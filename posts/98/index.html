<!doctype html><html lang=en><meta charset=UTF-8><meta content="width=device-width" name=viewport><meta content=#222 name=theme-color><meta content="Hexo 8.0.0" name=generator><link crossorigin href=https://cdnjs.cloudflare.com rel=preconnect><link href=/images/apple-touch-icon-next.png rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><link href=/css/main.css rel=stylesheet><link integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css rel=stylesheet><link integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css rel=stylesheet><script class=next-config data-name=main type=application/json>{"hostname":"cookiehcl.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"agate","dark":"agate"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":true,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script defer src=/js/config.js></script><meta content="Introduction https://arxiv.org/abs/1706.03762 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neur" name=description><meta content=article property=og:type><meta content="(2017) Attention Is All You Need" property=og:title><meta content=https://cookiehcl.github.io/posts/98/index.html property=og:url><meta content="기억 저장소" property=og:site_name><meta content="Introduction https://arxiv.org/abs/1706.03762 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neur" property=og:description><meta content=en_US property=og:locale><meta content=https://cookiehcl.github.io/posts/98/architecture.png property=og:image><meta content=https://cookiehcl.github.io/posts/98/multi_head_attetnion.png property=og:image><meta content=2025-07-21T14:01:56.000Z property=article:published_time><meta content=2025-07-21T14:01:56.000Z property=article:modified_time><meta content=CookieHCl property=article:author><meta content=summary name=twitter:card><meta content=https://cookiehcl.github.io/posts/98/architecture.png name=twitter:image><link href=https://cookiehcl.github.io/posts/98/ rel=canonical><script class=next-config data-name=page type=application/json>{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://cookiehcl.github.io/posts/98/","path":"posts/98/","title":"(2017) Attention Is All You Need"}</script><script class=next-config data-name=calendar type=application/json>""</script><title>(2017) Attention Is All You Need | 기억 저장소</title><script integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js></script><script defer src=/js/utils.js></script><script defer src=/js/motion.js></script><script defer src=/js/sidebar.js></script><script defer src=/js/next-boot.js></script><script integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js></script><script defer src=/js/third-party/search/local-search.js></script><script class=next-config data-name=mermaid type=application/json>{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.10.1/mermaid.min.js","integrity":"sha256-BmQmdWDS8X2OTbrwELWK366LV6escyWhHHe0XCTU/Hk="}}</script><script defer src=/js/third-party/tags/mermaid.js></script><script class=next-config data-name=enableMath type=application/json>true</script><link integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css rel=stylesheet><script class=next-config data-name=katex type=application/json>{"copy_tex_js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js","integrity":"sha256-Us54+rSGDSTvIhKKUs4kygE2ipA0RXpWWh0/zLqw3bs="}}</script><script defer src=/js/third-party/math/katex.js></script><noscript><link href=/css/noscript.css rel=stylesheet></noscript><body class=use-motion itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><div class=column><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=site-brand-container><div class=site-nav-toggle><div aria-label="Toggle navigation bar" class=toggle role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <i class=logo-line></i> <p class=site-title>기억 저장소</p> <i class=logo-line></i> </a></div><div class=site-nav-right><div class="toggle popup-trigger" aria-label=Search role=button><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>About</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>Tags</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>Categories</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>Archives</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>Search </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container><input autocapitalize=off autocomplete=off class=search-input maxlength=80 placeholder=Searching... spellcheck=false type=search></div><span class=popup-btn-close role=button> <i class="fa fa-times-circle"></i> </span></div><div class=search-result-container><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>Table of Contents<li class=sidebar-nav-overview>Overview</ul><div class=sidebar-panel-container><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>1.</span> <span class=nav-text>Introduction</span></a><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>2.</span> <span class=nav-text>Background</span></a><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>3.</span> <span class=nav-text>Architecture</span></a><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>4.</span> <span class=nav-text>Attention</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Scaled-Dot-Product-attention><span class=nav-number>4.1.</span> <span class=nav-text>Scaled Dot-Product attention</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Multi-Head-Attention><span class=nav-number>4.2.</span> <span class=nav-text>Multi-Head Attention</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Applications><span class=nav-number>4.3.</span> <span class=nav-text>Applications</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Why-Attention><span class=nav-number>4.4.</span> <span class=nav-text>Why Attention?</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>5.</span> <span class=nav-text>Embeddings</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Word-Embedding><span class=nav-number>5.1.</span> <span class=nav-text>Word Embedding</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Positional-Encoding><span class=nav-number>5.2.</span> <span class=nav-text>Positional Encoding</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>6.</span> <span class=nav-text>Training</span></a><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>7.</span> <span class=nav-text>Results</span></a><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>8.</span> <span class=nav-text>Conclusion</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop=author itemscope itemtype=http://schema.org/Person><p class=site-author-name itemprop=name>CookieHCl<div class=site-description itemprop=description></div></div><div class="site-state-wrap animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>109</span> <span class=site-state-item-name>posts</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>25</span> <span class=site-state-item-name>categories</span></a></div></nav></div><div class="links-of-author animated"><span class=links-of-author-item> <a rel="noopener me" title="GitHub → https://github.com/CookieHCl" href=https://github.com/CookieHCl target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a rel="noopener me" title="BOJ → https://www.acmicpc.net/user/CookieHCl" href=https://www.acmicpc.net/user/CookieHCl target=_blank><i class="fa fa-code fa-fw"></i>BOJ</a> </span><span class=links-of-author-item> <a rel="noopener me" title="solved.ac → https://solved.ac/profile/CookieHCl" href=https://solved.ac/profile/CookieHCl target=_blank><i class="fa fa-audio-description fa-fw"></i>solved.ac</a> </span></div><div class="cc-license animated" itemprop=license><a class=cc-opacity href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ko rel=noopener target=_blank><img alt="Creative Commons" src=https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg></a></div></div></div></div></aside></div><div class="main-inner post posts-expand"><div class=post-block><article class=post-content itemscope itemtype=http://schema.org/Article lang=en><link href=https://cookiehcl.github.io/posts/98/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=/images/avatar.gif itemprop=image> <meta content=CookieHCl itemprop=name> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="기억 저장소" itemprop=name> <meta itemprop=description> </span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork> <meta content="(2017) Attention Is All You Need | 기억 저장소" itemprop=name> <meta itemprop=description> </span><header class=post-header><h1 itemprop="name headline" class=post-title>(2017) Attention Is All You Need</h1><div class=post-meta-container><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>Posted on</span> <time itemprop="dateCreated datePublished" title="Created: 2025-07-21 23:01:56" datetime=2025-07-21T23:01:56+09:00>2025-07-21</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-folder"></i> </span> <span class=post-meta-item-text>In</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/PAPER/ itemprop=url rel=index><span itemprop=name>PAPER</span></a> </span> , <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/PAPER/AI/ itemprop=url rel=index><span itemprop=name>AI</span></a> </span> , <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/PAPER/AI/LLM/ itemprop=url rel=index><span itemprop=name>LLM</span></a> </span> </span><span title="Reading time" class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>Reading time ≈</span> <span>4 mins.</span> </span></div></div></header><div class=post-body itemprop=articleBody><h1>Introduction</h1><p><a href=https://arxiv.org/abs/1706.03762 rel=noopener target=_blank>https://arxiv.org/abs/1706.03762</a><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. <em>Advances in neural information processing systems</em>, 30.<p>일명 Transformer 논문. LLM 시대를 열 수 있게 만들어준 기념비적인 논문이다.<h1>Background</h1><p>Transformer 이전에는 RNN + Attention 또는 CNN + Attention으로 텍스트를 처리했다.<br> 하지만 RNN, CNN은 구조적인 문제가 있었고, Transformer는 최초로 Attention만 사용한 모델을 만들기로 한다.<ul><li>RNN: 선형 구조라 병렬화가 불가능하다.<li>CNN: 먼 거리의 토큰 연관성을 계산하려면 레이어 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mi>d</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(d)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord mathnormal">d</span><span class=mclose>)</span></span></span></span>개가 (구현에 따라서는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mi>log</mi><mo>⁡</mo><mi>d</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(\log d)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class=mop>lo<span style=margin-right:0.01389em;>g</span></span><span class=mspace style=margin-right:0.1667em;></span><span class="mord mathnormal">d</span><span class=mclose>)</span></span></span></span>개가) 필요하다. (effective receptive field 문제)</ul><h1>Architecture</h1><p><img alt="Transformer model architecture" src=architecture.png><p>사실 Transformer는 기존 방식을(RNN or CNN) 많이 참조하였다.<ul><li>Encoder는 input sequence <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo stretchy=false>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=true>,</mo><mo>…</mo><mo separator=true>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>(x_1, \ldots, x_n)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner>…</span><span class=mspace style=margin-right:0.1667em;></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>을 representations <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo stretchy=false>(</mo><msub><mi>z</mi><mn>1</mn></msub><mo separator=true>,</mo><mo>…</mo><mo separator=true>,</mo><msub><mi>z</mi><mi>n</mi></msub><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>(z_1, \ldots, z_n)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:0.04398em;>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:-0.044em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner>…</span><span class=mspace style=margin-right:0.1667em;></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.04398em;>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em;><span style=top:-2.55em;margin-left:-0.044em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>으로 바꾼다.<li>Decoder는 representations <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo stretchy=false>(</mo><msub><mi>z</mi><mn>1</mn></msub><mo separator=true>,</mo><mo>…</mo><mo separator=true>,</mo><msub><mi>z</mi><mi>n</mi></msub><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>(z_1, \ldots, z_n)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:0.04398em;>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:-0.044em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner>…</span><span class=mspace style=margin-right:0.1667em;></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.04398em;>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em;><span style=top:-2.55em;margin-left:-0.044em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>을 output sequence <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo stretchy=false>(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator=true>,</mo><mo>…</mo><mo separator=true>,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>(y_1, \ldots, y_m)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em;>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner>…</span><span class=mspace style=margin-right:0.1667em;></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em;>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em;><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>으로 바꾼다.<li>Output token <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>y</mi><mi>k</mi></msub></mrow><annotation encoding=application/x-tex>y_k</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em;>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em;><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em;>k</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span></span>가 나올 때마다 이전 토큰들 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo stretchy=false>(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator=true>,</mo><mo>…</mo><mo separator=true>,</mo><msub><mi>y</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>(y_1, \ldots, y_{k-1})</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em;>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner>…</span><span class=mspace style=margin-right:0.1667em;></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em;>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em;><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em;>k</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2083em;><span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>을 input으로 사용한다. (Auto-regressive)<li>Encoder, Decoder 모두 residual connection을 사용한다. (이 때문에 모든 layer의 dimension은 똑같다.)</ul><h1>Attention</h1><h2 id=Scaled-Dot-Product-attention>Scaled Dot-Product attention</h2><p>Transformer에서는 scaled dot-product attention을 사용한다.<br> Transformer가 정립시키고 난 이후부턴 이 attention만 사용되는 것 같다.<p><span class=katex-display><span class=katex><span class=katex-mathml><math display=block xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi mathvariant=normal>Attention</mi><mo>⁡</mo><mo stretchy=false>(</mo><mi>Q</mi><mo separator=true>,</mo><mi>K</mi><mo separator=true>,</mo><mi>V</mi><mo stretchy=false>)</mo><mo>=</mo><mi mathvariant=normal>softmax</mi><mo>⁡</mo><mrow><mo fence=true>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence=true>)</mo></mrow><mi>V</mi></mrow><annotation encoding=application/x-tex>\operatorname{Attention}(Q,K,V) = \operatorname{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V </annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class=mop><span class="mord mathrm">Attention</span></span><span class=mopen>(</span><span class="mord mathnormal">Q</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class="mord mathnormal" style=margin-right:0.22222em;>V</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:2.4684em;vertical-align:-0.95em;></span><span class=mop><span class="mord mathrm">softmax</span></span><span class=mspace style=margin-right:0.1667em;></span><span class=minner><span class="mopen delimcenter" style=top:0em;><span class="delimsizing size3">(</span></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.5183em;><span style=top:-2.2528em;><span class=pstrut style=height:3em;></span><span class=mord><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8572em;><span class=svg-align style=top:-3em;><span class=pstrut style=height:3em;></span><span class=mord style=padding-left:0.833em;><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em;>k</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span><span style=top:-2.8172em;><span class=pstrut style=height:3em;></span><span class=hide-tail style=min-width:0.853em;height:1.08em;><svg preserveaspectratio="xMinYMin slice" viewbox="0 0 400000 1080" height=1.08em width=400em xmlns=http://www.w3.org/2000/svg><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1828em;><span></span></span></span></span></span></span></span><span style=top:-3.23em;><span class=pstrut style=height:3em;></span><span class=frac-line style=border-bottom-width:0.04em;></span></span><span style=top:-3.677em;><span class=pstrut style=height:3em;></span><span class=mord><span class="mord mathnormal">Q</span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8413em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.13889em;>T</span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.93em;><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style=top:0em;><span class="delimsizing size3">)</span></span></span><span class=mspace style=margin-right:0.1667em;></span><span class="mord mathnormal" style=margin-right:0.22222em;>V</span></span></span></span></span><p>먼저 당시 사용되던 attention에는 additive attention과 dot-product(multiplicative) attention이 있었는데, 시간복잡도는 동일하지만 dot-product attention은 행렬 곱으로 구현할 수 있기 때문에 실제 연산 속도는 훨씬 빠르다.<p>여기에 추가로 Transformer는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mfrac><mn>1</mn><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mrow><annotation encoding=application/x-tex>\frac{1}{\sqrt{d_k}}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.3831em;vertical-align:-0.538em;></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8451em;><span style=top:-2.5864em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8622em;><span class=svg-align style=top:-3em;><span class=pstrut style=height:3em;></span><span class="mord mtight" style=padding-left:0.833em;><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3448em;><span style=top:-2.3488em;margin-left:0em;margin-right:0.0714em;><span class=pstrut style=height:2.5em;></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em;>k</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1512em;><span></span></span></span></span></span></span></span></span><span style=top:-2.8222em;><span class=pstrut style=height:3em;></span><span class="hide-tail mtight" style=min-width:0.853em;height:1.08em;><svg preserveaspectratio="xMinYMin slice" viewbox="0 0 400000 1080" height=1.08em width=400em xmlns=http://www.w3.org/2000/svg><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1778em;><span></span></span></span></span></span></span></span></span><span style=top:-3.23em;><span class=pstrut style=height:3em;></span><span class=frac-line style=border-bottom-width:0.04em;></span></span><span style=top:-3.394em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.538em;><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>로 scaling을 한다.<br> 당시 차원이 클 경우 additive attention이 dot-product attention보다 성능이 좋다는 결과가 있었는데, 이 논문에선 곱셈으로 인한 magnitude 문제라고 판단하고 scaling으로 보정해준다.<h2 id=Multi-Head-Attention>Multi-Head Attention</h2><p><img alt="Multi-Head Attention" src=multi_head_attetnion.png><p>Transformer는 1개의 Attention을 사용하는 대신 각 Q, K, V를 h개로 쪼개 h개의 Attention을 구한 뒤, h개의 Attenntion의 linear transformation을 attention 값으로 사용한다.<p>여러 개의 head를 사용하면 각 head가 문장의 서로 다른 구조를 학습할 것으로 기대했다고 한다.<h2 id=Applications>Applications</h2><p>Transformer는 (각 layer마다) 총 3종류의 attention이 사용된다.<ul><li>Encoder self-attention: QKV가 전부 Encoder에서 오므로, 각 position이 이전 layer의 모든 position을 볼 수 있다.<li>Decoder self-attention: QKV가 전부 Decoder에서 온다. 하지만 자기 이후에 나오는 토큰들을 보면 안 되므로 softmax 이전에 masking을 하여 보면 안 되는 토큰들의 attention을 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo>−</mo><mi mathvariant=normal>∞</mi></mrow><annotation encoding=application/x-tex>-\infty</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6667em;vertical-align:-0.0833em;></span><span class=mord>−</span><span class=mord>∞</span></span></span></span>로 만든다. (softmax 이후에는 0이 된다.)<li>Encoder-decoder attention: Q는 Decoder, KV는 Encoder에서 온다. Decoder의 각 position이 encoder의 모든 position을 (즉, 모든 input sequence를) 볼 수 있다.</ul><p>의외로 encoder-decoder attention은 Transformer에서 처음 고안한게 아니라 seq2seq 모델에서 사용되던 기법이라고 한다.<h2 id=Why-Attention>Why Attention?</h2><p>Transformer를 만들 때 computational complexity, parallelizability, maximum path length를 고려해서 self-attention을 사용했다고 한다.<p>먼저 RNN의 computational complexity는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(nd^2)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.0641em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord mathnormal">n</span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>으로, Attention의 computational complexity <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><msup><mi>n</mi><mn>2</mn></msup><mi>d</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(n^2d)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.0641em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class=mclose>)</span></span></span></span>보다 빠르지만, (논문을 쓸 당시에는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>n</mi><mo>&lt;</mo><mi>d</mi></mrow><annotation encoding=application/x-tex>n &lt; d</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.5782em;vertical-align:-0.0391em;></span><span class="mord mathnormal">n</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>&lt;</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.6944em;></span><span class="mord mathnormal">d</span></span></span></span>라서 Attention이 더 빠르다고 하긴 했다) parallelize가 안 되고, maximum path length가 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mi>n</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(n)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord mathnormal">n</span><span class=mclose>)</span></span></span></span>이다. 반면에 Attention의 maximum path length는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mn>1</mn><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(1)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class=mord>1</span><span class=mclose>)</span></span></span></span>이다. (즉, 연산 1번으로 long-range dependency를 학습할 수 있음)<p>또한 CNN의 computational complexity는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mi>k</mi><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(knd^2)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.0641em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord mathnormal">kn</span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>고, maximum path length는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mfrac><mi>n</mi><mi>k</mi></mfrac><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(\frac{n}{k})</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.095em;vertical-align:-0.345em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6954em;><span style=top:-2.655em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em;>k</span></span></span></span><span style=top:-3.23em;><span class=pstrut style=height:3em;></span><span class=frac-line style=border-bottom-width:0.04em;></span></span><span style=top:-3.394em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.345em;><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mclose>)</span></span></span></span>라 Attention의 하위호환이다. Separable convolution의 computational complexity는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mi>k</mi><mi>n</mi><mi>d</mi><mo>+</mo><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(knd + nd^2)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord mathnormal">kn</span><span class="mord mathnormal">d</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:1.0641em;vertical-align:-0.25em;></span><span class="mord mathnormal">n</span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>이고, dilated convolution의 maximum path legnth는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mi>k</mi></msub><mo stretchy=false>(</mo><mi>n</mi><mo stretchy=false>)</mo><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(\log_k(n))</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class=mop><span class=mop>lo<span style=margin-right:0.01389em;>g</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.242em;><span style=top:-2.4559em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em;>k</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2441em;><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal">n</span><span class=mclose>))</span></span></span></span>이지만 그래도 Attention이 더 좋거나 똑같다.<p>그리고 Attention 모델은 해석하기가 더 쉬운데, attention weight를 보고 각 head가 어떤 단어들이 연관 있다고 생각했는지 파악할 수 있기 때문이다.<br> 실제로 이를 통해 transformer의 각 head가 서로 다른 문법적/의미적 구조를 파악했다는 것을 알 수 있었다.<p>마지막으로, 논문에서 후속연구로 Attention의 computational complexity를 줄이기 위해 주변 r개의 토큰들만 참조해 시간복잡도를 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mi>r</mi><mi>n</mi><mi>d</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(rnd)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.02778em;>r</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class=mclose>)</span></span></span></span>로 줄이는 방법을 설명한다. 하지만 maximum path length가 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><mfrac><mi>n</mi><mi>r</mi></mfrac><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(\frac{n}{r})</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.095em;vertical-align:-0.345em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6954em;><span style=top:-2.655em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em;>r</span></span></span></span><span style=top:-3.23em;><span class=pstrut style=height:3em;></span><span class=frac-line style=border-bottom-width:0.04em;></span></span><span style=top:-3.394em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.345em;><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mclose>)</span></span></span></span>로 늘어나게 된다는 문제가 있고, 일단 Transformer 논문에서는 단순 아이디어 단계에서 그친 것 같다.<h1>Embeddings</h1><h2 id=Word-Embedding>Word Embedding</h2><p>Transformer는 learned embeddings을 사용했고, embedding layer와 pre-softmax linear transformation에서 같은 weight matrix를 사용하여 input, output의 embedding의 일관성을 챙겼다.<p>또한, embedding layer에서 weight에 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msqrt><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></msqrt></mrow><annotation encoding=application/x-tex>\sqrt{d_{model}}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.04em;vertical-align:-0.1828em;></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8572em;><span class=svg-align style=top:-3em;><span class=pstrut style=height:3em;></span><span class=mord style=padding-left:0.833em;><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style=margin-right:0.01968em;>l</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span><span style=top:-2.8172em;><span class=pstrut style=height:3em;></span><span class=hide-tail style=min-width:0.853em;height:1.08em;><svg preserveaspectratio="xMinYMin slice" viewbox="0 0 400000 1080" height=1.08em width=400em xmlns=http://www.w3.org/2000/svg><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1828em;><span></span></span></span></span></span></span></span></span>을 곱해주는데, 아쉽게도 논문에 명확한 이유는 나오지 않는다.<br> Embedding vector과 positional encoding를 더하는 과정이 있는데, 두 벡터의 scale을 맞추기 위해서 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msqrt><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></msqrt></mrow><annotation encoding=application/x-tex>\sqrt{d_{model}}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.04em;vertical-align:-0.1828em;></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8572em;><span class=svg-align style=top:-3em;><span class=pstrut style=height:3em;></span><span class=mord style=padding-left:0.833em;><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style=margin-right:0.01968em;>l</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span><span style=top:-2.8172em;><span class=pstrut style=height:3em;></span><span class=hide-tail style=min-width:0.853em;height:1.08em;><svg preserveaspectratio="xMinYMin slice" viewbox="0 0 400000 1080" height=1.08em width=400em xmlns=http://www.w3.org/2000/svg><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1828em;><span></span></span></span></span></span></span></span></span>을 곱하는 것으로 추측된다.<h2 id=Positional-Encoding>Positional Encoding</h2><p>Attention은 position에 대한 정보가 없기 때문에, Transformer는 따로 positional encoding을 더하는 방식으로 position에 대한 정보를 추가했다.<p><span class=katex-display><span class=katex><span class=katex-mathml><math display=block xmlns=http://www.w3.org/1998/Math/MathML><semantics><mtable columnalign="right left" columnspacing=0em rowspacing=0.25em><mtr><mtd><mstyle displaystyle=true scriptlevel=0><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy=false>(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator=true>,</mo><mn>2</mn><mi>i</mi><mo stretchy=false>)</mo></mrow></msub></mrow></mstyle></mtd><mtd><mstyle displaystyle=true scriptlevel=0><mrow><mrow></mrow><mo>=</mo><mi>sin</mi><mo>⁡</mo><mrow><mo fence=true>(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mfrac><mrow><mn>2</mn><mi>i</mi></mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mfrac></msup></mfrac><mo fence=true>)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle displaystyle=true scriptlevel=0><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy=false>(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator=true>,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=false>)</mo></mrow></msub></mrow></mstyle></mtd><mtd><mstyle displaystyle=true scriptlevel=0><mrow><mrow></mrow><mo>=</mo><mi>cos</mi><mo>⁡</mo><mrow><mo fence=true>(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mfrac><mrow><mn>2</mn><mi>i</mi></mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mfrac></msup></mfrac><mo fence=true>)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding=application/x-tex>\begin{align*} PE_{(pos, 2i)} &= \sin\left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right) \\ PE_{(pos, 2i+1)} &= \cos\left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right) \end{align*}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:5.5239em;vertical-align:-2.5119em;></span><span class=mord><span class=mtable><span class=col-align-r><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:3.0119em;><span style=top:-5.0119em;><span class=pstrut style=height:3.45em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class=mord><span class="mord mathnormal" style=margin-right:0.05764em;>E</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3448em;><span style=top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.3552em;><span></span></span></span></span></span></span></span></span><span style=top:-2.25em;><span class=pstrut style=height:3.45em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class=mord><span class="mord mathnormal" style=margin-right:0.05764em;>E</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3448em;><span style=top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.3552em;><span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.5119em;><span></span></span></span></span></span><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:3.0119em;><span style=top:-5.0119em;><span class=pstrut style=height:3.45em;></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span><span class=mop>sin</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner><span class="mopen delimcenter" style=top:0em;><span class="delimsizing size3">(</span></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.1076em;><span style=top:-2.11em;><span class=pstrut style=height:3.1219em;></span><span class=mord><span class=mord>1000</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:1.1219em;><span style=top:-3.5234em;margin-right:0.05em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8551em;><span style=top:-2.656em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3448em;><span style=top:-2.3448em;margin-left:0em;margin-right:0.1em;><span class=pstrut style=height:2.6944em;></span><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style=margin-right:0.01968em;>l</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.3496em;><span></span></span></span></span></span></span></span></span></span><span style=top:-3.2255em;><span class=pstrut style=height:3em;></span><span class="frac-line mtight" style=border-bottom-width:0.049em;></span></span><span style=top:-3.384em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.5937em;><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.3519em;><span class=pstrut style=height:3.1219em;></span><span class=frac-line style=border-bottom-width:0.04em;></span></span><span style=top:-3.7989em;><span class=pstrut style=height:3.1219em;></span><span class=mord><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.0119em;><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style=top:0em;><span class="delimsizing size3">)</span></span></span></span></span><span style=top:-2.25em;><span class=pstrut style=height:3.45em;></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span><span class=mop>cos</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner><span class="mopen delimcenter" style=top:0em;><span class="delimsizing size3">(</span></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.1076em;><span style=top:-2.11em;><span class=pstrut style=height:3.1219em;></span><span class=mord><span class=mord>1000</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:1.1219em;><span style=top:-3.5234em;margin-right:0.05em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8551em;><span style=top:-2.656em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3448em;><span style=top:-2.3448em;margin-left:0em;margin-right:0.1em;><span class=pstrut style=height:2.6944em;></span><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style=margin-right:0.01968em;>l</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.3496em;><span></span></span></span></span></span></span></span></span></span><span style=top:-3.2255em;><span class=pstrut style=height:3em;></span><span class="frac-line mtight" style=border-bottom-width:0.049em;></span></span><span style=top:-3.384em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.5937em;><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.3519em;><span class=pstrut style=height:3.1219em;></span><span class=frac-line style=border-bottom-width:0.04em;></span></span><span style=top:-3.7989em;><span class=pstrut style=height:3.1219em;></span><span class=mord><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.0119em;><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style=top:0em;><span class="delimsizing size3">)</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.5119em;><span></span></span></span></span></span></span></span></span></span></span></span><p>Transformer는 positional encoding으로 sinusoid를 사용했는데, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi></mrow></msub></mrow><annotation encoding=application/x-tex>PE_{pos+k}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em;></span><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class=mord><span class="mord mathnormal" style=margin-right:0.05764em;>E</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em;><span style=top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style=margin-right:0.03148em;>k</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em;><span></span></span></span></span></span></span></span></span></span>를 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding=application/x-tex>PE_{pos}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em;></span><span class="mord mathnormal" style=margin-right:0.13889em;>P</span><span class=mord><span class="mord mathnormal" style=margin-right:0.05764em;>E</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em;><span style=top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em;><span></span></span></span></span></span></span></span></span></span>의 linear function으로 나타낼 수 있기 때문에 relative position을 잘 학습할 것으로 생각했다고 한다.<p>이 당시에도 positional encoding을 학습시킬 수 있었지만, Transformer 모델은 sinusoid와 learned positional encoding의 결과가 큰 차이가 없었다고 한다.<h1>Training</h1><p>영어와 프랑스어/독일어 데이터셋을 사용하였고, byte-pair encoding을 사용했다. (자주 등장하는 토큰 쌍을 묶어서 새로운 토큰으로 취급함)<p>훈련은 Adam optimizer를 사용하였고, learning rate는 warmup_steps 까지는 step 수에 선형으로 비례하고, 그 이후에는 <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mfrac><mn>1</mn><msqrt><mrow><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>s</mi></mrow></msqrt></mfrac></mrow><annotation encoding=application/x-tex>\frac{1}{\sqrt{steps}}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.3831em;vertical-align:-0.538em;></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8451em;><span style=top:-2.6293em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8009em;><span class=svg-align style=top:-3em;><span class=pstrut style=height:3em;></span><span class="mord mtight" style=padding-left:0.833em;><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">s</span></span></span><span style=top:-2.7609em;><span class=pstrut style=height:3em;></span><span class="hide-tail mtight" style=min-width:0.853em;height:1.08em;><svg preserveaspectratio="xMinYMin slice" viewbox="0 0 400000 1080" height=1.08em width=400em xmlns=http://www.w3.org/2000/svg><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2391em;><span></span></span></span></span></span></span></span></span><span style=top:-3.23em;><span class=pstrut style=height:3em;></span><span class=frac-line style=border-bottom-width:0.04em;></span></span><span style=top:-3.394em;><span class=pstrut style=height:3em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.538em;><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>에 비례해 점점 감소하게 된다.<br> 또한, 모델 크기가 크면 learning rate를 줄여 더 오랫동안 학습한다.<p><span class=katex-display><span class=katex><span class=katex-mathml><math display=block xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>l</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>=</mo><msubsup><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow><mrow><mo>−</mo><mn>0.5</mn></mrow></msubsup><mi>min</mi><mo>⁡</mo><mo stretchy=false>(</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><msup><mi>s</mi><mrow><mo>−</mo><mn>0.5</mn></mrow></msup><mo separator=true>,</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>s</mi><mo>⋅</mo><mi>w</mi><mi>a</mi><mi>r</mi><mi>m</mi><mi>u</mi><mi>p</mi><mi mathvariant=normal>_</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><msup><mi>s</mi><mrow><mo>−</mo><mn>1.5</mn></mrow></msup><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>lrate = d_{model}^{-0.5} \min(steps^{-0.5}, steps \cdot warmup\_steps^{-1.5}) </annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6944em;></span><span class="mord mathnormal" style=margin-right:0.01968em;>l</span><span class="mord mathnormal" style=margin-right:0.02778em;>r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:1.1555em;vertical-align:-0.2914em;></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8641em;><span style=top:-2.4086em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style=margin-right:0.01968em;>l</span></span></span></span><span style=top:-3.113em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">0.5</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2914em;><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em;></span><span class=mop>min</span><span class=mopen>(</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class=mord><span class="mord mathnormal">s</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8641em;><span style=top:-3.113em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">0.5</span></span></span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord mathnormal">s</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:1.1741em;vertical-align:-0.31em;></span><span class="mord mathnormal" style=margin-right:0.02691em;>w</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style=margin-right:0.02778em;>r</span><span class="mord mathnormal">m</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class=mord style=margin-right:0.02778em;>_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class=mord><span class="mord mathnormal">s</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8641em;><span style=top:-3.113em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1.5</span></span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span></span><p>Regularization을 위해 각 레이어마다 dropout을 넣고, label smoothing을 했다.<br> Label smoothing을 하면 perplexity는 안 좋아지지만, (확률 분포가 얼마나 맞는지 나타내는 지표), accuracy는 좋아진다. (예측한 가장 확률이 높은 토큰이 정답인 비율)<h1>Results</h1><p>결과는 다들 알다시피 Attention만으로 기존 RNN, CNN 모델들의 번역 성능을 뛰어넘었다.<p>논문에선 추가로 Transformer 파라미터를 조정했을 때 결과도 포함되어 있다.<ul><li>그냥 attention보단 multi-head attention이 더 성능이 좋았지만, head 개수를 너무 늘리면 오히려 성능이 떨어졌다.<li>layer 수, 각 벡터의 차원 등 모델 크기를 늘리면 성능이 더 좋아졌다.<li>dropout은 있는게 더 성능이 좋다.<li>label smoothing은 perplexity가 안 좋아지지만, accuracy가 좋아져 결과적으로 성능이 더 좋다. (유일하게 perplexity, accuracy가 모두 좋아지진 않는 변화다.)<li>learned positional encoding으로 대체했을 때 성능 변화는 거의 없었다.</ul><h1>Conclusion</h1><p>Transformer는 RNN, CNN을 사용할 필요 없이 Attention만 사용하면 더 학습도 빠르고 결과도 좋다는 것을 증명해낸 논문이다.<br> 이 논문 이후로 LLM은 모두 Transformer를 기반으로 하게 되었다.<p>하지만 논문에서 잠깐 언급하고 지나갔던 Attention의 단점이 점점 부각되게 되었는데, Transformer 시절에는 짧았던 input sequence 길이가, LLM 시대부턴 너무 길어지면서 Attention의 computational complexity <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>O</mi><mo stretchy=false>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>O(n^2)</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.0641em;vertical-align:-0.25em;></span><span class="mord mathnormal" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>이 문제가 되기 시작했다.<br> 이 문제는 현재진행형으로, 하드웨어 최적화나 효율적인 Attention 구조 등 다양한 방법으로 해결하려고 노력 중이다.</div><footer class=post-footer><div class=post-nav><div class=post-nav-item><a href=/posts/97/ rel=prev title=Dynamics> <i class="fa fa-angle-left"></i> Dynamics </a></div><div class=post-nav-item><a href=/posts/99/ rel=next title=Introduction> Introduction <i class="fa fa-angle-right"></i> </a></div></div></footer></article></div><div class="comments utterances-container"></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© 2023 – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>CookieHCl</span></div><div class=powered-by>Powered by <a href=https://hexo.io/ rel=noopener target=_blank>Hexo</a> & <a href=https://theme-next.js.org/muse/ rel=noopener target=_blank>NexT.Muse</a></div></div></footer><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div><div class=sidebar-dimmer></div><div aria-label="Back to top" class=back-to-top role=button><i class="fa fa-arrow-up fa-lg"></i><span>0%</span></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><script class=next-config data-name=utterances type=application/json>{"enable":true,"repo":"CookieHCl/cookiehcl.github.io","issue_term":"pathname","theme":"github-light"}</script><script defer src=/js/third-party/comments/utterances.js></script>