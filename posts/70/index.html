<!doctype html><html lang=en><meta charset=UTF-8><meta content="width=device-width" name=viewport><meta content=#222 name=theme-color><meta content="Hexo 7.3.0" name=generator><link crossorigin href=https://cdnjs.cloudflare.com rel=preconnect><link href=/images/apple-touch-icon-next.png rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><link href=/css/main.css rel=stylesheet><link integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css rel=stylesheet><link integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css rel=stylesheet><script class=next-config data-name=main type=application/json>{"hostname":"cookiehcl.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"agate","dark":"agate"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":true,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script defer src=/js/config.js></script><meta content="Word Embedding Embedding is a mapping between anything and a vector. Each word is represented as a vector. Similarity between words can be measured in the vector space called embedding space. Word2Vec" name=description><meta content=article property=og:type><meta content="Large Language Model" property=og:title><meta content=https://cookiehcl.github.io/posts/70/index.html property=og:url><meta content="기억 저장소" property=og:site_name><meta content="Word Embedding Embedding is a mapping between anything and a vector. Each word is represented as a vector. Similarity between words can be measured in the vector space called embedding space. Word2Vec" property=og:description><meta content=en_US property=og:locale><meta content=https://cookiehcl.github.io/posts/70/multi_head_attention_transformer.png property=og:image><meta content=https://cookiehcl.github.io/posts/70/parallelism.png property=og:image><meta content=https://cookiehcl.github.io/posts/70/3d_parallelism.png property=og:image><meta content=2025-04-01T00:23:15.000Z property=article:published_time><meta content=2025-04-01T00:23:15.000Z property=article:modified_time><meta content=CookieHCl property=article:author><meta content=summary name=twitter:card><meta content=https://cookiehcl.github.io/posts/70/multi_head_attention_transformer.png name=twitter:image><link href=https://cookiehcl.github.io/posts/70/ rel=canonical><script class=next-config data-name=page type=application/json>{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://cookiehcl.github.io/posts/70/","path":"posts/70/","title":"Large Language Model"}</script><script class=next-config data-name=calendar type=application/json>""</script><title>Large Language Model | 기억 저장소</title><script integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js></script><script defer src=/js/utils.js></script><script defer src=/js/motion.js></script><script defer src=/js/sidebar.js></script><script defer src=/js/next-boot.js></script><script integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js></script><script defer src=/js/third-party/search/local-search.js></script><script class=next-config data-name=mermaid type=application/json>{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.10.1/mermaid.min.js","integrity":"sha256-BmQmdWDS8X2OTbrwELWK366LV6escyWhHHe0XCTU/Hk="}}</script><script defer src=/js/third-party/tags/mermaid.js></script><script class=next-config data-name=enableMath type=application/json>true</script><link integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css rel=stylesheet><script class=next-config data-name=katex type=application/json>{"copy_tex_js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js","integrity":"sha256-Us54+rSGDSTvIhKKUs4kygE2ipA0RXpWWh0/zLqw3bs="}}</script><script defer src=/js/third-party/math/katex.js></script><noscript><link href=/css/noscript.css rel=stylesheet></noscript><body class=use-motion itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><div class=column><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=site-brand-container><div class=site-nav-toggle><div aria-label="Toggle navigation bar" class=toggle role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <i class=logo-line></i> <p class=site-title>기억 저장소</p> <i class=logo-line></i> </a></div><div class=site-nav-right><div class="toggle popup-trigger" aria-label=Search role=button><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>About</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>Tags</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>Categories</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>Archives</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>Search </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container><input autocapitalize=off autocomplete=off class=search-input maxlength=80 placeholder=Searching... spellcheck=false type=search></div><span class=popup-btn-close role=button> <i class="fa fa-times-circle"></i> </span></div><div class=search-result-container><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>Table of Contents<li class=sidebar-nav-overview>Overview</ul><div class=sidebar-panel-container><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>1.</span> <span class=nav-text>Word Embedding</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Word2Vec><span class=nav-number>1.1.</span> <span class=nav-text>Word2Vec</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Any2Vec><span class=nav-number>1.2.</span> <span class=nav-text>Any2Vec?</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>2.</span> <span class=nav-text>Recurrent Neural Networks (RNN)</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Neural-Machine-Translation-NMT><span class=nav-number>2.1.</span> <span class=nav-text>Neural Machine Translation (NMT)</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Alignment-problem><span class=nav-number>2.2.</span> <span class=nav-text>Alignment problem</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Bottleneck-problem><span class=nav-number>2.3.</span> <span class=nav-text>Bottleneck problem</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>3.</span> <span class=nav-text>Transformer</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Attention><span class=nav-number>3.1.</span> <span class=nav-text>Attention</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Terms-of-transformer><span class=nav-number>3.2.</span> <span class=nav-text>Terms of transformer</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#BERT-Bi-directional-Encoder-Representation-from-Transformers><span class=nav-number>3.3.</span> <span class=nav-text>BERT (Bi-directional Encoder Representation from Transformers)</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Multi-Head-Attention><span class=nav-number>3.3.1.</span> <span class=nav-text>Multi-Head Attention</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Masked-language-model-MLM><span class=nav-number>3.3.2.</span> <span class=nav-text>Masked language model (MLM)</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#ChatGPT-GPT-3><span class=nav-number>3.4.</span> <span class=nav-text>ChatGPT (GPT-3)</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>4.</span> <span class=nav-text>Training Transformer</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Model-size-of-Multi-Head-Attention-Transformer><span class=nav-number>4.1.</span> <span class=nav-text>Model size of Multi-Head Attention Transformer</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Computation-cost-of-Multi-Head-Attention-Transformer><span class=nav-number>4.2.</span> <span class=nav-text>Computation cost of Multi-Head Attention Transformer</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Trillion-Parameter-Models><span class=nav-number>4.3.</span> <span class=nav-text>Trillion Parameter Models</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Parallelism><span class=nav-number>4.4.</span> <span class=nav-text>Parallelism</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#All-Reduce-vs-All-Gather><span class=nav-number>4.4.1.</span> <span class=nav-text>All Reduce vs. All Gather</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#ZeRO-DP-Zero-Redundancy-Optimizer><span class=nav-number>4.4.2.</span> <span class=nav-text>ZeRO-DP (Zero Redundancy Optimizer)</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#ZeRO-Offload><span class=nav-number>4.4.3.</span> <span class=nav-text>ZeRO-Offload</span></a></ol></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>5.</span> <span class=nav-text>Test-time Scaling</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#DeepSeek-R1-Training><span class=nav-number>5.1.</span> <span class=nav-text>DeepSeek-R1 Training</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop=author itemscope itemtype=http://schema.org/Person><p class=site-author-name itemprop=name>CookieHCl<div class=site-description itemprop=description></div></div><div class="site-state-wrap animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>104</span> <span class=site-state-item-name>posts</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>24</span> <span class=site-state-item-name>categories</span></a></div></nav></div><div class="links-of-author animated"><span class=links-of-author-item> <a rel="noopener me" title="GitHub → https://github.com/CookieHCl" href=https://github.com/CookieHCl target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a rel="noopener me" title="BOJ → https://www.acmicpc.net/user/CookieHCl" href=https://www.acmicpc.net/user/CookieHCl target=_blank><i class="fa fa-code fa-fw"></i>BOJ</a> </span><span class=links-of-author-item> <a rel="noopener me" title="solved.ac → https://solved.ac/profile/CookieHCl" href=https://solved.ac/profile/CookieHCl target=_blank><i class="fa fa-audio-description fa-fw"></i>solved.ac</a> </span></div><div class="cc-license animated" itemprop=license><a class=cc-opacity href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ko rel=noopener target=_blank><img alt="Creative Commons" src=https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg></a></div></div></div></div></aside></div><div class="main-inner post posts-expand"><div class=post-block><article class=post-content itemscope itemtype=http://schema.org/Article lang=en><link href=https://cookiehcl.github.io/posts/70/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=/images/avatar.gif itemprop=image> <meta content=CookieHCl itemprop=name> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="기억 저장소" itemprop=name> <meta itemprop=description> </span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork> <meta content="Large Language Model | 기억 저장소" itemprop=name> <meta itemprop=description> </span><header class=post-header><h1 itemprop="name headline" class=post-title>Large Language Model</h1><div class=post-meta-container><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>Posted on</span> <time itemprop="dateCreated datePublished" title="Created: 2025-04-01 09:23:15" datetime=2025-04-01T09:23:15+09:00>2025-04-01</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-folder"></i> </span> <span class=post-meta-item-text>In</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/SNU/ itemprop=url rel=index><span itemprop=name>SNU</span></a> </span> , <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/SNU/4-1/ itemprop=url rel=index><span itemprop=name>4-1</span></a> </span> , <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/SNU/4-1/%ED%95%98%EB%93%9C%EC%9B%A8%EC%96%B4%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%84%A4%EA%B3%84/ itemprop=url rel=index><span itemprop=name>하드웨어시스템설계</span></a> </span> </span><span title="Reading time" class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>Reading time ≈</span> <span>5 mins.</span> </span></div></div></header><div class=post-body itemprop=articleBody><h1>Word Embedding</h1><p>Embedding is a mapping between anything and a vector.<p>Each word is represented as a vector.<br> Similarity between words can be measured in the vector space called embedding space.<h2 id=Word2Vec>Word2Vec</h2><p>Given a center word (with one-hot coding input), Word2Vec predicts context words (words before and after center word).<br> Because we are using one-hot vector, a column of weight becomes the hidden layer - called word vector.<h2 id=Any2Vec>Any2Vec?</h2><p>Using same idea, anything can be represented as a vector.<br> e.g. product vector, hotel vector, youtube vector (in recommender system)<h1>Recurrent Neural Networks (RNN)</h1><ul><li>Each input word change hidden state.<li>The input sequence and output sequence can be any length. (one to one, one to many, many to one, many to many)</ul><h2 id=Neural-Machine-Translation-NMT>Neural Machine Translation (NMT)</h2><p>Input sequence are fully feeded without any output. (Encoder RNN)<br> START token indicates start of translation. Decoder RNN outputs each word and use it as the next input.<h2 id=Alignment-problem>Alignment problem</h2><p>Input sequence and output sequence don't always match one-by-one.<br> Alignment can be many-to-one.<h2 id=Bottleneck-problem>Bottleneck problem</h2><p>RNN only has 1 encoding for the source sentence.<br> Can it capture all the infromation from thousands of words before?<h1>Transformer</h1><h2 id=Attention>Attention</h2><p>Instead of single hidden state, every hidden states of encoder RNN are used!<br> In decoder RNN, each hidden state determines attention scores.<br> With attention scores and encoder RNN's hidden state, attention vector is obtained and concatenated with decoder hidden state.<h2 id=Terms-of-transformer>Terms of transformer</h2><ul><li>Query: Decoder's intermediate output (hidden state)<li>Key: Used to calculate attention scores<li>Value: Encoder's hidden state</ul><p><span class=katex-display><span class=katex><span class=katex-mathml><math display=block xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mtext>Attention</mtext><mo stretchy=false>(</mo><mi>Q</mi><mo separator=true>,</mo><mi>K</mi><mo separator=true>,</mo><mi>V</mi><mo stretchy=false>)</mo><mo>=</mo><mi mathvariant=normal>softmax</mi><mo>⁡</mo><mrow><mo fence=true>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence=true>)</mo></mrow><mi>V</mi></mrow><annotation encoding=application/x-tex>\text{Attention}(Q,K,V) = \operatorname{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V </annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord text"><span class=mord>Attention</span></span><span class=mopen>(</span><span class="mord mathnormal">Q</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class="mord mathnormal" style=margin-right:0.22222em;>V</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:2.4684em;vertical-align:-0.95em;></span><span class=mop><span class="mord mathrm">softmax</span></span><span class=mspace style=margin-right:0.1667em;></span><span class=minner><span class="mopen delimcenter" style=top:0em;><span class="delimsizing size3">(</span></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.5183em;><span style=top:-2.2528em;><span class=pstrut style=height:3em;></span><span class=mord><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8572em;><span class=svg-align style=top:-3em;><span class=pstrut style=height:3em;></span><span class=mord style=padding-left:0.833em;><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em;>k</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span><span style=top:-2.8172em;><span class=pstrut style=height:3em;></span><span class=hide-tail style=min-width:0.853em;height:1.08em;><svg preserveaspectratio="xMinYMin slice" viewbox="0 0 400000 1080" height=1.08em width=400em xmlns=http://www.w3.org/2000/svg><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1828em;><span></span></span></span></span></span></span></span><span style=top:-3.23em;><span class=pstrut style=height:3em;></span><span class=frac-line style=border-bottom-width:0.04em;></span></span><span style=top:-3.677em;><span class=pstrut style=height:3em;></span><span class=mord><span class="mord mathnormal">Q</span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8413em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.13889em;>T</span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.93em;><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style=top:0em;><span class="delimsizing size3">)</span></span></span><span class=mspace style=margin-right:0.1667em;></span><span class="mord mathnormal" style=margin-right:0.22222em;>V</span></span></span></span></span><h2 id=BERT-Bi-directional-Encoder-Representation-from-Transformers>BERT (Bi-directional Encoder Representation from Transformers)</h2><p>Input can affect hidden state from the past!<br> Use whole sentence to produce output.<p>If the BERT model is sufficiently pre-trained, the larger BERT models give better accuracy without overfitting.<p>Usually transformer's input is token embedding vector + position embedding vector.<p>Pre-trained BERT model can used in different tasks:<ul><li>Sentence pair classification (2 input sentences, 1 output class)<li>Single sentence classification (1 input sentence, 1 output class)<li>Question answering (2 input sentences (question, paragraph), 1 output sentence)<li>Single sentence tagging (1 input sentence, 1 output sentence)</ul><h3 id=Multi-Head-Attention>Multi-Head Attention</h3><p>For each head, we use different dimensions.<br> It's similar to the ensemble model - use different attention patterns to learn multiple relationships.<h3 id=Masked-language-model-MLM>Masked language model (MLM)</h3><p>BERT is pre-trained with BooksCorpus and Wikipedia.<br> Random tokens were masked and BERT had to predict a masked word in a given sentence.<ul><li>This can make BERT learn context naturally!<li>We don't have to label datas or create input-output pairs, dataset itself is a training data!</ul><h2 id=ChatGPT-GPT-3>ChatGPT (GPT-3)</h2><ol start=0><li>Pre-train GPT.<br> Masked language model and 570GB of datasets were used.<li>Supervised learning with good examples.<br> Examples of input and output are prepared by human labelers or existing sets of text.<li>Reward model training<br> Human labeler gives a rating to the output of GPT-3.<br> OpenAI trained reward model which predicts the rating on the given input.<li>Reinforcement Learning <ul><li>ChatGPT generates output.<li>Reward model evaluates the output.<li>Fine-tune the GPT-3 based on the reward.</ul></ol><p>ChatGPT is still RNN based - it only generate one word at a time!<h1>Training Transformer</h1><p><img alt="Multi-Head Attention Transformer" src=multi_head_attention_transformer.png><h2 id=Model-size-of-Multi-Head-Attention-Transformer>Model size of Multi-Head Attention Transformer</h2><p>Input matrix: L x H (L words of vector embedding size H)<br> Head size, dimension: h, d (i.e. H = h x d)<p>For each head, Q, K, V is calculated from weight matrix H x d.<br> Each head (L x d) is concated to L x H matrix, then output is calculated from weight matrix H x H. (i.e. weight size is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>H^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>)<br> Therefore, total model size of MH attention is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>h</mi><mo>⋅</mo><mn>3</mn><mo>⋅</mo><mi>H</mi><mi>d</mi><mo>+</mo><msup><mi>H</mi><mn>2</mn></msup><mo>=</mo><mn>4</mn><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>h \cdot 3 \cdot Hd + H^2= 4H^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6944em;></span><span class="mord mathnormal">h</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.6444em;></span><span class=mord>3</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.7778em;vertical-align:-0.0833em;></span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class="mord mathnormal">d</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>4</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>.<p>Finally, feed-forward network (FFN) is used at output, from H to 4H and back to H.<br> Each step's weight is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>H</mi><mo>⋅</mo><mn>4</mn><mi>H</mi><mo>=</mo><mn>4</mn><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>H \cdot 4H = 4H^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.6833em;></span><span class=mord>4</span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>4</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>, so total weight size is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>8</mn><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>8H^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>8</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>.<p>Therefore, model size per layer is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>12</mn><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>12H^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>12</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>, and parameter ratio for MH attention and FFN is 1:2.<h2 id=Computation-cost-of-Multi-Head-Attention-Transformer>Computation cost of Multi-Head Attention Transformer</h2><p>Assume matrix multiplication of N x K and K x M is NKM.<p>For each head, Q, K, V is caclculated for each word: multiplication of L x H and H x d (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>L</mi><mi>H</mi><mi>d</mi></mrow><annotation encoding=application/x-tex>LHd</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6944em;></span><span class="mord mathnormal">L</span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class="mord mathnormal">d</span></span></span></span>)<br> For each head, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding=application/x-tex>QK^T</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.0358em;vertical-align:-0.1944em;></span><span class="mord mathnormal">Q</span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8413em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.13889em;>T</span></span></span></span></span></span></span></span></span></span></span> is calculated: multiplication of L x d and d x L (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup><mi>d</mi></mrow><annotation encoding=application/x-tex>L^2d</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord><span class="mord mathnormal">L</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span></span></span></span>)<br> For each head, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mo>⋅</mo><mi>V</mi></mrow><annotation encoding=application/x-tex>QK^T \cdot V</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1.0358em;vertical-align:-0.1944em;></span><span class="mord mathnormal">Q</span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8413em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.13889em;>T</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.22222em;>V</span></span></span></span> is calculated (assume softmax is calculated): multiplication of L x L and L x d (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup><mi>d</mi></mrow><annotation encoding=application/x-tex>L^2d</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord><span class="mord mathnormal">L</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span></span></span></span>)<p>Each head is concated to L x H matrix, then output is calculated: multiplication of L x H and H x H (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>L</mi><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>LH^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class="mord mathnormal">L</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>)<br> Therefore, total multiplications of MH attention is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>h</mi><mo stretchy=false>(</mo><mn>3</mn><mi>L</mi><mi>H</mi><mi>d</mi><mo>+</mo><msup><mi>L</mi><mn>2</mn></msup><mi>d</mi><mo>+</mo><msup><mi>L</mi><mn>2</mn></msup><mi>d</mi><mo stretchy=false>)</mo><mo>+</mo><mi>L</mi><msup><mi>H</mi><mn>2</mn></msup><mo>=</mo><mn>4</mn><mi>L</mi><msup><mi>H</mi><mn>2</mn></msup><mo>+</mo><mn>2</mn><msup><mi>L</mi><mn>2</mn></msup><mi>H</mi></mrow><annotation encoding=application/x-tex>h(3LHd + L^2d + L^2d) + LH^2 = 4LH^2 + 2L^2H</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:1em;vertical-align:-0.25em;></span><span class="mord mathnormal">h</span><span class=mopen>(</span><span class=mord>3</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class="mord mathnormal">d</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8974em;vertical-align:-0.0833em;></span><span class=mord><span class="mord mathnormal">L</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:1.0641em;vertical-align:-0.25em;></span><span class=mord><span class="mord mathnormal">L</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class="mord mathnormal">L</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.8974em;vertical-align:-0.0833em;></span><span class=mord>4</span><span class="mord mathnormal">L</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>2</span><span class=mord><span class="mord mathnormal">L</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span></span></span></span>.<p>Finally, feed-forward network (FFN) is used at output, from H to 4H and back to H.<br> Multiplication of L x H and H x 4H (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>4</mn><mi>L</mi><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>4LH^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>4</span><span class="mord mathnormal">L</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>), and L x 4H and 4H x H (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>4</mn><mi>L</mi><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>4LH^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>4</span><span class="mord mathnormal">L</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>)<br> Therefore, total multiplications of FFN is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>8</mn><mi>L</mi><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>8LH^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>8</span><span class="mord mathnormal">L</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>.<p>Therefore, multiplications per layer is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>2</mn><msup><mi>L</mi><mn>2</mn></msup><mi>H</mi><mo>+</mo><mn>12</mn><mi>L</mi><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>2L^2H + 12LH^2</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8974em;vertical-align:-0.0833em;></span><span class=mord>2</span><span class=mord><span class="mord mathnormal">L</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>12</span><span class="mord mathnormal">L</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>.<br> If we assume <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>L</mi><mo>=</mo><mi>H</mi></mrow><annotation encoding=application/x-tex>L = H</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal">L</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span></span></span></span>, total multiplications per layer is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>14</mn><msup><mi>L</mi><mn>3</mn></msup></mrow><annotation encoding=application/x-tex>14L^3</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>14</span><span class=mord><span class="mord mathnormal">L</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>, and parameter ratio for MH attention and FFN is 6:8.<h2 id=Trillion-Parameter-Models>Trillion Parameter Models</h2><p>Transformers now have more than trillion parameters.<p>e.g. BERT has N=298 transformer layers, and H=17480 hidden dimension, resulting in total <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>N</mi><mo>⋅</mo><mn>12</mn><msup><mi>H</mi><mn>2</mn></msup><mo>≈</mo><mn>1.08</mn><mo>×</mo><msup><mn>10</mn><mn>12</mn></msup></mrow><annotation encoding=application/x-tex>N \cdot 12H^2 \approx 1.08 \times 10^{12}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.10903em;>N</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>12</span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>≈</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.7278em;vertical-align:-0.0833em;></span><span class=mord>1.08</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>×</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span></span></span></span></span></span></span></span></span> parameters.<br> If we assume sequence length (number of tokens in a sentence) is L=2048, and batch size is B=2048, per-layer activation size in training is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>H</mi><mo>⋅</mo><mi>L</mi><mo>⋅</mo><mi>B</mi><mo>=</mo><mn>7.3</mn><mo>×</mo><msup><mn>10</mn><mn>10</mn></msup></mrow><annotation encoding=application/x-tex>H \cdot L \cdot B = 7.3 \times 10^{10}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal">L</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.05017em;>B</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.7278em;vertical-align:-0.0833em;></span><span class=mord>7.3</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>×</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span>.<p>Obviously, GPU memory is not enough to hold 1T parameters.<br> Optimizer like adam requires two more parameter for each weight, so we need more than 10 times memory.<br> We need to use thousands GPUs to train parameters!<h2 id=Parallelism>Parallelism</h2><p><img alt=Parallelism src=parallelism.png><ul><li>Data Parallelism (DP): Each GPU has a entire model, and data is distirbuted among GPUs. Weight should be synchronized!<li>Model Parallelism (MP): Each layer is divided to multiple GPUs. Activation should be transfered among GPUs!<li>Pipeline Parallelism (PP): Weight, activation is too big for GPU to handle. We divide each stage to multiple GPUs.</ul><p><img alt="3D Parallelism" src=3d_parallelism.png><p>DP, MP, PP can be used together to train model across multiple GPUs.<br> e.g. Model with 2TB parameters, 256 layers can be trained with 4096 GPUs using 8-way MP, 64-way PP, and 8-way DP.<h3 id=All-Reduce-vs-All-Gather>All Reduce vs. All Gather</h3><ul><li>All Reduce (in DP): Accumulate weight updates across different mini-batches to obtain global weight updates. Computation is used.<li>All gather (in MP): Copying a set of weights or activations to other nodes. Only collect without computation.</ul><h3 id=ZeRO-DP-Zero-Redundancy-Optimizer>ZeRO-DP (Zero Redundancy Optimizer)</h3><p>Model size is too large, we divide parameters into N GPUs!<p>Instead of holding every parameter in each GPUs, each GPU hold portion of parameters.<br> When we need to compute, use all gather to get all parameters, compute, then hold portion of parameters.<p>Unlike DP, we don't have to hold every parameters!<br> Unlike MP, we use all gather on parameters instead of activation!<h3 id=ZeRO-Offload>ZeRO-Offload</h3><p>We need large memory because optimizer like adam need states.<p>Solution: Store optimizer states in CPU memory for entire training!<br> CPU runs Adam code to update weight.<p>With ZeRO-Offload, we can even train large model in single GPU, and model can be scaled very easily.<br> However, we now have a bottleneck in CPU-GPU bandwidth (ultimately bottleneck in CPU memory).<br> Big companies use DRAM per GPU, so we can achieve higher bandwidth and larger memory.<h1>Test-time Scaling</h1><p>ChatGPT o1 use self refinement with chain of thought: every input, reasoning, output from previous turn is used as next turn's input.<br> Log of compute cost is proportional to accuracy.<p>AlphaGo and Poker AI was able to achieve better performance with searching (i.e. thinking) for 30 seconds before answering.<br> In natural language, chain-of-thought prompting is used. Instead of just giving input and output, giving input, output with reasoning can get better output.<p>Large models on difficult problems can benefit the most.<br> The more time it takes to search, the better the answer will be.<h2 id=DeepSeek-R1-Training>DeepSeek-R1 Training</h2><ul><li>Phase 1: Cold Start (Same supervised learning)<li>Phase 2: Reasoning (Learn how to search, by giving longer time to answer)<li>Phase 3: Rejection Sampling SFT (Self training with good results)<li>Phase 4: Diverse RL (Check helpfulness and harmfulness)</ul><p>Even when DeepSeek-R1 was not instructed to use chain of thought, it was able to learn chain of thought on its own!<br> More steps and longer output resulted in better answer.</div><footer class=post-footer><div class=post-nav><div class=post-nav-item><a title="Multiplexing & UDP" href=/posts/55/ rel=prev> <i class="fa fa-angle-left"></i> Multiplexing & UDP </a></div><div class=post-nav-item><a title="Introduction to Geometry" href=/posts/56/ rel=next> Introduction to Geometry <i class="fa fa-angle-right"></i> </a></div></div></footer></article></div><div class="comments utterances-container"></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© 2023 – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>CookieHCl</span></div><div class=powered-by>Powered by <a href=https://hexo.io/ rel=noopener target=_blank>Hexo</a> & <a href=https://theme-next.js.org/muse/ rel=noopener target=_blank>NexT.Muse</a></div></div></footer><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div><div class=sidebar-dimmer></div><div aria-label="Back to top" class=back-to-top role=button><i class="fa fa-arrow-up fa-lg"></i><span>0%</span></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><script class=next-config data-name=utterances type=application/json>{"enable":true,"repo":"CookieHCl/cookiehcl.github.io","issue_term":"pathname","theme":"github-light"}</script><script defer src=/js/third-party/comments/utterances.js></script>