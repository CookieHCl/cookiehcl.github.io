<!doctype html><html lang=en><meta charset=UTF-8><meta content="width=device-width" name=viewport><meta content=#222 name=theme-color><meta content="Hexo 7.3.0" name=generator><link crossorigin href=https://cdnjs.cloudflare.com rel=preconnect><link href=/images/apple-touch-icon-next.png rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><link href=/css/main.css rel=stylesheet><link integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css rel=stylesheet><link integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css rel=stylesheet><script class=next-config data-name=main type=application/json>{"hostname":"cookiehcl.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"agate","dark":"agate"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":true,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script defer src=/js/config.js></script><meta content="Pipelined tile based multiplications  We need to multiply large matrices with small systolic array. Systolic array use tile pipelining! It keeps get next tile's input to remove idle cycles. Convolutio" name=description><meta content=article property=og:type><meta content="NPU (Hardware Accelerator for Neural Networks)" property=og:title><meta content=https://cookiehcl.github.io/posts/71/index.html property=og:url><meta content="기억 저장소" property=og:site_name><meta content="Pipelined tile based multiplications  We need to multiply large matrices with small systolic array. Systolic array use tile pipelining! It keeps get next tile's input to remove idle cycles. Convolutio" property=og:description><meta content=en_US property=og:locale><meta content=https://cookiehcl.github.io/posts/71/tile_pipelining.png property=og:image><meta content=https://cookiehcl.github.io/posts/71/memory_compute_balance.png property=og:image><meta content=https://cookiehcl.github.io/posts/71/tpuv2.png property=og:image><meta content=https://cookiehcl.github.io/posts/71/tpu_board.png property=og:image><meta content=https://cookiehcl.github.io/posts/71/tpuv3_scaling.png property=og:image><meta content=https://cookiehcl.github.io/posts/71/tpuv4i.png property=og:image><meta content=https://cookiehcl.github.io/posts/71/d1_chip.png property=og:image><meta content=2025-04-15T00:02:14.000Z property=article:published_time><meta content=2025-04-15T00:02:14.000Z property=article:modified_time><meta content=CookieHCl property=article:author><meta content=summary name=twitter:card><meta content=https://cookiehcl.github.io/posts/71/tile_pipelining.png name=twitter:image><link href=https://cookiehcl.github.io/posts/71/ rel=canonical><script class=next-config data-name=page type=application/json>{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://cookiehcl.github.io/posts/71/","path":"posts/71/","title":"NPU (Hardware Accelerator for Neural Networks)"}</script><script class=next-config data-name=calendar type=application/json>""</script><title>NPU (Hardware Accelerator for Neural Networks) | 기억 저장소</title><script integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js></script><script defer src=/js/utils.js></script><script defer src=/js/motion.js></script><script defer src=/js/sidebar.js></script><script defer src=/js/next-boot.js></script><script integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin defer src=https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js></script><script defer src=/js/third-party/search/local-search.js></script><script class=next-config data-name=mermaid type=application/json>{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.10.1/mermaid.min.js","integrity":"sha256-BmQmdWDS8X2OTbrwELWK366LV6escyWhHHe0XCTU/Hk="}}</script><script defer src=/js/third-party/tags/mermaid.js></script><script class=next-config data-name=enableMath type=application/json>true</script><link integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM=" crossorigin href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css rel=stylesheet><script class=next-config data-name=katex type=application/json>{"copy_tex_js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js","integrity":"sha256-Us54+rSGDSTvIhKKUs4kygE2ipA0RXpWWh0/zLqw3bs="}}</script><script defer src=/js/third-party/math/katex.js></script><noscript><link href=/css/noscript.css rel=stylesheet></noscript><body class=use-motion itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><div class=column><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=site-brand-container><div class=site-nav-toggle><div aria-label="Toggle navigation bar" class=toggle role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <i class=logo-line></i> <p class=site-title>기억 저장소</p> <i class=logo-line></i> </a></div><div class=site-nav-right><div class="toggle popup-trigger" aria-label=Search role=button><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>About</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>Tags</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>Categories</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>Archives</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>Search </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container><input autocapitalize=off autocomplete=off class=search-input maxlength=80 placeholder=Searching... spellcheck=false type=search></div><span class=popup-btn-close role=button> <i class="fa fa-times-circle"></i> </span></div><div class=search-result-container><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>Table of Contents<li class=sidebar-nav-overview>Overview</ul><div class=sidebar-panel-container><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>1.</span> <span class=nav-text>Pipelined tile based multiplications</span></a><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>2.</span> <span class=nav-text>Convolution on GPU</span></a><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>3.</span> <span class=nav-text>Weight stationary in Systolic Array</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Hiding-latency-of-reading-weights><span class=nav-number>3.1.</span> <span class=nav-text>Hiding latency of reading weights</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>4.</span> <span class=nav-text>Improvements in TPUs</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#TPUv2><span class=nav-number>4.1.</span> <span class=nav-text>TPUv2</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#TPUv3><span class=nav-number>4.2.</span> <span class=nav-text>TPUv3</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#TPUv4i><span class=nav-number>4.3.</span> <span class=nav-text>TPUv4i</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>5.</span> <span class=nav-text>Vision for driving assistance and autonomy</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Teslar-FSD-Chip><span class=nav-number>5.1.</span> <span class=nav-text>Teslar FSD Chip</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#ExaPOD-Teslar-Supercomputer><span class=nav-number>5.2.</span> <span class=nav-text>ExaPOD: Teslar Supercomputer</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Dojo-compiler><span class=nav-number>5.3.</span> <span class=nav-text>Dojo compiler</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>6.</span> <span class=nav-text>Other NPUs</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#AMD-MI300X-Series><span class=nav-number>6.1.</span> <span class=nav-text>AMD MI300X Series</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#SambaNova-s-Chiplet-based-Accelerator><span class=nav-number>6.2.</span> <span class=nav-text>SambaNova's Chiplet based Accelerator</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Cerebras-Wafer-Scale-Engine-WSE><span class=nav-number>6.3.</span> <span class=nav-text>Cerebras Wafer Scale Engine (WSE)</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>7.</span> <span class=nav-text>Zero skipping</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#SwarmX><span class=nav-number>7.1.</span> <span class=nav-text>SwarmX</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#NVIDIA-Tensor-Core><span class=nav-number>7.2.</span> <span class=nav-text>NVIDIA Tensor Core</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Zero-Skipping-in-Nvidia-A100><span class=nav-number>7.3.</span> <span class=nav-text>Zero Skipping in Nvidia A100</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Samsung-NPU-v1><span class=nav-number>7.4.</span> <span class=nav-text>Samsung NPU v1</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Samsung-NPU-v2><span class=nav-number>7.5.</span> <span class=nav-text>Samsung NPU v2</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Precision-in-Nvidia-A100-and-H100><span class=nav-number>7.6.</span> <span class=nav-text>Precision in Nvidia A100 and H100</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link><span class=nav-number>8.</span> <span class=nav-text>Future of accelerator</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop=author itemscope itemtype=http://schema.org/Person><p class=site-author-name itemprop=name>CookieHCl<div class=site-description itemprop=description></div></div><div class="site-state-wrap animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>96</span> <span class=site-state-item-name>posts</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>20</span> <span class=site-state-item-name>categories</span></a></div></nav></div><div class="links-of-author animated"><span class=links-of-author-item> <a rel="noopener me" title="GitHub → https://github.com/CookieHCl" href=https://github.com/CookieHCl target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a rel="noopener me" title="BOJ → https://www.acmicpc.net/user/CookieHCl" href=https://www.acmicpc.net/user/CookieHCl target=_blank><i class="fa fa-code fa-fw"></i>BOJ</a> </span><span class=links-of-author-item> <a rel="noopener me" title="solved.ac → https://solved.ac/profile/CookieHCl" href=https://solved.ac/profile/CookieHCl target=_blank><i class="fa fa-audio-description fa-fw"></i>solved.ac</a> </span></div><div class="cc-license animated" itemprop=license><a class=cc-opacity href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ko rel=noopener target=_blank><img alt="Creative Commons" src=https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg></a></div></div></div></div></aside></div><div class="main-inner post posts-expand"><div class=post-block><article class=post-content itemscope itemtype=http://schema.org/Article lang=en><link href=https://cookiehcl.github.io/posts/71/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=/images/avatar.gif itemprop=image> <meta content=CookieHCl itemprop=name> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="기억 저장소" itemprop=name> <meta itemprop=description> </span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork> <meta content="NPU (Hardware Accelerator for Neural Networks) | 기억 저장소" itemprop=name> <meta itemprop=description> </span><header class=post-header><h1 itemprop="name headline" class=post-title>NPU (Hardware Accelerator for Neural Networks)</h1><div class=post-meta-container><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>Posted on</span> <time itemprop="dateCreated datePublished" title="Created: 2025-04-15 09:02:14" datetime=2025-04-15T09:02:14+09:00>2025-04-15</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-folder"></i> </span> <span class=post-meta-item-text>In</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/SNU/ itemprop=url rel=index><span itemprop=name>SNU</span></a> </span> , <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/SNU/4-1/ itemprop=url rel=index><span itemprop=name>4-1</span></a> </span> , <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/SNU/4-1/%ED%95%98%EB%93%9C%EC%9B%A8%EC%96%B4%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%84%A4%EA%B3%84/ itemprop=url rel=index><span itemprop=name>하드웨어시스템설계</span></a> </span> </span><span title="Reading time" class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>Reading time ≈</span> <span>6 mins.</span> </span></div></div></header><div class=post-body itemprop=articleBody><h1>Pipelined tile based multiplications</h1><p><img alt="Tile Pipelining" src=tile_pipelining.png><p>We need to multiply large matrices with small systolic array.<br> Systolic array use tile pipelining! It keeps get next tile's input to remove idle cycles.<h1>Convolution on GPU</h1><p>We use BLAS (Basic Linear Algebra Subprograms) library for matrix multiplication.<p>cuBLAS and cuDNN both duplicate input image into large array, so we can conmpute convolution with matrix multiplication.<br> cuBLAS actually created large matrix, while cuDNN stores original image and reformat when it goes to GPU cache.<br> cuDNN better utilizes main memory at a small cost of software's on-the-fly data reformatting!<p>However, in hardware accelerator, hardware should reformat image.<br> In TPU architecture, systolic data setup will convert 2D window into 1D vector to perform matrix multiplication.<h1>Weight stationary in Systolic Array</h1><p>Google found out memory bandwidth determines performance. Why?<p>Reason 1: M*V multiplication of MLP and RNN is bandwidth-intensive operation because weights are used only once.<p>c.f. M*V multiplication is highly used in recommendation system.<p>Solution 1: Large batch changes matrix-vector multiplication to matrix-matrix multiplication which offers higher data reuse.<p>In case of interactive service, (e.g. recommendation) batch formation will delay user service therby limit batch size.<p>Solution 2: Weight stationary in Systolic Array<p>Instead of pipelining weights, we load weights at once in systolic array.<p>During MM, we need high bandwidth only for one of the two matrices from the internal buffer.<br> Thus, in case of low memory bandwidth, weight stationary can offer much better performance than output stationary!<p>Internal buffer -> high bandwidth, provide each tile<br> DRAM -> low bandwidth, we store each tile into systolic array and reuse it.<p>Problem 1: latency exists! We should hide latency of reading weights with computation.<br> We should compare latency of reading the weights of next tile from slow DRAM vs. latency of systolic array multiplication on current tile.<p>Problem 2: We need large partial sum buffer and accumulate partial sums.<h2 id=Hiding-latency-of-reading-weights>Hiding latency of reading weights</h2><p>Due to low bandwidth of DRAM, systolic array have to wait until DRAM read the next tile.<p>We can use higher DRAM bandwidth, better DRAM bandwidth utilization (e.g. int8), or ... just increase systolic array computation time?!?!?!<p>Computing larger matrix can reuse DRAM's tile more, so we can hide latency.<br> By having larger batch size, we make input matrix taller, thus reusing DRAM's tile more!<br> In case of interactive service, we can process multiple user's input as a batch.<p><img alt="Memory-Compute Balance" src=memory_compute_balance.png><p>Memory-Compute Balance: At ideal case, computation time and memory access time should be same!<br> If memory access is longer, we have memory bottleneck.<br> If computation takes longer, we have compute bottleneck.<br> This is why batch size is important!<p>e.g. Unlike TPUv1, TPUv2 used High Bandwidth Memory (HBM) and 16-bit bfloat.<br> Since memory access time is faster, we should make computation time faster to maintain the memory-compute balance.<p>TPUv2 also use bfloat16 instead of float32.<br> Unlike float16, bfloat16 have same exponent as float32.<br> Theorically, represenatable value range are same with float32! (From <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mn>10</mn><mrow><mo>−</mo><mn>38</mn></mrow></msup></mrow><annotation encoding=application/x-tex>10^{-38}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">38</span></span></span></span></span></span></span></span></span></span></span></span> to <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>3</mn><mo>⋅</mo><msup><mn>10</mn><mn>38</mn></msup></mrow><annotation encoding=application/x-tex>3 \cdot 10^{38}</annotation></semantics></math></span><span aria-hidden=true class=katex-html><span class=base><span class=strut style=height:0.6444em;></span><span class=mord>3</span><span class=mspace style=margin-right:0.2222em;></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em;></span></span><span class=base><span class=strut style=height:0.8141em;></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em;><span style=top:-3.063em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">38</span></span></span></span></span></span></span></span></span></span></span></span>)<h1>Improvements in TPUs</h1><h2 id=TPUv2>TPUv2</h2><p><img alt=TPUv2 src=tpuv2.png><p>TPUv2 have interconnect router that can directly connect multiple TUP chips.<p>TPUv2 uses two TPU core, PCIe queues (to connect with CPU, GPUs can't run without CPU's command!), and bfloat16.<p>TPU core has vector unit. Vector unit have 32K x 32bit vector memory, can perform elementwise operation with 2 ALUs, and send and retrieve vectors from matrix multipy unit.<p>TPUv2 has 128x128 systolic array!<br> Smaller matrix have better utilization, larger matrix have better data reuse.<br> By considering 256x256, 128x128, 64x64, they chose 128x128 as the best option.<h2 id=TPUv3>TPUv3</h2><p>TPUv3 is similar to TPUv2.<p>Now we use 2 systolic arrays per TPU Core, HBM with better bandwidth and capacity, interconnect router with more nodes, etc.<p><img alt="TPU Board" src=tpu_board.png><p>Each TPU Board has 4 TPU chips.<br> TPUs are interconnected in 2D Torus.<br> Each TPU Board is connected to eac Host (CPU) wiht PCI-e.<p><img alt="TPUv3 Scaling" src=tpuv3_scaling.png><p>Google run benchmarks with 6 production applications they're using, 2 of each MLP, CNN, RNN.<p>RNN, CNN had linear scaling, but MLP (recommendation system) didn't had linear scaling.<h2 id=TPUv4i>TPUv4i</h2><p><img alt=TPUv4i src=tpuv4i.png><p>TPUv4i have single core (Tensor core) with 4 systolic arrays connected to vector memory.<br> CMEM is a cache between HBM and VMEM.<br> It can cache previously computed matrix!<p>Each systolic array compute 32x32 matrix multiplication with 4x4 dot product.<br> 4 inputs and 4 weights are computed at once in 4 input-adder tree.<br> By removing 3 accumulators, We could reduce 40% in area and 25% in power with respect to 128x128 systolic array.<p>TPUv4i also used bfloat16 and int8.<br> Problem: poor segmentation degrades the result of the image quality enhancement.<h1>Vision for driving assistance and autonomy</h1><p>Tesla Hydranet has 8 cameras, 16 timesteps, and 32 batch size.<br> We need to process 4096 HD images in a single forward pass!<h2 id=Teslar-FSD-Chip>Teslar FSD Chip</h2><p>First chip to actually accelerate convolution!<br> They used systolic array with 8bit input.<p>To provide and store large input datas, Teslar have large SRAMs with high bandwidth.<h2 id=ExaPOD-Teslar-Supercomputer>ExaPOD: Teslar Supercomputer</h2><p>Self-driving cars need lots of training data. Tesla used 1.5PB for final dataset!<p><img alt="D1 Training chip" src=d1_chip.png><p>Single D1 Training chip has 5x5 chips that can be connected in four directions to arrange in tiles.<br> It is connected to CPU with PCIe, and every training data is stored in CPU's memory.<br> There is no large memory in D1 chip! Therefore we want to avoid data repetation.<ol><li>Parameters for two layers are distributed across two tiles, splitting across channels.<li>Inputs are shared across four tiles, splitting across batches. (1/4 of batch)<li>First layer's parameter are replicated to create full parameters in two tiles.<li>These parameters are replicated into other two tiles.<li>First layer is run<li>Second layer's parameter are replicated into other two tiles. (1 copy per 2 tiles for MP)<li>Remove replicated first parameters and input<li>Split each first layer's output (per 1/4 of batch) into two tiles, spliting across channels<li>Second layer is run (output two partial sums per 1/4 of batch)<li>Add each two partial sums (per 1/4 of batch)<li>Remove replicated second parameters<li>Now we've done two layers of CNNs, and parameters are still in chips! (can be reused)</ol><h2 id=Dojo-compiler>Dojo compiler</h2><p>Most engineers in Teslar is software engineer!<br> Because their design was unique, they had to make their own compiler for pytorch.<p>Result was better than NVIDIA's solution!<h1>Other NPUs</h1><h2 id=AMD-MI300X-Series>AMD MI300X Series</h2><ul><li>Use 8 HBM stacks for memory<li>Stacks of silicon dies for compute<li>Tend to have smaller server size, and consumes less power</ul><h2 id=SambaNova-s-Chiplet-based-Accelerator>SambaNova's Chiplet based Accelerator</h2><ul><li>Best chiplet-based inference accelerator<li>Large silicon die in the middle, surrounded by DRAM and HBM</ul><h2 id=Cerebras-Wafer-Scale-Engine-WSE>Cerebras Wafer Scale Engine (WSE)</h2><ul><li>Proposes wafer as a large silicon chip<br> c.f. Usually you cut wafer to get desired amount of chips<li>About 1 million cores are inside one wafer!<li>Zero skipping - faster computation for sparse weights</ul><h1>Zero skipping</h1><p>Significant portion of input values in a CNN is zero!<br> Pruning and ReLU makes zero values.<h2 id=SwarmX>SwarmX</h2><p>Motivation: matrix-matrix multiplication can be viewed as a sum of scalar-vector multiplication.<p>If scalar is zero, we skip!<h2 id=NVIDIA-Tensor-Core>NVIDIA Tensor Core</h2><p>Tensor Core performs 64 multiplications per cycle. (4 instances of 4x4 outer product multiplier)<br> This can multiply 4x4 matrix in 1 cycle.<p>Each outer product multiplier gets row/column of matrix.<br> In 1 cycle, outer product performs outer product, (similar to systolic array) and adder tree adds outer product's elements.<h2 id=Zero-Skipping-in-Nvidia-A100>Zero Skipping in Nvidia A100</h2><p>For each 4 parameter, we prune 2 parameters.<br> We can shrink parameters into half, but we need memory for non-zero indices.<br> Sparse Tensor Core will select input activation at non-zero indices with mux.<p>We need 2 bits for indices - 12.5% for 16bit data, 25% overhead for 8bit data<p>When training, We prune half of parameters and update weight only for parameters that aren't pruned.<br> Sparse Tensor Cores will multiply 16x32 and 32x8 matrix in 2 cycles.<h2 id=Samsung-NPU-v1>Samsung NPU v1</h2><p>Exynos has zero weight skipping!<p>For each kernel element, we produce intermediate output by multiplying kernel element and corresponding input feature map.<p>If kernel is 3x3, we need 9 cycles. ' However, if kernel has zero values, we can skip to save cycles.<p>This is the first commercial neural network accelerator!<h2 id=Samsung-NPU-v2>Samsung NPU v2</h2><p>Zero-activation skipping!<ul><li>Feature-map-aware zero skipping: Move feature map and weight to fill in zero features, so we can perform less dot products.<li>Feature-map lossless compressor: If feature map has many zero features, we can compress into smaller feature map.</ul><h2 id=Precision-in-Nvidia-A100-and-H100>Precision in Nvidia A100 and H100</h2><p>We use more smaller types - float8, int8, etc.<p>If the data size is halved, we only need to fetch half the weights (or we can compute twice the weight in the same time), so performance is doubled!<p>NVIDIA has found that smaller types don't affect accuracy as much for different networks.<h1>Future of accelerator</h1><p>Reasoning model needs more token generations.<br> But power limits data center; We should maximize number of token generated per power!<p>Power efficiency is becoming more and more important.<br> Probably the only hope for startup companies to beat big tech?<p>Modern GPU can access other GPU memories. (NVLINK)<br> GPU can now access 155TB of memory at high bandwidth!<br> Configurations may change in 1T parameter model training.<br> Network speed is also becoming important.</div><footer class=post-footer><div class=post-nav><div class=post-nav-item><a title="Congestion Control" href=/posts/67/ rel=prev> <i class="fa fa-angle-left"></i> Congestion Control </a></div><div class=post-nav-item><a title="Roller Coaster Physics" href=/posts/64/ rel=next> Roller Coaster Physics <i class="fa fa-angle-right"></i> </a></div></div></footer></article></div><div class="comments utterances-container"></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© 2023 – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>CookieHCl</span></div><div class=powered-by>Powered by <a href=https://hexo.io/ rel=noopener target=_blank>Hexo</a> & <a href=https://theme-next.js.org/muse/ rel=noopener target=_blank>NexT.Muse</a></div></div></footer><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div><div class=sidebar-dimmer></div><div aria-label="Back to top" class=back-to-top role=button><i class="fa fa-arrow-up fa-lg"></i><span>0%</span></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><script class=next-config data-name=utterances type=application/json>{"enable":true,"repo":"CookieHCl/cookiehcl.github.io","issue_term":"pathname","theme":"github-light"}</script><script defer src=/js/third-party/comments/utterances.js></script>