---
title: 딥기 Lec 11
categories:
  - SNU
  - 4-1
  - 딥러닝의 기초
date: 2023-04-06 12:35:15
tags:
---

# Recurrent Neural Networks

Feedfoward Neural Network와 달리 one-to-one 말고도 many-to-one, one-to-many, many-to-many 등의 구조를 가질 수 있다!

참고) seq to seq는 many-to-one 이후 one-to-many

hidden vector h를 가지고 있음

$$
\begin{align*}
    h_t &= tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\\
    &= tanh\left( W \begin{pmatrix}h_{t-1} \\ x_t\end{pmatrix} + b_h\right)\\
    y_t &= W_{hy}h_t + b_y
\end{align*}
$$

ex) Language model: 문자 c~1~, c~1~, ..., c~t-1~을 보고 다음에 올 문자 c~t~를 예측

## Backpropagation Through Time (BPTT)

전체 sequence foward 후 전체 sequence backpropagation

단점: 너무 메모리가 많이 필요함

## Truncated backpropagation through time (TBPTT)

일부 sequece에서만 forward 후 backpropagation

# Long Short Term Memory (LSTM)

$$
h_t = tanh\left( W \begin{pmatrix}h_{t-1} \\ x_t\end{pmatrix} + b_h\right)
$$

문제점: W가 계속 곱해짐

W의 largest singular value > 1이면 무한대로 발산  
W의 largest singular value < 1이면 vanishing gradients

LSTM은 4개의 gate를 추가!

i: Input gate, whether to write to cell  
f: Forget gate, whether to erase cell  
o: Output gate, how much to reveal cell  
g: Gate gate, how much to write to cell

$$
\begin{align*}
    \begin{pmatrix}i \\ f \\ o \\ g\end{pmatrix} &= \begin{pmatrix}\sigma \\ \sigma \\ \sigma \\ tanh\end{pmatrix}\left( W \begin{pmatrix}h_{t-1} \\ x_t\end{pmatrix} + b_h\right)\\
    c_t &= f_t \odot c_{t-1} + i_t \odot g_t\\
    h_t &= o_t \odot tanh(c_t)
\end{align*}
$$

왜 이랬는지는 아무도 모르지만 잘 작동한다

![RNN](rnn.png)

![LSTM](lstm.png)

RNN과 달리 LSTM은 uninterrupted gradient flow를 가진~~다고 논문에서 주장함~~

but 요즘은 RNN, LSTM 아예 안 씀

# Self-Attention layer

![Attenion](attention.png)

위 그림은 batch size 3 가정

Input $X (N_X \times D_X)$ ($N_X$는 batch size, $D_X$는 dimension size, i.e. vocabulary 수)  
Key matrix $W_K (D_X \times D_Q)$  
Value matrix $W_V (D_X \times D_V)$  
Query matrix $W_Q (D_X \times D_Q)$

Query vectors $Q = XW_Q (N_X \times D_Q)$  
Key vectors $K = XW_K (N_X \times D_Q)$  
Similarities $E = QK^T / \sqrt(D_Q) (N_X \times N_X)$  
Attention weights $A = softmax(E, dim = 1) (N_X \times N_X)$  
Value vectors $V = XW_V (N_X \times D_V)$  
Output vectors $Y = AV (N_X \times D_V)$

왜 이랬는지는 아무도 모르지만 잘 작동한다 2222

Attention weight은 input vector마다 softmax 해준 결과!!  
softmax하면 합이 1이므로 masking 효과가 있음

문제점) RNN, LSTM과 달리 input 순서가 바뀌면 output 순서만 바뀜 (위치 고려를 아예 안 함!)

그래서 positional encoding을 input에 추가해줘야 함

대신 RNN, LSTM과 달리 순서가 없으므로 병렬 처리가 아주 쉬움 (행렬 곱셈밖에 없음)

# Transformer

![Transformer](transformer.png)

Layer Normalization, MLP는 vector마다 개별적으로 적용됨  
유일하게 vector끼리 연결되는 부분은 Self-Attention(위에서 본 것) 뿐

그래서 논문 제목이 **Attention is all you need**

위 블럭 그대로 여러 개 연결하면 Transformer; 물론 자유롭게 수정 가능

이전까진 분류, 요약, 등 분야마다 연구자가 달랐지만 Transformer가 모든 분야 씹어먹은거 보고 분야마다 연구자가 사라짐;

그냥 훈련시킨 Transformer 뒤에 추가적인 레이어 달아서 좀 fine-tuning 해주면 됨 ~~아니 내 인생을 바친 연구가!!!~~

# GPT

Transformer 나온 이후 다시 무지성 레이어 늘리기 시작  
그냥 Transformer 길게 달면 다 해결된다~

GPT-3은 1750억개 parameters

아무리 그래도 1750억개 parameters는 너무 많은거 아님?  
왠진 몰라도 regularization도 잘 되고 훈련도 잘 되고 결과도 잘 나옴  
~~연구자들 오열~~

GPT-3은 autoregressive model; 토큰 t~1~, t~2~, ..., t~n-1~이 주어졌을 때 t~n~이 나올 확률 구함

## Greedy Search

EOS(End of sentence) 토큰이 나올 때까지 항상 확률이 제일 높은 토큰을 고름

단점: suboptimal solution에 도달할 수 있음

ex: 0.4 -> 0.9 토큰들이 제일 확률 높은데 0.5 -> 0.4 토큰들이 골라질 수 있음

## Beam Search

얘도 suboptimal solution을 찾지만 greedy search보다는 나음

1. beam width B를 정하고 candidate를 빈 문장으로 시작
1. candidate들로부터 다음 토큰을 구함
1. 구한 문장들 중 가장 확률이 높은 B개를 새 candidate로 정하고 반복

## Top K sampling

다음 토큰으로 올 수 있는 토큰 중 제일 확률 높은 K개를 구하고, 다음 토큰을 K개로부터 확률에 비례해서 sampling

## Nucleus sampling

다음 토큰으로 올 수 있는 토큰들을 확률이 높은 순서대로 정렬 후 누적 확률이 p를 넘길때까지 토큰들을 구하고, 구한 토큰들로부터 확률에 비례해서 sampling

## Reinforcement learning from human feedback (RLHF)

1. Pretrain Language Model(LM)
1. Sample prompts from prompts dataset, and generate responses
1. Let human score the responses
1. Train reward model on {response, score} pairs
1. Finetune the LM with reinforcement learning with the trained reward model
