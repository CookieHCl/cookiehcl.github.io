---
title: ë”¥ê¸° Lec 13
categories:
  - SNU
  - 4-1
  - ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆ
date: 2023-04-13 12:38:26
tags:
---

# Generative model

low dimensional vector zë¡œë¶€í„° meaningful sample xë¥¼ ë§Œë“¤ì–´ëƒ„

ì¦‰, $\mathbb{E}_{x \sim \mathcal{D}}[\log p_\theta(x)]$ë¥¼ ìµœëŒ€í™”í•´ì•¼ í•¨ (ğ““ëŠ” data sampleì˜ distribution)

ë¬¸ì œì ) $p(x)$ê°€ ì£¼ì–´ì§€ëŠ”ê²Œ ì•„ë‹ˆë¼ $p(x)$ì—ì„œ ë½‘ì•„ë‚¸ data sampleì¸ ğ““ë§Œ ì£¼ì–´ì§  
ê·¸ë˜ì„œ ì§„ì§œ ë¬´ì§€ì„±ìœ¼ë¡œ ìµœëŒ€í™”í•˜ë©´ ğ““ë¥¼ ë§ì¶œ ìˆ˜ëŠ” ìˆì§€ë§Œ $p(x)$ë¥¼ ë§ì¶œ ìˆ˜ëŠ” ì—†ìŒ

ê·¸ëŸ¼ ì–´ë–»ê²Œ í•´ì•¼í•¨?  
ë‹µ ì—†ìŒ ê·¸ëƒ¥ ê¸°ë„í•´ì•¼í•¨

$p_\theta(x) = \int p(z)p_\theta(x \vert z)dz$ë¥¼ ìµœëŒ€í™”í•˜ì!  
ê·¼ë° integralì´ analytically intractable(ê³„ì‚° ë¶ˆê°€)

zëŠ” $\mathcal{N}(0, I)$ì—ì„œ, xëŠ” $\mathcal{N}(\mu_{x \vert z}, \sum_{x \vert z})$ì—ì„œ ë½‘ëŠ”ë‹¤ê³  ê°€ì •í•˜ë©´

$$\textstyle p_\theta(x) = C\int \exp\left( -\frac{1}{2} z^Tz \right)\exp\left( -\frac{1}{2}(x-\mu_{x \vert z})^T\sum_{x \vert z}^{-1}(x-\mu_{x \vert z}) \right)dz$$

ê·¼ë° ì´ê²ƒë„ $\mu_{x \vert z}, \sum_{x \vert z}$ê°€ zë‘ nonlinearí•˜ë©´ (ì¦‰ NNì„ ì“°ë©´) ê³„ì‚° ë¶ˆê°€

Bayes rule? $p_\theta(x) = \frac{p_\theta(x \vert z)p(z)}{p_\theta(z \vert x)}$ ì ë¶„ì€ ì‚¬ë¼ì¡Œì§€ë§Œ $p_\theta(z \vert x)$ëŠ” ì–´ë–»ê²Œ êµ¬í•¨?

# Variational autoencoders(VAE)

![VAE](vae.png)

$p_\theta(z \vert x)$ë¥¼ approximateí•˜ëŠ” encoder network $q_\phi(z \vert x)$ë¥¼ ì¶”ê°€!

ì…ë ¥ìœ¼ë¡œë¶€í„° $\mu_{z \vert x}, \sum_{z \vert x}$ë¥¼ êµ¬í•œ í›„ ì´ ê°’ë“¤ë¡œë¶€í„° zë¥¼ multivariate normal distributionìœ¼ë¡œ sampling í›„ decoderë¡œ xë¥¼ multivariate normal distributionìœ¼ë¡œ sampling

## ELBO

f(x)ê°€ êµ¬í•˜ê¸° ë„ˆë¬´ ì–´ë ¤ìš°ë©´ f(x)ì˜ lower bound h(x)ë¥¼ êµ¬í•œ ë‹¤ìŒ h(x)ë¥¼ maximize

PPT ì°¸ê³ ) $\log p_\theta(x^{(i)}) \geq \mathbb{E}_{Z \sim q_\theta(\cdot\vert x^{(i)})}\log p_\theta(x^{(i)} \vert Z) - D_{KL}(q_\theta(\cdot\vert x^{(i)}) \Vert p(\cdot))$

$x^{(i)}$ëŠ” íŠ¹ì • data sample

ë’¤ìª½ ë¶€ë¶„ì„ $\mathcal{L}(x^{(i)}, \theta, \phi)$ìœ¼ë¡œ í‘œê¸°í•˜ê³  Differentiable lower bound/Variational lower bound/Evidence lower bound(ELBO)ë¼ê³  í•¨

$\log p_\theta(x^{(i)})$ëŠ” ë„ˆë¬´ ì–´ë ¤ìš°ë‹ˆê¹Œ ELBOë¥¼ ìµœëŒ€í™”í•¨!

### ELBO ì˜ë¯¸

Recall: encoder $q_\phi(z \vert x)$, decoder $p_\theta(x \vert z)$

$$\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_{Z \sim q_\theta(\cdot\vert x^{(i)})}\log p_\theta(x^{(i)} \vert Z) - D_{KL}(q_\theta(\cdot\vert x^{(i)}) \Vert p(\cdot))$$

$$\mathbb{E}_{Z \sim q_\theta(\cdot\vert x^{(i)})}\log p_\theta(x^{(i)} \vert Z)$$

$x^{(i)}$ë¥¼ encoderì— ë„£ì–´ì„œ ë‚˜ì˜¨ sampleë“¤ì¸ Zì˜ decoder log-likelihoodì˜ ê¸°ëŒ“ê°’. decoderê°€ ì–¼ë§ˆë‚˜ reconstructionì„ ì˜ í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ„

$$D_{KL}(q_\theta(\cdot\vert x^{(i)}) \Vert p(\cdot))$$

encoderì™€ decoderì˜ distributionì´ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ ë‚˜íƒ€ëƒ„ (- ë¶™ì–´ìˆìœ¼ë‹ˆê¹Œ ë‘ ë¶„í¬ ê°™ì•„ì§ -> ì–˜ ìµœì†Œí™” -> ELBO ìµœëŒ€í™”)

### Reparameterization trick

ë¬¸ì œì ) Z samplingì€ backpropagationì´ ì•ˆë¨

í•´ê²°ì±…) samplingì„ inputìœ¼ë¡œ ë°€ì!

$\epsilon \sim \mathcal{N}(0, I)$ sampleë¡œë¶€í„° $\mu(x^{(i)}) + A(x^{(i)}) \epsilon \sim \mathcal{N}(\mu(x^{(i)}), \sum(x^{(i)}))$ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ (ì´ë•Œ $A(x^{(i)})$ëŠ” Cholesky factor or $QD^\frac{1}{2}$ where $\sum(x^{(i)}) = QDQ^T$)

$$\therefore \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)}\log p_\theta(x^{(i)} \vert Z) \approx \log p_\theta(x^{(i)} \vert \mu(x^{(i)}) + A(x^{(i)})\epsilon)$$

![Reparameterization trick](reparameterization_trick.png)

ì™¼ìª½ì€ ê¸°ì¡´ ë°©ë²•, ì˜¤ë¥¸ìª½ì€ reparameterization trick!

non-differentiable ë¶€ë¶„ì¸ samplingì´ inputìœ¼ë¡œ ë¹ ì ¸ì„œ ì˜¤ë¥¸ìª½ì€ backpropagationì´ ê°€ëŠ¥í•´ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ
