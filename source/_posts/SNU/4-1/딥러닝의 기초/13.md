---
title: 딥기 Lec 13
categories:
  - SNU
  - 4-1
  - 딥러닝의 기초
date: 2023-04-13 12:38:26
tags:
---

# Generative model

low dimensional vector z로부터 meaningful sample x를 만들어냄

즉, $\mathbb{E}_{x \sim \mathcal{D}}[\log p_\theta(x)]$를 최대화해야 함 (𝓓는 data sample의 distribution)

문제점) $p(x)$가 주어지는게 아니라 $p(x)$에서 뽑아낸 data sample인 𝓓만 주어짐  
그래서 진짜 무지성으로 최대화하면 𝓓를 맞출 수는 있지만 $p(x)$를 맞출 수는 없음

그럼 어떻게 해야함?  
답 없음 그냥 기도해야함

$p_\theta(x) = \int p(z)p_\theta(x \vert z)dz$를 최대화하자!  
근데 integral이 analytically intractable(계산 불가)

z는 $\mathcal{N}(0, I)$에서, x는 $\mathcal{N}(\mu_{x \vert z}, \sum_{x \vert z})$에서 뽑는다고 가정하면

$$\textstyle p_\theta(x) = C\int \exp\left( -\frac{1}{2} z^Tz \right)\exp\left( -\frac{1}{2}(x-\mu_{x \vert z})^T\sum_{x \vert z}^{-1}(x-\mu_{x \vert z}) \right)dz$$

근데 이것도 $\mu_{x \vert z}, \sum_{x \vert z}$가 z랑 nonlinear하면 (즉 NN을 쓰면) 계산 불가

Bayes rule? $p_\theta(x) = \frac{p_\theta(x \vert z)p(z)}{p_\theta(z \vert x)}$ 적분은 사라졌지만 $p_\theta(z \vert x)$는 어떻게 구함?

# Variational autoencoders(VAE)

![VAE](vae.png)

$p_\theta(z \vert x)$를 approximate하는 encoder network $q_\phi(z \vert x)$를 추가!

입력으로부터 $\mu_{z \vert x}, \sum_{z \vert x}$를 구한 후 이 값들로부터 z를 multivariate normal distribution으로 sampling 후 decoder로 x를 multivariate normal distribution으로 sampling

## ELBO

f(x)가 구하기 너무 어려우면 f(x)의 lower bound h(x)를 구한 다음 h(x)를 maximize

PPT 참고) $\log p_\theta(x^{(i)}) \geq \mathbb{E}_{Z \sim q_\theta(\cdot\vert x^{(i)})}\log p_\theta(x^{(i)} \vert Z) - D_{KL}(q_\theta(\cdot\vert x^{(i)}) \Vert p(\cdot))$

$x^{(i)}$는 특정 data sample

뒤쪽 부분을 $\mathcal{L}(x^{(i)}, \theta, \phi)$으로 표기하고 Differentiable lower bound/Variational lower bound/Evidence lower bound(ELBO)라고 함

$\log p_\theta(x^{(i)})$는 너무 어려우니까 ELBO를 최대화함!

### ELBO 의미

Recall: encoder $q_\phi(z \vert x)$, decoder $p_\theta(x \vert z)$

$$\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_{Z \sim q_\theta(\cdot\vert x^{(i)})}\log p_\theta(x^{(i)} \vert Z) - D_{KL}(q_\theta(\cdot\vert x^{(i)}) \Vert p(\cdot))$$

$$\mathbb{E}_{Z \sim q_\theta(\cdot\vert x^{(i)})}\log p_\theta(x^{(i)} \vert Z)$$

$x^{(i)}$를 encoder에 넣어서 나온 sample들인 Z의 decoder log-likelihood의 기댓값. decoder가 얼마나 reconstruction을 잘 하는지 나타냄

$$D_{KL}(q_\theta(\cdot\vert x^{(i)}) \Vert p(\cdot))$$

encoder와 decoder의 distribution이 얼마나 다른지 나타냄 (- 붙어있으니까 두 분포 같아짐 -> 얘 최소화 -> ELBO 최대화)

### Reparameterization trick

문제점) Z sampling은 backpropagation이 안됨

해결책) sampling을 input으로 밀자!

$\epsilon \sim \mathcal{N}(0, I)$ sample로부터 $\mu(x^{(i)}) + A(x^{(i)}) \epsilon \sim \mathcal{N}(\mu(x^{(i)}), \sum(x^{(i)}))$을 만들 수 있음 (이때 $A(x^{(i)})$는 Cholesky factor or $QD^\frac{1}{2}$ where $\sum(x^{(i)}) = QDQ^T$)

$$\therefore \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)}\log p_\theta(x^{(i)} \vert Z) \approx \log p_\theta(x^{(i)} \vert \mu(x^{(i)}) + A(x^{(i)})\epsilon)$$

![Reparameterization trick](reparameterization_trick.png)

왼쪽은 기존 방법, 오른쪽은 reparameterization trick!

non-differentiable 부분인 sampling이 input으로 빠져서 오른쪽은 backpropagation이 가능해진 것을 볼 수 있음
