---
title: 딥기 Lec 10
categories:
  - SNU
  - 4-1
  - 딥러닝의 기초
date: 2023-04-04 12:41:03
tags:
---

# CNN 역사

## AlexNet

![AlexNet](alexnet_layers.png)

- 224x224 input
- 5 CNN, 3 FC
- Max pooling & ReLU

FLOPS
: Floating point operations, 곱셈 덧셈 연산 수

AlexNet이 최초로 ImageNet classification에 8-layer짜리 Deep Learning 들고옴 -> 이후에 무지성 레이어 늘리기 대회 개최

![AlexNet Statistics](alexnet_stats.png)

Parameter는 거의 FC에 몰림, FLOPS는 거의 CNN에 몰림

## VGG-16

![VGG-16](vgg_layers.png)

VGG는 3x3 CNN을 여러 개 씀  
왜? 3x3 CNN 2개는 5x5 CNN 1개와 receptive field가 똑같지만 parameter, FLOPS가 더 작음 (하지만 ReLU 때문에 정확히 똑같은 결과가 나오진 않음)

![VGG-16 Statistics](vgg_stats.png)

## ResNet

이건 아직도 쓰고 있음

불행히도 무지성 레이어 늘리기 대회는 잘 안 먹혔음  
에러 까보니까 underfitting (training error 더 낮게 나옴) 이슈임

왜 모델이 더 복잡해졌는데 underfitting이 일어남?

가설: 아마 optimization이 잘 안 되는거 같다, 특히 deeper model은 shallow model도 재현할 수 있어야 되는데 identity function 훈련이 잘 안 됨

해결책: identity function을 훈련할 수 있도록 shortcut을 추가함!

![ResNet](resnet.png)

ResNet은 Residual block을 사용함  
x -> F(x)가 나오는 기존 block과 달리  
x -> F(x) + x가 나와서 identity function 훈련하기가 더 쉬움

사용한 레이어는 무려 152개 but 훈련 잘 됨

그 당시에 있었던 모든 인공지능 씹어먹었음

이 이후로 ImageNet 대회 폐지...

### PreActResNet

![PreActResNet](preactresnet.png)

기존 Residual block은 F(x) + x 뒤에 ReLU를 씌움

이러면 완벽한 identity를 못 만들지 않나?

그래서 Pre-Activation ResNet block에선 ReLU를 F(x) 안에 넣어서 완벽한 identity를 만들 수 있도록 만듦

성능은 ResNet보다 좀 더 좋음 but 실제로 사용되진 않는다고 한다

이거 외에도 SENet(Residual 이후 Global pooling 한걸 합쳐서 다시 scale??), DenseNet(Residual은 한번만 합침; Dense block은 아예 서너개씩 연결되어있음) 등 다양한 변형이 있음

## MobileNet

![MobileNet](mobilenet.png)

ResNet 때문에 이제 무지성 레이어 늘리면 무지성 성능 올라가는거 알겠음  
성능 올리는 방법은 알겠으니 효율성을 올려보자!

기존 convolution
: 3x3 kernel을 채널 수만큼 만듦

Depthwise Convolution
: 3x3 kernel을 1개만 만듦; 모든 채널에 같은거 씀

Pointwise Convolution
: 1x1 kernel을 채널 수만큼 만듦 (그냥 1x1 CNN임)

# CPU & GPU

CPU
: core 수 적은 대신 각 코어 성능이 높음(Ryzen 3970X는 64 * 3.7GHz)

GPU
: core 수 아주 많지만 각 코어 성능이 나쁨(RTX 3090은 10496 * 1.4GHz)

행렬 곱셈은 완벽하게 독립적이라서 병렬연산으로 만들기 아주 쉬움 -> 게임에만 쓰던 GPU가 세상에 나온 계기;

# Deep learning softwares

사실상 PyTorch랑 TensorFlow가 다 먹음

## PyTorch

Facebook이 만듦

Tensor
: Numpy array같은건데 GPU에서 돌릴 수 있음  

Autograd
: Tensor 생성할 때 `requires_grad=True`만 지정해주면 알아서 gradient 계산해주고 back propagation 해줌

Module
: Neural network layer, forward 부분만 정의해주면 됨 Optimizer, loss, back propagation, resetting gradient 등은 코드 한줄로 가능

PyTorch는 dynamic computation graph; 코드 불릴 때마다 새로 만들고 다 끝나면 없앰

참고) Tensorflow는 static computation graph; 최적화를 해준다는 장점이 있지만 중간에서 멈추고 디버깅 같은게 힘듦
