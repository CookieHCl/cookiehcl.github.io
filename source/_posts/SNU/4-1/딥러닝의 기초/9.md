---
title: 딥기 Lec 9
categories:
  - SNU
  - 4-1
  - 딥러닝의 기초
date: 2023-03-30 12:35:12
tags:
---

# Batch Normalization(BN)

Normalize output of layer so they have zero mean and unit variance.

$$\hat{x}=\frac{x-E[x]}{\sqrt{Var(X)}}$$

Why? It helps reduce the internal covariate shift and improves
optimization.

internal covariate shift????
: train set이랑 test set이랑 다른 분포를 가지고 있는걸 의미한다고 함

improves optimization
: normalize하면 더 generalization of features가 잘 일어날 거라고 생각됨

but 완벽한 정규분포는 문제가 있음  
ex) sigmoid function의 경우 정규분포로 normalize하게 되면 sigmoid function의 거의 직선인 부분만 사용하게 됨  
-> β, γ를 도입해서 평균과 표준편차를 학습할 수 있도록 함! ~~뭐이리 주먹구구식이야~~

참고) 위 가설은 반박된 적도 있고, 다른 가설들도 있음

- hidden layer간의 inter dependency를 줄임, but 정규분포는 아예 layer끼리 관계를 없애버리기 때문에 β, γ를 도입
- internal covariate shift는 iteration 전후의 gradient 차이를 의미한다고 봐야 한다는 설 -> 실제로 BN은 이걸 줄여줌, 더 정확히는 gradient를 적당하게 만들어줘서 landscape를 더 평평하게 만들어주는 효과가 있다고 함??? 그래서 higher learning rate를 사용해도 더 빨리 converge (but 이건 BN 말고도 L2 normalization같은 일반적인 normalization에서도 일어나는 현상이라고 함 BN은 사실상 regularization 역할?)

사실 batch normalization이 왜 잘 먹히는지 수학적으로 증명된건 없음

## Batch Normalization in Action

Input $x \in \mathbb{R}^{N \times D}$ (N: batch size, D: input dimension)

$$\mu_j = \frac{1}{N} \sum_{i=1}^{N}x_{i,j}$$

$$\sigma_j^2 = \frac{1}{N} \sum_{i=1}^{N}(x_{i,j} - \mu_j)^2$$

$$\hat{x}_{i,j} = \frac{x_{i,j} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}$$

$$y_{i,j} = \gamma_j \hat{x}_{i,j} + \beta_j$$

사실 $\mu, \sigma$를 test time에 못 구한다는 문제점이 있음 -> running average를 사용

Fully-connected: $x: N \times D$  
Normalize $\mu, \sigma, \gamma, \beta: 1 \times D$

CNN: $x: N \times C \times H \times W$  
Normalize $\mu, \sigma, \gamma, \beta: 1 \times C \times 1 \times 1$

보통 FC/CNN이랑 non-linearity 사이에 들어옴

## Batch Normalization 장단점

장점

- train 잘 됨
- higher learning rate, faster convergence
- more robust to initialization
- regularization 역할

단점

- 왜 되는지 모름
- train이랑 test에서 다르게 작동함 -> 버그의 주 원인

## Other normalizations

Batch Normalization이 잘 먹히니까 다른 Normalization도 등장

Layer Normalization: $\mu, \sigma, \gamma, \beta: N \times 1 \times 1 \times 1$

Instance Normalization: $\mu, \sigma, \gamma, \beta: N \times C \times 1 \times 1$

Group Normalization: 위 세 개를 적당히 섞음;

But Batch Normalization 빼고 다 망했다

# Overfitting

training set에서 너무 잘 함 but test set은 조짐

ML의 영원한 적! 해결책 -> Regularization

전에 봤던 해결책은 weight decay

$$L = \text{Loss function} + \lambda R(W)$$

L2 regularization이 주로 사용됨

## Dropout

~~이게 왜 먹힘???~~

p(주로 0.5)를 정함

training 도중에 1-p만큼 뉴런을 없애버림 (0.5면 절반을 없애버림)

```python
H = # last layer output
U = np.random.rand(*H.shape) < p # dropout mask
H *= U # drop!
```

참고: $Uni(0,1) < p \sim Bernoulli(p)$

그렇게 훈련시키면 잘 됨? ~~논문에 수식 하나도 없었다고 함~~

Forces network to have redundant representation; Prevents co-adaptation of features... 라고 설명하긴 함  
dropout의 optimal 모습은 뉴런을 어떻게 반으로 나눠도 잘 작동하는 NN이므로 적당히 정보가 복사되어 있으면서 뉴런 일부만으로도 결정될 정도로 핵심 정보들만 갖고 있는 NN이 될 거 같으니까 dropout을 적용시키고 optimal을 찾으면 우리가 원하는 갓-네트워크가 될거임???
~~무당식 사고방식~~

Test time에는 dropout 없는 대신 모든 뉴런 output에 p를 곱함  
각 뉴런이 영향 끼칠 확률이 p라서 p 곱해야 training 할때랑 기댓값이 같음

요즘은 FC 대신 global average pooling(GAP, feature들을 높이랑 너비에 대해 평균냄)을 사용해서 dropout을 아예 안 쓰기도 함

### Inverted Dropout

실제로는 training 때 `np.random.rand(*H.shape) < p / p`를 사용

training & test 때 전부 p를 곱하느니 trainig에서 p 곱하고 p 나눔

이러면 test때는 네트워크를 그대로 사용할 수 있음!

## DropConnect

Dropout -> 뉴런을 끔  
DropConnect -> 뉴런은 전부 살아있고 연결을 끔

## Randomness

Training: Add some kind of randomness

Test: Average out randomness (sometimes approximate)

Dropout, BN도 사실 위 공식을 따르는거임

### Data Augmentation

- Horizontal flips (but 뒤집으면 안 되는 이미지들도 존재)
- Random crops and scales  
    Training
    1. Resize image L*L (L in range [256, 480])
    1. Sample 224*224

    근데 Test 때는 randomness average 해야 함!

    Testing
    1. Resize image in 224, 256, 384, 480, 640
    1. Sample 224*224 from 4 corners + center
- Color jitter
- AutoAugment  
    알고리즘으로 제일 결과 잘 나오는 Augment를 찾으려고 했지만 결과 조짐
- CutOut (이미지 일부를 가려버림)  
    이미지에선 조졌지만 다른 곳에서 많이 응용됨  
    ex) language model에서 문장에서 단어를 가리고 학습
