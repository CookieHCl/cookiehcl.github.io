---
title: 딥기 Lec 12
categories:
  - SNU
  - 4-1
  - 딥러닝의 기초
date: 2023-04-11 15:27:06
tags:
---

# Unsupervised learning

Supervised learning
: data x, label y가 있고 f(x) = y인 f를 찾아야 함

Unsupervised learning
: data x만 있고 x에 있을 underlying hidden structure를 찾아야함???

# Autoencoders

$x \rightarrow z \rightarrow \hat{x}$

L2 loss function: $\lVert x - \hat{x} \rVert^2$

앞 부분을 encoder, 뒷 부분을 decoder라고 부름

z 차원은 x보다 작음! x의 meaningful factor(feature)를 찾아내는 목적이라고 생각하면 됨

옛날엔 Linear+sigmoid를 사용했다가 최근엔 FC Deep Layer쓰다가 ReLU CNN으로 갈아탐

Q. 그래서 자기자신 만드는 놈을 어따가 쓰는거임?  
A. 저기서 decoder를 버리고 encoder만 사용해서 supervised learning에서 입력들을 차원이 더 작은 feature로 만들어서 사용함  
즉, 기존 supervised learning이 $x \rightarrow \hat{y}$였다면 이제는 $x \rightarrow z \rightarrow \hat{y}$가 됨

# Variational autoencoders (VAE)

반대로 $x \rightarrow z \rightarrow \hat{x}$를 훈련시킨 다음 decoder 부분을 떼서 generative model로 사용함

근데 구조만 똑같지 원리나 발상 같은건 AE랑 아무런 관계가 없음

## KL-divergence(Kullback-Leibler divergence)

$$
\begin{align*}
  D_{KL}(p \Vert q) &= \mathbb{E}_{X \sim p} \left[ \log\frac{p(X)}{q(X)} \right] = \mathbb{E}_{X \sim q} \left[ \frac{p(X)}{q(X)} \log\frac{p(X)}{q(X)} \right]\\
  &= \begin{cases}
    \sum_{x\in\mathcal{X}} p(x) \log\frac{p(x)}{q(x)} & \text{if } p, q \text{ are discrete}\\
    \int_{-\infty}^{\infty} p(x) \log\frac{p(x)}{q(x)} dx & \text{if } p, q \text{ are continuous}
  \end{cases}
\end{align*}
$$

KL-divergence는 두 확률분포의 차이를 나타내는 함수! but 일반적으로 $D_{KL}(p \Vert q) \neq D_{KL}(q \Vert p)$라서 distance measure는 아님; 대신 $D_{KL}(p \Vert q) \geq 0, D_{KL}(p \Vert p) = 0$임

계산할 땐 아래 convention들을 따름

$$0\log\frac{0}{0} = 0, 0\log\frac{0}{q(x)} = 0, p(x)\log\frac{p(x)}{0} = \infty$$

Recall: Jensen's inequality

$f: \Omega\rightarrow\mathbb{R}$가 convex function이고 (즉, $\forall x,y\in\Omega, \forall\theta\in[0,1], f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta)f(y)$) $X$가 random variable일 때 $\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])$

∴ $f(x) = -\log(x)$는 convex function이므로

$$
\begin{align*}
  D_{KL}(p \Vert q) =& \mathbb{E}_{X \sim p} \left[ \log\frac{p(X)}{q(X)} \right] = \mathbb{E}_{X \sim p} \left[ -\log\frac{q(X)}{p(X)} \right]\\
  \geq& -\log\mathbb{E}_{X \sim p} \left[ \frac{q(X)}{p(X)} \right] = -\log\int q(x)dx = 0
\end{align*}
$$

### Divergence of KL-divergence

Support
: 함숫값이 0이 아닌 정의역 원소들의 집합

$\text{Supp}(p) \subseteq \text{Supp}(q)$, 즉 $q(x)=0 \Rightarrow p(x)=0$일 때만 finite KL 가능

이 외에는 KL-divergence는 ∞ (다만 조건을 만족해도 Cauchy와 Normal density처럼 KL-divergence가 ∞가 나올 수 있음)

### KL between Gaussians

$k$는 차원

$$
\textstyle
D_{KL}(\mathcal{N}(\mu_0,\sum_0) \Vert \mathcal{N}(\mu_1,\sum_1)) =\\
\frac{1}{2} \left( \text{Tr}(\sum_1^{-1}\sum_0) + (\mu_1-\mu_0)^T\sum_1^{-1}(\mu_1-\mu_0) - k + \log\frac{\det\sum_1}{\det\sum_0} \right)
$$

$$
\textstyle
D_{KL}(\mathcal{N}(\mu,\sum) \Vert \mathcal{N}(0,I)) =
\frac{1}{2} \left( \text{Tr}(\sum) + \mu^T\mu - k - \log\det\sum \right)
$$

If $\sum$ is diagonal,

$$
\textstyle
D_{KL}(\mathcal{N}(\mu,\sigma^2) \Vert \mathcal{N}(0,I)) =
\frac{1}{2} \sum_{j=1}^k \left( \sigma_j^2 + \mu_j^2 - 1 - \log\sigma_j^2 \right)
$$

## Jensen-Shannon divergence

$$JSD(p \Vert q) = \frac{1}{2}\left( D_{KL}(p \Vert m) + D_{KL}(q \Vert m) \right) \text{where } m = \frac{p+q}{2}$$

- $\sqrt{JSD(p \Vert q)}$ is a metric
- $JSD(p \Vert q) = JSD(q \Vert p)$ (JSD is symmetric)
- $0 \leq JSD(p \Vert q) \leq 1$ (base 2 logarithm일 때)

## f-divergence

$$D_f(p \Vert q) = \int_{-\infty}^\infty q(x)f\left( \frac{p(x)}{q(x)} \right) = \mathbb{E}_{X \sim q} \left[ f\left(\frac{p(X)}{q(X)}\right) \right]$$

- $f(t) = t\log(t)$면 KL-divergence가 됨
- $f(t) = \frac{1}{2}\left[ (t+1)\log\left( \frac{2}{t+1} \right) + t\log(t) \right]$면 JSD가 됨
- $D_f(p \Vert q) \geq 0$
