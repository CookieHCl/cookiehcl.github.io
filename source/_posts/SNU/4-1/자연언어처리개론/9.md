---
title: 자연언어처리 Lec 9
categories:
  - SNU
  - 4-1
  - 자연언어처리개론
date: 2023-04-11 15:27:49
tags:
---

# cos similarity

Document & Query를 모두 vector로 나타낼 수 있음  
Document과 Query vector 사이의 각도로 similiarity를 나타낼 수 있음!

$$\cos(\vec{q}, \vec{d}) = \frac{\vec{q}\cdot\vec{d}}{\lvert\vec{q}\rvert\lvert\vec{d}\rvert} = \frac{\vec{q}}{\lvert\vec{q}\rvert}\cdot\frac{\vec{d}}{\lvert\vec{d}\rvert}$$

그래서 vector들을 미리 normalize해준 다음에 내적해서 cos값을 구함

# tf-idf weighting variants

Term frequency

- natural $tf_{t,d}$
- logarithm $1 + \log(tf_{t,d})$
- augmented $0.5 + 0.5 \frac{tf_{t,d}}{\max_{t'} tf_{t',d}}$
- boolean $1$ if $tf_{t,d} > 0$, $0$ otherwise

Document frequency

- idf $\log \frac{N}{df_t}$
- prob-idf $\max(0, \log \frac{N-df_t}{df_t})$

1. 위에 나온 다양한 방법으로 query와 document의 tf-idf vector를 구함
1. query와 document의 cosine similarity를 구함
1. cosine similarity가 높은 순서대로 K개 document를 고름!

# Generative vs. Discriminative models

Generative models
: $P(x,y)$를 구하려고 함

Discriminative models
: $P(y|x)$를 구하려고 함

Conditional model
: $P(y|x)$를 직접 구함

Joint model
: $P(x,y)$를 구하고 joint likelihood를 최대화함 $\argmax_{c \in C} P(d \vert c)P(c)$

## Features

Features *f* are elementary pieces of evidence that link aspects of what we observe *d* with a category *c* that we want to predict

Model이 feature에 실수로 된 weight를 매김; positive일수록 맞는 feature

NLP에선 feature는 indicator function (boolean matching function)

$$f_i = \left[ c = c_j \land \Phi(d) \right]$$

안의 조건이 true면 1, false면 0

$\Phi(d)$ 예시

- 'c'로 끝남
- 이전 단어가 'in'임
- 대문자로 이루어져 있음
- 등등... 손으로 만든 조건들

## Feature-Based Models

Text Categoration
: 문장에 label 붙이고 문장 단어들 등등을 feature로

Word-Sense Disambiguation
: 단어에 label 붙이고 주변 단어들 등등을 feature로

POS Tagging
: 품사를 label로 붙이고 주변 단어들, 주변 품사들 등등을 feature로

### Feature-Based Linear Classifiers

$$\argmax_{c \in C}\sum \lambda_i f_i(c,d)$$

$d$는 data, $\lambda_i$는 각 feature의 weight

### Multinomial Linear Classifiers

$$P(c \vert d, \lambda) = \frac{\exp \sum_i \lambda_i f_i(c,d)}{\sum_{c'} \exp \sum_i \lambda_i f_i(c',d)}$$

$P(c \vert d, \lambda)$가 제일 큰 $c$를 찾으면 됨

$\exp$ 부분은 softmax

# Part-of-speech tagging

![Part-of-speech tagging](pos_tagging.png)

단어마다 품사(POS, part-of-speech)를 매겨줌

ex) back의 경우 JJ(Adjective), NN(Noun), RB(Adverb), VB(Berb) 등 여러 품사가 될 수 있음. 근데 품사마다 back이 의미하는 뜻이 달라짐 -> 이걸 표기해서 나타내자!

컴공에서 만든 분류는 아니고 언어학자들이 분류해놓은 대로 사용함 (그래서 쓸데없이 수십가지로 분류됨)

Text-to-speech 등 품사가 필요한 경우에 사용됨

현재 제일 잘하는 모델은 97% 정확도 but baseline(제일 멍청한 방법)도 90% 정확도를 보여줌!

- 대부분 단어들 품사는 명확함 (약 11% 정도만 모호함)
  - Prefix: un- 있으면 JJ(Adjective)
  - Suffix: -ly 있으면 RB(Adverb)
  - Capitalization: 대문자로 시작하면 NNP(Proper noun)
  - 그냥 단어 품사가 1개밖에 없는 경우도 있음
- the, a 등 날먹단어들이 많아서 정확도 뻥튀기됨
