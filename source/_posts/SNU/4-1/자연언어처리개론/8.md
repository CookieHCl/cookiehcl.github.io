---
title: 자연언어처리 Lec 8
categories:
  - SNU
  - 4-1
  - 자연언어처리개론
date: 2023-04-06 15:32:52
tags:
---

# ELMo

Forward LSTM이랑 Backward LSTM이 있음

John [] a paper를 훈련 중이라면 John은 Forward LSTM, a paper는 Backward LSTM으로 들어가서 [] 단어 맞춤

# BERT

Transformer라 방향이 없음; input 전부 넣음

훈련시킨건 총 2가지

1. 무작위로 masking 후 가린 단어 맞추도록 훈련
1. 문장 두 개가 이어져 있을지 아닐지 훈련 (사실 classification problem; IsNext인지 NotNext인지 구분)

이렇게 pre-train된 BERT를 가져와서 나한테 맞게 fine-tuning하면 됨

# Document vector

각 document를 vector of words로 나타냄

ex) battle, good, fool, wit, 총 4가지 분류로 단어들을 나눈 다음 개수를 셈

단어 대 단어 기준으로 세는 word-word matrix (term-context matrix)도 있음 (word context vector가 비슷하면 비슷한 의미인 단어)

# Web search

옛날엔 query-doc은 boolean matching; doc이 쿼리에 맞거나 안 맞거나 두 가지 경우밖에 없었음

이러니까 query가 좁으면 결과가 아예 안 나오고 query가 넓으면 너무 많이 나옴

## Ranked retrieval models

Document들한테 query와 맞는 정도 ranking을 매기고 상위 K개 결과만 보여줌

장점: 항상 K개의 결과가 보장됨  
단점: ranking을 잘 매겨야함

query마다 document한테 [0, 1] score를 매김

### Bag of words model

단어 위치를 고려하지 않고 단어가 몇 개 있는지만 봄

### Term frequency(Collection frequency) tf~t,d~

document d에 term t가 몇 번 나오는지 셈

tf~t,d~를 그대로 쓰기보단 log frequency를 씀  

$$
w_{t,d} = \begin{cases}
  1 + \log_{10}(tf_{t,d}) & tf_{t,d} > 0 \\
  0 & tf_{t,d} = 0
\end{cases}
$$

query score는 $\sum_{t \in q}w_{t,d}$

### Document frequency

term frequency는 좋지만 the, a 같은 흔한 단어들이 너무 frequency가 높음

query에 arachnocentric처럼 희귀한 단어가 있으면 더 중요할 확률이 높음!

document frequency df~t~
: term t가 나타난 document의 수

inverse document frequency idf~t~
: $idf_t = \log_{10}\frac{N}{df_t}$ where N is total number of documents

idf가 높을수록 그 term은 잘 안 나온다는 뜻 -> 더 가중치를 줘야 함!

### tf-idf weighting

$$tf.idf_{t,d} = w_{t,d} idf_t = (1+\log_{10}tf_{t,d}) \times \log_{10}\frac{N}{df_t}$$

$$Score(q,d) = \sum_{t \in q}tf.idf_{t,d}$$

tf랑 idf 곱해서 weight로 씀

이제 weight을 미리 계산해두면 document를 vector로 만들 수 있음!  
각 term마다 tf-idf weight를 계산하면 |V|차원 document vector가 됨

|V| 차원은 너무 커서 대부분 sparse vector임

document vector는 길이가 달라서 단순 distance로 계산하지 않음  
대신 각도를 통해서 similarity를 계산함
