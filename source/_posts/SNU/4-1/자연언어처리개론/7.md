---
title: 자연언어처리 Lec 7
categories:
  - SNU
  - 4-1
  - 자연언어처리개론
date: 2023-03-30 15:28:48
tags:
---

# Word meanings

Lemma
: 단어 자체 (pepper)

Sense
: 단어의 의미 (사전 1, 2, 3 ... 번 뜻 같은거)

Definition
: 단어 뜻 (pepper의 Sense 1번 뜻은 머시기머시기)

## Sense간의 관계

Synonymy
: 같은 의미를 가짐 (big, large)

Similarity
: 비슷한 의미를 가짐 (car, bicycle)
사람들이 두 단어가 얼마나 비슷한지 점수를 매긴 SimLex-999 dataset이 있음

Word relatedness/Word association
: 두 단어가 연관되어있음 (car, gasoline)

Antonymy
: 서로 반대되는 의미를 가짐 (dark, light)

Subordinate
: 더 범위가 작은 단어 (car is subordinate of vehicle)

Superordinate
: 더 범위가 큰 단어 (car is superordinate of sedan)

[WordNet](http://wordnetweb.princeton.edu/perl/webwn)에 가면 위에서 말한 단어 사이의 관계들을 보여줌

# Word Embeddings

단어들을 usage에 따라 분류할 수 없을까? 여기서 usage는 word's environment (주변 단어들)

주변 단어들이 같다면, synonym이라고 말할 수 있다! (Zellig Harris, 1954)

## word as a vector

단어들을 vector로 나타냄, 비슷한 단어들은 진짜로 vector space 근처에 있음

vector space에 embedded되어 있어서 **embedding**이라고 부름

사실상 현대 NLP는 전부 word embedding을 사용함

단어 자체를 사용하던 기존 연구와 달리 word embedding을 사용하면 비슷한 단어를 숫자를 사용해서 고를 수 있음 -> training set에 없는 단어들도 사용가능!!

아쉽게도 의미가 여러 개인 단어들은 고려하지 않음;

word embedding은 일반적으로 dense, 길이는 짧고 대부분 원소들이 0이 아님

- 길이가 짧으면 머신러닝에서 사용해야 할 weight 수가 줄어듦
- dense vector가 sparse vector보다 generalize를 더 잘 함
- synonymy를 더 잘 찾음
- 그냥 해보니까 잘 되더라

의미가 여러 개인 단어는 어떻게? 아쉽게도 그건 고려 안 함

## Word2vec

[오픈소스](https://code.google.com/archive/p/word2vec/)고 훈련 속도가 빨라서 사람들이 자주 씀

단어를 count하는게 아니라 predict한다!

기존 방식: 단어 w가 이 단어 주변에서 몇 번 나옴?  
Word2Vec: 단어 w가 이 단어 주변에서 나올 것 같음?

또한, Word2vec은 self-supervision이 가능하다! 사람이 일일히 레이블링 안 해도 됨

### Skip-gram with negative sampling(SGNS)

1. target word t와 neighboring context word c를 positive example로 정함
2. lexicon에서 단어들 랜덤으로 뽑은 다음 negative example로 정함
3. logistic regression으로 positive example과 negative example을 구별하는 classifier를 훈련시킴
4. 훈련된 weight를 embedding으로 사용

이때 positive example 정하는 방식이 skip-gram임

### Skip-gram

target word 앞뒤로 2개의 단어를 본다고 하자. (이걸 window라고 부름)

주어진 문장이 a tablespoon *of* apricot jam이라면,  
(of, a)  
(of, tablespoon)  
(of, apricot)  
(of, jam)  
을 훈련 데이터로 정하고 나올 확률을 지정해줌

#### Window가 작은 경우 (±2)

같은 자리에 들어갈 수 있는 단어들이 나옴 (words in same taxonomy)

ex) 호그와트의 경우 다른 가상의 학교 이름들이 나옴 (Sunnydale, Evernight 등)

#### Window가 큰 경우 (±5)

단어랑 관련있는 단어들이 나옴 (words in same semantic field)

ex) 호그와트의 경우 해리포터랑 관련된 단어들이 나옴 (Dumbledore, Half-blood 등)

### Word2vec으로 할 수 있는 것들

Analogical relations
: 한국-서울, 일본-도쿄 같은 관계를 벡터 연산으로 계산할 수 있음 (서울-한국+일본 = 도쿄)  
단점: 생각보다 작동을 잘 안 함; 자주 사용되고 특정 관계에 대해서만 잘 계산함

Historical semantics
: 시대별로 단어들을 다르게 표기한 다음 word2vec를 돌리면 시대에 따라 달라지는 의미를 볼 수 있음  
(ex: gay는 즐겁다 -> 게이게이야, broadcast는 널리 알리다 -> 방송)

Cultural bias?
: 프로그래머-남자+여자 = 주부? 과거 설문조사에 나타났던 bias랑 word2vec이 보여주는 bias랑 일치한다고 함

# ELMo: context2vec

단어만 보고 어떻게 사용될지 예측할 수가 없음 (동음이의어 문제 등) -> word 말고 context 2 vector!

Forward LSTM과 Backward LSTM을 사용해서 context를 vector로 만듦, target 단어는 word embedding -> context embedding과 word embedding을 합쳐서 계산
