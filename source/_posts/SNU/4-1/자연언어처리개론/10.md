---
title: 자연언어처리 Lec 10
categories:
  - SNU
  - 4-1
  - 자연언어처리개론
date: 2023-04-13 15:27:41
tags:
---

# Information extraction (IE)

텍스트에서 중요한 정보들을 추출

사람이 잘 알아볼 수 있도록 & 컴퓨터 알고리즘 돌리기 쉽도록

ex) Gmail에서 자동으로 캘린더 항목 만들어줌  
ex) 구글이 웬만한 질문은 답변을 줌

## Named Entity Recognition (NER)

Named entity를 찾아내고, relation을 구함

ex) Andrew, Billiton, London, 2010

POS 처럼 단어 tagging 필요

- O: tag 없음(Other)
- ORG: Organization
- PER: Person
- 등등...

IOB encoding에선 시작하는 단어는 B, 그 외에는 I로 시작함

ex) Sue Mengqiu Huang은 B-PER, I-PER, I-PER

### Precision/Recall/F1 in IE/NER

First Bank of Chicago에서 모델이 Bank of Chicago만 찾아냈다면?  
false positive여 false negative여?

사실 이런 boundary error가 대부분임 -> 맞은 만큼 부분점수 주는 MUC scorer 같은것도 있음

### Features for sequence labeling

- words: 단어 뜻으로부터
- inferred linguistic classification (ex: POS 보고 noun인지 확인)
- label context: 앞 뒤 label로부터
- word shapes: 길이, 대소문자 여부, 특수문자 여부 같은걸로부터

## Hidden Markov Models

$$\begin{align*}
  P(t_1 \ldots t_n \vert w_1 \ldots w_n) =& \frac{P(w_1 \ldots w_n \vert t_1 \ldots t_n)P(t_1 \ldots t_n)}{P(w_1 \ldots w_n)}\\
  \propto& P(w_1 \ldots w_n \vert t_1 \ldots t_n)P(t_1 \ldots t_n)\\
  \approx& \prod_{i=1}^n P(w_i \vert t_i)P(t_i \vert t_{i-1})\\
\end{align*}$$

$P(w_i \vert t_i)$는 output probability, $P(t_i \vert t_{i-1})$는 transition probability

HMM: {S, K, Π, A, B}

S: hidden state values  
K: observations  
Π: initial state probabilities  
A: transition probabilities  
B: observation probabilities

### Greedy Inference

제일 좋은 label을 계속해서 고름

장점

- 빠름, 메모리 필요 X
- 구현 쉬움

단점

- 한 번 잘못 들어가면 못 빠져나옴

### Beam Inference

제일 좋은 k개의 sequence들로부터 다음 sequence들을 만듦; 그 중 제일 좋은 k개의 sequence를 남기고 반복 (보통 k는 3~5)

장점

- 빠름
- 구현 쉬움

단점

- 이래도 optimal 찾는다는 보장은 없음
- 메모리 필요

### Viterbi Inference

Dynamic programming을 이용해서 optimal sequence 찾음

장점

- 확실하게 optimal을 찾음

단점

- 구현 어려움
- 긴 sequence에선 하기 힘듦

# Conditioned Language Models

Input X, Output as Y

ex) English -> Japanese (Translation)  
ex) Image -> Text (Image captioning)  
ex) Document -> Short Description (Summarization)  
ex) Speech -> Transcript (Speech Recognition)

$$P(Y \vert X) = \prod_{j=1}^J P(y_j \vert X, y_1, \ldots, y_{j-1})$$

기존 $P( X) = \prod_{i=1}^I P(x_i \vert x_1, \ldots, x_{i-1})$과 달리 context X가 들어감

## Generation

Sampling
: P(Y|X)에서 random하게 sampling

Argmax
: 가장 높은 확률을 가지는 문장을 찾음; Greedy search와 Beam search가 있음

## Attention

문장의 각 단어들을 vector로 나타냄

query vector (decoder state) & key vector (encoder state) pair마다 weight을 구하고 softmax; 이후 value vector (encoder state)와 weighted sum
