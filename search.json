[{"title":"굳이굳이 Hexo로 블로그 시작한 이유","url":"/posts/1/","content":"동기\nGithub Pages랑 Markdown을 이용해서 블로그를 만들고 싶어서 시작했다.\n처음에는 직접 html이랑 javascript를 배워서 만들어볼까 했지만, 맨땅에서 시작하기엔 웹프로그래밍에 대해 아는게 없었고, 블로그의 주 목적은 게시글을 작성하는 건데, html 단계부터 만들기 시작하면 내가 게시글을 만들 때마다 html을 새로 만들거나, 티스토리에 있는 것마냥 게시글 편집기를 직접 만들어야 할 것 같아서 포기했다.\n결국 블로그의 주 목적은 정확히는 블로그가 망하지 않으려면 게시글을 작성하는 것이고, 이를 위해서는 게시글을 쉽고 빠르게 작성할 수 있어야 한다고 생각해서 Markdown 블로그를 만들었다.\n물론 티스토리 블로그 같은 서비스를 이용해도 되지만, 혹시나 먼 미래에 갑자기 'html부터 새로 만들어보는 나만의 블로그' 같은 짓을 할 수도 있을 것 같아서 게시글들을 쉽게 이전할 수 있도록 Markdown으로 게시글을 작성했다.\n그래서 왜 Hexo?\n사실 가장 잘 알려진 방법은 Jekyll을 사용하는 것이다. 일단 Github Pages에서 공식적으로 권장하기도 하고, Markdown을 사용한 static site generator중에서는 가장 유명할 것이다.\n하지만 굳이굳이 Hexo를 사용해서 블로그를 시작한 이유는 게시글 주소 때문인데, Jekyll은 주소로 게시글 제목, 날짜 등을 사용하지만 (ex: blog.com/2023/11/30/blog_sijak) Hexo는 티스토리처럼 주소로 숫자를 사용하는 플러그인이 있었다. (ex: blog.com/123)\n막상 시작해보니 Hexo를 사용하게 된 계기인 주소 플러그인이 잘 작동하지 않는 문제점이 있긴 했지만, Hexo에서 지원하는 기능들이 상당히 많아서 Jekyll로 시작했으면 더 불편했을 것 같다.\n장점\n\n대부분 Hexo 유저가 사용하는 Next Theme이 정말 이쁘다!\nruby를 사용하는 Jekyll과 달리, node.js를 사용해서 npm으로 쉽게 설치가 가능하다.\n\n솔직히 ruby는 아무도 안 쓰잖아...\n\n\n중국인이 사용하기 때문에 다국어 지원이 잘 되어있어 겸사겸사 한국인도 이득을 봤다.\nJekyll은 진짜 글만 모아둔 느낌인데 Hexo는 카테고리, 댓글, 글 검색 기능 등 일반적인 블로그 서비스들이 제공하는 기능들도 지원하려고 노력한다.\nsyntax highlighting, LaTeX 지원이 기본으로 되어있다.\n이외에도 유튜브 동영상, 그래프 등 Markdown으로 표현할 수 없는 부분을 지원하기 위한 플러그인들이 많다.\n광고, 블로그 홍보에 미쳐있어서 블로그 홍보하는 documentation까지 있다.\n\n단점\n\n수상할 정도로 중국인들만 사용해서 그냥 귀찮게 영어 쓰는 대신 중국어로 issue, plugin 등을 남기는 경우가 많다. 심하면 그냥 README를 중국어로 적는 경우도 있다.\nHexo도 광고에 미쳐있다. 무려 공식 documentation에 광고가 뜬다....\nHexo의 기능이 뛰어나기 보단 Next Theme에서 지원하는 기능이 많아 테마가 제한되는 느낌이 있다.\n\nHexo plugin들도 Next Theme을 사용하는 가정 하에 만드는 경우도 있다.\n위 장점들 중에서도 Next Theme에만 해당되는 장점들도 있다.\n아마 다른 테마도 맞는 plugin들만 깔아주면 Next Theme과 같은 기능을 할 것 같긴 한데 다른 테마는 안 써봐서 모르겠다.\n\n\n유저 수 적은 서비스가 늘 그렇듯이 API 등 심화된 부분으로 가면 documentation이 잘 안 되어있다. 덕분에 plugin 만들 때 좀 고생했다.\n\n결론\n그래서 Hexo를 써야 하냐고 묻는다면 쓰는게 좋은 것 같다. 본인처럼 주는 것에 만족하지 못 하고 직접 건드리려고 하면 조금 애먹을 수 있지만 Next Theme이 워낙 잘 되어있어서 건드릴 일이 거의 없다.\nJekyll은 정말 기본적인 기능만 지원해서 카테고리, 검색 기능 같은 것을 지원하려면 어차피 plugin들을 찾아보거나, 직접 코딩하는 등 힘들게 살아야하는데 Hexo는 수많은 중국인들이 이미 귀찮은 작업들을 다 끝내줘서 찾아볼 필요가 없다. 감사합니다 따-거\n","categories":["BLOG"]},{"title":"Hexo로 블로그 시작하기","url":"/posts/2/","content":"설치\n먼저 node.js와 git을 깔아준다.\n그 후 hexo를 설치한 다음 블로그 폴더를 만들어준다.\nnpm install -g hexo-clihexo init &lt;folder&gt;cd &lt;folder&gt;npm install\n실행\nhexo new --path &lt;path&gt; &lt;title&gt;로 원하는 경로에 게시글을 만들 수 있다. 물론 Front matter만 알맞게 작성한다면 직접 Markdown 파일을 만들어도 상관없다.\n게시글을 다 작성했다면 hexo generate로 블로그를 생성하고, hexo server로 블로그를 로컬 서버로 구동시킨다.\nhexo generate -w를 사용하면 파일이 편집될 때마다 블로그를 계속 생성해준다.\n문제는 hexo server가 hot/live reloading을 지원하지 않아 블로그가 새로 생성돼도 브라우저를 새로고침 해야 반영이 된다.\n그래서 어쩔 수 없이 Browsersync를 깔아야 했는데, npx browser-sync ./public -w를 사용하면 파일이 편집될 때마다 새로 생성된 블로그를 볼 수 있다.\n일부 변경사항은 hexo generate를 아무리 해도 적용이 안 되는데, (_config.yml 변경 등) 이때는 hexo clean으로 캐시를 지워줘야 한다.\n배포\nGitHub Pages니까 GitHub Actions을 사용하기로 했다.\n먼저 repository 설정에서 GitHub Pages를 활성화시켜야 한다.\n\nCode and automation - Pages에서 Source를 Deploy from branch로 해준다.\nBranch는 gh-pages로 선택한다.\n\n이후 아래 파일을 .github/workflows/pages.yml으로 저장한다.\nname: Deploy Github Pageson:  push:    branches:      - mainjobs:  pages:    runs-on: ubuntu-latest    permissions:      contents: write    steps:    - name: Checkout code      uses: actions/checkout@v4    - name: Setup LTS Node.js      uses: actions/setup-node@v4      with:        node-version: &#x27;lts/*&#x27;    - name: Cache NPM dependencies      uses: actions/cache@v3      with:        path: node_modules        key: npm-cache-$&#123;&#123; runner.os &#125;&#125;-$&#123;&#123; hashFiles(&#x27;**/package-lock.json&#x27;) &#125;&#125;        restore-keys: |          npm-cache-$&#123;&#123; runner.os &#125;&#125;          npm-cache    - name: Build      run: |        npm install        npx hexo generate    - name: Deploy      uses: peaceiris/actions-gh-pages@v3      with:        github_token: $&#123;&#123; secrets.GITHUB_TOKEN &#125;&#125;        publish_dir: ./public\nactions/checkout, actions/setup-node는 말 그대로의 일을 하고, actions/cache는 workflow가 빠르게 돌 수 있도록 node_modules 폴더를 캐싱해둔다.\n그 후 npx hexo generate로 블로그를 생성한 다음 peaceiris/actions-gh-pages로 ./public 폴더를 GitHub Pages branch에 올리면 된다.\nNexT Theme 설치\nNexT Theme 설치는 npm install hexo-theme-next로 간단하게 할 수 있다. 이후 _config.yml에서 theme: next로 설정해주면 된다.\n그 후 NexT Theme 설정 파일을 따로 만들어줘야 하는데, node_modules/hexo-theme-next/_config.yml을 _config.next.yml로 복사해주고 _config.next.yml만 수정하면 된다.\nWhat's Next...?\n이 이후로는 자유롭게 hexo documentation과 NexT documentation을 참고하면서 원하는 대로 수정하면 된다.\n다른 plugin들도 살펴보면서 필요한 것이 있다면 다운받아도 된다. 몇 가지 추천 플러그인들은 다음과 같다.\n\nhexo-auto-category: post의 경로대로 category를 만들어준다.\nhexo-excerpt: 블로그 메인화면에서 post가 너무 길 경우 일부만 보여준다.\nhexo-generator-searchdb: 검색기능을 만들어준다! 자세한 설명은 NexT documentation을 참고하자.\n\n","categories":["BLOG"]},{"title":"Hexo permalink 숫자로 나오게 하기","url":"/posts/3/","content":"문제점\n분명 Hexo에는 이걸 지원하는 plugin이 있고, 이것 때문에 Hexo로 블로그 만들기를 시작했는데, 정작 이 plugin이 작동하질 않았다...\n아직 Hexo 작동원리를 정확히 아는 것은 아니라 모르겠지만, 대충 이유를 추측하자면 저 plugin은 post_permalink filter에서 주소를 계산하는데, 게시글 주소가 여러 곳에서 쓰이다 보니 (메인화면, 이전/다음 글, 카테고리 등...) post_permalink filter가 여러 번 불린다.\n근데 plugin 제작자가 이럴거라고 생각을 못 했는지, 아니면 post_permalink가 불릴때마다 순서가 뒤죽박죽이라던지, 여러가지 이유로 게시글 주소를 계산할 때마다 permalink를 다르게 계산하고, 결국 링크가 제대로 작동하지 않는 문제가 생겼다.\n해결책\n결국 plugin을 직접 만들기로 했다... 아래 파일을 scripts/abbrlink.js로 저장한다.\nlet abbrlinks = &#123;&#125;;hexo.extend.filter.register(&#x27;before_generate&#x27;, () =&gt; &#123;    const posts = hexo.locals.get(&#x27;posts&#x27;).data;    let undefined_posts = [];    let max_abbrlink = 0;    // get max abbrlink    for (const post of posts) &#123;        if (post.abbrlink) &#123;            abbrlinks[post._id] = post.abbrlink;            max_abbrlink = Math.max(max_abbrlink, post.abbrlink);        &#125;        else &#123;            undefined_posts.push(post);        &#125;    &#125;    // generate abbrlink for undefined posts    undefined_posts.sort((a, b) =&gt; a.date - b.date);    for (const post of undefined_posts) &#123;        abbrlinks[post._id] = ++max_abbrlink;    &#125;&#125;);hexo.extend.filter.register(&#x27;post_permalink&#x27;, (data) =&gt; &#123;    // if abbrlink is already set, do nothing    if (data.abbrlink) &#123;        return data;    &#125;    // set abbrlink if abbrlink is generated    const abbrlink = abbrlinks[data._id];    if (abbrlink) &#123;        data.abbrlink = abbrlink;    &#125;    return data;&#125;, 1);\n항상 게시글 주소가 똑같이 유지되도록 before_generate filter에서 미리 주소를 전부 계산한 다음, post_permalink filter에서는 그냥 계산한 주소를 붙이기만 하기로 했다.\n계산한 숫자 주소는 원본 plugin에서 쓰던 대로 abbrlink라고 부르기로 했다.\n웃기게도 가장 애먹은 부분은 모든 게시글을 불러오려면 const posts = hexo.locals.get('posts').data;를 써야 된다는 것을 알아내는 일이였다... 제발 documentation 똑바로 좀 만들어라...\n이후는 abbrlink가 없는 post들을 찾은 다음, 날짜 순으로 정렬해 쓴 순더대로 abbrlink를 붙여주는게 끝이다.\n","categories":["BLOG"]},{"title":"Hexo tag plugin 만들기","url":"/posts/10/","content":"Tag Plugin이란?\nHexo에는 Tag Plugin이라는 개념이 있는데, Markdown으로 작성하지 못 하는 내용들을 작성할 때 사용한다. 쉽게 말하면 임의의 html 태그를 삽입할 수 있는 기능이다. Hexo에도 다양한 Tag Plugin들이 있지만, Next Theme에서도 버튼, 그래프 등 다양한 Tag Plugin들을 제공한다.\nTag Plugin 만들기\n다행히도 Hexo에서 Tag Plugin 만드는 방법을 지원한다. sciprts/ 폴더에 아래 두 코드 중 하나가 있는 파일을 저장하면 된다.\nhexo.extend.tag.register(&#x27;name&#x27;, function(args)&#123;  return `&lt;div class=&quot;hello&quot;&gt;hi&lt;/div&gt;`;&#125;);\n이렇게 하면 게시글에 &#123;% name args %&#125;라고 쓰면 &lt;div class=&quot;hello&quot;&gt;hi&lt;/div&gt;로 렌더링된다.\nhexo.extend.tag.register(&#x27;name&#x27;, function(args, content)&#123;  return `&lt;div class=&quot;hello&quot;&gt;$&#123;content&#125;&lt;/div&gt;`;&#125;, &#123;ends: true&#125;);\n이렇게 하면 게시글에\n&#123;% name args %&#125;content&#123;% endname %&#125;\n라고 쓰면 &lt;div class=&quot;hello&quot;&gt;content&lt;/div&gt;로 렌더링된다.\nCSS 입히기\n불행히도 이렇게 하면 원하는 html 태그를 넣을 수는 있지만 CSS가 없기 때문에 원하는 대로 꾸미지는 못 한다. 다행이도 Next Theme에서 CSS를 삽입하는 방법을 지원한다.\n먼저 Next Config File인 _config.next.yml에 아래 설정을 추가한다.\ncustom_file_path:  style: source/_data/styles.styl\n이후 source/_data/styles.styl에 Stylus 문법으로 원하는대로 스타일을 지정해주면 된다.\n","categories":["BLOG"]},{"title":"<경제 읽어주는 남자의 15분 경제 특강>","url":"/posts/8/","content":"\n    \n    경제 읽어주는 남자의 15분 경제 특강김광석\n    \n  \n경제의 기본 개념\n경제의 3대 주체: 정부, 가계, 기업\n\n정부: 공공재 제공\n가계: 노동, 세금 재공\n기업: 민간재, 세금 제공\n\n\nGDP = C + I + G + netEx\n\n\nGDP: Gross domestic product, 총 생산\nC: Consumption, 소비\nI: Investment, 투자\nG: Government expenditure, 정부지출\nnetEx: net Export, 순수출 (수출 - 수입)\n\n경제의 선순환 구조: 투자 -&gt; 고용 -&gt; 소득 -&gt; 소비 -&gt; 투자\n1. 금리\n금리 = 돈의 가치, 물가 = 물건의 가치\n따라서 금리와 물가는 반비례함\n물가 상승 -&gt; 금리 인상 -&gt; 소비, 투자 위축 -&gt; 물가 하락\n경기 침체 -&gt; 금리 인하 -&gt; 소비, 투자 확대 -&gt; 경기 부양\n보통 물가상승률 2%를 기준으로 잡음!\n2. 물가\n\n\n인플레이션(Inflation): 물가 상승\n\n\n디스인플레이션(Disinflation): 물가 상승세가 없음\n\n\n디플레이션(Deflation): 물가 하락\n\n\n디프레션(Depression): 경기 침체\n\n\n스태그플레이션(Stagflation): 인플레이션 + 디프레션 (물가는 높은데 경제성장률이 저성장)\n\n\n소비자물가와 체감물가와 다른 이유\n\n소비자물가지수\n실생활에서 소비하는 총 460여 개의 품목 및 서비스를 대상으로 평균 가격을 조사한 후 지난해 대비 등락을 측정한 지수\n\n\n내가 사는 품목들이 460여 개의 일부거나, 포함되어 있지 않음\n소비자물가지수는 전년 동월 대비 등락을 측정해서, 어제나 지난달 등의 짧은 기간의 등락을 고려하지 않음 (Ex: 설 연휴철을 앞두고 폭등)\n\n(소비자)물가는 중앙은행에서, 체감물가는 기획재정부에서 고려함\n3. 무역\n무역: 국가간의 분업 체계\n각자 더 잘하는 것을 생산하고 서로 교환하면 전체의 효용이 증가함!\nFTA: Free Trade Agreement, 관세 철폐\n하지만 상대국보다 불리한 자국의 산업이 사라질 수도 있으므로 단서 조항 등 특약을 체결하여 일부 산업은 FTA를 적용하지 않을 수 있음\n생각할거리\n\n디지털 위안화\n디지털 화폐(CBDC, Central Bank Digital Currency)의 중국 버전, 글로벌 통화로 만들기 위해 노력 중\n디지털세(DST, Digital Service Tax)\nGAFA세(Google, Amazon, Facebook, Apple)로도 불림, 무형자산을 바탕으로 이익을 버는 디지털 기업들에 부과하는 세금이 필요하다는 논의\nIPEF(Indo-Pacific Economic Framework)\n미국이 중국 배제하고 만든 경제 협력체\n\n4. 환율\n환율: 다른 나라 화폐와 우리나라 화폐의 교환 가치\n\n한 나라 경제의 건전성을 보여줌\n특히, 달러는 안전자산으로 달러의 가치는 불확실성과 변동성으로 판단할 수 있음 (Ex: 코로나, 전쟁 때 달러 가치 급상승)\n달러 가치가 강해지면 신흥국의 수익을 달러로 바꿨을 때 얼마 되지 않아 신흥국 투자가 줄어듦\n수출 기업은 달러 가치가 강해지면 이득, 수입 기업은 달러 가치가 약해지면 이득\n\n5. 주식\n주가에 영향을 미치는 경제 지표\n\n경제성장률\n산업이 성장중이므로 주식 투자를 적극적으로 해야 함. 특히 유망 산업인 신성장 동력 산업(더 많이 성장하는 사업)에 투자해야 함.\n경기선행지수\n주식 투자는 선행 지표이므로, 경기보다 먼저 작용할 것 같은 지표인 경기선행지수를 봐야 함.\n실업률\n고용은 시간이 걸리므로 고용 지표는 가장 경기 후행적인 지표임. 따라서 가장 믿고 판단할 수 있는 지표고, 앞으로 경제가 탄탄하게 성장할지 판단하는데 도움을 줌.\n금리와 물가\n일반적으로 금리는 주가와 역행함. 그리고 금리는 보통 물가를 보고 결정함.\n통화량\n통화량은 유동성을 얼마만큼 공급하느냐는 지표이므로, 통화량이 높을수록 주가가 상승함.\n\n좋은 기업을 고르는 방법\n\n전자공시 시스템 다트 활용\n기업의 경영진, 학력, 경력, 앞으로의 비즈니스 목적, 분기별 사업 보고서 등을 전부 볼 수 있음.\n재무재표 활용\n기업의 현재 매출, 영업이익 등을 확인할 수 있음. 수익성, 안전성, 성장성, 활동성, 채권 의존도, 자기 자본 비중 등을 읽을 수 있어야 함.\n\n기업의 지표들\n\nEPS(Earnings Per Share, 당기순이익)\n주당 순이익을 의미함, 발행 주식 수 대비 얼마만큼의 순이익을 내고 있는지 보여주는 지표\nROE(Return On Equity, 자기자본이익률)\nROE = 당기순이익(EPS)/평균 자기자본\n\n기업이 자기 자본을 가지고 얼만큼 돈을 벌고 있는지, 즉 수익성을 보여줌.\n부채에 관한 정보는 없으므로 ROE 값이 높아도 부채 위험이 있는지 확인해야 함.\n\nPER(Price Earning Ratio, 주가수익비율)\nPER = 주가/당기순이익(EPS)×100\n\nPER 값이 높을수록 회사가 버는 이익금에 비해 주가가 고평가되어 있다고 판단할 수 있음.\n\nPBR(Price Book-value Ratio, 주당순자산비율)\nPBR = 주가/주당 순자산가치×100\n\nPBR 값이 높을수록 회사의 순자산(청산 가치)에 비해 주가가 고평가되어 있다고 판단할 수 있음.\nPER은 순수익, PBR은 순자산 대비 주가의 고평가 여부를 판단한다고 보면 됨.\n생각할거리\n\n테이퍼링\n통화량을 서서히 공급하거나 서서히 줄여나가는 것. 한번에 늘리거나 줄이면 시장에 긴축 발작이 일어나므로 테이퍼링을 하는데, 이걸 보고 앞으로의 통화량을 예측해 주가를 예측할 수 있음.\n금리\n물가가 너무 높으면 금리를 인상하고 주가가 내려감. 테이퍼링 계획을 미리 발표하므로, 이를 보고 기준금리 인상 타이밍도 예측할 수 있음.\n나스닥\n미국 장이 우리나라보다 먼저 열리고, 우리나라에 큰 영향을 끼쳐서 나스닥 시장이 코스피, 코스닥 시장의 선행 지표 역할을 함.\n경기 변동\n경기가 좋다면 주가가 상승함. 특히, 여러 산업 중 더 유망한 산업은 더 상승함.\n이슈\n코로나 -&gt; 재택근무 -&gt; 가구 수요 증가 등 이슈에 따라서 저점 매수가 가능한 종목이 존재함.\n경상수지\n우리나라 기업들은 수출 의존도가 높아서 경상수지가 흑자면 주가가 상승함.\n기업 실적\n기업은 한 해 4번의 사업 보고서를 발표해야 함 -&gt; 이 사업 보고서 실적이 좋으면 주가가 상승함.\n환율\n환율이 높으면 한국주식에서 돈을 벌어도 달러로 환전하면 얼마 안 되기 때문에 외국인 투자자들의 자금 이탈이 벌어져서 주가가 떨어짐.\n\n6. 채권\n회사, 국가, 지자체 같은 기관이 은행이나 그 밖의 투자자로부터 돈을 빌릴 때 (자금을 조달할 때) 채권을 발행함.\n영구채를 제외한 일반적인 채권은 만기일과 만기일에 갚아야 할 원금+이자(액면가)가 정해져 있음.\n국가에서 발행하면 국채, 회사에서 발행하면 회사채라고 부름.\n주식과 채권의 차이\n\n\n\n구분\n주식\n채권\n\n\n\n\n자금조달 방법\n자기 자본\n타인 자본\n\n\n증권 소유자의 위치\n주주로서의 지위\n채권자로서의 지위\n\n\n소유자로부터의 권리\n결산 시 사업이익금에 따른 배당을 받을 권리\n확정이자 수령 권리\n\n\n존속기간\n영속성\n기한적(영구채 제외)\n\n\n원금상환\n원금상환이 불확실\n원금상환\n\n\n자본의 조달형태\n출자증권\n대부증권\n\n\n\n주식을 사면 실제로 회사의 주주가 되어 경영 의사결정에 참여하지만, 채권을 사면 단순히 돈을 받는 채권자가 될 뿐임.\n채권 가격과 채권금리의 관계\n채권은 갚아야 하는 금액인 액면가가 정해져 있으므로 채권가격이 상승하면 채권금리(수익률)이 줄어듦.\n따라서 금리와 채권 가격은 서로 반대로 움직임.\nex) 액면가 105만원인 채권이 100만원이였는데 (수익률 5%) 가격이 105만원으로 상승하면 수익률이 0%가 됨.\nex) 시장 금리가 10%면 액면가 105만원인 채권을 아무도 100만원에 안 사주므로 가격이 하락함.\n7. 부동산\n보통 부동산은 주택담보대출에 의존하여 구매하므로 금리가 인상되면 부동산 매매 수요가 감소함.\n특히, 전월세 시장을 통해 생각해보면 비싼 금리의 전세대출을 받는 대신 월세를 선택해 전세 가격이 내려가고 급매물이 발생함. (임차인의 월세 선호현상)\n부동산은 수요, 공급 말고도 정부의 부동산 정책에 따라서 급격하게 변하기도 하므로 정책도 눈여겨봐야 함.\n부동산 수요지표들\n수요 = 구매 의사 × 구매 여력\n\n주택가격전망 CSI\n100을 상회한다면 집값이 오를 것으로 믿으므로, 구매하려는 의사가 올라감. 반대로 100을 밑돌 때는 집값이 내릴 것으로 믿으므로, 구매하려는 의사가 줄어듦.\nPIR(Price to Income Ratio, 소득대비주택가격비율)\n연소득을 모두 모아 주택을 구입하는 데 걸리는 기간으로, 주택가격을 가구 소득으로 나눈 수치. PIR가 높다면 주택 구매 여력이 낮다는 뜻임.\nHAI(House Affordability Index, 주택 구매력지수)\n연소득 대비 주택담보대출의 원리금을 상환할 수 있는 능력을 나타내는 지표로, HAI가 낮다면 주택 구매 여력이 낮다는 뜻임.\n\n부동산 공급지표들\n\n전세수급지수\n전세 수요를 확인할 수 있는 지표로, 전세 수요가 떨어지면 갭투자자들이 급매물을 내놓아 공급이 늘어남.\n미분양주택 건수\n신규 주택의 물량을 확인할 수 있는 지표로, 미분양주택이 많으면 집값이 하락함.\n주택건설 인허가실적\n인허가 -&gt; 착공 -&gt; 준공에 이르기까지 2~3년의 시간이 걸리므로 주택 공급 규모를 결정짓는 선행변수로서 작동함.\n\n부동산 투자 팁\n\n국토교통부 보도자료 꼼꼼히 읽기\n국토교통부가 부동산에 관련한 다양한 자료를 올리므로 전체적으로 기조가 어떻게 흘러가는지 볼 수 있음.\n분양 기회 모색\n거주 목적이라면 시세 대비 저렴한 분양을 통해 집을 얻는 것이 좋음.\n가치 상승 지역을 찾아서 투자\n가치가 올라가는 1인가구, 2인가구, 많게는 3인가구에 초점을 둔 소형 아파트나 도시계획상 교통이나 편의시설 등이 생길 예정인 지역에 투자해야 함.\n\n8. 국제유가\n일반적으로 세계 경제가 회복세를 보이면 원유 수요가 증가 국제유가도 회복세를 보임.\n석유감산 합의, 경제 제재 등으로 인해 원유 공급이 변할 수 있음.\n석유는 사용되지 않는 곳을 찾기가 힘든 원자재임. 특히, 우리나라는 원유를 100% 수입해서 정제 후 고부가가치 석유 제품을 수출하는 구조라 국제유가가 상승하면 우리나라 물가가 상승함.\n또한, 국제유가는 일반적으로 달러 가격과 반대로 움직이기 때문에 환율에 영향을 줄 수 있음.\n9. 가계부채\n소비에 영향을 미치는 요소 - 소득, 자산, 부채, 물가\n가계부채도 GDP처럼 경제규모가 증가할수록 늘어나는게 정상이나, 너무 늘어난다면 문제임.\n따라서 소득 수준 대비 부채가 얼마나 늘어나는지 봐야 함.\n\nDSR(Debt Service Ratio, 채무상환비율)\n원리금 상환액 / 가처분소득 × 100\n원리금 상환액\n매월 갚는 원금 + 매월 갚는 이자\n가처분소득\n가구 처분(소비 또는 저축)할 수 있는 소득, 가구 전체 연봉에서 세금과 대출이자 등을 제외한 소득\n\n보통 DSR이 20~29%면 안정군, 30~39%면 위험군, 40% 이상이면 고위험군으로 분류함.\nDSR 외에도 은행대출금 연체율, 부실채권비율, 취약차주 비중 추이를 보고 가계부채를 진단할 수 있음.\n대출받을 때 팁\n\n반드시 만기일시상환(이자만 내다가 만기일에 원금을 갚는 방식) 대신 균등분할상환(주기적으로 이자와 원금을 함께 상환하는 방식)으로 대출을 받아야 함.\n고정금리, 변동금리는 금리의 향방을 가늠하고 선택해야 함. 변동금리가 금리는 더 낮지만 고정금리는 갚아야 할 원리금이 정해져 있다는 장점이 있음. (보통 대출은 10년 이상 빌리는데 10년 이상의 금리 행방을 예측하기는 힘듦.)\n1금융권에서만 대출을 받아야 함.\n전국은행연합회 소비자포털 등을 참고해 가장 낮은 금리를 제공하는 은행을 찾아야 함.\n\n10. 추경\n기획재정부, 한국개발연구원(KDI), 산업연구원(KIET), 한국은행, 현대경제연구원, LG경제연구원, 삼정KPMG경제연구원 등 수많은 기관에서 경제 전망을 함.\n그러나 기획재정부는 매년 경제성장률 전망치를 높게 잡아 세금을 덜 걷은 다음 경제상장률을 하향조정해 추경을 하는 문제있는 행보를 지속하고 있음.\n하지만 추경이 어느 분야에 집중 편성되는지 살펴보면 우리나라 경제의 흐름을 이해할 수 있음.\n11. 실업률\n고용은 경구 후행적인 변수기 때문에, 코로나 시대가 끝나가는 지금도 고용은 아직 회복이 되지 않고 있음.\n생산가능인구의 분류\n\n생산가능인구: 만 15세 이상 65세 미만인 모든 사람\n\n경제활동인구\n\n취업자\n\n임금근로자: 남에게 봉급을 받는 사람\n\n상용 근로자\n임시 근로자\n일용 근로자\n\n\n비임금근로자(자영업자): 자신에게 봉급을 주는 사람\n\n고용주: 고용원이 있는 사람\n자영자: 고용원이 없는 사람\n무급가족종사자: 가족일이라 도와주는 사람 (ex: 남편 치킨집을 도와주는 아내)\n\n\n\n\n실업자\n\n\n비경제활동인구\n\n주부\n학생\n'쉬었음' 인구: 뚜렷한 이유 없이 구직활동을 하지 않은 사람들\n취업준비자\n구직단념자\n취업무관심자\n\n\n\n\n\n고용률과 실업률\n\n고용률\n취업자 / 생산가능인구 × 100\n실업률\n실업자 / 경제활동인구 × 100\n\n분모가 다르기 때문에 고용률과 실업률의 합계는 100%가 아니며, 고용률과 실업률이 같이 증가할 수 있음.\n또한, 비경제활동인구가 아닌 '실업자'가 되기 위해서는 조건 3가지를 만족해야 함.\n\n현재 일을 하지 않는 상태임 (알바생도 취업자로 취급함)\n일이 주어지면 일을 할 수 있음\n지난 4주간 적극적인 구직활동을 한 사람임\n\n상당수는 3가지 조건을 만족하지 않기 때문에 비경제활동인구로 빠져서 실업률이 사람들 인식보다 낮음.\n사람들이 느끼는 실업률에 가까운 수치는 체감실업률이 있음.\n\n체감실업률\n(실업자 + 취업준비자 + 구직단념자 + 취업무관심자 + ⋯) / 경제활동인구\n\n12. 인구\n\n생산가능인구: 만 15세 이상 65세 미만\n고령자: 만 65세 이상\n\n고령자가 7% 이상이면 고령화사회, 14% 이상이면 고령사회, 20% 이상이면 초고령사회임.\n우리나라는 다른 나라들보다 상당이 빨리 고령화가 진행중이며, 2025년에 초고령사회에 진입할 것으로 예상됨.\n고령화가 진행되며 독거노인이 늘어나 1인가구가 주가 되고 있으며, 노인 빈곤율, 노동력 부족 등의 문제가 심해지는 중임.\n하지만 지금의 시니어들은 액티브 시니어, 즉 노년이 새로운 인생이라 생각하며 활동적으로 소비하는 계층이므로 시니어들을 위한 에이징 테크 산업이 유망할 수 있음.\n13. 디지털 트랜스포메이션\n산업혁명은 생산성이 급증하는 현상을 가리킴.\n4차 산업혁명은 인공지능, 빅데이터, 사물인터넷 등 지능정보기술을 활용해 산업의 생산성이 급증하는 현상으로, '4차 산업'은 존재하지 않음. 대신 1, 2, 3차 산업의 생산성이 급증할 수 있음.\n\n\n\n산업혁명\n기술적 동인\n\n\n\n\n1차 산업혁명\n증기기관 기반의 기계화 혁명\n\n\n2차 산업혁명\n전기 에너지 기반의 대량생산 혁명\n\n\n3차 산업혁명\n컴퓨터와 인터넷 기반의 지식정보 혁명\n\n\n4차 산업혁명\n지능정보기술 (AI, 빅데이터, IoT, 클라우드 등)\n\n\n\n\n디지털 트랜스포메이션\n삶이 아날로그에서 디지털로 전환되는 현상 (ex: 신문 대신 포털 사이트에서 검색)\n\n2000년대 까지만 해도 상위 10대 기업은 대부분 제조사였지만, 현재는 상위 10대 기업이 IT 기업임.\n모든 기업의 첫 번째 경영 전략은 디지털 트랜스포메이션이 되어야 함.\n디지털 트랜스포메이션의 6가지 물결\n1. 비대면화(Untact, New Normal)\n대표적으로 오프라인 쇼핑에서 온라인 쇼핑으로의 전환이 이루어지고 있음.\n그마저도 PC에서 모바일로 전환되고 있으며, 모바일 기반 플랫폼들이 PC를 압도적으로 이기는 중임.\n비대면은 더이상 이상한 것(Abnormal)이 아니라 새로운 표준(New Normal)이 되었음.\n2. 탈경계화(Borderless)\nAI를 이용한 모션 캡쳐로 홈 트레이닝을 진행하는 서비스 기업은 통신, 요가, 인공지능 기술을 모두 가지고 있음.\n플랫폼 안에서 어떤 것도 서비스가 될 수 있기 때문에 산업의 경계가 허물어지며고 있음.\n3. 초맞춤화(Hyper-customizaton)\n스포티파이 등의 음액 앱은 나의 감정, 현재 상황, 태도나 기분 등을 실시간으로 인식해 맞춤화된 음원을 스트리밍해줌.\n의료 서비스도 초맞춤화되어 개인마다 맞는 건강 관리 서비스를 제공해줄 것으로 보임.\n4. 서비스화(Servitization)\n제품의 생산, 공급, 판매로 끝나는 것이 아니라 제품을 이용한 추가적인 서비스를 공급함.\nex) 나이키는 운동화 깔창에 바이오 빅데이터를 실시간으로 축적하는 시스템을 마련해 바이오 헬스케어 서비스를 제공하고 있음.\n5. 실시간화(Real Time)\n사물인터넷을 기반으로 실시간 정보를 제공함.\nex) 웬만한 은행들이 도입항 '동산 담보 관리시스템'은 사물인터넷을 이용해 동산의 위치, 가동 상태 등을 실시간으로 분석해 담보가치를 실시간으로 확인해줌.\n6. 초실감화(UX, User Experience)\n비대면화 때문에 실제로 체감하기 어려워지는 상황이 발생하자 VR, AI 등을 이용해 수십가지 제품을 체험할 수 있도록 만듦.\nex) 일본의 안경 브랜드 진스가 만든 진스브레인(Jins Brain)은 여러 가지 안경을 써본 모습을 보여줄 수 있음.\n구매를 해야만 경험을 할 수 있었던 기존의 비즈니스 모델과 달리 가상 증강현실 기술로 경험 후 구매를 하도록 만듦.\n14. 기후변화와 ESG 경영\n\nESG\nEnvironment(환경) + Social(사회) + Governance(지배구조)\n\n기업의 사회적 책임인 CSR(Corporate Social Responsibility)가 발전한 모습이라고 생각하면 됨.\n전 세계적으로 유행중이며, CITI 그룹의 &lt;Environmental, Social and Governance Report&gt;가 대표적임. (기업이 얼마나 ESG에 맞춰 잘 운영했고, 앞으로의 지속 가능성이 얼마나 되는지 보고함.)\n하지만 겉으로만 ESG 기업인척 하는 그린워싱(Greenwashing)도 늘어나고 있으므로 소비자가 진짜 ESG 기업인지 아닌지 선별할 눈을 가져야 함.\n1. Environment(환경)\n마스터카드는 탄소 계산기라는 애플리케이션을 개발해 소비자가 결제한 탄소 배출량을 알려줌.\n글로벌 환경규제가 본격화되고 있음으며, 유렵에서는 2025년부터 내연기관 엔진을 탑재한 차는 판매가 금지되며, 탄소배출권 거래제가 시행되고 있음. (탄소를 배출 허용량보다 적게 배출한 금액은 탄소배출권을 판매할 수 있고, 탄소를 배출 허용량보다 더 많이 배출하고 싶은 기업은 탄소배출권을 사와야 함.)\n이로 인해 전기차, 이차전지 시장이 폭발적으로 성장중이며, 엔진 등 내연기관차 부품시장이 쇠퇴하고 모터 등 전기차 부품시장이 성장중임.\n2. Social(환경)\n유한킴벌리는 시니어들을 고용해 시니어가 시니어를 돕는 돌봄 서비스를 제공해 고령자 일자리도 제공하고 고령자 돌봄 서비스도 제공하는 공익 유통기업으로서 성장하는 모습을 보여줌.\n3. Governance(지배구조)\n대한항공의 땅콩 회항 사건 등이 기업 가치를 추락시킬 수도 있음.\n풀무원 남승우 전 대표의 경우 자식에게 물려주며 2세대, 3세대로 경영권을 승계하는 대신 1호 사원인 이효율 대표에게 경영권을 일임해 좋은 모습을 보여줌.\n","categories":["BOOK","경제경영"]},{"title":"<랑과 나의 사막> 로봇은 감정을 느낄 수 있을까?","url":"/posts/4/","content":"\n    \n    랑과 나의 사막천선란\n    \n  \n이 소설은 주인공 랑의 죽음으로부터 시작된다. 모든 것이 멸망한 아포칼립스 세계관에서 지구는 이미 거의 모든 동식물이 사라져 황폐해진 지 오래고 인간이 노인까지 살다가 죽기에는 너무 위험한 세계가 되었다. 그렇게 끝없는 사막 속에서 랑의 엄마인 조도 죽었고, 랑도 결국 죽게 된다. 특이하게도 랑과 같이 살던 동거인은 같은 인간이 아니라 로봇인 고고였는데, 로봇이라 인간의 감정을 이해하지 못 하는 모습을 보여준다. 랑의 장례식은 아직까지 랑의 집에 남아있던 고고와 랑의 친구인 지카가 치렀는데, 고고는 지카가 왜 그렇게 허탈해하고 왜 그렇게 쓸쓸해하는지, 그리고 왜 랑의 시체를 땅속 깊이 묻어두는지 이해하지 못한다.\n지카가 갈 곳이 없으면 같이 바다로 가자고 했을 때, 그제야 고고는 자신이 살아가야 할 목적이 없어졌다는 사실을 깨닫는다. 태어날 때부터 만들어져 목적을 가지고 살아가는 다른 로봇들과 달리, 고고의 기억은 랑이 모래 속에 파묻혀있던 자신을 구해줬을 때부터 시작된다. 어떠한 명령이나 지시도 받지 않은 채 유일하게 알고 지내던 랑이 세상의 전부였던 고고는 어떻게 할까 고민하던 도중 결국 새로운 삶의 목적을 찾지 못한 채 랑을 행복하게 해준다는 마지막 명령을 따르기 위해 랑이 가고 싶었던 과거로 가는 땅을 향한 여정을 출발하게 된다. 이 점에서 고고는 사람과 다르지 않다. 흔히들 상실을 이겨낸다고 하지만, 상실을 완전히 잊어버릴 수는 없으며 소중한 사람의 상실인 경우 이겨내지 못하고 매몰되는 경우도 생긴다. 물론 고고의 경우에는 마지막으로 하던 일을 계속해서 반복하는 것에 가까운, 완전 다른 이유로 행하는 것이지만, 그런데도 랑을 잊어버리지 못하고 랑에 대한 감정이 남아있는 것처럼 행동한다.\n사막에서 정처 없이 떠돌던 도중 고고는 다른 로봇인 알아이아이를 만나게 된다. 알아이아이는 이미 죽어버린 주인을 기다리며 마지막 명령인 트랙터의 경로를 바꾸는 일을 수행하기 위해 트랙터에 몸을 부딪쳐가며 자신을 부숴가고 있었다. 우리가 익히 알고 있는 로봇의 모습처럼 마지막 명령을 그대로 수행하는 모습이다. 고고도 똑같은 로봇이고, 똑같이 마지막 명령을 그대로 수행하고 있지만 알아이아이를 이해하지 못 하는 모습이 인상적이다. 같은 로봇이지만, 고고가 더 효율적인 방법을 제시해주고 직접 자신의 팔을 떼주는 등 랑이 했던 것처럼 쓸데없이 오지랖이 넓은 모습은 마치 두 로봇이 서로 대화하는 장면이 아니라 사람과 로봇이 대화하는 장면처럼 보이기도 한다.\n마지막에 도달한 과거로 가는 땅에서 만난 살리는 고고와 사람의 차이가 있는 것인지 물어본다. 그렇다. 고고가 사막으로 가는 여정을 결정한 일, 그리고 여정 중에 행동한 행위들은 사람과 다를 바가 없었다. 고고는 자신한테 감정을 느끼는 회로가 없다고 하지만, 감정은 기본적으로 실체적인 현상이 아닌 추상적인 개념이다. 인간의 감정, 기억, 생각 등도 표면적으로는 뇌 속에 있는 시냅스를 통한 전기신호일 뿐이다. 어쩌면 우리도 고고처럼 화학적인 전자회로로만 이루어진 존재라고 볼 수도 있다. 그렇다면 감정을 느낀다는 것은 무엇일까? 작가는 여기서 실제로 감정을 느끼는지 아닌지가 왜 중요하냐고 역으로 물어본다. 고고가 감정이 있는 것처럼 행동한다면 다른 사람들한테 고고가 감정을 느끼는 회로가 있는지 정말 중요하게 생각할까? 우리가 감정에 의해 행동하는 행위 그 자체가 감정을 느낀다는 근거가 되는 것이다. 애초에 감정이 추상적인 개념이기 때문에, 겉모습이 사람인지 로봇인지는 중요하지 않다. 감정이 있는 것처럼 행동한다면, 고고도 로봇임에도 불구하고 '인간다움'을 가지고 있다고 생각할 수 있을 것이다.\n","categories":["BOOK","소설"]},{"title":"<외모 대여점> 무엇이든 빌려드립니다","url":"/posts/5/","content":"\n    \n    외모 대여점이사카와 히로치카\n    \n  \n이 책은 하루 동안 원하는 외모를 빌릴 수 있는 외모 대여점에 방문하는 10명의 손님들에 관한 이야기다. 외모 대여점의 점장인 아즈마 안지는 사실 여우술사라 4명의 변신여우들을 다룰 수 있으며, 변신여우들은 원하는 외모로 변신할 수 있어, 변신여우들이 손님들이 원하는 외모로 변신한 뒤, 변신여우와 손님이 서로 외모를 맞바꾸는 방식으로 외모를 '대여'할 수 있다. 이런 소설도 나오는 것을 보면 더 잘생겨지고 싶거나 더 예뻐지고 싶은 마음은 일본에서도 한국에서도 똑같은 사람의 본성인 것 같다.\n하지만 외모 대여점에서 자신이 가장 가지고 싶었던 외모를 빌려본 10명의 손님들은 '외모가 중요하지 않다'는 결론을 얻고 돌아간다. 공공장소에서 시끄럽게 떠드는 여자애들한테 조용히 하라고 말하려고 하거나, 자신이 좋아하는 남자한테 고백하려고 하거나, 혼자 있는 직장 동료에게 하고 싶은 말을 하려고 하는 등 10명의 손님들은 다양한 이유로 외모를 빌려봤지만, 정작 알아낸 것은 사람들이 '원하는 외모'를 빌린 자기 자신보다, 외모가 뒤바뀌어서 '원래 자신의 외모'를 가지고 있는 변신여우의 말을 더 잘 듣는다는 사실을 알아낸다. 외모를 빌리려고 한 손님들은 자신의 외모로는 다른 사람들한테 얘기할 수 없을 것 같아, 다른 사람들이 더 잘 들어줄 것 같은 외모를 빌려보지만, 가장 중요한 것은 자신감이 있는 외모가 아니라 자신감이 있는 내면의 모습 그 자체임을 알게 된다.\n다만 교육적인 내용과는 별개로 소설 자체는 아쉬운 편이다. 외모 대여점은 옴니버스 형식을 사용한 소설로, 손님마다 1개의 챕터를 맡아 총 10개의 챕터로 나누어져 있는데, 책 자체가 300쪽도 안 되는 짧은 분량이기 때문에 각 손님의 이야기가 몇십 쪽 만에 끝나버려 깊은 이야기를 다루지 못 한 것 같다. 책 내용도 매번 등장인물 소개 후 외모를 바꿔본 뒤에 생각처럼 안 된다는 결론이 계속 반복되니 같은 내용만 보고 있다는 느낌도 받았다. 가장 아쉬운 점은 주인공의 얘기가 소설에 잘 안 나타난다는 점이다. 여우술사와 변신여우라는 특이한 설정을 채용했음에도 불구하고, 정작 소설 내에서의 활용도는 원하는 외모를 바꿔주는 도구로만 쓰이는 것이 대부분이다. 주인공들의 설정은 정말 매력적임에도 불구하고 그에 걸맞은 뒷이야기가 풀리지 않은 것 같아 매우 아쉽다. 아무래도 소설이 옴니버스 형식이다 보니 주인공의 이야기를 우선적으로 풀기보단, 각 챕터마다 등장하는 손님들의 이야기를 먼저 얘기하다 보니 정작 주인공에 대해 할 이야기가 줄어들어 소설에 묻힌 감이 있다. 막상 각 손님의 이야기도 손님마다 감정이입 하기에는 너무 짧아서 주인공한테도, 손님들한테도 감정을 이입할 수가 없었다. 챕터 수를 좀 더 줄이고 각 챕터를 조금 늘리면 더 나은 소설이 될 것 같다.\n그럼에도 불구하고 워낙 소설 설정 자체가 매력적이다보니 재밌게 읽은 것 같다. 깊이 있는 소설을 원하는 사람한테는 별로지만 간단하게 읽을 수 있는 책으로는 나쁘지 않다. 몇몇 챕터는 여장남자에 관한 얘기가 나오는 등 생각해볼 만한 주제가 있는 챕터도 있었고, 전체적으로 책이 주는 교훈이 나쁘지 않다. 내가 무슨 짓을 해도 바꿀 수 없는 외모에 대해 집착하는 것보단, 내 노력으로 바꿀 수 있는 마음가짐을 바꾸는 것이 더 중요하다. 결국, 사람의 본질은 외모에 있는 것이 아니라 마음에 있다는 사실을 알게 된다.\n","categories":["BOOK","소설"]},{"title":"<내게 남은 사랑을 드릴게요> 사랑을 SF적으로 표현하는 방법","url":"/posts/6/","content":"\n    \n    내게 남은 사랑을 드릴게요이유리, 김서해, 김초엽, 설재인, 천선란\n    \n  \n이 책을 처음 봤을 때는 '내게 남은 사랑을 드릴게요'라는 제목과 하트로 가득 찬 표지 때문에 로맨스 소설인 줄 알았지만, 읽어보니 이 책은 사실 SF 소설 앤솔러지였다. 그런데 SF 소설 앤솔러지라고는 하지만 'SF 소설'은 '판타지 소설'처럼 너무 광범위하고 다양한 작품들을 지칭하는 말이다. 장르의 방대함을 증명하듯 이 책을 이루는 5개의 소설에서도 명확한 공통점을 찾기는 쉽지 않다. 굳이 따지자면 이 책의 제목에서도 나타나듯, '사랑' 자체가 이 책을 관통하는 공통점일 것이다. 5명의 작가가 'SF 소설'만큼이나 정의하기 어려운 '사랑'을 나름의 방식대로 SF 소설로 표현한 모습을 볼 수 있었다.\n5개의 소설 중 인상 깊었던 2개의 소설이 있는데, 먼저 김초엽 작가의 &lt;수브다니의 여름휴가&gt;가 있다. 인공장기를 배양하는 회사에서 일하던 주인공이 인공피부를 배양하는 곳에서 일한다는 내용인데, 역사가 깊은 SF 장르에서도 언급되지 않은 주제인 인공피부를 찾아내서 소설로 쓴 것을 보고 작가가 매우 똑똑하다는 생각을 했다. 또한, 인공장기는 단순히 심장을 다치면 교체하고, 눈을 다치면 교체하는 등 단순히 수술을 대체해줄 의료적인 목적만 달성한다면, 인공피부는 곰, 고양이, 용, 늑대 등 인간과 완전히 다른 종이 되고 싶은 사람들을 위해 만들어진다. 물론 피부만이 인간의 본질을 나타내는 요소는 아니겠지만, 이를 통해 '피부가 달라지면 우리도 인간이 아니게 되는 걸까?'라는 생각을 하도록 만든다. 이 외에도 인간과 안드로이드의 사랑과, 금속 피부를 이식받은 후 스스로 녹슬어가는 작품의 일부가 되는 결말까지, 짧은 소설임에도 불구하고 생각해볼 점이 많았다. 읽으면서도 쏟아져 내리는 새로운 설정들 때문에 이야기가 어떻게 흘러갈지 열심히 기대하면서 읽었다.\n다음으로는 천선란 작가의 &lt;뼈의 기록&gt;이 있다. 소설 배경은 미래시대로, 안드로이드가 보급되면서 장의사 대신 장례를 치르는 장의사 안드로이드가 생겼다는 설정이다. 장의사 안드로이드인 로비스는 고독사한 노인, 자살한 여자, 사고사한 아이 등 여러 시체들의 장례를 치르며, 마지막에는 자신과 같이 말동무를 해주던 청소부 모미의 장례를 치러준다. 같은 작가의 &lt;랑과 나의 사막&gt;에서도 느낀 거지만 천선란 작가는 안드로이드 같은 기계적인 생각으로 인간의 마음을 표현하는 것을 잘하는 것 같다. 과학적인 사실인 적혈구의 기능을 얘기하면서 유가족들을 위로해주거나, 아름다움의 정의를 듣고 나서 뼈가 아름답다고 정의를 내리는 등의 모습을 보고 느꼈다. 또한, 이 소설은 발단-전개-위기-절정-결말로 이루어진 소설의 구성단계를 완벽하게 지킨 소설이기도 하다. 45페이지의 짧은 소설에 복선들도 넣어가며 우주선에서 장례를 치르는 클라이맥스로 달려가는 모습은 읽은 사람들에게 말 그대로 '절정'을 느끼게 해준다. 너무 짧아서 아쉬웠던 다른 소설들과 달리 짧은 글임에도 탄탄한 구성으로 이루어져 있어 수백 페이지의 소설을 다 읽은 느낌을 얻을 수 있었다.\n이 책에서 말하는 '사랑'은 남녀 사이의 사랑도 있긴 하지만, 그것보다 보편적인 인간성에 관한 이야기다. 혼자서만 지내는 학우한테 먼저 다가가 친구가 돼주는 일, 가정폭력을 당하고 있는 아이한테 다가가서 안아주는 일, 애인과 만들었던 마지막 작품을 스스로 완성하는 일, 평생 볼일 없을 유가족들을 위로해주고, 함께 일하다 죽은 동료를 위해 그 마음을 헤아리고 평생 내본 적 없던 용기를 내본 일. 이 모든 것들은 인간만이 가능한 비논리적인 행동이고, 그러므로 '사랑'으로 밖에 표현할 말이 없다. 5명의 작가는 서로 다른 세상의 모습을 그렸지만, 그 속에서도 여전히 '사랑'이 존재할 것이다.\n","categories":["BOOK","소설"]},{"title":"<푸른 낙엽> 또 다른 한국인들의 삶","url":"/posts/14/","content":"\n    \n    푸른 낙엽김유경\n    \n  \n푸른 낙엽은 탈북민 작가 김유경이 쓴 단편 소설집이다. 무려 9개의 단편 소설로 이루어진 소설집인데, 북한 사회, 더 나아가서 탈북민의 사회에 관한 소설이다. 북한에 대해서는 당연히 바로 옆에 붙어있는 국가이기 때문에 많이 들어봤지만, 북한 사람들이 어떻게 살고 있는지는 잘 몰랐다. 그저 가난한 나라인 만큼 우리나라랑 달리 굶주리면서 살겠지 하면서 생각했다. 이 책은 북한 안에서의 삶, 탈북 도중의 중국에서의 삶, 그리고 탈북한 후 한국에서의 삶 등 다양한 각도에서 북한 사람들이 어떻게 살아가는지 보여준다. 작가는 탈북민들을 단풍으로 미처 물들지 못한 채 갑작스럽게 떨어진 푸른 낙엽에 비유하면서, 북한에서 태어났기에 어쩔 수 없이 도피처를 찾게 된 북한 사람들의 생활을 소설에 나타내고자 했다.\n특이하게도 이 책에서 탈북 과정에 대한 묘사는 잘 나오지 않는다. 이 책의 소개에도 다른 탈북민들의 작품처럼 '북한 인권문학'이 아니라고 소개된다. 다른 탈북민들이 쓴 작품은 탈북 과정에서 벌어지는 인신매매와 납치 등의 일을 소개하면서 인권 유린의 현장을 보여주지만, 이 책은 오히려 탈북 과정에 대한 소개 없이 북한에서의 삶과 남한에서의 삶에 초점을 맞춘다. 분명 북한 사람들의 삶과 남한 사람들의 삶은 너무나 많이 달라졌고, 해가 지날수록 탈북민들이 사회에 적응하기 더 어렵게 만든다. 작가는 북한 인권문제의 실태를 고발하는 대신, 북한과 남한이 얼마나 다른지를 보여주며 북한과 남한과의 차이가 조금이라도 줄어들기를, 그럼으로써 탈북민이 남한에 더 잘 적응할 수 있고 통일이 더 빨라지기를 바라고 있다.\n9개의 소설 중 크게 인상 깊었던 소설이 2개 있는데, &lt;자유인&gt;과 &lt;붉은 낙인&gt;이다. &lt;자유인&gt;에서는 북한을 위해 유럽에서 필요한 물품들을 제재에 걸리지 않고 평양으로 빼돌리는 외교관 출신 탈북자가 나온다. 하지만 이 탈북자는 북한에서 누구보다 많은 것을 누리며 상류층으로 살았던 모습을 버려버리고, 조용히 바닷가 관리원 일을 하거나 산골에서 배추를 심으면서 산다. 어쩌면 자본주의 세상에 빠져 재산과 지위를 차지하려 노력하는 우리나라 사람들과 달리, 자유를 찾아 탈출한 탈북민이기 때문에 우리보다 더 진정으로 자유만을 추구하는 삶을 살 수 있었던 것 같다.\n&lt;붉은 낙인&gt;에서는 먼저 탈북했던 진옥이 아직 북한에 남아있던 진미를 탈북시키기 위해 브로커를 찾는데, 알고 보니 진미는 탈북하려고 했던 게 아니라 북한한테 세뇌된 상태로 오히려 보위원의 말을 듣고 진옥을 납북시키려고 했다. 상당히 충격적인 결말에 도달하는 소설이 책의 마지막을 장식하는 이유는 무엇일까. 작가는 진옥과 진미를 통해 남한에서 바라보는 북한의 시각과 북한에서 바라보는 남한의 시각의 차이를 나타내려고 한 것 같다. 거의 80년을 향해가는 분단의 역사는 같은 민족의 생각을 이렇게나 바꾸어놓은 것이다. 하지만 북한 사람들도 남한과 말이 통하는 같은 민족임이 틀림없다. 실제로 문화어는 한국어와 크게 차이가 없어 통역 없이 서로 소통이 가능한 정도라고 한다. 이 책에서도 문화어에서만 쓰는 표현들이 일부 나오지만, 읽는 데 불편함을 느끼지 못했다. 같은 말을 쓰는 같은 민족이 80년째 분단되어 있다는 사실은 전 세계를 찾아봐도 얼마 없는 크나큰 비극이다. 비록 소설에서는 진옥과 진미가 합의점을 찾지 못하고 다시 헤어지지만, 과연 가까운 미래에는 서로의 차이를 극복하고 진옥과 진미가 함께 떠날 수 있을까.\n","categories":["BOOK","소설"]},{"title":"<일머리 문해력> 사람이라면 읽어야 할 책","url":"/posts/7/","content":"\n    \n    일머리 문해력송숙희\n    \n  \n세대가 지날수록 문해력이 점점 떨어지고 있다는 지적이 계속해서 나온다. '심심한 사과', '사흘' 등 지금까지 잘 사용되던 단어들을 이해하지 못 하는 사람들이 늘어나고 있다. 보통 이런 경우 어휘력이 문제라고 생각하는 사람들이 많다. 하지만 진짜 문제는 문해력이 부족하다는 것이다. 문해력은 '글을 읽고 의미를 파악하고 이해하는 능력'으로, 문해력이 높은 사람들은 읽기-생각하기-쓰기 순서로 생각하며 글을 이해하고 평가하며 사용한다. '심심하다'의 뜻이 '마음의 표현 정도가 매우 깊고 간절하다'라는 것을 아는 사람들은 그리 많지 않다. 하지만 문해력이 높은 사람들은 글의 의도를 생각하면서 읽기 때문에 '심심한 사과'를 보고도 사과가 지루하고 재미가 없냐면서 화를 내진 않는다.\nOECD가 &quot;어떤 능력이 정보기술 위주의 디지털 시대에서 경쟁력을 높여줄까?&quot;라는 문제에 답하기 위해 성인 경쟁력에 대한 국제조사를 시행한 적이 있다. 이 조사에서는 특이하게도 관련 없어 보이는 문해력, 수리력, 컴퓨터를 사용한 기술적 문제해결 능력이 일하는 사람의 경쟁력을 좌우한다는 전제를 한다. 그런데 놀랍게도 문해력, 수리력, 컴퓨터를 사용한 기술적 문제해결 능력의 상관성이 강하고, 문해력이 다른 두 능력을 좌우한다는 결론이 나왔다. 컴퓨터공학부 학생으로서 왜 이런 결과가 나왔는지 생각해봤는데, 프로그래밍도 일종의 '글쓰기'라서 그런 것 같다. 같은 기능을 하는 프로그램은 다양한 방법으로 작성할 수 있지만, 프로그래머는 그중에서 다른 사람이 봤을 때 가장 읽기 쉬운 코드를 작성하려고 노력한다. 본인이 옛날에 작성한 코드를 다시 수정하거나, 다른 사람들과 같이 코드를 작성할 때 읽기 쉬운 코드일수록 이해하기가 쉽기 때문이다.\n글쓰기도 마찬가지다. 글의 목적은 읽히는 것이다. 나 혼자만 볼 수 있고 이해할 수 있는 글이 필요한 때도 있을 수 있지만, 대부분 글은 다른 사람도 읽을 수 있도록 쓴다. 그런데 다른 사람들이 정작 내 글을 읽었을 때 이해를 못 한다면 좋은 글이 아닐 가능성이 크다. 글쓰기를 잘하는 사람은 읽는 사람 입장에서 생각할 줄 안다. 어떻게 하면 더 많은 사람이 이해할 수 있을지 생각하고, 본인의 주장을 간단하고 명료하게 주장한다. 그래서 더 효과적으로 자기의 주장을 전달할 수 있고, 다른 사람들에게 더 큰 영향력을 끼칠 수 있는 거물이 될 수 있다. 문과와 이과만큼 전혀 다른 분야인 것처럼 보이는 문해력, 수리력, 컴퓨터를 사용한 기술적 문제해결 능력이 큰 상관관계가 있는 이유다.\n이 책은 여러 거물들의 말을 인용했다. 빌 게이츠, 워렌 버핏, 일론 머스크 등 전 세계 사람들이 알고 있는 유명인들이 모두 읽고, 생각하고, 쓰라고 권유한다. 책에서 문해력에 악영향을 끼치는 행동들을 소개해줬는데 아마 현역 군인이라면 모두 해당하는 내용일 것이다. 종이책 대신 전자책 읽기, 긴 글 대신 세줄요약 읽기, 유튜브 알고리즘에 몸을 맡기기 등 요즘 10~20대한테서 흔하게 볼 수 있는 모습이다. 심지어 세대가 지날수록 문해력이 떨어지는 것은 우리나라뿐만 아니라 전 세계에서 나타나고 있는 현상이라고 한다. 하지만 문해력은 동물이 아닌 인간이 되기 위해서 갖춰야 할 최소한의 능력이다. 단순히 보고서, 자소서, 독후감 등 글 쓸 일이 많아서 필요하다는 뜻이 아니다. 문해력은 사람이 이해하고, 생각하고, 비판적인 사고를 하기 위한 필수적인 능력이다. 남들의 생각만 따라 하는 동물 대신 스스로 생각할 수 있는 사람이 되기 위해서는 읽고, 생각하고, 쓰는 연습을 하며 문해력을 키워야 한다. 그러면 우리도 언젠가 전 세계에서 알아주는 거물이 될 수 있을 것이다.\n","categories":["BOOK","자기계발"]},{"title":"<서사의 위기> 의미 없고 공허한 정보의 시대","url":"/posts/13/","content":"\n    \n    서사의 위기한병철\n    \n  \n&lt;서사의 위기&gt;는 아주 짧은 책이다. 137페이지에 크기도 작아서 다른 책들의 1/3 정도 분량밖에 되지 않는다. 하지만 이 짧은 책에 담겨 있는 고찰은 절대 짧지 않다. 왜 우리 현대 사회에는 목적과 방향 없이 그냥 살아가는 사람들이 많은지, 니트족 등 취업 준비도 하지 않고 살려는 어떠한 노력도 하지 않는 백수들이 점점 늘어나고 있는지 설명해준다. 작가는 이 해답을 우리 시대가 너무 스마트해졌기 때문이라고 설명한다. 이제 스마트폰은 아무리 가난해도 있어야 할 필수품으로 여겨지고 있다. 영화 &lt;기생충&gt;에서도 인터넷 요금제도 없을 만큼 가난해도 스마트폰을 가지고 있는 모습이 나온다. 누구나 인터넷을 통해서 수많은 정보를 얻거나 생산할 수 있으며, 이제 많은 사람이 책 같은 오프라인 매체보다 스마트폰을 통해서 정보를 얻는다.\n문제는 정보를 생산하면서 돈을 벌 수 있는 사회가 되면서 시작된다. 유튜브, 인스타그램 등의 플랫폼에서는 더 많은 사람이 플랫폼을 쓰도록 만들기 위해서 광고비 일부를 콘텐츠 크리에이터들에게 나눠주기 시작했다. 그런데 오히려 유튜브 등의 플랫폼이 전 세계 모든 사람이 사용할 정도로 커지고, 광고비 수익이 엄청나게 커지게 된다. 유튜브, 인스타그램만으로도 먹고 살 수 있을 뿐만 아니라 누구나 알아주는 부자가 될 수 있게 된 것이다. 일이 이렇게 되자 사람들은 경쟁적으로 콘텐츠 시장에 뛰어들게 된다. 싸이월드 같은 예전의 SNS는 자신의 이야기를 공유하는 목적으로 사용됐다면, 지금의 유튜브, 인스타그램 등의 SNS는 어떤 방식으로든 조회수를 더 얻거나, 좋아요를 더 얻으려는 목적으로 변질되었다. 작가는 이를 보고 '스토리텔링'이 아니라 '스토리셀링'이라고 비판한다. 사람들은 이제 SNS를 자신을 이야기하려는 목적으로 사용하지 않는다. 오히려 자신의 이야기를 상품으로 만들어 시장에 내놓으며, 어떻게든 다른 사람이 읽게 만들기 위해 더 자극적이고 단편적인 주제들로 꾸미게 된다.\n이 과정에서 서사의 위기를 만든 획기적인 발명품 &quot;숏츠&quot;가 탄생하게 된다. 플랫폼 회사들은 하나의 긴 이야기보다 짧은 수많은 이야기를 판매하는 것이 플랫폼에 더 도움이 된다는 사실을 알아내게 된다. 이제 유튜브도, 인스타그램도 모두 숏츠 시장에 적극적으로 집중하고 있다. 사람들은 이제 사람에 대한 하나의 이야기 대신 끝도 없이 쏟아지는 30초짜리 숏츠들을 보게 된다. 작가는 이 과정을 정보화라고 표현한다. 사람에 대한 서사는 더 이상 존재하지 않는다. 대신 사람을 30초짜리 숏츠로 분해한 정보들만이 존재할 뿐이다. 이 정보들은 무질서하고, 관련 없다. 딱 30초 동안만 유효한 곧 소멸할 정보들일 뿐이다. 처음에는 수많은 정보가 쏟아지는 정보의 바다에서 강력한 자극을 얻고 빠져들게 되지만, 곧이어 이러한 정보들은 서사가 없는 무의미한 것뿐임을 알게 된다. 그래서 현대인들은 공허해지고, 허무감에 빠지게 된다.\n불행히도, 작가는 이런 서사의 위기를 해결할 방법을 설명해주지 않는다. 과거와 달리 우리 사회는 현재 서사의 위기에 빠져있고, 어떻게 빠지게 됐는지 '정보'와 '이야기'의 비유로 명쾌하게 설명해주지만, 아쉽게도 그 문제에 대한 해결방법은 이 책에 남겨져 있지 않다. 어쩌면 문제의 핵심을 단번에 파악한 작가조차도 해결방법에 대해서는 아직 고민 중일지도 모르겠다. 하지만 우리 개개인이 정보의 바다에서 스스로 빠져나와야겠다고 생각하게 된다면 어떨까. 이미 오래전부터 숏츠를 보는 것은 무의미하다고 생각했던 나는 숏츠가 보이지 않도록 개조한 유튜브 앱을 사용하고 있다. 우리 스스로가 정보 대신 이야기를 찾기 시작한다면 이 사회는 다시 서사를 되찾을 수 있지 않을까?\n","categories":["BOOK","인문"]},{"title":"<유리 멘탈이지만 절대 깨지지 않아> 일상 속에서 소소한 행복을 찾자","url":"/posts/9/","content":"\n    \n    유리 멘탈이지만 절대 깨지지 않아기무라 코노미\n    \n  \n이번 달에는 어떤 책을 읽을까 고민하며 책들을 살펴보던 도중 이 책을 발견하게 됐다. ChatGPT 등 재미있어 보이는 책들이 많았지만 지금 당장 필요한 책은 이 책인 것 같아서 바로 골랐다. 이번 달에는 여러 가지 일들이 겹치면서 스트레스를 굉장히 많이 받았다. 아무래도 너무 많은 일을 준비하다 보니 몸과 마음이 지쳤던 것 같다. 상병 진급 시험도 있었고, 자격증 공부와 대학 학점취득 원격강좌도 준비하고 있었는데 뜬금없이 구글에서 약관을 변경하면서 안드로이드 개발자 계정이 삭제될 위기에 처해 군대 안에서 앱 개발을 할 방법도 고민해야 했다. 하필 입대 날짜도 애매하게 잡은 덕분에 이번 학기가 아니면 할 수가 없는 일들이 너무 많았다. 한두 달 정도 일찍 오거나 늦게 왔었다면 여유가 있었을 텐데 군대 오기 전에 이런 일들에 대해선 고민해보지 않아서 아쉬웠다. 그 와중에다 새해라서 그런지 부대 내에 있는 온갖 것들이 바뀌다 보니 적응하다 지친 나는 결국 번아웃이 와버렸다.\n하고 싶은 일은 많은데 아이러니하게도 시작하고 싶은 생각은 들지 않는 기분을 모두 알고 있을 것이다. 개인정비 시간 때마다 별생각 없이 누워서 스마트폰만 보면서 시간을 보내는 도중에 이 책을 보게 되었다. 이 책은 정확하게 지금과 같은 상황에서 어떻게 벗어나야 하는지 알려주고 있었다. 정신병원에서 주기적으로 약을 처방받아야 할 정도로 힘든 사람들은 많지 않지만, 사회에서 살아가면서 힘든 적이 한 번도 없는 사람은 없을 것이다. 하지만 조금이라도 감기에 걸리면 바로 병원에 가는 대한민국에서도 정신병원에 대한 인식은 심각하게 안 좋아서, 기분이 우울하면 그냥 우울한 채로 살면서 지내는 경우가 많다. 우리는 몸에 생긴 사소한 병들은 즉시 치료받으려고 들면서, 정작 정신에 대한 치료는 시간이 지나면 알아서 해결될 것이라 믿는다. 면역력만을 믿고 버티는 것인데, 이 책은 단순히 기다리는 대신 어떻게 하면 부정적인 생각 대신 긍정적인 생각을 할 수 있는지 알려준다.\n책을 읽으면서 내 얘기인가 싶을 정도로 공감되는 얘기가 많았다. 계속 최악의 상황만 가정한다던가, 오히려 항상 최선의 상황만 생길 거라고 막연히 기대하던가, 우울한 상황에서 억지로 힘을 내려고 하면서 더 힘들어지는 등 내가 최근에 하던 생각들이 전부 기록되어 있었다. 나도 모르게 일본 정신과 의사랑 상담이라도 했던가? 읽으면서 내 얘기 같은 얘기가 계속해서 반복되는 걸 보니 전 세계 사람들도 나랑 같은 고민을 하면서 지내는구나 하는 생각이 들었다. 어쩐지 나만 심각한 게 아니라 누구나 겪는 일이라는 생각이 들면서 안심이 됐다.\n특이하게도 이 책에서 제시하는 방법들은 백천만 감사나눔 운동과 겹치는 면이 많았다. 내가 전입을 했을 때 선임들이 힘들게 써봤자 휴가를 하루나 이틀밖에 안 줘서 그냥 안 쓰는 게 낫다고 말해줬고, 나도 딱히 쓸만한 공책도 시간도 없어서 백천만 감사나눔 운동은 고려조차 안 했었다. 그런데 군 생활이 너무 힘들어서 일병 때부터 공책을 구해 감사노트를 쓰기 시작했는데, 감사노트를 채우려고 억지로라도 감사할 일을 찾으려고 하다 보니 자연스럽게 일상 속에서 조금이라도 기분 좋았던 일들을 되짚어보게 되었다. 하루 동안 감사할 일을 많이 찾으면 기분도 좋아졌다. 군대에서는 더욱더 그렇지만, 사실 사회에서도 만날 기분이 좋아지려고 해외여행 같은 스펙타클한 일을 찾아다닐 수는 없다. 우리가 일상 속에서 있었던 일들 때문에 지친다면, 일상을 무리하게 바꾸려고 하기보다는 일상 속에서 행복한 일을 찾아야 하지 않을까. 아직 많이 쓰진 못 했지만, 오늘도 꾸준히 일상 속에서 감사할 일들을 찾고 있다.\n","categories":["BOOK","자기계발"]},{"title":"<한석준의 말하기 수업> 나의 말하기를 되돌아볼 시간","url":"/posts/11/","content":"\n    \n    한석준의 말하기 수업한석준\n    \n  \n초등학교 때 국어 과목으로 읽기와 듣기, 말하기, 쓰기를 배운 기억이 난다. 학교에서 언어로 할 수 있는 일을 총 4가지로 나누었던 것이다. 저번 독후감을 쓸 때 읽었던 &lt;일머리 문해력&gt;에서 읽기와 쓰기를 어떻게 해야 하는지 배웠는데, 이번 독후감을 쓸 때 읽은 &lt;한석준의 말하기 수업&gt;에서는 말하기를 어떻게 해야 하는지 배운다. 물론 의무교육을 들은 사람들은 누구나 한국어로 읽기, 듣기, 말하기, 쓰기를 할 수 있다. 하지만 남들의 생각을 정확하게 이해하면서 읽고, 자신의 생각을 명료하게 쓰는 것은 어려운 일이고, 학습이 필요하다. 말하기도 마찬가지다. 자기가 말하고자 하는 바를 상대가 이해하도록 정확하게 말하는 일은 쉬운 일이 아니다. 그렇기 때문에 말을 전달해주는 직업인 아나운서라는 직업이 있는 것이다.\n이 책에선 한석준 아나운서가 직접 사용했던 말하기 기법들이 적혀 있다. 사실 다들 한국어를 자연스럽게 쓰기 때문에 당연히 올바르게 한국어를 발음하고 있다고 착각하고 있다. 하지만 많은 사람이 잘못된 맞춤법을 사용하는 것처럼, 많은 사람이 잘못된 발음으로 한국어를 발음하고 있다. 대학교 재학 중 &quot;한글맞춤법의 이론과 실제&quot; 강좌를 들으면서 알게 됐던 사실인데, 나도 '닭이 난다', '닭을 잡았다' 등의 문장을 잘못 발음하고 있다는 사실을 알게 되었다. 올바르게 발음하려면 [달기], [달글]으로 발음하는 것이 맞는다. 솔직히 말하자면 너무 오랫동안 잘못된 발음으로 발음한 탓에 아직도 올바른 발음이 입에 붙진 않는다. 이 책에서는 이런 식의 잘못된 발음 말고도 사투리 등의 어조나 어투를 고치는 방법을 알려준다. 한석준 아나운서의 유튜브에 올라와 있는 영상자료 링크도 함께 책에 나와 있어서 쉽게 영상을 보면서 연습할 수 있었다. 가장 특이했던 훈련 중 하나는 모음 훈련으로, 대부분 사람이 모음을 올바르게 발음하지 못하기 때문에 문장에서 자음을 제외한 모음만 발음하도록 만든 훈련이다. 이 책을 보지 않았다면 내가 모음 발음이 잘 안 된다는 사실도 몰랐을 것이다.\n하지만 이 책에서는 말하기 기법 말고도 어떤 내용을 말해야 좋은지도 나와 있다. 어쩌면 말하기 기법들보다도 훨씬 중요한 내용일지도 모른다. 힘들어하는 사람을 위로해주는 일, 상대방의 잘못을 지적하는 일, 상대방의 부탁을 거절하는 일, 상대방에게 조언하는 일 등 일상에서 자주 일어나지만 어떻게 말해야 할지 난감한 일들이 많다. 특히 일상 속에서 대화하다 보면 흔히 나오는 주제이기 때문에 몇 시간 동안 공들여서 생각하면서 글을 쓸 수 없고 즉석에서 말을 꺼내야 하며, 그러다 보면 갈등이 생기기 마련이다. 이 책에서는 어떻게 말하면 현명하게 일을 해결할 수 있는지 알려준다. 개인적으로 말하기 기법들보다 더 중요하게 여겨졌던 챕터였다.\n이 책의 마지막에서 내리는 결론은 좋은 말을 하면 좋은 사람이 온다는 것이다. 대화 중에는 잠시 말을 끊고 몇 시간 동안 생각한 뒤 대화를 이어나갈 수가 없다. 한순간의 실수로 서로를 이해하지 못하고 서로한테 상처를 줄 수가 있다. 이런 일이 생기지 않으려면 평소에 배려하는 태도를 가지고 상대방을 존중하는 마음을 가져 즉석에서 좋은 말이 나올 수 있도록 해야 한다. 상대방을 배려하고 존중하는 사람 주변에는 좋은 사람들만 모일 수밖에 없다. 평소의 태도에 따라서 말 한마디로 좋은 사람을 얻거나 잃을 수도 있다. 언제든지 좋은 말이 나올 수 있도록 연습해둬야겠다.\n","categories":["BOOK","자기계발"]},{"title":"<사이버전의 실체, 전술 그리고 전략> 적과 나는 이미 연결되어있다","url":"/posts/12/","content":"\n    \n    사이버전의 실체, 전술 그리고 전략김광석\n    \n  \n요즘에는 인터넷을 안 쓰는 곳이 더 드물다. 전국 어디서나 인터넷이 되며, 스마트폰은 물론이고 냉장고, 자동차, 로봇청소기 등 웬만한 전자제품들도 모두 인터넷을 사용한다. 옛날에 유비쿼터스 시대가 올 것이라면서 미래 모습을 상상하곤 했는데, 이미 그 시대가 와버린 것 같다. 인터넷이 자연스러워진 세상인 만큼 스마트폰 어플만으로도 은행업무, 물품 구매 등 원하는 업무를 진행할 수 있게 됐다. 인터넷 덕분에 언제, 어디서든, 즉석에서 일처리를 할 수 있게 된 것이다.\n하지만 편의성이 증가한 만큼 보안은 위험해졌다. 인터넷으로 모든 것이 연결되어있다는 것은 반대로 말하면 인터넷으로 적과 우리도 연결되어있다는 뜻이다. 다양한 어플을 사용하면서 무엇이든지 인터넷으로 할 수 있게 되었지만, 사용하는 어플이 많아질수록 보안이 뚫리는 경우의 수도 늘어나게 되었다. 특히 일반적으로 비밀번호를 인증 방식으로 설정하는데, 10자리 정도의 단어만 알면 모든 권한이 허용되는 비밀번호는 굉장히 안 좋은 인증 방식이다. 수많은 어플의 비밀번호를 전부 똑같이 설정할수록 더더욱 그렇다. 이처럼 인터넷이 더 발달하고 더 중요해졌지만, 오히려 인터넷을 보호하는 보안은 더 취약해졌다. 그리고 점점 더 중요한 정보를 담게 된 인터넷이 정보의 노다지라는 것을 깨달은 사람들은 사이버전에 집중하고 있다.\n사이버전은 과거에는 단순히 바이러스를 퍼트리거나 해킹을 하는 정도였다. 전쟁이 실제 물리적인 공간에서 일어나는 일이라면, 사이버전은 말 그대로 사이버 공간에서만 일어나는 일이었다. 하지만 어디에서나 인터넷을 쓰는 요즘 발생하는 사이버전의 모습은 많이 달라졌다. 북한은 인도의 원자력 발전소를 해킹해 원자력 발전소에 피해를 주는 등 사이버전을 통해 실제 물리적인 피해를 주기 시작했다. 더 놀라운 점은 사이버전에서만 가능한 공격도 있다는 것이다. 단순히 파괴만 하던 전쟁과 달리 사이버전은 여론조작을 통해 사람들의 생각을 조종할 수 있게 됐다. 이미 러시아 같은 곳에서는 국가 차원에서 인터넷을 이용해 가짜 뉴스를 퍼트리고, 사람들을 속이고 있다. 별 볼 일 없는 일처럼 보이지만, 이미 미국 대선에 영향을 줄 정도로 발전했다. 사이버전은 이제 전쟁의 일부가 아니라 또 다른 전쟁이며, 어쩌면 거의 죽어버린 기존 전쟁과 달리 지금도 활발하게 일어나고 있는 전쟁이다.\n인터넷에 연결되어있는 이상 우리도 사이버전의 참전용사다. 사이버전에 어떻게든 영향을 받을 수밖에 없는 지금 우리는 어떻게 행동해야 할까? 먼저 비밀번호 같은 너무 쉬운 방법으로 인증이 되도록 두면 안 된다. 2차 인증으로 일시적인 비밀번호인 OTP를 사용하는 2FA를 활성화해야 한다. 아예 비밀번호를 없애버리고 지문, 뇌파 등의 생체인증을 사용하거나, 주변 소음을 통해 주변 환경을 인증해주는 방식도 있다. 그리고 인터넷에 있는 모든 것을 의심해야 하고 안전하다는 생각을 버려야 한다. 아무것도 안 했는데 해킹당하는 경우는 거의 없다. 대부분의 해킹은 안전하다고 생각한 파일을 무턱대고 실행하거나 안전하다고 생각한 사이트를 무턱대고 들어가면서 시작된다. 특히 요즘은 딥러닝을 이용해 여론조작이나 가짜 뉴스를 퍼트리는 방식의 사이버전도 가능하다. 인터넷에 있는 정보들을 그대로 믿지 말고 진위를 스스로 생각할 줄 알아야만 적들에게 휘둘리지 않을 수 있다. 적과 나는 이미 인터넷으로 연결되어있고, 어떤 식으로든 영향을 받을 수밖에 없다. 안전하게 인터넷을 사용하려면 인터넷에 있는 대로 생각하지 말고 스스로 생각할 줄 아는 사람이 되어야 한다.\n","categories":["BOOK","정치사회"]},{"title":"(2017) Attention Is All You Need","url":"/posts/98/","content":"Introduction\nhttps://arxiv.org/abs/1706.03762\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n일명 Transformer 논문. LLM 시대를 열 수 있게 만들어준 기념비적인 논문이다.\nBackground\nTransformer 이전에는 RNN + Attention 또는 CNN + Attention으로 텍스트를 처리했다.\n하지만 RNN, CNN은 구조적인 문제가 있었고, Transformer는 최초로 Attention만 사용한 모델을 만들기로 한다.\n\nRNN: 선형 구조라 병렬화가 불가능하다.\nCNN: 먼 거리의 토큰 연관성을 계산하려면 레이어 O(d)O(d)O(d)개가 (구현에 따라서는 O(log⁡d)O(\\log d)O(logd)개가) 필요하다. (effective receptive field 문제)\n\nArchitecture\n\n사실 Transformer는 기존 방식을(RNN or CNN) 많이 참조하였다.\n\nEncoder는 input sequence (x1,…,xn)(x_1, \\ldots, x_n)(x1​,…,xn​)을 representations (z1,…,zn)(z_1, \\ldots, z_n)(z1​,…,zn​)으로 바꾼다.\nDecoder는 representations (z1,…,zn)(z_1, \\ldots, z_n)(z1​,…,zn​)을 output sequence (y1,…,ym)(y_1, \\ldots, y_m)(y1​,…,ym​)으로 바꾼다.\nOutput token yky_kyk​가 나올 때마다 이전 토큰들 (y1,…,yk−1)(y_1, \\ldots, y_{k-1})(y1​,…,yk−1​)을 input으로 사용한다. (Auto-regressive)\nEncoder, Decoder 모두 residual connection을 사용한다. (이 때문에 모든 layer의 dimension은 똑같다.)\n\nAttention\nScaled Dot-Product attention\nTransformer에서는 scaled dot-product attention을 사용한다.\nTransformer가 정립시키고 난 이후부턴 이 attention만 사용되는 것 같다.\nAttention⁡(Q,K,V)=softmax⁡(QKTdk)V\\operatorname{Attention}(Q,K,V) = \\operatorname{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V\nAttention(Q,K,V)=softmax(dk​​QKT​)V\n먼저 당시 사용되던 attention에는 additive attention과 dot-product(multiplicative) attention이 있었는데, 시간복잡도는 동일하지만 dot-product attention은 행렬 곱으로 구현할 수 있기 때문에 실제 연산 속도는 훨씬 빠르다.\n여기에 추가로 Transformer는 1dk\\frac{1}{\\sqrt{d_k}}dk​​1​로 scaling을 한다.\n당시 차원이 클 경우 additive attention이 dot-product attention보다 성능이 좋다는 결과가 있었는데, 이 논문에선 곱셈으로 인한 magnitude 문제라고 판단하고 scaling으로 보정해준다.\nMulti-Head Attention\n\nTransformer는 1개의 Attention을 사용하는 대신 각 Q, K, V를 h개로 쪼개 h개의 Attention을 구한 뒤, h개의 Attenntion의 linear transformation을 attention 값으로 사용한다.\n여러 개의 head를 사용하면 각 head가 문장의 서로 다른 구조를 학습할 것으로 기대했다고 한다.\nApplications\nTransformer는 (각 layer마다) 총 3종류의 attention이 사용된다.\n\nEncoder self-attention: QKV가 전부 Encoder에서 오므로, 각 position이 이전 layer의 모든 position을 볼 수 있다.\nDecoder self-attention: QKV가 전부 Decoder에서 온다. 하지만 자기 이후에 나오는 토큰들을 보면 안 되므로 softmax 이전에 masking을 하여 보면 안 되는 토큰들의 attention을 −∞-\\infty−∞로 만든다. (softmax 이후에는 0이 된다.)\nEncoder-decoder attention: Q는 Decoder, KV는 Encoder에서 온다. Decoder의 각 position이 encoder의 모든 position을 (즉, 모든 input sequence를) 볼 수 있다.\n\n의외로 encoder-decoder attention은 Transformer에서 처음 고안한게 아니라 seq2seq 모델에서 사용되던 기법이라고 한다.\nWhy Attention?\nTransformer를 만들 때 computational complexity, parallelizability, maximum path length를 고려해서 self-attention을 사용했다고 한다.\n먼저 RNN의 computational complexity는 O(nd2)O(nd^2)O(nd2)으로, Attention의 computational complexity O(n2d)O(n^2d)O(n2d)보다 빠르지만, (논문을 쓸 당시에는 n&lt;dn &lt; dn&lt;d라서 Attention이 더 빠르다고 하긴 했다) parallelize가 안 되고, maximum path length가 O(n)O(n)O(n)이다. 반면에 Attention의 maximum path length는 O(1)O(1)O(1)이다. (즉, 연산 1번으로 long-range dependency를 학습할 수 있음)\n또한 CNN의 computational complexity는 O(knd2)O(knd^2)O(knd2)고, maximum path length는 O(nk)O(\\frac{n}{k})O(kn​)라 Attention의 하위호환이다. Separable convolution의 computational complexity는 O(knd+nd2)O(knd + nd^2)O(knd+nd2)이고, dilated convolution의 maximum path legnth는 O(log⁡k(n))O(\\log_k(n))O(logk​(n))이지만 그래도 Attention이 더 좋거나 똑같다.\n그리고 Attention 모델은 해석하기가 더 쉬운데, attention weight를 보고 각 head가 어떤 단어들이 연관 있다고 생각했는지 파악할 수 있기 때문이다.\n실제로 이를 통해 transformer의 각 head가 서로 다른 문법적/의미적 구조를 파악했다는 것을 알 수 있었다.\n마지막으로, 논문에서 후속연구로 Attention의 computational complexity를 줄이기 위해 주변 r개의 토큰들만 참조해 시간복잡도를 O(rnd)O(rnd)O(rnd)로 줄이는 방법을 설명한다. 하지만 maximum path length가 O(nr)O(\\frac{n}{r})O(rn​)로 늘어나게 된다는 문제가 있고, 일단 Transformer 논문에서는 단순 아이디어 단계에서 그친 것 같다.\nEmbeddings\nWord Embedding\nTransformer는 learned embeddings을 사용했고, embedding layer와 pre-softmax linear transformation에서 같은 weight matrix를 사용하여 input, output의 embedding의 일관성을 챙겼다.\n또한, embedding layer에서 weight에 dmodel\\sqrt{d_{model}}dmodel​​을 곱해주는데, 아쉽게도 논문에 명확한 이유는 나오지 않는다.\nEmbedding vector과 positional encoding를 더하는 과정이 있는데, 두 벡터의 scale을 맞추기 위해서 dmodel\\sqrt{d_{model}}dmodel​​을 곱하는 것으로 추측된다.\nPositional Encoding\nAttention은 position에 대한 정보가 없기 때문에, Transformer는 따로 positional encoding을 더하는 방식으로 position에 대한 정보를 추가했다.\nPE(pos,2i)=sin⁡(pos100002idmodel)PE(pos,2i+1)=cos⁡(pos100002idmodel)\\begin{align*}\nPE_{(pos, 2i)} &amp;= \\sin\\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right) \\\\\nPE_{(pos, 2i+1)} &amp;= \\cos\\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n\\end{align*}PE(pos,2i)​PE(pos,2i+1)​​=sin(10000dmodel​2i​pos​)=cos(10000dmodel​2i​pos​)​\nTransformer는 positional encoding으로 sinusoid를 사용했는데, PEpos+kPE_{pos+k}PEpos+k​를 PEposPE_{pos}PEpos​의 linear function으로 나타낼 수 있기 때문에 relative position을 잘 학습할 것으로 생각했다고 한다.\n이 당시에도 positional encoding을 학습시킬 수 있었지만, Transformer 모델은 sinusoid와 learned positional encoding의 결과가 큰 차이가 없었다고 한다.\nTraining\n영어와 프랑스어/독일어 데이터셋을 사용하였고, byte-pair encoding을 사용했다. (자주 등장하는 토큰 쌍을 묶어서 새로운 토큰으로 취급함)\n훈련은 Adam optimizer를 사용하였고, learning rate는 warmup_steps 까지는 step 수에 선형으로 비례하고, 그 이후에는 1steps\\frac{1}{\\sqrt{steps}}steps​1​에 비례해 점점 감소하게 된다.\n또한, 모델 크기가 크면 learning rate를 줄여 더 오랫동안 학습한다.\nlrate=dmodel−0.5min⁡(steps−0.5,steps⋅warmup_steps−1.5)lrate = d_{model}^{-0.5} \\min(steps^{-0.5}, steps \\cdot warmup\\_steps^{-1.5})\nlrate=dmodel−0.5​min(steps−0.5,steps⋅warmup_steps−1.5)\nRegularization을 위해 각 레이어마다 dropout을 넣고, label smoothing을 했다.\nLabel smoothing을 하면 perplexity는 안 좋아지지만, (확률 분포가 얼마나 맞는지 나타내는 지표), accuracy는 좋아진다. (예측한 가장 확률이 높은 토큰이 정답인 비율)\nResults\n결과는 다들 알다시피 Attention만으로 기존 RNN, CNN 모델들의 번역 성능을 뛰어넘었다.\n논문에선 추가로 Transformer 파라미터를 조정했을 때 결과도 포함되어 있다.\n\n그냥 attention보단 multi-head attention이 더 성능이 좋았지만, head 개수를 너무 늘리면 오히려 성능이 떨어졌다.\nlayer 수, 각 벡터의 차원 등 모델 크기를 늘리면 성능이 더 좋아졌다.\ndropout은 있는게 더 성능이 좋다.\nlabel smoothing은 perplexity가 안 좋아지지만, accuracy가 좋아져 결과적으로 성능이 더 좋다. (유일하게 perplexity, accuracy가 모두 좋아지진 않는 변화다.)\nlearned positional encoding으로 대체했을 때 성능 변화는 거의 없었다.\n\nConclusion\nTransformer는 RNN, CNN을 사용할 필요 없이 Attention만 사용하면 더 학습도 빠르고 결과도 좋다는 것을 증명해낸 논문이다.\n이 논문 이후로 LLM은 모두 Transformer를 기반으로 하게 되었다.\n하지만 논문에서 잠깐 언급하고 지나갔던 Attention의 단점이 점점 부각되게 되었는데, Transformer 시절에는 짧았던 input sequence 길이가, LLM 시대부턴 너무 길어지면서 Attention의 computational complexity O(n2)O(n^2)O(n2)이 문제가 되기 시작했다.\n이 문제는 현재진행형으로, 하드웨어 최적화나 효율적인 Attention 구조 등 다양한 방법으로 해결하려고 노력 중이다.\n","categories":["PAPER","AI","LLM"]},{"title":"Object-Oriented Programming (OOP)","url":"/posts/29/","content":"What is OOP?\nOOP (Object-Oriented Programming) is a way of looking at a software system as a collection of interactive objects.\nObjects\nObjects represents an entity.\n\nPhysical entity: e.g. Car\nConceptual entity: e.g. Bank account\nSoftware entity: e.g. Linked List\n\nClasses\nClass is the blueprint or template of objects.\n\nAttributes: states, properties, etc.\nMethods: ways to interact with the object\n\nOOP concepts\nEncapsulation\nEncapsulation is bundling non-visible attributes/methods with read/write methods to them.\nIt's usually implemented using access modifiers(public/private), setters, and getters.\n\nAbstraction: Hide unnecessary details\nDefensive Programming: Protect data from misuse by the outside world\n\nInheritance\nInheritance is making specialization and extension from existing modules/systems.\nIs-a relationship indicates inheritance.\nHas-a relationship indicates composition.\n\nModularity: Can provide extended/modified functionality without breaking old methods (Can adopt a much more incremental approach!)\nResuability: Can create new class from existing class\n\nTimeliness: Faster development time\nReliability: Existing class has testing &amp; validations\nConsistency: A strict emphasis on regular, coherent design\n\n\n\nCircle-Ellipse Problem\nclass Ellipse&#123;  float axisX, axisY;  Ellipse(float x, float y)&#123;axisX = x; axisY = y;&#125;  public void stretchX(float scaleX)&#123; axisX *= scaleX; &#125;  public void stretchY(float scaleY)&#123; axisY *= scaleY; &#125;&#125;class Circle extends Ellipse&#123;  Circle(float radius)&#123; super(radius,radius); &#125;&#125;\nCircle is an Ellipse, but Circle.stretchX loses its characteristic as a circle!\nOOP is not the best way...\nInterface and Abstract class (in Java)\nInterface and abstract class can have abstract method - method without body.\nInterface can have multiple inheritance, but can't extend classes.\nAbstract class can extend class and implement inheritance.\nInterface provides a common interface between several unrelated classes.\nAbstract class provides a blueprint for closely related classes.\nPolymorphism\nPolymorphism provides a way to perform a single action (with the same name) in different ways.\nOverloading\nClass can have two or more methods with same name but different signatures!\nOverriding\nSubclass can redefine an inherited instance method!\nHiding (not polymorphism)\nSubclass can hide superclass's static method, static variable, instance variable.\nOverridden method can't be accessed by casting to superclass, but hidden method/variable can be accessed by casting to superclass.\n","categories":["SNU","1-2","컴퓨터프로그래밍"]},{"title":"SOLID principles","url":"/posts/30/","content":"SOLID principles\nSOLID is a OOP design principles proposed by Robert C. Martin in his paper Design Principles and Design Patterns. (2000)\nSingle responsibility principle (SRP)\nA class should handle one and only one job.\ni.e. a class should have one and only one reason to change.\nOpen-Closed principle (OCP)\nObjects or entities should be open for extension, but closed for modification.\nLiskov substitution principle (LSP)\nEvery subclass should be substitutable for their superclass.\nFormally, if q(x)q(x)q(x) is a property provable about objects of xxx of type TTT, then q(y)q(y)q(y) should be provable for objects yyy of type SSS where SSS is a subtype of TTT.\nInterface segregation principle (ISP)\nClients shouldn't be forced to depend on methods they do not use.\ni.e. clients should never be forced to implement an interface that it doesn't use.\nDependency inversion principle (DIP)\nEntities must depend on abstractions (e.g. interface) not on concretions (e.g. class).\nHow do I do this???\nSolution: split your code! (into interfaces or classes)\n","categories":["SNU","1-2","컴퓨터프로그래밍"]},{"title":"제1강 그리스로마신화의 특징과 의미","url":"/posts/15/","content":"강의를 시작하며\n소크라테스의 변명\n김헌 교수님은 사실 학부 때 불어교육과였음\n하지만 학부때 독후감 과제였던 플라톤의 &lt;소크라테스의 변명&gt;을 보고 서양철학에 빠져들게 되고, 서양고전학과로 전공을 바꾸게 됨\n소크라테스는 알다시피 서양철학에 매우 큰 영향을 준 인물이고, 플라톤은 소크라테스의 제자로서 소크라테스의 이야기를 책으로 남겼는데, 특이하게도 소크라테스가 주인공인 연극 형식으로 책을 썼음\n소크라테스의 주제: 어떻게 사는 것이 올바른 삶인가? &quot;어떻게 하면 인간이 덕(Arete)을 실현해 인간답게 살 수 있을까?&quot;\n소크라테스 방식: 일단 질문함 -&gt; 상대방이 대답함 -&gt; 소크라테스 반박 -&gt; 데꿀멍 -&gt; 너도 알못인데 같이 합심해서 진리를 탐구해보자\n이상: 아 나도 무지했구나 같이 진리를 탐구해보자\n현실: 점마 거지같네 -&gt; 교모한 말재주로 청년들을 타락시키고 전통적인 신(제우스, 아테네)들을 부인하는 종교적인 이단이다\n그래서 소크라테스를 싫어하는 사람들이 많았고 결국 고발되어 재판에 서게 됨\n당시 아테네는 검사/변호사라는 직업이 없었고 분쟁 당사자가 직접 검사/변호사가 됐어야 함;\n그래서 소크라테스가 직접 자신은 선량한 아테네 시민이고 그냥 알고싶은게 많은 진리를 추구하는 철학자일 뿐이라고 변호하는 내용을 이걸 책으로 쓴게 소크라테스의 변명\nTMI) 소크라테스는 본인이 직접 글을 쓴 적이 없고 대화만 했음;;\n하지만 소크라테스한테 영향을 받아 같이 대화하면서 진리를 탐구하는 사람들이 많았음 (이들을 소크라테스의 제자로 부름)\n그러나 소크라테스가 직접 글을 쓴 적은 없기 때문에 이게 정말 옮겨적은건지 아니면 플라톤이 제멋대로 각색한건지는 논란거리임\n하여튼 플라톤 피셜 소크라테스 왈: &quot;숙고하지 않는 삶은 살 가치가 없습니다&quot;\n숙고하는 삶이란, 어떤 주어진 명제를 금과옥조로 여기고 계속 되뇌며 신념을 확고히 하는 사람이 아니라, 질문을 던지는 삶.\nㄴ 이게 잘한 일일까? 못한 일일까?\nㄴ 내가 어떤 행동을 했지?\nㄴ 내가 옳은 행동을 했나?\nㄴ 옳은 행동이 뭐지?\nSophos와 Philosophos\nPhilo: 사랑하는\nsophos: 지혜로운 사람(sopia: 지혜/지식)\n숙고하는 삶을 사는 사람은 질문을 던지고 답을 찾아 나가는 사람, 즉 '지혜를 사랑하는 사람(philosophos)'이다.\n근데 왜 지혜로운 사람이 아님?\n플라톤의 &lt;파이드로스&gt;에서 소크라테스 &amp; 파이드로스의 대화에서 나온 얘기:\n만약 연설가/시인/정치가가 진실을 알고서 자신들의 작품을 구성했다면, 그리고 그들이 쓴 것에 대해 가해지는 논박을 잘 막아내고 자신을 지킬 수 있을 때 philosophos\n근데 왜 지혜로운 사람(sophos)이라고 안 함? 이건 신에게만 적합한 말이라고 생각했음;;; 사람은 philosophos 정도가 맞다\nPhilokalos\nkalos: 아름다움\n근데 그리스에서 아름다움은 외모만 말하는게 아니라 내 말, 태도, 사고방식, 사상 등도 다른 사람들에게 아름답게 느껴지도록 가꿔나가야 한다는 것을 말함\n플라톤의 &lt;향연&gt;: &quot;인간의 삶이 살 가치가 있는 것은 아름다움 바로 그 자체를 바라보면서 살 때입니다.&quot;\n플라톤의 &lt;파이드로스&gt;: &quot;참된 것들을 가장 많이 본 영혼은 장차 '지혜를 사랑하는 사람(Philosophos)' 또는 '아름다움을 사랑하는 사람(Philokalos)' 또는 '음악적 재능이 있는 사람(Musikos)' 또는 '사랑에 충만한 사람(Erotikos)'의 종족에 깃들 것입니다.\n근데 왜 여기에 음악이 낌? 그 당시에는 지혜, 지식, 정보를 음악적인 운율에 담아서 기억하기 좋게 만들었기 때문에 철학자가 음악도 했음;;\n사랑은 왜 낌? 여기서 사랑은 에로가 아니라 무언가를 갈망하고 열정을 가진 사람임\n페리클레스가 스파르타와 전쟁 전 아테네인들에게 자부심을 불러일으키기 위해 한 연설에도 있음\n투퀴디데스의 &lt;펠로폰네소스 전쟁사&gt;: &quot;우리는 아름다움을 사랑하되(philokaloumen) 소박함이 있고, 지혜로움을 사랑하되(philosophoumen) 유약함은 없습니다. 우리는 부를 일의 적절한 수단으로 사용하고, 말로 자랑할 대상으로 사용하지 않습니다. 우리는 가난을 수치라 여기지 않고 오히려 가난에서 벗어나려고 일하지 않음을 수치로 여깁니다.&quot;\nPhilomuthos\n아니 그럼 철학과나 갈것이지 뭔 놈의 그리스 로마신화여??\n아리스토텔레스의 &lt;형이상학&gt;: &quot;'신화를 사랑하는 사람(Philomuthos)'은 어떤 뜻에서는 '지혜를 사랑하는 사람(Philosophos)'인데, 그 까닭은 신화가 놀라운 사건들로 이루어져 있기 때문이다.&quot;\n그 당시 그리스 로마신화는 만화책 수준의 지위가 아니었음;; 무려 신화를 철학 수준으로 취급함\n사람들은 대상에 대한 무지 때문에 놀라고, 놀라움에서 궁금증이 생기며, 궁금증을 풀기 위해 탐구한다. -&gt; 이런 사람이 바로 Philosophos\n신화는 세상의 수많은 현상들과 인간들의 운명에 관한 놀라움과 궁금증에서 시작하여 지어진(Poiein) 이야기(Muthos)다. -&gt; 즉 신화는 논리를 만들어낸 철학과 달리 이야기를 만들어냈을 뿐 궁금증을 풀기 위해 탐구한 것은 똑같음\n신화는 철학 이전의 철학\n월터 옹 (대충 유명한 사람임): 문자가 발명되어 상용화되고 역사와 철학이 본격적으로 시작되기 전, 사람들은 세상을 이해하고 삶을 살아갈 수 있는 지식과 정보, 지혜를 신화 속에 담아 기억에서 기억으로, 입에서 입으로 전하면서 다듬고 보전하고 전승하였다. 신화는 지식과 정보와 지혜의 보물 창고이며, 철학 이전의 철학, 역사 이전의 역사였고, 철학과 역사 이후에도 또 다른 결을 가진 철학으로 존속했다.\n즉 월터 옹 생각대로면 인간의 지적 활동과 생산, 보전과 전승의 단계는 구술문화, 문자문화, 디지털 문화로 나뉘고, 문자가 없던 시절에 신화를 통해 지식, 정보를 전달했고, 문자가 생기면서 신화에서 멀어지고 역사, 과학, 철학이 생겨났다고 함.\n'신화를 위한 우화'\n막간 책 홍보(아님): 김헌의 &lt;그리스 로마 신화&gt;\n우리가 사는 세상을 배로 비유 -&gt; 배 탔는데 사람들이 말하는 배 행선지가 다 다름; 근데 막 사람들이 사라지기도 하는데 이것도 얘기가 다 다름; 바다로 빠지는거임? 다시 돌아온다고 함? 다른 개쩌는 배로 간다고 함? 하지만 전부 소문이고 증명할 수 없었음 -&gt; 그럴바엔 진리에 얽매이지 말고 이 배 자체를 즐겨라\n내가 이 배 안에 있다는 사실은 확실하다! 라고 생각했지만.... 아바타였다면? 매트릭스라면? 여기가 배가 아니라면? 진짜 나는 어디있는거지? -&gt; 결국 사람들은 소문들에 미쳐버리고 각자 원하는 소문을 기둥으로 삼아서 사는 것 같음 (여담: 잘만 살면 좋겠는데 꼭 내 기둥이 옳다고 남의 기둥 무너뜨려리는 사람이 있음) 하지만 모든 소원을 물리칠 수 있는 진리를 바라는 것은 다 똑같음....\n믿을만한 이야기를 찾기 위해 사람들은 자신의 이야기를 지어 나가며 살고 있음 (or 쓰여져 있는 연극대로 따라하는 중?) 우리 이전에 살던 사람들도 낯선 세계와 무서운 현상과 허무하기 그지없는 삶을 이해하고 값진 의미를 부여하기 위해 이야기를 짓다가 어다론가 사려졌다 -&gt; 이렇게 만들어진게 그리스로마신화 -&gt; 우리도 그리스로마신화를 읽으면서 자신의 이야기를 채워나가지 않을까\n\n&quot;즉, 인문학적인 행위를 한다는 것, 그게 무엇인지 여러분들이 생각할 수 있는 시간이 되었다면 좋겠습니다.&quot; - 김헌\n\n문 풍 당 당\n이과는 웁니다 ㅠㅠ\n그리스로마신화의 특징\n대체 왜 그리스 로마 신화만 이렇게 유명할까?\n그 전에 신화는 누가 만들었을까? -&gt; 알 수 없음\n그래서 보통 신화는 집단무의식의 창작, 즉 한 민족, 한 국가의 모든 사람들이 다듬고 전승한 것이라고 얘기함. (난 안 했는데요? 하지만 너가 입에서 입으로 전승해주면서 신화는 만들어졌음)\n그리스 로마 신화도 마찬가지지만... 다름!\n장 피에르 베르낭: 그리스의 경우, 신화는 대게 문자화된 텍스트 형태로 오랜 세월 동안 우리에게 전달되었으며, 그 중 가장 오래된 것들은 서사시, 서정시, 비극, 역사 또는 철학 등과 같은 문자화된 작품들에 속한다.&quot;\n아폴로도로스의 &lt;신화집&gt;: 그 이야기들은 대부분의 경우에는 단편적이고 종종 암시적인 방식으로 분산된 모양새를 띤다. 나중에, 서기 년도가 시작될 무렵에 석학들이 다소 간은 서로 일치하지 않고 분산된 수많은 전통들을 수집하였다. 그것들을 하나의 동일한 전집 안에 통합하여 그야말로 '도서관'의 서가 위에 차례차례로 정돈하여 보여주기 위해서였다. 그 때문에 아폴로도로스가 자신이 작성한 (그 이야기들의) 목록에다 붙인 바 있는 그 제목('도서관')을 다시 취했던 것이며, 그 분야의 위대한 고전들 가운데 하나가 되었다. 이렇게 그리스 신화라고 부르는 것이 관례가 된 것이 구축되었다.\n즉, 아폴로도로스도 그리스 로마신화 정리할 때 주워들은게 아니라 문자화된 텍스트를 읽고 정리한거였음!!\n보통 신화는 집단의 기억력과 전승에 의해서 보존되어왔음. 하지만 그리스 로마신화는 천재적인 작가들의 문학작품 속에서 새롭게 구성되고 진화하였으며, 위대한 문학적 성취로 보존되고 전승되었음.\n즉, 다른 신화들과 달리 작가가 알려져 있으며, 이야기 형태인 다른 신화와 달리 텍스트로 남기다 보니까 표현이 복잡해지고 멋을 부리고 아름다워지면서 문학작품 형태를 가짐.\n신화적 상상력\n인문학의 세 분야\n\n\n\n분야\n정의\n질문\n가치\n\n\n\n\n철학\n개별적인 개체들의 보편적 개념화\n인간은 무엇을 해야 하는가?\n도덕적, 윤리적 당위성\n\n\n문학\n인간의 가능세계를 상상\n인간은 무엇을 할 수 있는가?\n가능성에 대한 상상력\n\n\n역사\n인간이 했던 사실의 기술\n인간은 무엇을 했는가?\n사실에 대한 실증성\n\n\n\n아폴로도로스의 &lt;아르고호의 모험&gt;\n아폴로도로스는 알렉산드리아 도서관장으로 활동함; 당시 알렉산드리아 도서관은 모든 지식과 지혜를 모으려고 하고 있었음\n그 중 하나가 황금 양털을 찾아 떠나는 이아손의 이야기임\n왕이였던 아이손이 삼촌한테 배신당하고 쫓겨남. 덩달아 왕자인 이아손도 쫓겨남; 근데 이아손이 삼촌 다시 찾아와서 왕 다시 달라고 함;;;;;\n삼촌이 대충 쫓아내려고 황금 양털 가져오라고 함; 근데 황금 양털은 콜키스를 수호하는 역할을 하고 있어서 당연히 콜키스에서 안 주려고 했음;;; 그래서 그냥 양털 얻기 빡세니까 삼촌 쿠데타 하려다가 이러면 너무 삼촌같다고(?) 아르고 호 만들어서 황금 양털 가져오려고 감\n그리스 신화에서의 영웅: 신과 인간의 자식\n황금 왕털이 있는 콜키스의 왕 아이에테스도 영웅이였음 (영웅같은 짓은 안 했고 그냥 신과 인간의 자식이라 영웅)\n물론 아이에테스는 황금 왕털 안 준다고 했지만 아이에테스의 딸인 메데이아 공주가 사랑에 빠져서 황금 양털 준다고 빼돌림;\n근데 왜 갑자기 사랑에 빠진거임??? -&gt; 이 과정을 묘사할 때 신화적 상상력이 들어감\n\n그녀는 계속해서 빛나는 눈길을 아이손의 아들에게로 마주 던졌고, 그녀의 현명한 마음은 가슴 속에서 고통으로 흔들렸으며, 어떤 기억도 남지 않고, 달콤한 괴로움에 정신이 녹아 내렸다. 그리고 그녀의 &quot;부드러운 뺨은 창백하게, 어떤 때는 붉게, 마음 속 고뇌를 좇아 변하였다.&quot;\n\n역사와 달리 작가의 상상력을 첨가하여 심리적 변화를 묘사함 아니 걍 완전 창작이네\n여기까진 사실 그냥 일반적인 문학작품; 그리스가 아닌 다른 나라에서도 이정도 문학작품은 많았음\n\n그 사이 에로스는 회색 구름에 가리어 보이지 않게 다가왔다... 곧 그는 현관의 문설주에 기대어 활에 시위를 얹고, 화살 통에서 아직 쓴 적이 없는, 많은 고통을 불러일으키는 화살을 꺼냈다. 그러고는 빠른 발로 몰래 문지방을 넘었다. 날카롭게 주위를 살피며, 그래서 아이손의 아들에게 가까이 접근하여, 화살 오뉘를 시위 중간에 먹였고, 곧바로 두 손으로 버티고 당겨 메데이아에게 쏘아 보냈다. 그러자 매혹이 그녀의 가슴을 사로잡아 말문을 막았다. 그는 지붕 높은 방으로부터 날개 쳐 돌아, 즐거워하며 날아 나갔다. 하지만 화살은 소녀의 심장 밑에서 불꽃같이 깊숙히 타 들어갔다. 그녀는 계속해서 빛나는 눈길을 아이손의 아들에게로 마주 던졌고, 그녀의 현명한 마음은 가슴 속에서 고통으로 흔들렸으며, 어떤 기억도 남지 않고, 달콤한 괴로움에 정신이 녹아 내렸다... 그와 같이 파괴적인 에로스는 남몰래 가슴 밑으로 기어들어 타올랐다. &quot;부드러운 뺨은 창백하게, 어떤 때는 붉게, 마음 속 고뇌를 좇아 변하였다.&quot;\n\n그리스 로마 신화의 특징: 신화를 동원하여 사태를 설명함 이제 역사는 온데간데없음\n이런 방식은 그리스 로마 신화에만 있었음!\n신화란 무엇인가?\n조지프 캠벨의 &lt;네가 바로 그것이다&gt;: 전체적으로 신화란 상징적인 이미지들과 이야기들을 조합해 놓은 것이며, 인간 경험의 가능성들에 대한 은유이고, 특정한 시대에 이루어진 특정한 문화적 성취입니다.\n에로스 같은 신들은 상징적인 이미지에 이야기를 결합시킨 것!\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제2강 최초의 신들과 신들의 권력 투쟁","url":"/posts/16/","content":"그리스어 공부\n\nPhilobiblos\n책을 읽기 좋아하는 사람\nPhilosophos\n지혜를 사랑하는 사람\nPhilotimos\n명예를 사랑하는 사람\nPhilopatris\n조국을 사랑하는 사람\nPhilologos\n(논리적이고 의미가 잘 통하는) 말을 사랑하는 사람\nPhilomathes\n배우기를 좋아하는 사람\nPhilokhrematos\n돈만 밝히는 사람\nPhilokalos\n아름다움을 사랑하는 사람\nPhilokosmos\n질서를 사랑하는 사람/장식하기를 좋아하는 사람 (후자로 쓰이면 허언기가 있는 사람 등 부정적으로 쓰이기도 함)\nPhilonikos\n승리를 좋아하는 사람\nPhilomuthos\n신화를 사랑하는 사람\nPhiloinos\n술을 좋아하는 사람\n\n최초의 신 카오스\n카오스(Khaos/Chaos)\n헤시오도스(BC 8-7세기 사람)의 &lt;신통기(Theogonia)&gt;: 태초에 가장 먼저 카오스(Khaos/Chaos)가 생겨났다.\n신통기는 신의 계통을 기록한 거라는 뜻인데 다들 신통방통하다고 해서 최근엔 신들의 계보로 책 제목을 바꿈;;;\n참고) Theo: 신, gonia: 탄생, 족보, 혈통\n카오스는 요즘엔 혼란, 혼돈이라고 쓰이지만 원래 뜻은 아무 것도 없는 텅 빈 공허라는 뜻임\n왜 이딴게 먼저 생겨남? -&gt; 모든 것들이 자리잡을 수 있고, 모든 것들을 품을 수 있는 공간이 생겨났다는 뜻\n또한 태초에(ex arkhes) -&gt; 처음이 있는 세계관임을 보여줌 (우주에는 처음과 끝이 없다는 식의 세계관도 존재함)\n아리스토텔레스의 아르케(arkhe/arche) 정의: 그 앞에는 아무것도 없고, 그 뒤에는 무언가가 있는 것\n반대로 끝은 그 앞에는 무언가가 있고, 그 뒤에는 아무것도 없는 것으로 정의했고, 중간은 그 앞에도 그 뒤에도 무언가가 있는 것으로 정의함.\n카오스 다음에는 가이아, 에로스, 타르타로스가 태어났음.\n성경과의 비교\n창세기: 태초에 신께서 하늘과 땅을 창조하셨다.\n근데 어디다가 하늘과 땅을 창조했다는거임??\n그리스 사람들은 이걸 논리적이지 않다고 생각했음;;; 그래서 공간부터 먼저 만들고 시작.\n카오스에서 코스모스로?\n기독교의 반박: 아무것도 없는 상태에서 어떻게 무언가가 생겨날 수 있음?? 아니 뭐가 있어야 생겨나지;;\n사실 그리스인들도 맞는 말이라고 생각해서 고민했었음;;; 둘이 뭐하냐\nEx nihilo nihil fit!: 아무것도 없는 곳에서는 아무것도 생겨날 수 없다! (무에서 유가 생겨날 수 없다)\n로마의 시인 오비디우스: 카오스가 완전 텅 빈 공간인건 아니고 뭐가 들어있는 혼돈이라고 생각해야할 듯? 모든 것들이 뒤엉켜있는 상태가 유지되고 있었는데 어떤 신비한 조화로운 손길에 의해 혼돈(카오스)에서 질서(코스모스)로 변신(metamorphosis)했다고 생각함.\n그리스 신화: 아무것도 없는 텅 빈 공간에서 하나씩 하나씩 생겨났다고 함\n로마 신화: 태초에 엉켜있는 혼돈의 상태에서 점점 질서를 잡아갔다고 함\n여담) 코스모스도 카오스처럼 많이 쓰이는 개념인데 미국의 천체물리학자 칼 세이거는 우주를 질서가 잡혀있다고 생각했는지 책 제목을 &lt;코스모스&gt;라고 지음\n미국의 수학자 애드워드 로렌츠는 '나비효과'를 설명하면서 무질서해보이는 것들도 나름의 질서가 있다고 설명함\n카오스와 야누스\n로마 신화에선 카오스는 신이 아니라 그냥 혼돈 그 자체임;;;;\n그럼 카오스에 해당하는 로마의 신은 뭐임?\n오비디우스의 &lt;로마의 축제일&gt; 1월: 카오스(그리스 신)의 로마 버전은 야누스(로마 신)다!\n참고로 12월까지 쓰려고 했는데 중간에 그만둠;;\n야누스는 문의 신(문의 라틴어는 lanua, 야누스는 lanus)이며 지혜의 신\n한 해의 문을 여는 달이 야누스의 달(lanuarius Mensis) -&gt; 여기서 January가 나옴\n\n로물로스가 로마를 세웠을 때, 적들이 침략할 때마다 야누스 신이 도와주었다. 전쟁이 터지면 야누스의 문을 열고, 평화가 찾아오면 야누스의 문을 닫았다.\n\n야누스의 문은 지금도 로마에 있음\n문은 안쪽과 바깥쪽을 향하는 두 얼굴이 있기 때문에 야누스도 두 얼굴을 가지고 있음; 자신의 안과 밖을 두루 성찰한다고 해서 지헤의 신이라고 불림\n또는 미래를 바라보는 얼굴과 과거를 돌아보는 얼굴로 설명하기도 함; 지혜는 과거를 반성하고 통찰함으로써 미래를 직시하는 것\n야누스는 카르나(경첩의 신)과 결혼해 티베리누스를 자식으로 가짐 문과 경첩은 좀;;\n경첩은 문을 열게 할 수 있는 상징적인 의미\n티베리누스는 강이 되어서 아니 신들의 자식인데 왜 나만 신이 아니라 강이야 로마를 지켜줬다고 함\n카오스의 자손들\n기울인 애들은 여신\n카오스(공허)\n-&gt; 혼자서 에레보스(어둠)과 뉙스(밤)을 낳음; 남자인데 자웅동체\n-&gt; 이 둘이 또 애들을 겁나게 낳음;;;;\n근데 카오스가 에레보스, 뉙스를 낳기 전에는 무슨 상태인건가? 밤도 낮도 없었는데 어둡지도 환하지도 않은 상태인건가?? -&gt; 김헌 교수님은 안개가 낀 상태로 생각하심\n\n아이테르(천공)\n헤메라(낮)\n모로스(운명)\n케르(급사)\n타나토스(죽음)\n휘프노스(잠)\n오네이로스(꿈)\n클로토\n라케시스\n아트로포스\n네메시스(응보/복수)\n아파테(기만)\n필로테스(우정)\n게라스(노령)\n에리스(불화)\n\n클로토, 라케시스, 아트로포스는 운명의 신이라고 함; 그리스 사람들은 운명이 이미 정해져있다고 생각했음\n\n클로토는 운명의 실을 잣는 신\n라케시스는 잣아놓은 운명의 실을 나눠주는 신\n아트로포스는 운명의 실을 잘라서 돌이킬 수 없게 만드는 신\n\n에리스는 또 혼자서 자식을 겁나게 낳았음 여자인데 자웅동체\n\n포노스(노고)\n레테(망각)\n리모스(기아)\n알고스(고통)\n휘스미네(전투)\n마코스(싸움)\n포노스(살육)\n안드로크타시아(살인)\n네이코스(다툼)\n프세우도스(거짓말)\n로고스(말)\n암피로기아(말다툼)\n뒤스노미아(무법)\n아테(미망)\n호로코스(맹세)\n\n그리스 사람들은 불화때문에 일어나는 일들을 불화의 여신 에리스가 여러 자식을 낳았다는 식으로 표현한거임!\n여담: 로고스는 아까 논리래면서요 그래서 처음에 로고스는 아첨, 아부 같은 뜻으로 해석했었음\n이와중에 뉙스가 혼자서도 자식을 낳음!!\n\n모모스(비난)\n오이쥐스(고통)\n헤스페리데스 3자매\n\n헤스페리데스는 서쪽을 지키는 요정들이라고 불림 (해가 서쪽으로 지니까 밤과 서쪽이 관련있다고 생각한듯)\n카오스의 자식들은 인간의 어두운 측면들을 가리키는 신들이라고 생각하면 될듯\n그 이후의 원초적 신들\n태초에 카오스(공허)가 있었는데 이후에 가이아(대지), 타르타로스, 에로스(사랑) 세 신들이 그냥 태어남 (에로스는 좀 늦게 태어남)\n가이아는 태초의 네 신 중에서 유일하게 여신인데 세 자식들을 낳음\n\n우라노스(하늘)\n우레아(산)\n폰토스(바다)\n\n가이아 이론\n영국의 과학자, 환경학자, 미래학자인 러브락(James Lovelock)은 지구는 하나의 생명체 같은 유기체로서 움직인다고 주장함 말이 안 되니까 주장을 신화에서 끌어옴\n인간은 지구 환경을 멸망시키는 존재 (팩트임)\n그래서 땅을 무겁게 하고 더럽히는 인간을 쓸어버리려고 했던 가이아 여신처럼 지구도 우리를 쓸어버릴 것이다..?\n타르타로스\n헤시오도스의 &lt;신통기&gt;: &quot;넒은 길의 대지 깊은 곳에 어두컴컴한 타르타로스도 생겨났다.&quot;\n하늘 꼭대기에서 대지까리의 거리만큼이나 대지에서 타르타로스의 바닥까지도 멀게 떨어져 있다는 얘기도 있음\n즉 타르타로스는 지하세계를 지배하는 신!\n신들 사이의 투쟁에서 승자는 하늘과 땅 사이에 자리잡았지만, 패자는 어둠의 장소인 타르타로스로 내려감\n또한 스튁스 강에 맹세하고 지키지 않은 신들도 타르타로스에서 1년 동안 금식하며 갇혀 지냈음\n약간 감옥같은 역할....?\n여담) 베드로후서 2장 4절: &quot;하나님의 범죄한 천사들을 용서하지 않고 타르타로스에 던져 어두운 구덩이에 두어 최후의 심판 때까지 지키게 하셨다.&quot;\n아니 기독교에서 그리스 로마신화가??? 뿌슝빠슝\n사실 원래 타르타로스는 신을 가리키는 단어였지만 이게 지하공간을 가리키는 단어가 되면서 성경에도 무사정착했다고 함\n저승세계와 타르타로스?\n호메로스는 땅이 하데스로 떨어진 곳에 타르타로스가 있다고 함\n플라톤은 인간이 죽으면 타르타로스의 입구 앞에서 최후의 심판을 받은 다음 그곳으로 들어간다고 함\n로마의 시인 베르길리우스도 죽은 자들의 혼백이 머무는 저승세계를 타르타로스라고 함\n엄밀히 따지면 하데스랑 타르타로스는 다른게 맞긴 한데, 둘을 거의 같은 곳으로 이야기한 사람들도 많았음\n세상을 움직이는 힘, 에로스\n에로스는 불사신들 가운데 가장 아름답고, 사지를 풀리게 하는 힘을 가지고 신과 인간들의 이성과 현명한 의지를 압도한다고 노래함. (참고로 모든 그리스 신은 불사신)\n근데 후대의 많은 작가들은 에로스(=쿠피도=아모르)가 아름다움의 여신 아프로디테(=베누스)와 전쟁의 신 아레스(=마르스)의 아들이라고 함. 아니 나 나름 태초신인데\n희극작가 아리스토파네스(기원전 5~4세기)의 &lt;새&gt;: 태초에 카오스, 뉙스, 에레보스, 타르타로스만 있을 때, 뉙스가 에레보스의 품속에서 알을 낳았는데, 그 알에서 에로스가 태어났다고 함.\n플라톤은 아프로디테가 태어났을 때 신들이 성대한 잔치를 벌였고, 그곳에 참석했던 궁핍의 여신 페니아가 술에 취한 방책의 신 포로스와 결합하여 에로스를 낳았음.\n사람마다 이야기가 다름;; 점점 격이 떨어지는 중\n신들의 권력 투쟁 1부\n최초의 권력투쟁\n헤시오도스는 &lt;신통기&gt;에서 세상을 처음으로 지배한 신이 카오스고, 이후에 가이아와 세 자식들이 카오스를 밀어내고 세상을 지배했다고 함. -&gt; 그래서 땅이 세상의 중심에 자리를 잡았고, 하늘, 바다, 산이 자신을 보호하고 감싸는 질서가 세워졌다고 함.\n근데 이후에 우라노스가 어머니가 정해놓은 틀에 갇히지 않고 어머니를 딛고 일어서서 새로운 질서를 만들고 권력을 차지함;;;; 이걸 신들 사이에서 벌어진 최초의 권력투쟁이라고 함. (참고로 그리스 로마 신화에서는 자식이 부모 엿먹이는 경우가 많음)\nQ. 근데 카오스랑 가이아도 있었잖아요\nA. 그건 투쟁이 아니라 그냥 스윽 밀어내고 스윽 밀려난겨(??)\n우라노스와 가이아의 결합\n근데 이 이후에 우라노스가 가이아랑 결혼함???? 심지어 애도 개많이 낳음\n\n12명의 티탄산족 (오케아노스, 코이오스, 크레이오스, 휘페리온, 아이페토스, 크로노스, 테튀스, 포이베, 테미스, 테이아, 므네모쉬네, 레아)\n퀴클롭스(외눈거신) (브론테스(천둥), 스테로페스(번개), 아르게스(벼락))\n헤카톤케이르(백수거신) (콧토스, 브리아레오스, 귀게스)\n\n우라노스가 티탄산족 낳고 보니까 엄마 밀어냈는데 엄마가 애들 데리고 복수해서 권력 다시 빼앗기는거 아니냐고 걱정함 그러면 애는 왜 낳는데 미친놈아\n근데 애를 또 낳으니까 눈알 하나인 퀴클롭스들이 나옴;;; 얘들은 진짜 무서워서 좀 쫄음\n그래도 참고 아니 그니까 왜 참는데 애를 또 낳았는데 이젠 손이 백개인 헤카톤케이르들이 나옴;;;;\n이건 진짜 안되겠다 싶어서 우라노스는 낳은 애들을 전부 땅 속에 가둬놓고 더 이상 애를 낳지 않음\n이를 우라노스의 폭력이라고 함.\n여담) 새로운 세대를 자신의 틀 속에 가두려는 기성세대의 특징을 신화적 상징으로 보여준 것이라고 함\n여담) 근데 가이아가 땅의 여신이잖아 -&gt; 그래서 땅 속에 가둬놓은 것을 가이아의 자궁 속에 다시 넣은거라고 보기도 함??!?!!?!?!?! 그래서 가이아는 고통스러웠다고 함????????\n진짜여담) 헤시오도스의 &lt;신통기&gt;는 가이아의 깊숙한 곳이라고 묘사함\n크로노스의 도전\n가이아가 자식들을 모아서 선동함(사실 선동이 아니라 팩트임)\n\n니들 자궁 속에 있는거 갑갑하지 않냐?? 이놈의 니 아빠는 사실 내 아들인데 나는 몰아내고 지가 땅을 지배하고 있고 니들은 낳고선 땅속에 쳐박아놨다! 그러니까 니들 중에 아무나 나서서 아빠 좀 몰아내고 새로운 세상을 만들어라!\n\n근데 이미 땅에 쳐박을 정도의 권력과 힘을 가진 아버지한테 다들 PTSD가 걸려서 아무도 안 나서려고 함 사실 가이아도 안 나섬\n이 때 티탄신족 중에서 제일 막내인 크로노스가 나서는데, 가이아가 너는 용기가 있다면서 아다만트로 만든 낫을 주면서 이걸로 밤에 아빠가 어둠을 끌어내리면서 땅으로 내려오면 찍으라고 함...\n크로노스는 가이아가 준 낫으로 아빠를 거세시켜버림;;;; 고자가 된 우라노스는 멀리 달아나버리고 크로노스가 새로운 권력자가 됨\n우라노스의 저주와 새로운 신들의 탄생\n헤시오도스의 &lt;신통기&gt;: &quot;그들을 아버지는 티탄신족(Titenas)이라는 별명으로 불렀다.(사실 이때부터 티탄신족이라고 불렀음) 위대한 우라노스는 직접 낳은 자식들을 꾸짖으면서, 그리고 말했다. 사악한 마음으로 '손을 뻗어(titainontas)' 엄청난 짓을 저질렀으니, 다음으로 그들에게도 징벌이 훗날 있으리라.&quot;\n쉽게 말해서 너도 자식한테 거세당하라고 저주함\n티탄신족은 의외로 거신이라는 뜻이 아니라 (기성세대에게) 손을 뻗는 자, 즉 저항적이고 혁신적이라는 뜻임.\n우라노스를 거세하면서 떨어진 남근이 정액이랑 피가 섞인 채로 쓸데없이 묘사가 자세하다 땅에 닿았는데 이때 신들이 태어남?????!?!!?!?!?! (아마도 땅이 가이아 여신이라서...? 그래서 얘들을 가이아의 자식으로 봄....)\n이떄 태어난 신들이 에리뉘스(복수의 여신: 알렉토, 티시포네, 메가이라)들, 기가스들(거신족), 메리아(물푸레나무)들이 태어남...\n심지어 크로노스는 거세된 남근을 잡고 바다에 던져버렸다는데, 이때 남근 주변에 하얀 거품(aphros)이 일어나면서 아프로디테가 태어났다고 함...... (사실 아프로디테의 뜻은 거품에서 태어난 자임) 이때 아프로디테가 도착한 섬이 퀴프로스임.\n이 개막장 스토리를 인정하지 못한 호메로스와 베르길리우스 등 다른 작가들은 아프로디테가 제우스와 디오네 사이에서 태어났다고 함.\n플라톤은 &lt;향연&gt;에서 아예 아프로디테는 두 명이 있다면서 남근에서 태어난 '아프로디테 우라니아'(천상의 아프로디테)와 제우스와 디오네 사이에서 태어난 '아프로디테 판데모스'(지상의 아프로디테)가 있다고 했고, 각각 사랑의 두 종류인 고결한 사랑과 육체적인 사랑을 상징한다고 함.\n참고로 로마에서는 남근 얘기는 생략하고 그냥 매력의 신으로 나옴...\n로마의 시인 카툴루스의 시: &quot;퀸티아는 얼굴도 예쁘고, 피부도 곱고, 날씬하지만 아름답다고 할 수는 없어. 베누스가 없으니까. 하지만 레스비아는 아름다워. 머리 끝에서 발끝까지 사랑스러워. 그녀가 모든 여성들에게서 베누스를 앗아갔거든.&quot;\n여담) 베누스는 매력이라는 뜻을 가지고 있음\n크로노스의 권력\n크로노스는 시간의 신으로 여겨지는데, 낫을 든 모습으로 그려짐.\n낫은 농사의 의미인데 농사에서 가장 중요한게 시간이라 시간의 신으로 여겨진다고... 함?\n또한 모든 것을 무너뜨리는 것이 시간의 힘이라 모든 남근을 거세시켜버린 크로노스를 시간의 신으로 본다고 함.\n참고로 이 낫의 재료는 '아다마스'라는 금속인데, 아다마스는 모든 것을 제압하며 무엇에도 제압되지 않는다는 뜻을 가지고 있음.\n로마신화에서는 사투르누스(Saturnus)라고 불리고, '씨를 뿌리는 파종'을 뜻하는 Satus라는 말에서 왔다고 함. -&gt; 로마에서는 농업의 신이라고 봤음\n좀 헷갈리지만 농업에서 제일 중요한게 시간이라? 농업의 신 크로노스랑 시간의 신 크로노스는 같은거라고 함;;;\n여담) 그리스는 상업, 로마는 농업이 중요한 사회였음. 이게 영향을 끼친게 아닐까?\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제3강 제우스의 도전(티타노마키아)과 승리","url":"/posts/17/","content":"티탄의 자식들\nRecall: 우라노스와 가이아의 자식인 12명의 티탄신족: 오케아노스, 코이오스, 크레이오스, 휘페리온, 아이페토스, 크로노스, 테튀스, 포이베, 테미스, 테이아, 므네모쉬네, 레아\n근데 12명의 티탄신족들이 지들끼리 자식을 낳았음;;;\n오케아노스와 테튀스\n3000명의 강물의 신들(보통 남신)과 뉨페(샘물, 숲을 관장하는 요정들, 보통 여신)들을 낳음\n당연히 3000명을 다 알 필요는 없고 도리스가 있는데 얘가 네레우스와 네리테스와 50명의 네레이데스를 낳음(네레우스는 폰토스와 가이아의 자식)\n네리테스는 아프로디테가 보자마자 사랑에 빠지고 네레이데스 중에는 트로이 전쟁의 영웅 아킬레우스를 낳은 테티스가 있음\n코이오스와 포이베\n얘들은 아스테리아와 레토를 낳는데, 아스테리아는 헤카테를 낳고, 레토는 아폴론과 아르테미스를 낳음\n헤시오도스의 &lt;신통기&gt;에서 헤카테는 제우스를 비롯해서 가장 큰 존경을 받은 신이고, 아낌없이 사람들에게 축복을 주고, 전쟁터에서는 헤카테가 사랑한 전사가 승리를 거두고, 재판할 때는 왕들의 옆에서 공정하게 재판하도록 도와주고, 운동 시합에서는 헤카테가 원하는 사람이 승리하고, 어부들은 많은 고기를 잡게 하고, 농부들은 풍작을 이루게 하고, 목동들은 가축들의 수가 늘어나게 하고, 인간들이 지혜롭고 씩씩하게 자라도록 양육함.\n헤카테 혼자서 온갖 축복을 다 주면서 승리의 신, 전쟁의 신, 풍요의 신 등 온갖 역할을 혼자 다 함....\n하지만 왠지 모르게 후대에는 잘 안 알려진 신\n크레이오스\n크레이오스는 티탄신족에서 짝을 찾지 않고 에우뤼비아와 아스트라이오스라는 애를 낳고, 아스트라이오스는 새벽의 신 에오스와 애를 낳아서 바람의 신들인 보레아스(북풍), 노토스(남풍), 에우로스(동풍), 제퓌로스(서풍)를 낳음.\n옛날에는 NSEW 대신 보레아스, 노토스, 에우로스, 제퓌로스로 방위를 표시했다고 함!\n테미스\n법도의 신 테미스도 티탄신족에서 짝을 찾지 않고 제우스와 애들을 낳음\n\n디케(정의)\n에우노미아(법치)\n에이레네(평화)\n세 여신 호라이(좋은 시절?)\n\n운명의 세 여신 모이라도 얘들 사이에서 태어났다는 설도 있음\n휘페리온과 테이아\n휘페리온: 하늘을 떠드는 자\n테이아: 신성한 것\n그래서 얘들은 하늘을 빛내는 헬리오스(태양), 셀레네(달), 에오스(새벽)를 낳음\n이아페토스\n얘도 티탄신족에서 짝을 찾지 않고 에우뤼노메와 애를 낳음 (아시아라는 설도 있음)\n\n아틀라스\n프로메테우스\n에피메테우스\n메노이티오스\n\n므네모쉬네\n기억의 여신 므네모쉬네도 티탄신족에서 짝을 찾지 않고 제우스와 12명의 무사들을 낳음\n무사는 칼 들고 싸우는게 아니라 Muse로, 학문과 예술을 관장하는 여신이라고 함.\n크로노스와 레아\n크로노스: 시간의 신\n레아: 흐름\n시간의 흐름을 거스를 수 있는 존재가 없기 때문에 얘들이 낳은 애들은 막강한 힘을 가짐;\n크로노스의 폭력\n권력을 잡은 크로노스\n크로노스가 우라노스를 거세하자, 티탄신족들은 우라노스를 권좌에서 완전히 몰아내고 타르타로스에 갇혀있던 티탄신족들, 퀴클롭스 3형제, 헤카톤케이르 3형제를 꺼내줌\n근데 권력을 잡은 크로노스는 얘들을 다시 묶어서 타르타로스에 가두고 권력을 독점하려고 했음;;\n우라노스와 가이아는 개빡쳐서 크로노스가 자식에게 권력을 빼앗기리라는 신탁을 내림\n본인이 직접 신탁으로 우라노스를 쫓아냈다보니 두려워진 크로노스는 자식들을 다 먹어버림;;;;\n여섯번째 아이 제우스\n여섯번째 아이를 임신한 레아는 이 아이라도 살리기 위해 가이아 여신을 찾아감\n가이아 여신은 크레타 섬에서 애를 몰래 낳고 감춘 후 돌덩이를 크로노스한테 주라고 함\n크로노스는 담보에 싼 돌덩이가 애인줄 알고 먹어버림;;;\n크레타의 딕타이온 안트론 동굴에 애를 숨겼는데 현재는 제우스의 동굴이라는 관광지가 됐음\n레아는 여섯번째 아이를 쿠레테스들(호위무사들)과 멜리세우스의 딸들인 뉨페(요정) 아드라스테이아와 이데에게 맡김\n이 아이는 아말테이아(신비로운 염소)의 젖을 먹고 자랐고, 쿠레테스는 아이가 크로노스에게 들키지 않도록 지켜줌\n이때 지켜준 방식이 웃긴게 아이 울음소리가 들리지 않도록 창과 방패를 두들기며 노래를 불렀다고 함\n여섯번째 아이가 장성했을 때, 가이아가 찾아와 크로노스에게 도전할 것을 당부함.\n티타노마키아\n여섯번째 아이는 크로노스에게 도전하기로 하고 협력자로 메티스를 정함 크로노스는 혼자 했는데 ㅉㅉ\n메티스는 오케아노스와 테튀스의 딸로, 신들과 필멸의 인간들 중에서 가장 아는 것이 많은 지혜로운 여신임\n메티스는 크로노스에게 모든 것을 토해내는 약을 먹이고, 토해져 나온 다섯 형제 자매들과 함께 크로노스와 티탄신족들과 전쟁을 시작함. 티탄신족은 호구인가\n참고로 삼킨 순서의 역순으로 나와서 맨 먼저 돌덩이가 나옴....\n전쟁의 양상\n전쟁은 무려 10년 동안 지속됨;;\n여섯 형제자매는 올륌포스산에, 티탄신족은 더 남쪽인 오트뤼스 산에 있었음\n근데 전쟁이 안 끝나니까 가이아 여신에게 도움을 청함...\n가이아는 타르타로스에게 갇혀있는 자들을 동맹자로 삼아 싸우면 승리할 것이라고 조언해줌\n이에 여섯째는 타르타로스를 지키는 캄페를 죽이고 헤카톤케이르 3형제와 퀴클롭스 3형제를 구출함!\n퀴클롭스는 여섯째에게 번개와 벼락, 다섯째에게 삼치창, 넷째에게 투구(쓴 사람이 안 보이는 투명 투구임)를 선사함.\n헤카톤케이르(콧토스, 브리아레오스, 귀게스)는 삼백 개의 바위를 일제히 날려 보내(Recall: 각 헤카톤케이르는 손이 100개임) 티탄 신족의 하늘을 날아다니는 돌덩이로 어둡게 했음.\n이때 날린 돌들이 섬이 돼서 그리스에 섬이 수천개나 있다고 함...\n이제 제우스 원맨쇼 &quot;하늘과 올륌포스에서 쉴새 없이 번개를 치자, 벼락들이 천둥과 섬광과 함께 그의 억센 손에서 잇달아 날았다.&quot;\n마침내 여섯 형제들이 승리하고 티탄 신족들을 타르타로스에 가둠! 제일 빡친 헤카톤케이르가 티탄 신족을 감시하는 역할을 하기로 함\n참고) 모든 티탄 신족들이 타르타로스로 간건 아님;; 예를 들어 오케아노스는 중립을 지켰기 때문에 아무 일도 없었음\n일부는 제우스 편을 들어서 당연히 감금 안 당했고, 아틀라스 같은 경우는 너무 괘씸해서 하늘을 짊어지는 벌을 받았음\n권력을 나눠 가진 여섯 형제들\n신탁대로 크로노스를 이겨버린 여섯 형제들은 자기들도 똑같이 될까봐 고민함;; 그래서 권력을 독점하지 않고 나눠갖기로 했음\n근데 제비뽑기가 가장 공정하다고 제비뽑기로 나눠갖음;;;;;;;;;;;\n여섯째는 하늘의 신(정확히는 하늘을 관장하는 신인데 귀찮아서 하늘의 신이라고 적음) 제우스, 다섯째는 바다의 신 포세이돈, 넷째는 지하의 신 하데스가 되었음\n(여담: 제우스가 하늘이라는 뜻이기 때문에 아마 제비뽑기로 역할이 정해진 뒤에야 이름이 정해졌을 것이라고 추측됨)\n하늘, 바다, 지하는 세 형제가 나눠갖고 땅은 세 누이가 나눠가졌는데, 결혼의 신 헤라, 곡물/농사의 신 데메테르, 가정/화로의 신 헤스티아가 되었음\n여담) 이름의 뜻\n\n제우스: 빛남, 하늘\n포세이돈: 물(돈)의 주인(포세이)\n하데스: 안 보이는 자\n헤라: 적기/사랑\n데메테르: 땅(데)의 어머니(메테르)\n헤스티아: 화로\n\n그리스인들에게 정의란 각자에게 적절한 몫을 나눠주는 것인데, 그런 점에서 보면 제우스가 권력을 분배하는 과정을 정의라고 할 수 있음. (그래서 하극상 안 당함)\n여담) 지도자가 가져야 할 미덕?\n\n용기: 제우스는 안전한 크레타 섬에서 안주하지 않고 용기있게 크로노스에게 도전함\n지혜: 성공을 거두기 위해 가장 먼저 지혜를 구함\n절제: 권력을 독점하지 않음\n정의: 권력을 적절하게 나눠줌\n\n제우스가 보여준 4가지 모습이 지도자가 가져야 할 미덕이지 않을까?\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제4강 올륌포스 12신 체제","url":"/posts/18/","content":"티타노마키아에 참여한 신들\n이아페토스의 아들 아틀라스, 프로메테우스, 에피메테우스, 메노이티오스도 티타노마키아에 참여했는데,\n아틀라스와 메노이티오스는 크로노스와 티탄신족 편에서, 프로메테우스와 에피메테우스는 제우스 편에서 싸웠음.\n결국 제우스가 이긴 덕분에 아틀라스는 하늘을 짊어지는 벌을 받았고, 메노이티오스는 에레보스에(타르타로스에서 가장 깊고 어두운 곳) 갇힘.\n반면 프로메테우스와 에피메테우스는 생명체를 만드는 일에 함께 참여하면서 제우스의 측근으로 활동함.\n오케아노스와 테튀스의 딸인 스튁스도 제우스의 편에서 싸웠기에 맹세의 신이 되어 신들과 인간들의 존경을 받게 되었고, 그녀의 네 자식(젤로스, 니케, 크라토스, 비아)은 항상 제우스의 곁을 지키는 존재가 됨. (이래서 스튁스 강에 맹세한다는 말이 나옴)\n참고) 젤로스는 질투, 부러움, 경쟁심, 니케는 승리의 여신, 크라토스는 권력, 비아는 힘을 뜻함.\n제우스의 아내들\n참고) 제우스는 아내가 상당히 많은데 그냥 여미새였던게 아니라 자식들이 권력 확장에 도움이 될 것이라고 생각해서 아내를 많이 가졌음. (고대 그리스에서도 부모 자식 관계가 가장 강하다고 생각했음)\n첫 번째 아내 메티스\n최초의 협력자인 메티스는 제우스의 아내기도 함. (제우스 생각: 메티스가 똑똑하니까 메티스와 영원히 함께한다면 내 권력도 영원할 것이다!)\n근데 제우스에게 불길한 신탁이 내려짐;\n\n메티스가 처음에는 딸을 낳고 그 다음에는 아들을 낳을텐데, 이 아들이 제우스를 밀어내고 권력을 잡을 것이다!\n\n신탁을 들은 제우스는 메티스를 어떻게 처리할지 궁리함;;; 완전 쓰레기\n고민하던 제우스는 메티스를 집어삼키고, 메티스가 제우스의 머리에 자리잡게 됨;;;;\n근데 이 당시에 메티스가 첫 번째 아이를 임신중인 상태였음;;;;;;;\n어느 날 머리가 너무 아팠던 제우스는 프로메테우스에게 찾아가는데 프로메테우스가 도끼로 제우스 머리를 내려침;;;;;;;\n그러자 깨진 머리에서 갑옷을 입고있는 완전무장한 아테나 여신이 태어남! (아테나는 전쟁, 전략의 신)\n참고) 플라톤은 아테나의 테나 부분이 '신적인 것'(Theia)이라는 뜻이라고 생각해, '보고 생각하고 알고 있는'(Nousa) 여신이라는 이름을 붙여 신의 마음을 가진 여신(Theonoe, 테오노에)라고 해석하기도 함.\n두 번째 아내 테미스\n테미스는 우라노스와 가이아 사이에서 태어난 티탄 신족으로, 정의, 법도, 원칙, 관습의 여신임. (제우스 생각: 안정적인 권력을 위해서는 법과 질서가 필요하다!)\n테미스는 제우스와 함께 세 명의 딸, 정의의 여신 디케, 올바른 법과 질서의 여신 에우노미아, 평화의 여신 에이레네를 낳음. (호라이 3자매라고 함)\n운명의 여신 모이라 3자매(운명의 실을 잣는 클로토, 운명의 베를 나눠주는 라케시스, 운명의 실과 베를 자르는 아트로포스)도 제우스와 테미스의 딸이라는 설이 있음, but 뉙스의 딸로도 알려져 있음.\n호라이 3자매\n호라이는 계절, 시기, 적기라는 뜻임. 올바른 법과 질서가 오면 좋은 시기라서 호라이 3자매라고 불림.\nbut 일반적으로는 계절, 적기, 꽃의 여신 탈로, 성장의 여신 아욱소, 열매, 추수의 여신 카르포를 호라이 3자매로 부르기도 함\n세 번째 아내 에우뤼노메\n에우뤼노메는 오케아노스와 테튀스의 딸로, 널리(Euru) 다스리고 분배하는 자(Nome)라는 뜻임. (제우스 생각: 나도 널리 다스리고 분배해야겠다!)\n에우뤼노메와 제우스는 세 명의 우아한 카리스 여신을 낳음.\n\n세상을 환하게 비치는 아글라이아\n현명한 생각으로 즐거움이 넘치게 하는 에우프로쉬네\n향기롭고 아름답게 꽃을 피어나게 하는 탈리아\n\n헤시오도스의 &lt;신들의 계보&gt;: &quot;이들이 쳐다 볼 때면 그들의 눈에서는 사지를 풀어버리는 사랑이 흘러내리니, 이들의 눈썹 아래 시선은 그토록 아릅답다.&quot;\n여담) 그리스에서 카리스마는 부드럽고 우아하고 아릅답게 자신의 능력과 권한은 열어주면서 상대에게 은혜를 베풀고 상대가 감동하게 만드는 것을 뜻함.\n네 번째 아내 데메테르\n데메테르는 크로노스와 레아의 두 번째 딸로, 대지의 여신임. (로마 신화에서는 곡물의 신 케레스, 시리얼의 어원임)\n제우스와 페르세포네를 낳는데, 하데스가 너무 아름다워서 납치해서 지하세계로 데려가서 아내로 삼음\n데메테르가 딸을 찾아 온 세상을 헤메느라 땅이 황폐해져가자 킹갓제우스가 중재해서 데메테르와 하데스 사이에 타협이 이뤄짐.\n1년의 반은 땅에서(봄, 여름), 반은 지하세계에서(가을, 겨울) 보내는 것으로 결정되고, 이 때문에 계절이 생겨났다고 함.\n(또는 겨울에 씨를 심어두면 봄에 피어나는 농업사회를 설명한거라고 해석하기도 함)\n다섯 번째 아내 므네모쉬네\n므네모쉬네는 가이아와 우라노스 사이에서 태어난 기억의 여신으로, 제우스와 9명의 무사(Mousa) 여신을 낳음.\n인간들의 행위인 음악, 예술, 역사를 표현한다고 함.\n여섯 번째 아내 레토\n티탄 신족 코이오스와 포이베의 딸은 아스테리아와 레토가 있음.\n제우스는 아스테리아를 사랑했지만, 아스테리아는 제우스의 손길을 피해 메추라기(오르튁스)로 변신해 도망감. 1고백 1차임\n근데 제우스는 독수리로 변신해서 메추라기를 잡을 뻔함;;; 그래서 아스테리아는 바다로 뛰어들어서 오르튀기아(메추라기의 섬) 섬이 되어버림.\n아쉬운대로 제우스는 레토를 아내로 삼고 아이를 갖게 되는데 이 때문에 헤라가 미워하게 됨.\n헤라는 레토가 출산할 수 없도록 출산의 여신 에일레이튀이아에게(헤라와 제우스 딸임) 명령하는 한편, 괴물 퓌톤을 보내 레토를 해치려고 함.\n하지만 북풍 보레아스와 포세이돈이 도와줘서 레토는 오르튀기아에 도착해 무사히 쌍둥이 남매 아르테미스와 아폴론을 출산하게 됨.\n이후 섬의 이름이 델로스(환하다, 찬란하다) 섬으로 바뀌었다고 함.\n일곱 번째 아내 헤라\n제우스 생각: 가정이 이루어져야 권력을 가질 수 있으므로 결혼의 신인 헤라를 가져야겠다!\nbut 헤라는 제우스를 거부함 0고백 1차임\n제우스는 포기하지 않고 천둥번개와 소나기를 퍼부은 후, 뻐꾸기로 변신하여 헤라한테 감.\n헤라는 비에 젖은 뻐꾸기가 불쌍해서 품으로 안자 제우스가 원래 모습으로 다시 변신해버림;;;\n근데 하찮은 뻐꾸기까지 되면서 자신을 원하는 것에 감동해서 제우스를 허락해줌(????)\n대신 정식으로 결혼식을 올려서 모든 인간과 신과 생명체들한테 자신이 제우스의 아내라는 것을 알려주라고 함.\n그래서 제우스는 처음으로 헤라와 함께 결혼식을 정식으로 가졌고, 칼리마코스라는 시인에 따르면 결혼식 연회가 무려 300년동안 있었다고 함.\n이 둘 사이에서 전쟁의 신 아레스, 대장장이의 신 헤파이스토스, 젊음의 신 헤베, 출산의 신 에일레이튀이아가 태어남.\n하지만 얘기가 신화마다 조금씩 다른데, 헤시오도스는 헤라가 혼자서 낳았다고 하고 오비디우스가 전해주는 로마 신화에서는 유노(헤라)가 꽃의 여신 플로라에게서 받은 꽃을 만져 임신해 마르스(아레스)를 낳았다고 함.\n올륌포스 12신\n제우스 형제/자식들 중에서 똘똘한 놈들을 모아서 12명 체제를 만듦\n왼쪽은 그리스 신화 이름, 오른쪽은 로마 신화 이름\n\n헤스티아/베스타, 화로의 신\n헤르메스/메르쿠리우스, 전령의 신\n아프로디테/베누스, 아름다움과 매혹의 신\n아레스/마르스, 전쟁의 신\n데메테르/케레스, 대지의 신\n헤파이스토스/불카누스, 대장장이의 신\n헤라/유노, 결혼의 신\n포세이돈/넵투누스, 바다의 신\n아테나/미네르바, 전쟁의 신 (여담: 지혜의 상징인 올빼미가 상징임)\n제우스/윱피테르(쥬피터), 하늘의 신\n아르테미스/다아나, 달의 신\n아폴론/아폴로, 태양의 신\n\n근데 디오뉘소스는 어디감?\n사실 12신도 지역, 시대마다 계속 변했다고 함;;;\n헤스티아가 그냥 땅에 머물겠다면서 빠지고 대신 포도주와 축제의 신 디오뉘소스가 올륌포스 12신에 들어왔다는 설도 있음.\n디오뉘소스는 한참 뒤 인간 세멜레와 제우스 사이에서 태어난 후대의 신임.\n헤라의 계략으로 세멜레는 타 죽고, 제우스가 타 죽는 세멜레한테 아이가 있는걸 보고 허벅지에 박아놔서 디오뉘소스는 제우스의 허벅지에서 자라남;;;\nbut 개빡친 헤라가 디오뉘소스를 광기에 휩싸이게 만들어서 디오뉘소스는 세상을 떠돌게 되다가 그리스 테베로 돌아오고, 올륌포스 12신에 합류하게 됨\n아테네의 정치인 페이시스트라토스에 의해 디오뉘소스 제전은 아테네의 대표적인 축제가 됨.\n전령의 신 헤르메스\n아틀라스와 플레이오네 사이에서 태어난 일곱 자매들 중 맏딸인 마이아와 제우스 사이에서 태어남.\n놀랍게도 제우스는 아내 말고도 첩이 있었음;;; 헤르메스는 유일하게 서자인데 올륌포스 12신이 되었음\n제우스는 마이아가 머무는 동굴을 찾아가 사랑을 나눴고, 갓 태어난 헤르메스는 아폴론이 돌보던 소 떼에게 다가가 소를 훔쳐 은밀한 곳에 숨김 첫날부터 도적질\n모든 사실을 알고 찾아온 아폴론을 상대로 거북이 껍데기와 소의 내장/힘줄로 만든 뤼라를 주면서 협상함. (아폴론도 음악에 관심이 많았음)\n이를 통해 아폴론이 호구다 헤르메스가 수완이 좋다는 것을 알 수 있다!\n헤르메스는 도둑과 협상, 상업, 부의 신으로 성장하였고, 제우스의 전령이 되어 신의 뜻을 인간에게, 인간의 뜻을 신에게 전해주는 전령 역할을 하게 됨.\n나그네와 여행자들의 수호자로도 역할하는데, 특히 죽은 자들의 혼백을 저승으로 인도하는 역할을 맡음.\nPsukho(프시코, 혼백)pompos(폼푸스, 인도하는자) Hermes, Argeiphontos(아르고스를 죽인 자) Hermes라고도 불림.\n제우스의 영광\n제우스의 영광을 찬양하는 무사 여신들\n문자가 없던 시절에는 잘 기억할 수 있도록 노래로 만들었기 때문에 사실상 기록과 같은 기억의 장치라고 생각하면 됨.\n근데 무사 여신들을 이용해서 제우스의 영광을 영원히 기억하게 만들려고 함;;;;\n각 무사 여신들은 담당하는 분야가 달랐음\n\n클레이오: 역사\n탈레이아: 희극\n에라토: 연가(연애)\n에우테르페: 서정시\n폴휩니아: 찬가\n칼리오페: 서사시\n테릅시코레: 춤과 합창\n우라니아: 천문학\n멜포메네: 비극\n\n제우스의 신전\n제우스의 신전은 아주 큰 규모로 올림피아에 있었음.\n그 옆에 약간 조그맣게 헤라 신전이 있었고,\n그 옆에 스타디온이라는 공터가 있었는데 여기서 제우스를 기리는 거대한 규모의 축제인 올림피아가 있었음. (추후에 올림픽이 됨)\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제5강 반란을 제압한 제우스","url":"/posts/19/","content":"무사 여신들\n알렉산드로스 대왕(너무 업적이 커서 왕 대신 대왕이라고 부름)은 이집트에 자신의 이름을 딴 도시 알렉산드리아를 세우고 그곳에 세상의 모든 지식을 담아내는 도서관과 박물관을 지으라고 했음.\n이 도서관 겸 박물관은 '무사 여신의 신전'이라는 이름으로 Mousaion이라고 불림 (영어로는 뮤지엄, Museum)\n무사 여신들은 므네모쉬네와 제우스의 딸들인데, 므네모쉬네는 기억의 여신이라 제우스가 시간에 지워지지 않고 영원히 기억되기 위해서 결혼했다고 함\n그래서 무사 여신들이 제우스의 영광을 영원히 기억하도록 노래를 만들게 됨;;;;\n여기서 영원히 기억되어야 할 것들을 보관하는 박물관이라는 단어 Museum이 나온 것\n참고) Music은 무사 여신의 기술이라는 뜻인 무시케(Mousike)에서 나온 말임.\n내부의 불협화음\n포세이돈의 불만\n티타노마키아의 승리 이후 제우스, 포세이돈, 하데스는 제비뽑기를 통해 각각 하늘과 바다와 지하세계를 나누어 다스림.\n하데스는 올륌포스를 떠나 지하세계를 충실하게 지킨 반면 (아예 안 온건 아니고 가끔 옴), 포세이돈은 계속 올륌포스에 머무르며 권력의 야심을 키워 나감.\n그러면서 제우스가 하늘을 차지하고 전체 통제권을 쥐고 최고의 권력을 누리는 것에 불만을 가지고 계속 제우스를 감시함.\n포세이돈의 반란\n네레우스의 딸 테티스를 놓고 결국 포세이돈과 제우스가 경쟁함.\n지혜, 미모, 덕성을 가진 흠이 없는 신이라고 함 ㄷㄷㄷ\n근데 나중에 테티스의 아들이 아버지의 권력을 능가할 것이라는 신탁을 듣게 되자 포세이돈과 제우스한테 버려짐....\n포세이돈은 제우스한테 불만을 품고 있는 세력을 규합하기로 함.\n\n헤라는 제우스가 너무 아내가 많아서 불만이 있었음\n아테나는 제우스가 자기 어머니인 메티스를 집어삼켜서 불만이 있었음 정작 제우스는 아테나를 제일 잘 챙겨줬음...\n아폴론은 제우스가 자기 아들인 의학/의술의 신 아스클레피오스가 죽은 사람도 살린다고(사실 하데스가 제우스한테 따지기도 함) 번개로 죽여버려서 불만이 있었음 (나중에 제우스가 화해하면서 살려냄)\n\n이 네 신들이 서로 힘을 합쳐서 제우스의 번개를 빼돌리고 제우스를 굵은 사슬로 꽁꽁 묶어버림\n테티스가 이걸 보고 깜짝 놀라서 지하세계를 관리하던 헤카톤케이르 3형제 중 가장 강력한 브리아레오스에게 도움을 요청함\n티타노마키아에서 도움을 받았던 브리아레오스는 제우스한테 좋은 감정이 있어 제우스를 묶어버린 사슬을 전부 풀어줌\n종살이 하는 포세이돈과 아폴론\n제우스는 아폴론과 포세이돈을 지하의 어두컴컴한 암흑의 호수 속으로 던져 버리려고 했으나, 아폴론의 엄마인 레토의 간청으로 포세이돈과 아폴론을 트로이아의 왕 라오메돈 아래 종살이를 하게 함. (신이 인간의 종이 되는 것은 굉장히 굴욕적인 일임!)\n포세이돈은 트로이아 도성을 쌓는 일을, 아폴론은 라오메돈의 소떼를 돌보는 일을 함.\n불경하고 오만한 라오메돈은 일 년 뒤, 두 신에게 노동에 약속한 보상을 하지 않고 두 신을 결박해 노예로 팔아버리겠다는 둥, 청동으로 귀를 잘라 버리겠다는 둥 협박을 가하면서 막 나감.\n개빡친 포세이돈은 트로이아에 바다괴물을 보내고, 아폴론은 역병을 보내 징계함. (아폴론은 병을 고치는 의술의 신이기도 하지만 병을 주기도 하는 신이기도 함)\n라오메돈은 GG치고 어떻게 하면 좋겠냐고 물어보니 딸 헤시오네를 제물로 바치라는 신탁을 받게되고, 그대로 함 진짜 쓰레기다\n결국 라오메돈은 바다괴물에게 딸 헤시오네를 바쳐버리지만, 다행히 지나가던 헤라클레스가 헤시오네를 구하게 됨\n기간토마키아\n가이아의 불만\n티타노마키아에서 승리한 뒤 제우스는 티탄신족들을 타르타로스에 감금하고 헤카톤케이르 3형제를 감독관으로 임명함.\n다같이 잘 사는 평화로운 사회를 원했던 가이아는 가이아의 자식들을 또다시 가둬버리는 제우스의 통치에 불만을 품게 됨.\n가이아는 제우스를 무너뜨리려고 기가스들을(Gigantes) 동원함.\nRecall) 우라노스가 거세될 때 흘린 정액과 피가 땅에 닿아서 태어난 거신족들임.\n기간토마키아\n기가스들은 거대한 크기와 무시무시한 모습을 가지고 있었는데 머리와 턱에 깊은 터럭을 가지고 다리에는 뱀이 꿈틀거렸다고 함.\n얘들은 올륌포스를 향해 바위와 불 붙은 참나무를 던지면서 공격함.\n포르퓌리온과 알퀴오네우스가 대표적인 기가스고, 특히 알퀴오네우스는 고향 땅에서 싸우는 한 불사의 존재였음.\n근데 심지어 알퀴오네우스만 그런게 아니라 대부분 기가스가 땅에 발을 붙이고 싸우는 한 불사의 존재였음;;;\n고전하던 올륌포스 신들에게 &quot;거신들은 신들의 공격으로는 절대 죽지 않고, 오직 필멸의 인간이 동맹자로 싸워야만 승리할 수 있다&quot;는 신탁이 내려짐. 가장 지혜로운 여신인 아테나가 이 인간이 헤라클레스라는 것을 알아냄.\n올륌포스의 신들이 중심이 되어 많은 신들이 힘을 합치고, 인간 헤라클레스가 동맹군으로 참여하여 대규모 전쟁을 치르게 됨.\n기가스들과 올륌포스 신들과의 전쟁을 기간토마키아라고 함.\n제우스의 두 번째 큰 고비로, 이전에는 신들끼리 계속 싸웠지만 외부 세력이 들어오자 신들이 올륌포스 12신 체계를 지키기 위해 단합하는 모습을 보여줌\n페르가몬의 제단\n여러 신전에 기간토마키아의 장면들이 새겨져 있다고 함!\n특히 페르가몬의 제단이 있는데 고고학자들이 발굴한 후 보존해야겠다는 마음을 먹고 전부 뜯어온 뒤 재현했다고 함;;;\n이 제단을 바탕으로 만든 박물관이 페르가몬 박물관임\n\n알키오네오스를 아테나와 헤라클레스가 칼과 화살로 죽이는 장면\n포르퓌리온을 제우스가 헤라를 이용해 미인계를 사용하여 죽이는 장면 (헤라한테 한눈에 반한 포르퓌리온이 숨은 헤라를 따라가면 숨어있던 제우스와 헤라클레스가 죽여버림)\n헤카테와 아르테미스가 기가스를 죽이는 장면\n레토가 기가스를 찌르는 장면\n아폴론이 기가스를 발로 제압하는 장면\n아프로디테와 미인계로 기가스를 죽이는 장면 (마찬가지로 아프로디테가 동굴로 숨어버리고 따라온 기가스를 숨어있던 헤라클레스가 죽여버림)\n아프로디테와 에로스가 직접 기가스를 발로 제압하는 장면 (미인계가 아님!!)\n디오네(아프로디테 판데모스의 어머니라고 알려져있는 신)가 싸우는 장면\n디오뉘소스가 솔방울 모양의 지팡이인 튀르소스를 휘두르며 싸우는 장면\n레아가 사자를 타고 다니며 싸우는 장면\n\n등등 여러 장면이 생생하게 부조에 남아있음\n기간토마키아의 승리\n\n제우스는 포르퓌리온과 나머지 기가스들을 번개로 물리침\n아테나는 알퀴오네우스를 무찌른 뒤, 에켈라도스에게 시켈리아 섬을 던져서 잡음\n아폴론과 헤라클레스는 에피알테스를 화살로 잡음\n디오뉘소스는 튀르소스로 에뤼토스를 잡음\n헤카테가 횃불로 클뤼티오스를 잡음\n헤파이스토스가 모루로 미마스를 잡음\n포세이돈은 폴뤼보테스를 코스섬으로 유도한 뒤 코스섬을 던져서 잡음\n헤르메스는 휩폴뤼토스를 잡음\n아르테미스는 그라티온을 잡음\n모이라는 청동의 곤봉으로 아그리오스와 토온을 잡음\n\n여러 신들이 활약하며 기간토마키아는 제우스의 승리로 끝남!\n하지만 가이아는 이 결과에 만족하지 않는데....\n튀포노마키아\n튀폰의 탄생\n기간토마키아에서 기가스들의 패배에 충격을 받은 가이아는 타르타로스와 결합하여 지금까지의 모든 존재들보다 가장 위협적이고 거대한 튀폰을 낳음. (튀폰은 나중에 typhoon, 태풍이 됨)\n아폴로도로스의 &lt;비블리오테케&gt;, 헤시오도스의 &lt;신들의 계보&gt; 등에서 묘사되었는데, 머리가 하늘에 닿고, 양팔과 양손이 세상에 닿고, 백 개나 되는 용의 머리가 양팔 위에 솟아 있고, 넓적다리 아래에는 거대한 뱀들이 꿈틀거리고(여담으로 기가스는 뱀이 한 마리만 있었음), 온몸에는 날개가 달렸고, 입에선 불을 뿜어낸다고 함.\n이건 못 이긴다고 보고 올륌포스의 신들은 짐승으로 변신해 아이귑토스(현재 이집트)로 피신함;;\n튀폰과 제우스의 대결\n제우스는 유일하게 도망가지 않고 튀폰과 싸웠는데, 사실 좆밥일줄 알고 그냥 무지성 정면대결 한거임\n제우스의 번개는 튀폰에게 큰 상처를 입혔지만 튀폰이 제우스를 손으로 잡아버리고 다른 손으로 사지의 힘줄을 끊어버림;\n튀폰은 사지가 끊긴 제우스를 칼리키아의 코리코스 동굴에 가둬버리고 잘라낸 힘줄을 곰의 가죽으로 싸서 아무도 모르는 곳에 숨겨놨다고 함.\n이렇게 튀폰이 승리한 것으로 보였으나...\n제우스의 승리\n제우스의 최측근인 헤르메스가 숨겨져있던 제우스의 힘줄을 찾아내고, 제우스를 찾아가 힘줄을 다시 연결시켜 제우스를 다시 되살림.\n냅다 도망친 제우스는 튀폰과 정면대결하면 안 된다는 것을 깨닫고 번개를 던지고 빠지는 것만 반복함.\n제우스는 약이 오른 튀폰을 시칠리아 섬 쪽으로 유인한 뒤 근처에 있던 에트나 화산으로 묻어버림\n근데 세상보다 크다는데 화산으로 덮힘? 그냥 그런갑다 하세요\n여담으로 에트나 화산이 활발한 활화산인 이유가 깔린 튀폰이 화나서 으아 하고 외쳐서 그런거라고 함;;\n이렇게 기간토마키아와 튀포노마키아를 승리로 이끈 제우스는 자신의 권력을 영원히 지켜나가게 됨.\n튀폰과 에키드나의 자식들\n튀폰은 에트나 화산에 깔리기 전에 에키드나 여신과 결합해 여러 자식들을 낳음.\n에키드나는 기가스와 비슷한 모습인데, 튀폰과 함께 낳은 자식들이 다 괴물임...\n\n머리 둘 달린 오르트로스(게리온의 소떼 지킴이)와 셋 달린 케르베로스(저승의 입구 지킴이)\n네르나 호수에 사는 머리가 아홉인 히드라\n사자와 염소와 뱀이 결합된 키마이라\n네메이아 사자\n크롬뮈온의 식인 멧돼지 파이아\n카륍디스의 스퀼라\n카우카소스 산(프로메테우스가 형벌을 받게 되는 곳)의 독수리\n헤스페리데스의 과수원 황금사과를 지키는 용 라돈\n콜키스의 황금양털을 지키는 용\n스핑크스 등등\n\n이 괴물들이 인간들을 괴롭히게 되고, 인간들 중에서 나타난 영웅들이 괴물들을 물리침.\n그리스 로마 신화의 앞 부분은 신들의 이야기, 그리스 로마 신화의 뒷 부분은 영웅들의 이야기라고 보면 됨!\n스포) 사실 헤라클레스가 거의 다 함;;\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제6강 인간의 탄생과 최초의 여인 판도라","url":"/posts/20/","content":"최초의 인간과 영웅들의 탄생\n헤시오도스의 &lt;일과 나날&gt; 인간의 다섯 종족\n헤시오도스의 &lt;일과 나날&gt;에서 인간이 본격적으로 나오게 됨 인간 = 나날이 일만하면서 보낸다는 뜻\n헤시오도스 왈: 인간은 5가지 종족이 연이어 나왔다!\n황금 종족\n최초의 인간은 황금으로 만들어졌음!\n제우스가 태어나기 전, 올림포스 신들이 활동하기 전에 크로노스가 만들었다고 함. (크로노스가 만들진 않고 그냥 그 때 나타났다고 하기도 함)\n근데 &lt;일과 나날&gt;에서는 올림포스 집들에 사시는 불사신들께서 맨 처음 만들었다고 함;;; 근데 크로노스랑 같이 살긴 살았다고 함;;;;; -&gt; 그래서 모순을 없애려고 크로노스가 올림포스에서 살았었다고 해석함 (또는 티타노마키아 도중에 만든거라고 해석함)\n마음에 아무 걱정 없이 신들처럼 살았다고 함. 일도 안 하고 궁핍하지도 않고 늙지도 않고 아프지도 않음 ㄷㄷㄷㄷ\n많은 가축 떼를 두고 제일 좋은 곡식과 열매를 얻을 수 있었고, 밭에서 한가롭게 일함\n그러던 도중 대지가 황금 종족들을 아래에 감춰버려서 멸종함. 이후 제우스의 뜻에 따라 착한 정령이 되어 죽게 마련인 인간들을 도와주고 정의를 수호하고 못된 짓을 막으며 온 세상을 돌아다니며 복을 주었다고 함.\n은의 종족\n황금 종족이 멸종한 후 은의 종족이 나타나는데, 제우스와 올림포스 신들이 권력을 잡았을 때 은의 종족이 나타났음. (제우스가 만들었다는 설도 있고 올림포스 신들이 만들었다는 설도 있음)\n태어난 아이는 100년 동안이나 자상한 어머니의 보살핌을 받으며 자라고 집에서 어리광을 피움\n그러나 성년이 되면 얼마 살지 못 한다고 함...\n서로 범죄행위를 억제할 능력이 없고, 불사신들을 섬기지 않아 신들의 제단에 제물을 바치려고 하지도 않았다고 함.\n청동 종족\n은의 종족이 멸종한 후 청동 종족이 나타났음.\n금-&gt;은-&gt;동.... 어째 가면 갈수록 안 좋아지는 것 같다?\n실제로 종족이 달라질 수록 인간의 퀄리티가 점점 떨어진다고 함!\n은의 종족과 마찬가지로 제우스와 올림포스 신들이 권력을 잡았을 때 나타났음.\n무섭고 사나운 종족인데 전쟁과 폭행에 몰두하고 곡식을 전혀 먹지 않고 억센 마음을 가졌다고 함.\n덩치만 큰 못난이들이고 엄청난 힘과 무적의 팔들이 어깨에서 자라나고 있다고 함 ㄷㄷㄷ\n무구도 집도 농기구도 청동이였다고 함.\n하지만 하데스에 의해 제압당하고 검은 죽음한테 사로잡혔다고 함...\n영웅 종족\n청동 종족이 활동하던 시기에 같이 태어났다고 함.\n즉 서로 섞여서 살았다고 함!\n(사실 &lt;일과 나날&gt;에서 나온대로면 청동 종족이 사라진 뒤에 나타났다고 하는데 그럼 신과 인간 사이에서 태어났다는 말이랑 모순임;;;)\n청동 종족 중에서 특별한 남성/여성을 신들이 관심을 가져서 결합한 결과가 영웅 종족이라고 함.\n즉, 신과 인간(청동 종족) 사이에서 태어난 반인반신의 자식이라는거임!\n청동 종족과는 달리 의롭고 선량한 반신들이라고 불렸다고 함;\n하지만 비교대상이 청동 종족인거지 정말 영웅 종족이 도덕적으로 뛰어났는지는 의문거리라고 함.....\n트로이아 전쟁이 끝난 후에 멸종하게 됨.\n&lt;일과 나날&gt;에서는 사악한 전쟁과 무시무시한 전투로 인해 멸종했다고 함.\n하지만 일부는 하데스로 내려가지 않고 제우스가 사람들에게서 멀리 떨어진 곳에서 생명과 거처를 주고 대지의 끝에서 살게 했다고 함! 오케아누스의 옆에 있는 축복받은 자들의 섬 엘리시온에서 살았다고 함.\n철의 종족\n영웅 종족이 사라질 때 청동 종족도 사라졌고, 철의 종족이 나타났음.\n헤시오도스가 (그리고 우리들이) 살고 있는 시기가 철의 종족의 시기라고 함!\n헤시오도스 왈: 나는 다섯 번째 종족과는 함께하지 말고 먼저 죽거나 나중에 태어나고 싶다고 함;;;\n밤이나 낮이나 노고와 곤궁에서 벗어나지 못 하고 신들께서 괴로운 근심거리를 준다고 함....\n하지만 완전히 사악하진 않고 악과 선이 섞여있다고 함.\n친구 사이, 형제 사이, 손님과 주인 사이에 우애가 없고 부모를 존경하질 않는다고 함;\n주먹이 곧 정의고 서로를 약탈하고 맹세를 지키거나 의로운 사람한테 감사하지 않는다고 함;;\n악행이나 범죄를 저지른 자를 존경하고 악한 자가 더 나은 사람들을 구박한다고 함;;;\n플라톤의 네 가지 종족\n\n플라톤의 &lt;국가&gt; 제 3권: &quot;이 도시국가에 있는 여러분 모두가 실은 형제입니다. 그러나 신은 여러분을 만들면서, 여러분 중에서도 다스릴 능력이 충분한 이들에게는 태어날 때 황금을 섞었고, 그래서 이들이 가장 존경을 받는겁니다. 한편 보조자들에게는 은을, 그리고 농부들이나 다른 장인들에게는 쇠와 청동을 섞었습니다.&quot;\n\n여기서는 완전 풀메탈 인간이 아니라 흙을 빚어서 인간을 만드는데 금속을 조금 섞었다고 함.\n황금 종족, 은 종족, 청동 종족, 철의 종족이 있었지만 이 종족들이 한꺼번에 어우러져서 살아갔음!\n좀 더 설득력(?) 있는게 철의 종족만 있었다면 세상이 망했겠지만 갓종족 똥종족이 어우러져서 사회가 멀쩡히 돌아갔다...?\n프로메테우스 vs 제우스\n인간을 사랑한 프로메테우스\n적어도 은의 종족부터는 제우스와 올림포스 신들이 인간들을 만들고 관리했음!\n신들이 생명체 만들던 시절 프로메테우스와 에피메테우스는 생명체에게 적절한 능력을 부여하는 임무를 맡음. (Recall: 티타노마키아에서 제우스 도와준 애들임)\n에피메테우스가 일단 능력을 나눠주고 프로메테우스가 적절했는지 검토했다고 함.\n근데 프로메테우스는 미리(프로) 생각하는자(메테우스)인데 에피메테우스는 나중에(에피) 생각하는자(메테우스)임;;;\n그래서 에피메테우스가 일단 저질러놓고 능력을 막 줘버리는데 인간을 까먹어서 인간한테 줄 능력을 안 남겨놓음;;;;\n이 문제를 해결하기 위해 프로메테우스는 신들의 것을 훔쳐서 인간에게 주는데, 이게 바로 불과 기술, 지혜, 정치적 지혜(인간들이 정치적인 공동체를 만들어서 살 수 있게 만든 힘)임.\n인간들에게 또 뭘 해줄 수 있을까 고민하던 프로메테우스는 제우스를 속여 인간들이 신들에게 제사를 드릴 때 제물의 좋은 부분을 인간의 몫이 되도록 만듦.\n정확히는 소를 잡았을 때 한쪽에는 뼈를 모은 뒤 맛있어 보이게 윤기있고 기름지게 치장하고 한쪽에는 고기들을 모은 뒤 가죽을 덮었음\n이를 안 제우스는 빡쳐서 인간들에게서 불을 회수해 고기를 못 굽도록 만듦... 근데 프로메테우스가 불을 다시 훔쳐서 인간들에게 몰래 가져다 줌!\n인간들을 위해 이렇게까지 한건 프로메테우스가 직접 인간을 만들었기 때문이라는 설이 있음.\n제우스는 자기 명령 어김 + 신의 것을 훔침 2단 콤보로 극대노해서 프로메테우스를 공개적으로 처벌함.\n사실 제우스의 전략?\n사실 제우스는 프로메테우스를 믿지 않았음. (이유: 애초에 배신때리고 내편으로 온 놈임)\n그래서 권력을 맡기진 않았지만 아예 공로를 무시할 수도 없으니까 생명체에게 능력을 부여하는 임무를 내린 것\n인간들을 괜히 괴롭힌 것도 프로메테우스를 제거할 빌미를 만들려고 그랬다는 해석도 있음.\n에피메테우스가 먼저 능력을 나눠주고 프로메테우스가 검토하는게 제우스의 명령이였는데, 인간들을 맨 마지막에 능력을 받도록 만들고 일부러 에피메테우스가 능력을 나눠주도록 해서 실수를 유도한게 제우스의 함정이라고 함 ㄷㄷㄷㄷ\n제물도 일부러 뼈를 골라서 프로메테우스에게(혹은 인간들에게) 화를 낼 명분을 얻었다고 함.\nQ: 프로메테우스는 앞을 내다보는 자, 예지의 신인데 제우스는 왜 못 보고 함정에 빠짐???\nA: 몰?루 프로메테우스가 정말 모든 것을 보는게 아니라 제한적으로만 예지가 가능하다고 생각됨\n인간을 위해 고통을 당하는 프로메테우스\n제우스는 스튁스의 자식인 크라토스(힘, 권력)와 비아(완력), 그리고 자신의 자식인 헤파이스토스를 보내 프로메테우스를 카우카소스 산의 절벽에 묶어 놓음.\n(Recall: 스튁스도 티타노마키아에서 제우스의 편을 들었는데, 그 보상으로 자신의 네 자식 젤로스, 니케, 크라토스, 비아가 항상 제우스 곁에 있게 해달라고 함)\n프로메테우스는 제우스에게 굴복하는 대신, 제우스의 약점을 이용하여 위협을 가함.\n제우스의 약점 = 제우스 가장 두려워하는 점 = 제우스가 권력을 잃게 되는 방법\n\n아테네의 뛰어난 비극 작가인 아이스퀼로스의 &lt;결박된 프로메테우스&gt;: '제우스가 권좌에서 떨어질 것이다. 언젠가는 후회할 결혼 때문에. 그녀는 아버지보다 더 강한 아들을 낳을 것이다.'\n\n프로메테우스는 오케아누스의 딸 헤시오네와 결혼 중이였는데,\n오케아누스가 찾아와서 프로메테우스한테 제우스와 타협하라고 하지만 프로메테우스는 절대 굴복하지 않겠다고 함.\n오케아누스의 딸들도 찾아와서 제우스에게 굴복하라고 하지만 프로메테우스는 제우스는 언젠가 권력을 잃게 될테니 절대 굴복하지 않겠다고 함. 망상충\n프로메테우스를 압박하는 제우스\n하지만 망상충에게 걸려든 제우스는 결국 헤르메스를 보내 프로메테우스에게 그 파멸의 결혼 상대자가 누구인지 밝히라고 압박함.\n프로메테우스는 거부하며 &quot;그대는 내가 겁을 먹고 새로운 신들 앞에 굽실거릴 줄 알았던 모양이지? 그건 결코 있을 수 없는 일이다.&quot; 라고 말함.\n헤르메스는 더 강한 협박을 하는데, 제우스가 프로메테우스가 묶여있는 땅을 아예 지하로 가라앉게 하고, 나중에 다시 햇빛을 보게 되었을 때, 독수리를 보내 간을 쪼아 먹게 하겠다고 함.\n프로메테우스가 ㅇㅋ 콜 해서 실제로 프로메테우스는 지하로 가라앉았고, 독수리한테 간을 쪼아 먹히는 신세가 됨.\n독수리는 낮에 프로메테우스의 간을 거의 다 먹고 밤에 날아가는데, 프로메테우스는 불사신이라 밤에 간이 재생돼서 영원히 간을 쪼아먹히게 됨;;\n(Recall: 이 독수리는 튀폰과 에키드나의 자식임)\n이래도 프로메테우스가 굴복하지 않으니까 제우스는 인간을 괴롭히기로 마음먹음.\n먼저 인간 여자를 만들고, 그 이후에는 거대한 홍수를 일으켜 인간을 멸종시키려고 했음.\n최초의 여인 판도라\n프로메테우스를 굴복시키려고 제우스가 만든 인간 여자임. (즉, 판도라 이전까지는 인간은 전부 남자였음)\n헤파이스토스가 여신들을 모델로 처녀의 모습을 흙으로 빚음.\n여신들은 빚어진 여자를 보고 선물들을 주는데,\n아테나는 예쁜 옷과 허리띠, 화환, 황금 머리띠 장식, 베 짜는 솜씨를,\n아프로디테는 남자들을 유혹할 수 있는 매력을,\n헤르메스는 아름다운 목소리, 영악한 마음, 교모한 말솜씨를 줌. 아니 넌 여신이 아니잖아\n신들에게 선물을 받은 이 여자는 모든(판) 것을 다 갖출 수 있도록 선물을(도라) 줬다는 뜻으로 이름을 판도라로 지음.\n프로메테우스는 모든 것을 예견하고 에피메테우스에게 절대 제우스의 선물을 받지 말라고 경고함.\n항아리를 연 판도라\n제우스는 판도라가 탄생하자 헤르메스를 시켜 에피메테우스만 남아있는 집으로 보냄.\n에피메테우스는 판도라를(=제우스의 선물) 보자 그 아름다움에 매료되어 프로메테우스의 경고를 잊고 집으로 받아들임.\n둘은 부부가 되어 퓌르라라는 여인이 태어나게 되고, 퓌르라는 나중에 프로메테우스의 아들 데우칼리온과 결혼함.\n에피메테우스는 판도라에게 집 안에 있는 항아리를 절대로 열어 보지 말라고 경고함.\n항아리에는 인간을 괴롭힐 수 있는 모든 나쁜 것들이 있었는데, 아마 인간을 사랑하는 프로메테우스가 나쁜 것들을 모아 항아리에 넣어놨던 것 같음.\n하지만 원래 항아리가 집에 있었던 것이 아니라 제우스가 보낸 선물이라는 설도 있음.\n물론 다들 알다시피 판도라는 항아리 뚜껑을 열어버림.\n제우스는 당연히 이 항아리를 열고 싶었는데, 제우스가 직접 가서 열기에는 에피메테우스가 (그리고 프로메테우스 있던 시절엔 프로메테우스도) 지키고 있었기 때문에 불가능했고, 호기심이 가득한 판도라를 집에 집어넣어서 대신 열게 하려고 했다고 함.\n항아리를 연 에피메테우스?\n판도라가 항아리랑 같이 에피메테우스의 집에 왔고, 판도라가 항아리를 열지 말라고 했는데 에피메테우스가 궁금해서 열었다는 설도 있음.\n제우스가 열었어도 됐는데 에피메테우스한테 책임전가를 하려고 (누가 열라고 칼들고 협박함?) 판도라랑 같이 보냈다고 함.\n하지만 판도라가 열었다는 설이 더 지배적임.\n판도라의 상자 아님??\n사실 상자가(퓍시스, puxis) 아니라 큰 항아리임(퓌토스, pithos)!\n르네상스 시기에 최초의 인문주의자였던 에라스뮈스가 판도라 이야기를 라틴어로 번역했는데,\n이때 항아리랑 상자를 헷갈려서 잘못 번역했다고 함...\n프시케와 에로스의 사랑 이야기(특히 로마 신화에 더 잘 나옴) 도중에 프시케가 하데스에 내려가서 페르세포네가 가지고 있던 화장품 상자를 받아 올림포스에 있는 베누스(아프로디테)에게 가져다주는 미션을 받는 장면이 있는데, 페르세포네가 베누스에게 주기 전까지 상자를 절대 열지 말라고 함. 프시케는 너무 궁금해서 가던 도중 상자를 열어보게 됨. 이거 판도라 표절이네\n아마 에라스뮈스가 이 두 이야기를 착각해서 판도라 이야기에서 상자가 튀어나온 것 같음...\n근데 이 글을 보고 감명받은 단테 가브리엘 로세티가 판도라 이야기를 그림으로 그리는데, 상자로 그려버렸고 이 그림이 너무 유명해짐...\n그래서 이거 보고 다들 상자로 따라그리면서 판도라의 상자가 유명해짐.\n항아리(pithos)는 크기가 보통 1m를 넘고 1.6m까지 가기도 한다고 함. 무게는 무려 2t임;;;\n판도라는 어떻게 선물로 가져온거지 그래서 원래 에피메테우스의 집에 있었다는 설이 더 지배적임!\n항아리에서 나온 것, 항아리에 남은 것\n어쨌든 판도라든 에피메테우스든 집에 있었던 것이든 판도라가 함께 가져온 것이든 항아리 뚜껑이 열렸고, 그 안에 있었던 인간을 괴롭히는 모든 부정적인 것들이(죽음, 급사, 질병, 미움, 다툼, 노고, 곤궁, 불화, 싸움, 기아, 살육, 살인, 다툼, 거짓말, 논쟁, 미망 등등) 튀어나옴.\n여담으로 이것들은 카오스의 자손들, 특히 뉙스의 자식들로 언급됨. (프로메테우스는 뉙스의 자식들을 잡아와서 항아리에 집어넣은 것일까?)\n깜짝 놀란 판도라는 항아리 뚜껑을 급히 닫았지만, 이미 모든 것이 날라가버리고 항아리 바닥에는 희망만이 남았다고 함.\n왜 희망이 항아리에 있었는가?\n인간을 괴롭히는 모든 부정적인 것들이랑 왜 희망이 같이 있었는가?\n희망을 품으면 품을수록 그것을 이루지 못 했을 때의 실망과 고통이 더 커지므로, 프로메테우스는 희망을 인간을 괴롭히는 것이라고 생각했나봄;\n희망이 욕망이 되고 욕망이 탐욕이 되기 전에 희망을 품지 말고 현재에 만족하라는 프로메테우스의 뜻?\n굳이 희망이 남았다는 뜻은 나머지 부정적인 것들은 인간이 통제하지 못 하지만 희망은 인간이 통제할 수 있다는 뜻?\n아니면 애초에 항아리 속에 있던 것들은 인간에게 고통을 주는 것들이 아니였다?\n인간에게 즐거움과 행복을 주는 것들이 잔뜩 담겨져 있었고, 프로메테우스가 인간에게 행복을 주기 위해서 인간 곁에 항아리를 놓았다고 함.\n그런데 항아리를 열면서 건강, 풍요로움 등 인간에게 좋은 것들이 날라가버렸고, (심지어 이 좋은 것들은 올림포스에 있는 신들에게 갔다고 함...) 좋은 것들을 잃어버린 인간들은 죽음, 급사, 질병 등을 겪게 됨.\n무엇이 맞는지는 아무도 모름...\n인류의 멸절, 그리고 새로운 인류의 탄생\n제우스: 하하하 항아리 열렸으니까 인간들은 이제 끝났다! 이제 입을 열어라\n프로메테우스: ㄲㅈ\n결국 항아리로는 만족하지 못 한 제우스는 아예 인간을 파멸시키기로 함.\n하지만 프로메테우스를 압박하려고 한게 아닌 가이아의 부탁 떄문이라는 설도 있음.\n&quot;인간들이 (판도라의 항아리 떄문에) 사악해지고 타락해버렸는데 좀 쓸어줘라!&quot;\n아니면 제우스 본인이 인간에 대한 실망과 혐오감을 갖고 있었다는 설도 있음.\n어쨌든 제우스는 인간을 파멸시키기로 하고, 거대한 홍수를 일으킴.\n이를 예견한 프로메테우스는 아들 데우칼리온과 며느리 퓌르라에게 배를 준비시키라고 함.\n배를 준비해놓은 부부는 홍수에서 살아남았고, (노아의 방주 표절?)\n홍수에서 살아남은 부부에게 &quot;어머니의 뼈를 등 뒤로 던져라. 그러면 이것이 너의 자손들이 되리라.&quot;라는 신탁이 내려짐.\n???: 아니 홍수 때문에 다 쓸렸는데 엄마 뼈를 어떻게 찾아요;;\n하지만 데우칼리온은 어머니를 모든 존재를 만들게 했던 가이아 여신이고, 어머니의 살은 흙, 뼈는 돌이라고 해석하고 돌을 등 뒤로 던짐.\n데우칼리온이 던진 돌은 남자가 되고, 퓌르라가 던진 돌은 여자가 되었고, 이렇게 새로운 종족이 태어나게 됨. 여담으로 헤시오도스의 인간의 다섯 종족이랑 안 맞는 면이 있음\n근데 돌만 던진게 아니라 부부가 자손도 새로 낳았음!\n이 자손들이 그리스의 여러 지역에 퍼져나가서 사는 종족이 됨.\n세계는 제우스가 지배하나, 그리스의 자손들은 프로메테우스의 자손들이 되었다는 해석이 가능함.\n신들의 세계의 지배자는 제우스지만, 프로메테우스는 땅의 지배자가 되었다고 보기도 함.\n프로메테우스의 자손들\n프로메테우스는 헤시오네와(또는 클뤼메네라는 설도 있음) 결혼해 데우칼리온을 낳고,\n에피메테우스는 판도라와 결혼해 퓌르라를 낳음.\n홍수로 인해 이 둘을 제외한 모든 인간들이 죽고, 데우칼리온과 퓌르라는 다시 자식을 낳는데, 여기서 헬렌이 태어남.\n그리스 사람들은 자신들을 그리스인 대신 헬라스 사람들이라고 지칭하는데, 헬렌에서 온 말임.\n그리스 사상이 중심이 되는 문화를 뜻하는 헬레니즘도 헬렌에서 온 말임.\n헬렌은 세 명의 아들 도로스, 크수토스, 아이올로스를 낳음.\n크수토스는 두 명의 아들 아카이오스, 이온을 낳음.\n도로스, 아카이오스, 이온, 아이올로스가 그리스의 여러 지역의 종족을 이루게 되고,\n이들이 사용하는 언어가 다양한 그리스의 방언이 됨.\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제7강 에우로페와 미노아 문명 신화","url":"/posts/21/","content":"이오에서 에우로페까지\n유럽 문명의 요람, 크레타\n유럽 문명은 고대 로마문명과 그리스 문명에 뿌리를 두고 있고, 그 출발점은 크레타 섬을 중심으로 BC 3000년경에서 1600년경까지 위세를 떨친 미노아 문명이었음.\n미노아 문명의 이름은 신화적 인물 &quot;미노스 왕&quot;에게서 온 것!\n미노스 왕은 에우로페와 제우스의 아들임.\n에우로페의 가계도\n제우스와 이오가 에파포스를 낳았고,\n에파포스와 멤피스가 뤼비아를 낳았고,\n포세이돈과 뤼비아가 아게노르를 낳았고,\n아게노르와 텔레팟사가 에우로페를 낳음.\n이때 에우로페의 아버지 아게노르는 페니키아 지역을 통치하고 있었는데,\n페니키아는 지금의 이스라엘, 시리아, 요르단 쪽 지역임.\n아게노르가 에우로페를 낳았다는 것은 역사적으로는 유럽 문명이 페니키아에서 출발했다는 뜻을 가지고 있음.\n뤼비아는 진짜 리비아, 멤피스는 이집트 도시 이름을 뜻한다고 함.\n이집트 -&gt; 리비아 -&gt; 페니키아 -&gt; 미노아 문명으로 유럽 문명이 이어져온 것 아니냐는 설이 유력함\n이오는 그리스의 아르고스 출신의 공주\n사실 미노아 문명의 출발점은 그리스인가...?\n근데 이건 그냥 그리스인들이 국뽕차서 그리스가 모든 문명의 시작이라고 구라친거라고 함;;\n제우스와 이오\n이오는 아르고스의 이나코스의 딸임.\n제우스는 이오를 사랑해서 달아나는 이오를 구름으로 변신하여 품에 안음.\n(인간세계의 문명에도 제우스가 간섭하기 위해 잘나가는 문명이랑 정략결혼 한거라고 보기도 함.)\n근데 이 현장을 헤라한테 급습당하고, 제우스는 이오를 암소로 변신시킨 뒤 여자 아니라 암소라고 안심시킴;;\n물론 헤라는 낚이지 않고 암소를 갖고 싶다고 그냥 달라고 했고 인간으로 돌아오지 못 하도록 아르고스를 감시자로 붙임.\n아르고스는 지역 이름이기도 하지만 괴물 이름이기도 한데, 온몸에 눈이 100개 있어서 밤에 잠을 자도 일부 눈만 감기 때문에 24시간 감시할 수 있음.\n제우스는 이오를 구출하기 위해 헤르메스를 보내 아르고스를 제거함;;\n헤르메스는 음악을 들려주고 이야기를 들려주면서 아르고스의 모든 눈을 감게 만든 다음 목을 침\n헤라는 죽은 아르고스의 몸에서 눈을 빼내 공작새 깃털에 붙였다고 함. (그래서 공작새 깃털에 눈이 많이 달림)\n사실상 암소 = 인간임을 인정해버린 제우스한테 뿔난 헤라는 피를 빨아먹는 쇠파리(등에)를 붙여 이오를 괴롭힘.\n너무 쇠파리에 고통을 당한 이오가 바다에 뛰어든게 이오니아가 됐다고 함;;\n제우스가 고통을 받는 이오를 보고 헤라한테 싹싹 빌면서 불쌍한 이오는 제 모습으로 돌아오게 해달라고 부탁함.\n헤라는 제우스와 화해하고 이오는 마침내 인간으로 돌아오게 됨.\n이시스 여신 이오?\n암소로 떠돌다가 바다에도 빠지고 마침내 인간이 된 이오는 이집트에서 인간이 됐는데,\n갑자기 암소가 사람이 되니까 이집트인들이 이시스 여신이라고 추앙했다고 함???\n그렇다면 이집트와 아프리카의 여러 도시를 건설한 사람들의 조상은 그리스에서 넘어온 이오인가??\n만물그리스설\n이시스 여신은 암소처럼 뿔이 나있기도 하고, 암소를 상징하는 초승달 모양의 장식이 머리에 있다고 함.\n이걸 보고 이오가 이시스 여신이라는 말이 나온게 아닐까...\n역사가 헤로도토스의 기록\n역사의 아버지라고 불리는 헤로도토스는 이오 이야기가 사실 역사를 신화의 형식으로 기록한거라고 함.\n그리스 인들과 동방 사람들 간의 경쟁을 이오로 나타냈다고 함!\n헤로도토스의 &lt;역사&gt;에는 페르시아 전쟁 이야기가 나오는데, 동방의 거대한 나라 페르시아가 서방의 그리스를 침략하면서 일어난 전쟁임.\n근데 페르시아 전쟁 중에 여자를 빼앗는 사건이 있었다고 함!\n\n헤로도토스의 &lt;역사&gt;: 포이니케인들은(페니키아) 아이깁토스와(이집트) 아시리아의 물품들을 싣고 다른 곳들도 갔고, 특히 아르고스에도(그리스) 들렸다고 한다. 당시 아르고스는 지금 헬라스라는 불리는 지역에 있는 나라들 가운데 모든 면에서 뛰어났다. 포이니케인들은 이 아르고스에 도착하여 그들의 물품을 내놓았다고 한다. 그들이 도착한지 닷새째인가 엿새째 날에 물건이 거의 다 팔렸을 때, 왕의 딸이 많은 여자들과 함께 바닷가로 나왔다고 한다.\n그녀의 이름은 이나코스의 딸 이오였다. 헬라스인들도 그렇게 말한다. &quot;그 여자들은 선미 근처에 서서 가장 맘에 드는 물건들을 사고 있었다고 한다. 그런데 그때 포이니케인들은 서로를 충동이며 그녀들에게 덤벼들었다고 한다. 여자들은 대부분 도망쳤지만, 이오는 다른 여자 몇 명과 함께 붙잡혔다고 한다. 이에 포이니케인들은 그녀들을 배에 태우고 아이깁토스를 향해 출항했다고 한다.&quot;\n\n이오는 실제 인물이였고 페니키아 상인들이 이집트로 이오를 납치해갔다고 주장함!\n에우로페를 납치한 제우스\n이오의 사례에서 배운 제우스는 에우로페한테 다가갈 때 제우스 모습으로 직접 가는 대신 황소로 변신해서 다가감\n원하는 것을 얻기 위해 먼저 낮추고 다가가는 모습을 제우스한테서 배울 수 있음...\n에우로페는 황소인 것을 보고 안심하고 황소를 쓰다듦는 등 가까이 지냄\n그러다 에우로페가 황소 등에 타자 제우스는 바다를 건너 크레타에 가버림;;;;\n크레타에 있는 딕테라는 산에 있는 들판에 도착한 뒤 제우스는 자신의 모습을 드러냄\nRecall) 제우스는 크레타 섬에서 태어났는데 아마도 자기가 태어난 고향에서 새로운 인간의 문명이 만들어지길 원했던 것 같음\n문명의 흐름이 페니키아에서 크레타로 갔다는 것을 의미함!\n참고) 에우로페 -&gt; Europe -&gt; 유럽임\n역사가 헤로도토스의 기록\n\n헤로도토스의 &lt;역사&gt;: 페르시아인들이 말하길... 어떤 헬라스인들이(그리스인) 포이니케의 튀로스에 가서 왕의 딸인 에우로페를 강탈해 갔다고 한다. 이 헬라스인들은 아마 크레타인들이었을 것 같다.\n\n이오를 납치해가니까 그리스인들이 에우로페를 납치해감...???\n이게 페르시아 전쟁의 원인....?\n하지만 진짜 역사인지는 확실하지 않음\n그래서 역사의 아버지가 아니라 거짓말의 아버지라고 부르는 사람도 있음\n권력을 잡은 미노스\n제우스와 에우로페의 세 아들\n크레타에 도착한 제우스와 에우로페는 미노스, 라다만튀스, 사르페돈 세 아들을 낳음.\n삼 형제는 크레타 섬을 누가 차지하는가를 두고서 싸우게 되는데\n신화에서는 밀레토스라는 미소년을 놓고 다투는걸로 묘사됨...\n미노스가 두 형제를 몰아내고 밀레토스를 차지하려고 하자\n미노스의 폭력적인 모습을 무서워한 밀레토스는 달아나버리고\n소아시아 서쪽 해변 카리아로 건너가 도시를 세우고 자신의 이름을 붙임.\n물론 역사의 관점으로 보면 밀레토스로 상징되는 권력을 놓고 삼형제가 권력투쟁을 벌인거라고 해석하는게 좋음.\n사르페돈\n미노스와의 경쟁에서 밀려나자 미련없이 크레타 섬을 떠나고 튀르키의 서남쪽에 있는 뤼키아인들과 전쟁을 하던 킬릭스의 동맹군이 되어 싸웠음. (킬릭스는 도시 이름이자 사르페돈의 외삼촌임)\n전쟁에서 승리를 거둔 킬릭스는 보상으로 사르페돈에게 땅을 주었고, 사르페돈은 뤼키아의 왕이 되었음.\n여담) 에우로페가 납치당하자 아버지 아게노르가 형제들을 불러서 흩어져서 여동생을 찾아오고 여동생을 찾아오지 못 하면 집에 돌아오지도 말라고 함;;;\n그래서 에우로페의 오빠들이 에우로페를 찾으러 전세계로 갔지만 못 찾아서 진짜 집으로 안 돌아옴(????)\n그래서 그냥 찾으러 간 곳에 정착해서 도시를 세우게 되는데 킬릭스도 그 중 하나임\n제우스는 사르페돈이 미노스에게 밀려나고 쫓겨난걸 불쌍하게 여겨서 장수의 복을 줘 사르페돈은 세 세대 동안 살 수 있었음.\n장수한 덕에 트로이아 전쟁에 참전하여 트로이아 편에서 싸웠고, 아킬레우스의 동료 파르토클로스에게 죽임을 당함.\n사르페돈은 가장 뛰어난 왕이 될 수 있었던 사람으로 알려져 있음!\n라다만튀스\n라다만튀스도 미노스와의 경쟁에서 밀려난 뒤 크레타 섬 남쪽에 있는 고르튀나에 정착함.\n라다만튀스는 두 아들이 있었는데, 고르튀나를 세운 고르튀스와 에뤼트라이를 세운 에뤼트로스가 있었음.\n라다만튀스는 고르튀나 사람들을 위해서 법을 만들어 준 다음에 그리스 본토에 있는 보이오티아로 도망침;;\n그곳에 있는 테베라는 도시에서 알크메네와 결혼을 하고 에게해의 여러 섬들에서 활동함.\n헤라클레스의 스승 노릇을 했다는 이야기도 있음!\n라다만튀스는 죽은 뒤에 하데스로 가서 미노스와 아이아코스와(아킬레우스의 할아버지) 함께 망자들의 혼백을 심판하는 재판관이 됨.\n(법을 잘 만드는 사람이니 재판관이 된 것은 당연한 것)\n플라톤에 따르면, 라다만튀스는 동방의 사람들을, 아이아코스는 서방의 사람들을 심판했고, 미노스는 최종 결정권을 행사했다고 함.\n미노스의 야망\n원래 제우스가 에우로페랑 애를 낳은 뒤 올림포스로 떠나면서 당시 크레타의 왕이였던 아스테리오스에게 세 아들을 맡겼었음\n그래서 아스테리오스는 에우로페가 아내인 것처럼, 자식들이 친자식들인것처럼 키웠음\n근데 미노스가 두 형제를 몰아내고 크레타의 왕이 됨;;\n(굳이 밀레토스 못 얻었다고 두 형제가 떠난것도 떠난게 아니라 미노스가 권력을 위해 쫓아냈다는 설이 있음)\n미노스의 계략과 배신\n미노스는 바닷가에 백성들을 모아두고 바다를 향에 &quot;포세이돈이시여, 튼튼하고 아름다운 황소를 보내주십시오!&quot;라고 외치며 기도함. 총선용 쇼\n그러자 정말로 바다가 열리면서 하얀 색의 황소가 나타남.\n백성들은 깜짝 놀랐고, 미노스가 크레타의 왕이 된다면 신들의 가호 아래 나라가 번성하리라 믿고 미노스를 왕으로 추대했음.\n근데 사실 이건 사전에 바다의 신이자, 자신의 숙부인 포세이돈과 거래를 한건데,\n포세이돈에게 제물을 바치고 기도하며 깊은 바다에서 황소 한 마리가 나타나게 해주면 다시 제물로 바치겠다고 약속함.\n포세이돈은 약속대로 황소를 보내주고, 백성들은 미노스를 왕으로 추대함.\n근데 미노스는 욕심이 생겨서 포세이돈과의 약속을 지키지 않고, 그 소를 빼돌리고 다른 황소를 제물로 바침;;;\n미노타우로스의 탄생\n포세이돈은 미노스가 배신한걸 당연히 알아채고 응징하는데, 미노스의 부인인 파시파에 왕비가 포세이돈이 보내고 미노스가 빼돌린 황소를 보고 반하게 만들었음.\n사랑에 빠진 파시파에는 크레타 최고의 장인이자 기술자이자 건축가였던 다이달로스를 찾아가 어떻게 황소와 사랑을 나눌지 고민을 털어놓음\n다이달로스는 가짜 암소를 만들테니 여기 들어가서 엎드려있으라고 함.\n결국 파시파에는 아이를 낳게 되고, 황소와 인간의 아이라서 머리는 황소고 몸은 인간인 반인반우의 모습을 가진 괴물이 태어남.\n미노스는 빡통이라 자기 자식인줄 알고 이름을 미노타우르스, 미노스의(미노) 황소(타우로스)라고 지음.\n하지만 미노타우로스는 점점 포악해지면서 가축을 잡아먹더니 마침내 사람을 잡아먹는 식인 괴물이 됨.\n미궁을 건설한 다이달로스\n자기 자식이자아님 제우스의 손자인아님 미노타우로스를 차마 죽이지 못 했던 미노스는 다이달로스한테 한번 들어가면 빠져나올 수 없는 미궁을 설계해달라고 함.\n여담) 근데 미궁을 진짜 만들었으면 진짜 있었을거 아님???\n영웅전을 쓴 플루타르코스는 미궁은 진짜 있었으나 미노타우로스는 반인반우의 괴물이 아니라 미노스가 굉장히 아끼는 장군이였다고 함.\n장군이 황소머리의 투구를 써서 반인반우의 괴물을 상상했다고 함;\n또 다른 가설로는 미노스 왕이 통치를 했다는 크노소스 궁전이(실제로 존재함) 있는데,\n중앙에 왕궁을 지은 후 세력이 커지면서 부속 건물을 계속 만든거라 구조가 개판이 됐고 궁전에서 길을 헤매서 미궁 신화가 생겼다고 함;;\n또 다른 가설로는 미궁이(Laburinthos) 아니라 양날 도끼(Labrus, 라브뤼스)의 집이라는 뜻이라는 설이 있음;;\n크노소스 궁전의 수많은 부속건물들 중 양날 도끼의 집이라는 건물이 있었다고 함.\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제8강 페르세우스와 뮈케네 문명","url":"/posts/22/","content":"다이달로스\n다이달로스는 원래 솜씨가 뛰어난 아테네 출신 기술자로, 아크로폴리스를 만들었다는 설도 있음.\n근데 조카가 다이달로스 밑에서 일을 했는데 너무 뛰어났음;;\n그래서 다이달로스가 시기심에 사로잡혀 조카를 아크로폴리스로 유인한 뒤 아크로폴리스 위에서 떨어뜨려 죽여버렸다고 함....\n사람들은 당연히 다이달로스가 죽였을 거라고 생각하고 살인죄로 고발했지만,\n다이달로스는 말재주가 좋아서 자신은 사고로 떨어진 조카를 잡으려고 했다고 변명함\n결국 살인죄는 피했지만 조카를 살해한 심증이 있어서 사람들이 추방시킴\n결국 다이달로스는 크레타에 머물면서 살게 됨;\n파에톤\n파에톤은 태양의 신 헬리오스의 아들이며, 이오의 자식인 에파포스와 친구였음.\n근데 에파포스가 자기는 제우스의 아들이라면서 너같이 천한 놈이 나와 노는걸 영광으로 여기라고 하면서 파에톤을 무시함.\n파에톤은 자기 아빠도 헬리오스라고 했지만 에파포스가 구라치지 말라면서 무시함;;\n빡친 파에톤은 헬리오스를 찾아가서 &quot;아버지, 당신의 아들 파이톤이 왔습니다&quot;라고 말하며 정말 자기 아빠인지 확인함...\n다행히 진짜 아빠여서 헬리오스는 파에톤을 껴안아줌\n하지만 파에톤은 아빠가 헬리오스라는 증거를 가져오기 위해 태양마차를(황금마차라고도 함) 한번만 몰게 해달라고 함.\n헬리오스는 이미 스틱스 강에 파에톤이 자신의 아들임을 증명하는 모든 요구를 들어주겠다고 맹세를 해버려서 태양마차를 진짜로 빌려주게 됨...\n하지만 태양마차를 모는 일은 제우스도 못 하고 오직 헬리오스만 할 수 있는 굉장히 어려운 일이였음;\n헬리오스는 계속 말렸지만 결국 스틱스 강에 맹세했기 때문에 태양마차를 빌려줌\n예상대로 파에톤은 태양마차를 잘 못 몰아서 너무 올라가 땅을 얼려버리거나 너무 내려가 땅을 불태워버림...\n세상에 혼란이 오자 제우스는 어쩔 수 없이 번개로 태양마차를 부숴버리고 파에톤은 죽게 됨\n의외로 그리스인들은 파에톤을 나쁘게 보지 않았고 파에톤처럼 떨어질지언정 남자라면 한번쯤 해보고 싶고 도전하고 싶은 일을 하는게 맞다고 생각했음\n다나에를 찾아 온 제우스\n본격적인 그리스 문명의 시작, 뮈케네 문명\nRecall) 크레타 섬에서 유럽의 최초 문명인 미노아 문명이 생김\n크레타 섬 위쪽에 테라 섬이라는 둥근 섬이 있었는데 이 섬이 화산폭발로 인해 2/3가 가라앉고 쓰나미가 크레타 섬으로 몰려옴;;\n여기에 외부 침략까지 더해지면서 미노아 문명은 쇠락하게 됨\n이후 펠로폰네소스 반도의 뮈케네를 중심으로 한 문명이 생기게 됨\n이 문명이 에게해의 주도권을 잡으며 본격적인 그리스 문명이 시작되었다고 봄\n페르세우스의 족보\nRecall) 티탄신족들 중 대양의 신 오케아노스와 테튀스가 이나코스를 낳음.\n이나코스는 아르고스를 감싸고 흐르는 이나코스 강의 신으로, 아르고스의 최초의 왕이기도 했음.\n이나코스는 딸 이오를 낳고, 이오가 제우스와 에파포스를 낳고, 에파포스가 멤피스와 딸 뤼비아를 낳고, 뤼비아가 포세이돈과 아들 아게노르를 낳고 아게노르가 에우로페를 낳고, 에우로페가 제우스와 미노스를 낳음.\n알다시피 미노스는 미노아 문명을 만들게 됨!\n참고) 멤피스는 네일로스의 딸인데, 네일로스는 그리스어로 나일 강이라는 뜻이 있고, 멤피스는 라는 도시의 기원이 됨.\n또한 아게노르는 페니키아 문명을 만들고, 에우로페는 유럽 문명을 만들게 됨.\n근데 뤼비아가 포세이돈과 벨로스도 낳음. 즉, 아게노르와 벨로스는 형제임!\n하지만 권력 투쟁 이후 그리스 중심인 아게노르와 달리 벨로스는 이집트가 중심인 아프리카 세계의 패권자가 됨.\n다나오스와 아이귑토스\n벨로스는 이후 앙키로에와 결혼해 쌍둥이 다나오스와 아이귑토스(그리스어로 이집트를 뜻함)를 낳게 됨.\n다나오스는 뤼비에를 다스리며 딸 50명을 낳고, 아이귑토스는 아이귑토스(도시 이름임)를 다스리며 아들 50명을 낳음\n그런데 이 둘도 권력투쟁을 하게 되고, 이긴 아이귑토스는 이집트에 남고, 다나오스는 아르고스로 도주함. (딸 50명 vs 아들 50명이라 불리했다고 함)\n당시 아르고스의 왕인 겔라노르는 갑자기 다나우스가 딸 50명과 군대와 재산을 싹 끌고 오니까 깜짝 놀랐음\n그런데 서로 얘기를 해보니까 알고보니 둘 다 이나코스의 자손이길래 왕권을 그냥 다나우스에게 넘겨줬고, 다나우스는 아르고스의 왕이 됨.\n이 과정에서 포세이돈을 화나게 해서 아르고스의 샘물이 말라버림.\n그러자 50명의 딸들이 물을 길어왔서 샘에 담아왔음. 하지만 시쉬포스의 형벌처럼 헛된 것이였음...\n그림 상으로는 밑빠진 샘(?)에 물을 붓는 것으로 그려짐\n다나오스와 아이귑토스의 자식들\n그런데 어느 날 아이귑토스의 아들들이 아르고스에 와서 평화를 선언하고 다나오스의 딸들과 결혼하기로 함.\n대규모 합동 결혼식이 성사되었지만 이들을 믿지 못 한 다나오스는 딸들에게 단도를 주고, 잠자리에서 신랑을 죽이라고 명령함.\n다나오스의 딸들은 아버지의 명령을 따랐으나, 유일하게 휘페름네스트라는 자신의 처녀성을 지켜준 륑케우스에게 감동해서 살려주었음.\n륑케우스는 다른 아들들과 달리 신사적으로 나오며 이런 결혼은 뭔가 께름칙하니 일단 상황을 보면서 정말 부부가 되어야 하는지 생각해보고, 정말 부부가 돼서 행복하게 살 수 있는 조건이 된다면 그때 정식으로 결혼하자면서 처녀성을 지켜줬음...\n다나오스는 49명의 남자들의 장례식을 성대하게 치르고, 륑케우스에게 돌아갈거냐고 물어봄\n하지만 형제들이 싹다 죽어버린 륑케우스는 어차피 결혼도 했으니 남는 것을 선택하고, 나중에 아르고스의 왕위를 물려받음.\n이후 륑케우스와 휘페름네스트라 사이에서 아바스가 태어났고, 아바스와 아글라이아 사이에서 쌍둥이 아들 아크리시오스와 프로이토스가 태어남.\n이후 아크리시오스가 다나에를 낳고, 다나에는 제우스랑 페르세우스를 낳음.\n페르세우스는 뮈케네 문명의 주역이 됨!\n여담) 그냥 제우스가 인간과 자식 낳으면 그게 문명이 되는 경향이 있음;;;\n문명이 태어난 과정을 신화로 설명한거라고 함; 제우스가 문명을 만들었다!\n아크리시오스와 프로이토스의 갈등\n다나우스, 륑케우스 이후에 아바스, 아크리시오스도 다 아르고스의 왕을 이어받음\n근데 사실 아크리시오스와 프로이토스는 아르고스 왕위를 두고 서로 전쟁을 벌임;\n아크리시오스가 승리햇고, 프로이토스는 아르고스에서 쫓겨나 소아시아의 뤼키아에 가서 이오바테스(또는 암피아낙스)의 딸 안테이아(또는 스테네보이아)와 결혼함.\n근데 프로이토스는 쫓겨난게 억울하다고 장인어른을 설득해 뤼키아 군대를 빌려옴;;\n결국 프로이토스는 티륀스를 점유하고, 퀴클롭스들이 프로이토스를 위해 성벽을 쌓아줬음.\n참고) 튀린스의 유적이라고 실제로 존재하는 벽이 있음\n근데 여기 돌이 인간보다 훨씬 커서 퀴클롭스들이 쌓아줬다는 신화가 생김;\n퀴클롭스 공법으로 만들어졌다고 불리기도 함;;;\n다행히 무력시위는 평화적인 타협으로 해결됐고, 아크리시오스는 아르고스를, 프로이토스는 티륀스를 다스리게 됨.\n아크리시오스에게 내려진 신탁\n라케다이몬과 스파르타의 딸 에우뤼디케는 아크리시오스와 결혼하여 다나에를 낳음.\n아크리시오스는 아들을 낳고자 신탁을 구했으나, 아들 대신 딸이 태어날 것이며, 그 딸이 낳은 아들의 손에 자신이 죽을 운명이라는 신탁을 받음.\n신탁이 두려워진 아크리시오스는 땅 속에 청동 방을 만들어 다나에를 가둠...\n근데 제우스가 황금소나기로 변신하여 다나에를 찾아가서 애를 낳음;;\n이게 바로 페르세우스!\n페르세우스의 모험\n아크리시오스가 다나에를 추방하다\n아크리시오스: 대체 애가 어떻게 생긴거임\n다나에: 빗물이 뚝뚝 떨어지더니 애가 생겼어요 (진짜임)\n아크리시오스는 당연히 안 믿었지만 혹시나 진짜 제우스의 아들일까봐 차마 죽이진 못 하고 다나에와 페르세우스를 궤짝에 넣어 바다에 던짐;\n궤짝은 아르고스 동남쪽에 있는 세리포스 섬에 도착하고, 어부로 지내던 왕족 딕튀스는 궤짝에서 나온 모자를 환대함.\n왕족인데 왜 어부를 하고 있냐면 세리포스의 왕이자 딕튀스의 형제인 폴뤼덱테스가 평소에 딕튀스가 자신을 몰아낼까봐 두려워해서 딕튀스를 왕궁에서 쫓아냄.\n근데 궤짝에서 여자를 낚았다는 소문을 듣고 온 폴뤼덱테스는 다나에한테 반해 결혼하려 했으나, 다나에는 제우스의 은총을 이미 받았고, 페르세우스를 지켜야 하기 때문에 결혼하지 않겠다고 함.\n폴뤼덱테스는 페르세우스만 없으면 결혼할 수 있겠다는 얘기로 알아듣고 (그남충 평균;;;) 페르세우스를 제거하기 위해 계략을 꾸밈;\n폴뤼덱테스는 이제 다나에는 질려서 힙포다메이아 공주(펠로폰네소스 반도 서쪽의 올륌피아의 피사의 공주이자 오이노마이오스와 스테로페의 딸)와 결혼하겠다고 선포함.\n그리고 사람들을 모은 뒤 결혼할테니 결혼 선물을 달라고 공표함;;\n아무것도 내놓을 것이 없는 페르세우스는 메두사의 목이라도 가져오겠다고 호언장담함.\n폴뤼덱테스는 페르세우스를 제거할 기회라고 생각하고 진짜 가져오라고 함;\n비운의 여신 메두사\n바다의 신 포르퀴스와 케토의 고르고 세 자매(스테노, 에우뤼알레, 메두사) 중 하나.\n근데 보통 신은 불멸의 존재인데 메두사는 죽을 수도 있는 존재로 태어남;\n뱀의 비늘로 덮인 머리, 멧돼지의 송곳니, 청동의 손, 황금 날개를 가지고 있는 무시무시한 모습을 가지고 있고,\n고르고 세 자매와 눈이 마주치면 돌로 굳어버린다고 함.\n메두사는 원래 매력적이고 아름다운 여신인데, 포세이돈이 납치해서 아테나 신전에서 겁탈해버림;\n(참고로 포세이돈은 아테나와 앙숙관계임; 일부러 남의 신전에서 겁탈해서 아테나에게 치욕을 남기려고 함)\n분노한 아테네 여신은 메두사가 평소 자랑하던 아름다운 머리카락을 뱀으로 만들어버림.\n참고로 왜 포세이돈이 아니라 메두사한테 이러냐면 메두사가 다른 모든 여신들보다 아릅답다고 사람들에게 칭찬받은걸 자랑하고 다녀서 열등감이 있었음;\n메두사의 목을 자른 페르세우스\n페르세우스가 메두사를 처치하러 모험을 떠나려 하자, 아테나 여신은 거울 같이 반짝이는 청동 방패를, 헤르메스는 날개 달린 샌들 탈라리아를, 제우스는 헤파이스토스가 아다마스로(크로노스가 우라노스를 거세할 때 썼던 낫의 재료) 만든 무적의 칼 하르페를, 하데스는 투명 투구를, 헤스페리데스(아틀라스의 딸들)는 튼튼한 가방 키비시스를 선물함.\n근데 막상 메두사가 어디 사는지 몰랐음; 그래서 고르고의 자매들인 그라이아이(데이노, 에뉘오, 펨프레도)를 만남.\n그라이아이 세 자매들은 이와 눈이 하나밖에 없어서 서로 돌려쓴다고 함;\n페르세우스는 이것을 가로채서 메두사의 거처를 알려달라고 협박함.\n(알아낸 다음에 순순히 눈을 돌려줬다는 얘기도 있고 보복이 무서워서 눈을 던져버렸다는 얘기도 있음)\n(그라이아이 세 자매들에게 키비시스, 탈라리아, 투명투구를 얻었고, 헤르메스에게서 하르페를 받았다는 얘기도 있음)\n메두사가 사는 곳을 알아낸 페르세우스는 잠든 메두사의 곁으로 아테나의 인도를 받아 접근한 후,\n청동방패로 메두사의 모습을 보며,(직접 안 마주치면 괜찮다고 아테나가 알려줌) 하르페로 목을 자름.\n근데 목이 잘린 몸뚱이에서 페가소스와 크뤼사오르(게뤼온의 아버지)가 튀어나옴.\n얘들은 사실 메두사와 포세이돈의 자식인거임;; 겁탈당했을 때 낳은 거라고 생각됨;\n안드로메다와 결혼한 페르세우스\n메두사의 목은 안전하게 키비시스에 넣었고, 몸뚱이에서 나온 페가소스는 잡아서 타고다님;;;\n다시 세르포스 섬으로 돌아오던 중, 케페우스가 다스리던 아이티오피아를 지나게 됨.\n그곳에서 바다 괴물의 먹이로 묶여 있던 안드로메다를 발견함.\n뭔 일이 있었나 했더니 케페우스의 부인인 카시오페이아가 자신이 네레우스의 딸들보다 더 아름답다고 오만한 발언을 해 신들의 노여움을 삼;;\n큰 홍수와 헤일이 일고 포세이돈의 바다괴물이 아이티오피아 사람들을 괴롭힘.\n신탁을 받아보니 케페우스의 딸인 안드로메다를 바쳐야한다고 함;\n상황을 알게 된 페르세우스는 안드로메다를 구해줄테니 결혼하게 해달라고 함.\n케페우스는 안드로메다를 살리고 싶어서 허락하고, 페르세우스는 바다괴물을 죽여버리고 안드로메다와 결혼하게 됨.\n근데 케페우스의 동생인 피네우스가 있었는데 얘가 원래 안드로메다와 결혼하려고 약혼까지 했었음;\n그러던 도중 페르세우스와 결혼하게 되니까 결혼식을 방해하려고 군대를 몰고 나타남;\n페르세우스는 처음엔 진짜 싸우다가 나중엔 그냥 메두사의 머리를 보여주고 전부 돌덩이로 만들어버림\n(케페우스랑 안드로메다 등등은 페르세우스가 엎드리라고 말해서 엎드림)\n케페우스는 페르세우스가 아주 휼륭하다는 것을 깨닫고 아이티오피아를 다스려달라고 요청함\n하지만 페르세우스는 어머니를 구하러 세르포스 섬에 가야 한다고 거절함.\n바로 떠난건 아니고 1년 정도는 머물었는데 안드로메다와 낳은 아들 페르세스도 놓고갔음;;\n세르포스 섬으로 돌아온 페르세우스는 메두사의 머리로 폴뤼덱테스도 돌덩이로 만들어버림;\n???: 제가 메두사의 목을 따왔습니다!\n???: 그래? 보여줘봐\n???: (진짜 보여줌)\n왕이 돌덩이가 되버린 시민들은 페르세우스보고 새로운 왕이 되어달라고 했지만 페르세우스는 고향으로 가야 한다면서 거절함\n그리고 신들에게 받은 선물들을 싹다 돌려주고 메두사의 머리도 아테네 방패에 달아서 선물로 줌;\n뮈케네 문명의 건설\n아크리시오스를 죽인 페르세우스\n페르세우스는 다나에와 안드로메다를 데리고 고향 아르고스로 돌아옴.\n근데 신탁에 의하면 아크리시오스는 죽을 운명;;\n신탁이 두려워진 아크리시오스는 왕이고 나발이고 아르고스를 떠나 텟살리아 지방의 펠라스고이족의 나라로 도피함.\n그 근처에 있던 라릿사라는 지역의 왕 테우타미데스는 돌아가신 아버지를 위한 장례식 추모 운동경기를 개최함.\n조문하러간 페르세우스도 그 경기에 참가함. 근데 원반던지기에 출전한 페르세우스가 너무 쎄서 원반이 운동장을 벗어나서 관중석까지 가버리고 관중 한 사람을 맞춤;;;\n하필 그게 아크리시오스라 신탁은 그대로 이루어짐....\n뮈케네를 세운 페르세우스\n왕이 죽어버린 바람에 페르세우스는 아르고스의 왕위를 이어받을 수 있는 유일한 존재가 되어버림.\n아르고스 사람들은 페르세우스에게 왕이 되어달라고 요청했지만 페르세우스는 또 거절하고 티륀스로 감.\n사유) 내가 왕을 죽여버렸는데 아르고스 왕 되는건 아닌 것 같다\nRecall) 티륀스는 아크리시오스에게 쫓겨난 쌍둥이 동생 프로이토스가 다스리던 곳. 지금은 프로이토스가 죽어서 프로이토스의 아들 메가펜테스가 다스리고 있었음.\n페르세우스는 메가펜테스와 협상을 하고 서로 나라를 바꾸어 다스리기로 함.\n페르세우스: 내가 아르고스 왕되는건 좀 그렇고 프로이토스는 원래 아르고스 다스려야 했는데 쫓겨났으니 이게 맞지 않냐 ㅇㅇ\n메가펜테스는 협상을 받아들이고 아르고스를 다스리고, 페르세우스는 티륀스를 다스리게 됨.\n이후 페르세우스는 세력을 확장하며 티륀스 옆에 미데아를, 아르고스와 티륀스 북쪽에 뮈케네를 건설함.\n페르세우스가 뮈케네에 도성을 세우고 거주하게 되면서 뮈케네는 지역 전체의 중심 도시가 되었고, 뮈케네 문명의 발상지가 됨.\n뮈케네를 발견한 하인리히 슐리만\n그런데 독일의 아마추어 고고학자 하인리히 슐리만이 실제로 뮈케네를 발견함!\n그냥 그리스 로마신화만 읽고 이거 진짜 있을 것 같다고 생각해서 무지성 발굴하다 발견함;;\n제일 유명한게 뮈케네의 사자의 문! 현재는 문 위에 있는 두 마리의 사자가 목이 잘린 채로 남아있음.\n원형의 왕족 무덤, 곡식 창고, 물을 구하기 위에 판 우물 등 여러 구조물이 남아있음.\n특이하게도 사자의 문으로 들어오자마자 왕족의 무덤이 보이는 구조; 메멘토 모리(죽음을 생각하라)가 생각나는 부분\n페르세우스의 자식들\n페르세우스와 안드로메다는 여러 자식들을 낳는데, 이들은 그리스 전역을 지배하는 중요한 인물들이 됨.\n속셈) 우리 그리스인들의 자손들이 전세계 문명을 만든거다!\n둘이서 낳은 첫 번째 아들은 페르세스인데, 부부가 아이티오피아를 떠나면서 페르세스를 케페우스 왕의 곁에 남겨두고 감. 이후 페르세스는 나중에 페르시아 왕족의 조상이 됨.\n뮈케네에 정착한 후에는 다섯 아들 알카이오스, 스테넬로스, 헬레이오스, 메스토르, 엘렉트뤼온과 딸 고르고포네를 낳음.\n엘렉트뤼온은 페르세우스를 이어 뮈케네를 통치함. 엘렉트뤼온은 딸 알크메네를 낳고, 알카이오스는 아들 암피트뤼온을 낳는데, 서로 부부가 되기 위해서 약혼을 함.\n근데 암피트뤼온이 엘렉트뤼온의 소떼를 몰던 도중 무리를 이탈하는 암소에게 막대기를 던졌는데 암소에 맞고 튕겨난 막대기가 엘렉트뤼온을 맞추고 엘렉트뤼온이 죽어버림;;;\n졸지에 살인자가 된 암피트뤼온은 스테넬로스에게 추방당하고, 암피트뤼온 대신 스테넬로스가 뮈케네와 티륀스의 왕권을 차지함. (원래 스테넬로스는 권력욕이 있어서 이 기회에 추방한 것일수도 있음)\n추방당한 암피트뤼온은 테베에 정착하고, 이때를 틈탄 제우스가 암피트뤼온의 모습으로 변신해 알크메네와 동침하고, 헤라클레스가 탄생함;;\n이때 심지어 암피트뤼온과 알크메네의 자식 이피클레스도 같이 있었기 때문에 헤라클레스와 쌍둥이가 됨...\n참고로 제우스 때문에 페르세우스와 헤라클레스는 증손자면서 형제임;\n제우스는 뮈케네 쪽으로 가기 위해 헤라클레스를 왕위에 올려 권력을 주려고 함.\n하지만 나중에 스테넬로스의 아들 에우뤼스테우스는 헤라클레스에게 12가지 과업을 부과하게 됨...\n몰리에르의 희극 &lt;암피트뤼온&gt;\n프랑스의 희극작가 몰리에르는 이 일을 희극으로 표현했는데,\n알크메네가 제우스랑 사랑을 나눈 이후 전쟁터에서 암피트뤼온이 돌아옴\n알크메네는 방금까지 사랑을 나눴는데 놀리는거냐면서 화를 냄\n암피트뤼온은 전쟁터에서 막 돌아왔는데 아내가 화를 내서 빡침\n갈통쇼가 벌어지던 와중 암피트뤼온은 알크메네가 자기 행세를 하는 다른 남자랑 잤다는 것을 알아냄\n암피트뤼온이 이걸로 따지니까 알크메네는 자기를 몰아내려고 시치미떼는거 아니냐고 함;;;\n결국 제우스가 미안해서 제우스가 찾아와서 내가 잘못했다고 서로 사랑하면서 지내라고 함\n로마의 희극작가 플라우투스도 비슷한 희극 &lt;암피트뤼온&gt;를 만듦\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제9강 아테네의 영웅 테세우스","url":"/posts/23/","content":"테세우스의 선택\n스파르타와 아테네의 영웅\n고대 그리스 시절에는 이탈리아 반도 일부, 이탈리아 남부, 시실리섬 동쪽, 튀르키예 일부도 그리스 땅이였음.\n이때 그리스 문명의 최전성기인 그리스 고전기를 이끈 두 도시국가 아테네와 스파르타가 있는데, 스파르타는 펠로폰네소스 반도 남쪽, 페르시아 제국을 두 번 승리한 아테네는 펠레폰네소스 반도 동쪽에 있음.\n각 도시마다 영웅이 있었는데, 스파르타의 영웅은 헤라클레스, 아테네의 영웅은 테세우스였음.\n도시국가 아테네\n아테네를 세운 왕은 케크롭스임. 그런데 사람인데 땅에서 혼자 태어났음; (굳이 따지자면 가이아의 아들로 봄)\n허리 위쪽은 인간이지만 아래는 뱀처럼 생겼음.\n케크롭스는 케크로피아를 다스리고 있었는데 케크로피아는 케크롭스가 다스리는 나라라는 뜻임.\n그런데 케크로피아를 두고 아테네와 포세이돈이 경쟁하게 됨.\n참고) 케크롭스의 자리를 빼앗으려고 한게 아니라 인간의 영역이 나눠진 것처럼 신들도 따로 영역을 나누고 있었음.\n아테네와 포세이돈의 경쟁\n포세이돈이 먼저 와서 케크로피아를 차지하겠다고 아크로폴리스에 삼지창을 박았고, 거기서 바닷물이 솟아올랐다고 함.\n근데 아테네도 와보니까 땅이 좋아보여서 아크로폴리스에 올리브 나무를 심음.\n결국 둘은 서로 경쟁하게 되고 케크롭스는 한순간에 포세이돈 아니면 아테네를 선택해야하는 곤란한 상황에 오게 됨;\n케크롭스가 결국 심판관이 되어서 선택했다는 이야기도 있고, 올림포스 12신이 심판관이 되었다는 이야기도 있고, 케크로피아의 시민들이 남성이든 여성이든(사실 중요한 대목임) 모여서 신들에게 우리 도시를 위해 무엇을 줄 수 있는지 말해주면 우리가 판단해서 수호신을 정하겠다고 했다는 이야기도 있음;;\n마지막 이야기에서는 포세이돈은 이동과 전쟁에서 유리한 말을 주겠다고 했고, 아테네는 먹을 수 있는 올리브 나무를 주겠다고 함.\n남자들은 전투력의 상징인 말을 주는 포세이돈을 찍었고, 여자들은 살림에 도움이 되는 올리브를 주는 아테네를 찍었다고 함; 근데 여성들의 수가 더 많아서 아테네가 뽑혔다고 함.\n포세이돈이 말 대신 물을 줬다는 이야기도 있는데, 아크로폴리스에서 물 대신 바닷물이 나와서 쓸모없다고 버려졌다고 함;;;\n어쨌든 아테네가 결국 이겨서 케크로피아의 수호신이 되고 도시 이름은 아테네가 됨.\n화가 난 포세이돈이 아테네를 물바다가 되게 했다는 이야기도 있음;;;\n아테네와 포세이돈의 경쟁 (다른 버전)\n케크롭스의 딸 중에 아그라우로스는 아레스와 사랑을 나누고 딸 알킵페를 낳았는데, 알킵페는 포세이돈의 아들 할리로티오스에게 겁탈을 당함.\n분노한 아레스는 할리로티오스를 혼내줬는데 너무 쎄게 때려서 죽여버림;;\n결국 아레스는 살인죄로 기소당하고 아레이오스파고스에서 신들에게 재판을 받음. 아니 평소에도 신들 살인 자주 하시던데\n아레이오스파고스는 최초의 살인죄 재판이 일어난 곳이라고 하고, 이름은 아레스의 할리로티오스 살인사건을 재판한 언덕이라는 뜻임.\n이정도는 아버지 된 도리로서 할만했다(?), 아레스가 진짜 살인하려고 했던건 아닌거 같다 등 여러 가지 이야기가 돌면서 아레스는 무죄판결을 받게 됨.\n이 이후로도 살인, 내란, 반란 등 큰 죄를 재판할 때 아레이오스파고스에서 재판을 했다고 함.\n케크롭스 이후의 이야기\n땅에서 태어난 크라나오스가 케크롭스의 뒤를 이어 아테네를 다스림.\n그런데 암픽튀온이 크라나오스를 쫓아냈고, 에릭토니오스가 암픽튀온을 쫓아냄...\n대장장이의 신 헤파이스토스에게 무기를 만들어달라고 아테나가 찾아왔는데, 헤파이스토스가 아테나를 보고 반해서 접근함.\n그런데 아테나는 순결한 처녀신이였고, 헤파이스토스를 굉장히 불결하다고 생각해 거부함;;;\n하지만 헤파이스토스는 참지 못 하고 사정을 하고 정액이 아테나의 허벅지에 묻음....\n아테나는 빡쳐서 솜으로 정액을 박박 닦아서 땅에 내던졌다고 함.\n근데 정액이 묻은 솜뭉치가 땅에 닿자마자 에릭토니오스가 생겼다고 함.... (에릭토니오스는 솜뭉치(에리)가 땅에(토니오) 닿아 태어난 자식이라는 뜻임)\n에릭토니오스는 샘의 요정 프락시테아와 결혼해 아들 판디온을 낳는데, 판디온에게 왕위를 물려줌\n세습이긴 하지만 최초의 권력투쟁 없는 평화로운 왕위 계승임;;\n여담) 판다온의 딸 프로크네는 아레스의 아들 테레우스와 결혼함. 근데 테레우스가 판다온의 다른 딸 필로멜라를 납치해 겁탈하고, 이 일에 대해서 프로코네한테 얘기할까봐 필로멜라 혀를 잘라버림;;;\n필로멜라는 이 사실을 프로코네에게 알리기 위해 수를 새겨서 프로코네한테 줌\n이걸 본 프로코네는 테레우스한테 보복하는데, 프로코네와 테레우스의 아들을 죽여서 요리하고 테레우스한테 먹임;;;;;;\n아들을 맛있게 먹었다는 사실을 깨달은 테레우스는 필로멜라를 겁탈했다는 사실은 까먹고 프로코네를 죽이려고 함;\n프로코네는 신들에게 살려달라고 요청하고, 신들은 프로코네를 나이팅게일(밤꾀꼬리)로, 필로멜라는 제비로 변신시켜서 살렸다고 함.\n그런데 테레우스도 후투티로 변신해서 새가 된 뒤 계속 쫓았다고 함;;;;;;\n델피신전을 찾은 아이게우스 왕\n판디온 이후로 여러 계보와 세대를 거친 후에 아이게우스가 아테네의 왕이 됨\n(정확히는 판디온 -&gt; 에레크테우스 -&gt; 케크롭스 2세 -&gt; 판디온 2세 -&gt; 아이게우스, 2세는 딱히 관련있는건 아니고 이전 아테네 왕들 이름을 그대로 붙여줌)\n하지만 주변에 너무 경쟁자가 많았고, 아들이 없어 후계자가 없어서 걱정이 많았음.\n그래서 파르나소스 산에 있는 아폴로 신전이 있는 델피라는 도시로 감\n여기서 아폴론의 여사제 피티아가 세발솥 위에 앉아서 신탁을 받고 의뢰인들에게 얘기해줬다고 함.\n아이게우스도 후계자를 얻으러 신탁을 받으러 갔는데, &quot;그대가 아들을 얻기 위해서는 아테네로 돌아올 때까지 포도주 부대의 끈을 풀지 말라&quot;는 신탁을 받음.\n술을 안 마시면 아들이 생긴다???\n하지만 아이게우스는 신탁이 이렇게 간단할리가 없다고 생각하고 트로이젠으로 가서 핏테우스를 찾아감.\n참고) 델피는 본토쪽, 트로이젠은 델피보다 남쪽에 있는 해안가에 있음\n핏테우스는 아이게우스의 친구인데 아이게우스가 신탁이 대체 무슨 뜻인지 알겠냐고 상담하러 옴\n근데 핏테우스가 그냥 술이나 한잔 하자고 함;;\n아이게우스: 아니 술 먹지 말라는 소리 아님??\n핏테우스: 신탁은 그렇게 간단한게 아니니 숨은 뜻을 찾기 위해서 술을 마셔보자\n그래서 술을 마시다가 아이게우스는 인사불성 상태가 됐는데 아침에 일어나니까 갑자기 아리따운 여자가 옆에 있었음;;\n알고보니 핏테우스의 딸 아이트라 공주였음\n핏테우스 생각: 끈이 포도주 부대가 아니라 바지끈인듯 (즉 야스를 의미하는 거였음)\n그냥 아테네를 갔으면 아무 문제가 없었을텐데 괜히 와서 고생중;;\n근데 핏테우스는 진짜로 아이게우스한테 아들을 만들어주기보단 자기 후계자를 만들려고 생각함;;;\n그래서 일부러 딸을 보내 외손자를 만들려고 했고, 만약 외손자가 아이게우스를 이어 아테네의 왕이 된다면 개이득\n하지만 아이게우스는 부인도 있고 아들을 낳는다는 보장이 없었기 때문에 아이트라를 데리고 갈 수가 없었음\n그래서 그냥 야스하고 런침;;;;\n그러면서 나중에 아들이 태어나면 보내달라고 함;\n아이트라와 테세우스\n아이트라는 나중에 실제로 아들 테세우스를 갖게 됨.\n어느 날 아이트라는 테세우스보고 집 앞에 있는 바위를 밀어보라고 함.\n테세우스가 밀어보니 아이게우스가 남겨둔 칼과 샌들이 나옴!\n알고보니 아이게우스가 바위 밑에 칼과 샌들 숨겨놓을테니 아들 낳으면 혼자의 힘으로 바위를 밀 수 있을 때 아들을 보내달라고 했음;;\n아이트라는 아이게우스가 말해준 대로 사실 너는 왕자니까 아테나로 가라고 했음;\n하지만 테세우스 입장에선 갑자기 자기랑 엄마를 버리고 간 아빠를 찾아가기도 애매함;\n심지어 테세우스가 아빠는 어디갔냐고 했을 때 아이트라가 포세이돈이 사실 너의 아버지라고 구라쳤어서 갑자기 평범한 인간(?)이 아빠라는 사실을 알고 충격받음;\n아이트라와 아이게우스가 야스한 밤에 꿈에 아테나 여신이 나타나 포세이돈 신전으로 가라고 했는데, 가보니 진짜 포세이돈이 있어서 포세이돈이랑도 야스했다는 신화도 있다고 함;\n근데 트로이젠에서 아테네로 가는 방법이 두 가지가 있었음.\n배를 타고 가면 안전하게 1~2일 이면 갈 수 있음.\n빙 둘러서 육로로 가면 강도들을 만나는 위협들이 있고, 몇 개월이 걸림.\n근데 테세우스는 굳이 육로로 감; 20살 동안 왕궁에서 너무 곱게 자랐는데 자기 능력과 한계를 모르겠다고 시험해보겠다고 굳이 어려운 길을 갔음;;\n여섯 과업의 도전\n곤봉 살인 악당 페리페테스\n결국 트로이젠에서 아테네로 빙 둘러서 가는데 에피다우로스에서 페리페테스를 만남\n얘는 강철로 된 철봉으로 지나가는 나그네한테 시비를 걸고 죽여버리는 악당임;;\n대장장이신 헤파이스토스의 아들으로 알려져 있음\n헤파이스토스가 아들한테 선물로 누구든지 다 이길 수 있게 곤봉을 만들어줬는데 그걸 가지고 나그네를 때려잡는 악당이 되어버림;;;\n테세우스는 결사적으로 싸워서 상대를 때려눕히고 피레페테스를 철봉으로 죽여버린 후 길을 계속 감\n신화는 상징적인 이미지와 이야기를 결합한 것! 진짜 철봉에만 집중하면 안 되고 이런 식으로 부모님한테 많은 것을 물려받았음에도 불구하고 선하고 아름다운 일에 쓰는 대신 약자를 괴롭히고, 갑질하고, 으스대고 멸시하는 사람들을 대표하는 인물이라고 생각하면 됨\n능지처참 악당 시니스\n이후 이스트피아로 가는데 사람들이 거긴 또 끔찍한 놈이 있다고 함;;\n지나가는 사람을 붙잡아 나뭇가지를 잡아당겨 팔다리를 묶은 다음 탁 놓아서 탄력으로 능지처참하는 악당 시니스가 있다고 함;\n테세우스는 상대를 때려눕히고 시니스를 똑같이 능지처참시킴\n크롬뮈온의 살인 멧돼지 파이아\n크롬뮈온으로 오니까 살인 멧돼지 파이아가 있음;;\nRecall) 튀폰과 에키드나의 자식들 중 하나\n얜 특이하게 인간이 아니라 괴물임\n발을 씻기는 불한당 스케이론\n메가라에는 지나가는 사람들한테 발을 씻게 하고 굴욕과 치욕을 준 다음에 절벽 밑으로 발로 뻥 차버리는 악당 스케이론이 있음;;;\n얘도 똑같이 발을 씻게 하고 절벽 밑으로 뻥 차버림\n씨름 깡패 케르퀴온\n엘레우시스에 오니까 얜 왕이 악당임;;\n씨름을 잘하는 것으로 소문이 나있는데 지나가는 사람을 붙들어놓고 씨름을 한 뒤 죽여버린다고 함\n살인마 침대주인 프로크루스테스\n호텔같은 곳이라고 생각하면 됨\n투숙객의 키가 크면 작은 침대에 재우고, 깨어나면 키가 너무 크다고 침대에 맞게 상대를 잘라버림\n투숙객의 키가 작으면 큰 침대에 재우고, 깨어나면 키가 너무 작다고 침대에 맞게 늘려버림\n테세우스는 하마터면 키가 커서 목이 잘릴뻔했지만 다행히도 이겨낸 뒤 프로크루스테스 키가 너무 크다고 잘라 죽였음\n당연히 침대 길이가 중요한게 아님! 자신의 기준을 놓고 상대를 거기에 맞추려고 강요하는 것을 프로크루스테스의 침대라고 함.\n자기 기준에 상대방을 맞추려는 사람들을 대표한다고 생각하면 됨.\n아테네의 왕자, 테세우스\n아이게우스를 만난 테세우스\n여섯 과업을 뚫은 테세우스는 마침내 아테네로 와서 아이게우스를 만남\n근데 만날 때 칼이랑 샌들을 감추고 가니까 아이게우스가 못 알아봄;;\n심지어 아이게우스 뒤에는 희대의 악녀 메데이아가 있었고, 이미 둘 사이에 남자아이가 있었음\n즉 테세우스만 없었어도 왕이 됐을 아이임\n근데 테세우스랑 아이게우스랑 닮은걸 보고 메데이아는 테세우스가 아이게우스의 숨겨진 아들임을 알아챔\n테세우스한테 자신이랑 아들이 쫓겨날 것을 염려한 메데이아는 아이게우스가 멍청하게 못 알아보자 이를 기회로 삼고 테세우스를 없애려고 함\n그래서 아이게우스한테 테세우스가 악당들과 괴물들을 때려잡으면서 왔다고 말해주니까 아이게우스가 얘가 나도 때려잡고 아테나 왕이 되려는 줄 알고 겁을 먹음;;;\n근데 악당들과 괴물들을 때려잡았다니까 아테네 시민들이 굉장히 기대를 함\n&quot;야 아이게우스 찌질하고 무능하지 않냐&quot;\n&quot;하 차라리 테세우스가 우리 왕이였으면...&quot;\n그래서 다짜고짜 없애기도 그렇고 사실 악당들과 괴물들도 때려잡았다는데 이기지도 못 할 것 같음;;\n마라톤의 황소를 잡은 테세우스\n테세우스를 어떻게 제거할지 고민하던 도중 마침 아테네 근방에 농작물은 물론이고 사람도 해치고 가축도 해치는 마라톤의 황소가 있었음\n근데 사실 이게 크레타의 황소랑 같은 황소임!\nRecall) 미노스가 왕이 되려고 포세이돈한테 요청했던 황소인데 미노스가 약속을 어기고 다시 안 돌려주자 포세이돈이 황소가 미쳐 날뛰게 만들어 크레타를 초토화시켰음;\n다행히 헤라클레스가 12개의 과업 중 하나인 크레타의 황소 잡아오기도 성공하면서 크레타도 평화로워지는데, 황소 잡아서 뮈케네 쪽인 아르고스로 온 뒤에 헤라클레스는 쓸데가 없는 황소를 그냥 풀어줘버림;;;\n미쳐 날뛰는건 그대로라 크레타의 황소 대신 마라톤의 황소가 되었는데, 아이게우스는 처리할 능력이 없어서 그냥 놔두고 있었음...\n메데이아는 아이게우스보고 잡으면 미친 황소 사라지니까 이득이고, 못 잡으면 테세우스가 죽으니까 이득이니 일단 마라톤의 황소를 잡아오라고 하자고 함\n아이게우스는 말을 열심히 포장해서 악당들이랑 괴물들 잡았다는 얘기 잘 들었는데 우리 땅에도 마라톤의 황소라는 미친 황소가 있으니까 잡아달라고 함\n근데 테세우스 입장에선 겨우 아빠를 만났는데 알아보지도 못 하고, 마녀라는 소문이 도는 새엄마도 있고, 이미 후계자인 아들도 있어서 딱히 자신이 필요도 없고, 딱 보니까 진짜 마라톤의 황소를 잡고 싶은게 아니라 자기를 죽이려고 하는 것 같음;;\n하지만 이미 여섯 과업을 이겨낸 테세우스는 이번에도 이겨낼 수 있을 것이라 믿고 용기를 가지고 마라톤의 황소를 잡아옴\n아테네의 영웅으로 환영받는 테세우스\n아이게우스가 런쳐버린 마라톤의 황소를 잡아오자 아테네 시민들은 테세우스를 영웅으로 인정함\n하지만 아이게우스는 오히려 테세우스를 더욱더 없애버려겠다고 생각함\n그래서 축하연회를 연 다음에 독을 단 포도주를 건네주려는 계획을 세움\n테세우스가 너무 빡쳐서 그냥 먹고 뒤져버리겠다고 생각하고 숨겨둔 칼을 꺼냄\n근데 칼을 본 아이게우스는 마침내 테세우스를 알아봄\n메데이아는 알아본 것을 보고 콜키스로 도망쳐버림;;\n다행히 아이게우스는 테세우스를 정당한 후계자로 공식 선언하고 큰 연회를 열어줌\n그리고 그 연회에서 운동경기를 열었는데, 주변의 각국에서 축하 사절들이 와서 축하해줌\n미노스 왕의 요구\n사절 중에는 크레타의 왕자인 미노스의 아들 안드로게오스도 있었는데, 모든 경기를 전부 참여해 전 종목을 석권함\n아테나인들은 남의 나라한테 따잇당하자 분노해서 미노스의 아들을 암살해버림;;;\n이 사실을 알게 된 미노스는 군대를 몰고 와 아테네를 공격했고, 아테네를 순식간에 함락시켜버림\n미노스 왕은 자기 아들 안드로게우스의 피 값으로 일 년에 한 번씩 아테네의 처녀 일곱, 총각 일곱을 바치라고 함;\n힘이 없고 나약하고 우유부단하고 무능력한 왕인 아이게우스는 어쩔 수 없이 매년 처녀 일곱, 총각 일곱을 바쳤음;;\n바쳐진 처녀 일곱과 총각 일곱은 미노타우로스가 있는 미궁 속에 넣어져 미노타우로스의 먹이가 됨;;\n미노타우로스와 테세우스의 결심\n테세우스는 어떻게든 해야겠다고 생각했으나 군사력으로 크레타를 이기긴 빡셌음\n그래서 테세우스는 미노타우로스를 잡아오겠다고 결심함!\n아이게우스는 깜짝 놀라서 너는 아테네의 미래를 짊어질 중요한 후계자니까 그냥 가만히 있으라고 함;\n다행히 테세우스는 이런 자신의 모습을 보면 백성들이 왕이 될 자격이 없다고 생각할거라면서 왕이 될 자격이 있다는 것을 보여주겠다고 비겁한 길을 가지 않고 미노타우로스를 잡으러 감\n성공하기 위해서 가는 것이 아니라 가야 한다는 의무감 때문에 가는 것\n왕자로서의 의무감, 가치, 책임감을 위해서 미노타우로스를 잡기로 결정함\n어떤 상황에서는 성공 확률보다는 그 일 자체의 의미와 가치 때문에 도전해야 하는 상황도 있다!\n테세우스를 돕는 아리아드네\n테세우스는 미노스 왕을 만나 미노스 왕의 분노를 이해하지만 본인도 아테네의 왕자인 만큼 이 일을 해결해야된다고 함\n그러면서 자신이 미노타우로스를 때려잡을테니 앞으로는 처녀와 총각을 요구하지 말라고 함\n미노스는 테세우스를 인정하고 한 번 잡아보라고 함\n이걸 보고 미노스의 딸 아리아드네는 테세우스한테 반함\n아리아드네는 테세우스한테 찾아가 이러지 말고 그냥 자기와 몰래 떠나자고 했지만 테세우스는 거절함\n더 반해버린 아리아드네는 미궁에서 나올 수 있는 방법을 알려줄테니 성공하면 자기와 결혼해달라고 함\n테세우스는 그냥 진짜 죽어야지 하고 왔는데 아리아드네의 말을 듣고 성공의 가능성을 보게 됨\n성공의 가능성이 없는 일처럼 보여도 도전하는 순간 성공의 가능성이 열릴 수 있음!\n아리아드네는 실타래를 주고 입구에 묶어놓은 뒤 풀면서 미궁에 들어가고, 미노타우로스를 잡은 후에는 실을 되감고 입구로 오면 된다고 함\n테세우스는 이 말대로 미궁으로 들어가서 미노타우로스를 때려잡고 미궁 밖으로 무사히 나옴\n아테네의 영웅이 된 테세우스\n미노타우로스까지 때려잡은 테세우스는 아테네의 영웅이 됨\n테세우스는 약속대로 아리아드네와 같이 크레타를 떠남\n근데 옛날에는 크레타에서 아테나까지 하루만에 못 가서 중간에 낙소스 섬에 들려서 쉬고 갔음\n테세우스와 아리아드네도 낙소스 섬에서 하루 묵고 가기로 했는데 테세우스는 아리아드네를 두고 몰래 떠나게 됨;;\n테세우스가 사랑 없이 그저 아리아드네를 이용만 했을 수도 있고, 적국의 공주를 데려오는게 꺼려졌을 수도 있음\n근데 신화에 따르면 테세우스가 묵는 동안 디오니소스 신이 테세우스의 꿈에 나타나서 자기가 아리아드네를 찜해놨으니 넌 조용히 떠나라고 했다는 신화도 있음;\n근데 미노타우로스를 잡으러 갈때 아이게우스가 테세우스 죽을거니까 검은 돛을 달아줬음;\n하지만 혹시나 성공한다면 검은 돛을 내리고 생명의 하얀 돛을 달고 돌아오라고 했음\n근데 수니온 곶에서 테세우스를 기다리던 아이게우스는 검은 돛을 달고오는 배를 보고 자살해버림\n알고보니 낙소스 섬에서 다급하게 나온 나머지 돛을 바꾸는 것을 까먹고 검은 돛을 달고 와버림;;;\n그래서 이 바다를 아이게우스가 빠져죽은 바다라는 뜻인 에게해(그리스식으로는 아이게우스의 바다)라고 부름\n하지만 진짜 그 영웅인 테세우스가 까먹었을까? 아니면 테세우스한테 다른 뜻이 있었을까?\n진실은 다음 시간에...\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제10강 스파르타의 영웅 헤라클레스","url":"/posts/24/","content":"헤라클레스의 탄생과 성장\n헤라클레스의 탄생\nRecall) 제우스가 암피트뤼온인 척 하고 알크메네랑 헤라클레스를 낳음;;;\n알크메네가 헤라클레스를 임신중일 때 제우스가 &quot;페르세우스의 후손으로써 태어날 자가 곧 뮈케네를 다스릴 것이다&quot;라고 말함.\n근데 헤라가 제우스 자식이 왕 되는게 싫었는지 헤라클레스를 자궁속에 묶어놔버림;;;\n결국 에우뤼스테우스가 8달만에 태어나면서 뮈케네의 왕이 됨\n근데 알크메네가 계속 아이를 못 낳고 산고를 치르고 있으니까 시녀였던 갈란티스가 아이가 나왔다고 큰소리로 어그로를 끎 소리침\n깜짝 놀란 출산의 여신 헤라가 내가 자궁을 막고 있는데 어떻게 애가 나왔지 하고 구경하다가 손을 놓게 된 바람에 헤라클레스가 태어나게 됨\n빡친 헤라는 괘씸한 갈란티스를 족제비로 만들어버림\n헤라클레스의 성장\n헤라클레스는 처음에 알카이오스의 자손이라는 뜻의 알케이데스로 불림\n뮈케네 왕은 안 됐지만 제우스는 어쨌든 헤라클레스를 큰 인물로 만들고 싶어서 헤라의 젖을 먹임\n심지어 헤라가 자고 있을 때 몰래 알케이데스를 달아서 젖을 먹였음;;;\n자다가 깜짝 놀란 헤라는 애를 탁 쳐버림\n근데 애가 너무 힘쎄게 젖을 빨아서 애를 탁 쳐버리자 젖이 분수처럼 나옴;;;;\n이게 갈락시아스(은하수, milky way)가 됐다고 함. 여담으로 milky인 이유는.... 이하생략\n근데 왜 하필 헤라의 젖임?\n알케이데스는 인간이기 때문에 불멸의 존재로 만들기 위해 여신의 젖을 먹임\n그리고 헤라한테 사과하려고 나중에 큰 인물이 됐을 때 헤라한테 영광을 돌리기 위해서 지분 만들어줌;;\n그리고 기간토마키아 때 인간이 필요해서 일부러 인간이랑 애를 낳은 것도 있음 (헤라랑 자식 낳으면 신 됨)\n하지만 헤라는 안 그래도 남의 자식인데 모유도둑질까지 한 헤라클레스를 끔찍하게 싫어해서 아기일 때 거대한 독사 2마리를 요람에 보냄\n하필 쌍둥이인 이피클레스는 요람에 있는 뱀을 보고 깜짝 놀라 무서워서 울고 있었는데\n헤라클레스는 양손으로 두 뱀을 잡아서 숨통을 끊어버렸다고 함;\n여담) 암피트뤼온이 쌍둥이 중에서 진짜 자기 자식이 누군지 알아내려고 뱀을 보냈다는 설이 있음\n뱀을 찢어버리는걸 보고 헤라클레스가 제우스 자식이고 이피클레스가 진짜 자기 자식이라는 것을 알아챔\n헤라클레스의 교육을 전담했던 이는 반인반마의 켄타우로스 족의 케이론임\n켄타우로스는 원래 괴물같은 이미지인데 케이론은 현자같은 이미지라고 함\n케이론은 여러 영웅들의 스승이었고, 아폴론의 아들 아스클레피오스, 트로이아 전쟁의 영웅 아킬레우스 등등의 스승임\n다른 스승들도 있었는데, 음악을 가르친 키타라 교사 리노스가 있었음\n참고로 하데스에 내려갔다가 자기 아내를 데려온 천재 음악가 오르페우스의 아들임\n근데 헤라클레스가 음악을 잘 못 함;; 그래서 리노스가 지적을 많이 함\n구박을 받던 도중 헤라클레스가 너무 화가 나서 악기로 리노스를 때려서 죽였음;;;\n테베 근처의 키타이론 산에 사자가 있었는데 주변의 가축과 사람들을 해하니까 헤라클레스가 가서 죽여버림;;;\n이때 주변 지역의 왕인 테스피오스가 같이 사자를 잡으러 갔는데 헤라클레스를 보고 테스피오스가 저런 아들을 얻고 싶었음;;\n그래서 헤라클레스를 궁으로 초대하고 매일마다 자기의 50명의 딸을 한 명씩 넣음;;\n그래서 50명의 딸들이 모두 헤라클레스의 아내가 됐고 자식들을 낳음;;;;\n근데 술을 많이 먹여서 헤라클레스는 한 명이 계속 들어오는 줄 알았다고 함;;;\n테베와 갈등 관계에 있던 뮈니아이인들의 왕 에르기노스가 테베랑 전쟁해서 이김\n사실 에르기노스의 아버지가 테베인들에게 죽임을 당해서 보복하려고 전쟁건거임\n승리한 에르기노스는 테베인들에게 매년 소 100마리씩 20년 동안 바치라고 함\n헤라클레스는 이걸 보고 아버지 암피트뤼온과 같이 전쟁을 치르고 에르기노스 왕의 사절들을 제거해버림\n결국 테베인들은 소를 더 이상 안 바치게 됨\n근데 사실 이렇게 많은 업적들을 이룰 동안 아직 이름은 알케이데스였음!\n헤라클레스의 실수와 정화\n당시 테베의 왕 크레온은 헤라클레스가 너무 마음에 들어서 자기 딸 메가라를 아내로 줘서 사위로 만듦\n이 때 세 명의 아들을 낳으면서 알콩달콩 지냄\n근데 헤라가 이걸 보고 빡쳐서 헤라클레스가 광기에 휩싸이게 하고,\n헤라클레스는 자기 아들들이 적이고 맹수인 줄 알고 화살로 쏴서 죽여버림\n헤라클레스는 이 사실에 충격을 먹고 자살을 시도함\n하지만 테세우스가 찾아와서 자살한다고 죄가 씻어지는게 아니니 자살하지 말고 죄를 씻는 방법을 찾아보라고 자살을 말림\n헤라클레스는 델피에 있는 아폴론 신전을 찾아갔음\n그랬더니 퓌티아 여사제가 튀린스에 살면서 에우뤼스테우스의 명령에 따라 12년동안 10개의 과업을 완수하면 정화될 뿐만 아니라 불멸의 존재가 될거라는 신탁을 내려줌\n그리고 이름을 더이상 알케이데스라고 하지 말고 헤라클레스로 부르라고 함\n헤라클레스의 열두 과업(1)\n1. 네메아의 사자 가죽\n사람들을 해치는 네메아의 사자를 잡아오라고 함\n이놈이 그 튀폰과 에키드나의 자식인 괴물임;;\n근데 이 사자는 가죽이 너무 두꺼워서 화살이 튕겨나감;;\n그래서 곤봉으로 굴속으로 사자를 몰아넣은 다음에 목을 졸라서 죽였다고 함\n참고) 엘리스 근처에 올륌피아가 있었는데 거기에 제우스 신전이 있음\n그 신전의 메토페에(지붕에 있는 부조/조각상) 헤라클레스의 열두 과업이 새겨져 있음\n물론 제우스 신전이 지금까지 남아있진 않고 메토페 일부만 올륌피아 박물관에 남아있음\n2. 레르나 호수의 히드라\n머리가 9개 달린 괴물인데 사람들을 잡아먹고 죽이고 그랬음\n특히 머리 9개인 뱀의 이빨에서 나오는 독이 치명적이라 스치기만 해도 죽었음\n심지어 잘려나간 자리에서 머리가 다시 솟아나고, 하나를 자르면 두 개가 나온다는 얘기도 있음;;;\n이건 너무 빡센거 같아서 자기 조카인 이올라오스를 데리고 다녔음\n그리고 머리를 잘랐을 때 그 자리를 불로 지져달라고 부탁함\n그랬더니 머리가 부활을 안 하길래 손쉽게(?) 잡음\n근데 이올라오스 도움을 받았다고 본인 힘으로 한게 아니라면서 에우뤼스테우스가 무효화시켜버림;;\n3. 케뤼니테스의 암사슴\n케뤼네이아라고도 하는 케뤼니테스 산에 아르테미스 여신의 상징물인 암사슴이 있는데 뿔이 황금뿔임\n괴물인건 아니고 그냥 이걸 잡아오라고 했음;;\n근데 암사슴이 너무 빨라서 헤라클레스가 아무리 쫓아가도 못 잡았음\n하지만 헤라클레스가 지구력이 더 좋아서 1년 내내 추격한 끝에 사슴을 잡았음\n물론 아무도 아르테미스 여신한테 밉보이고 싶지 않았기 때문에 그냥 잡아온거 본 뒤에는 풀어주라해서 풀어줬음\n4. 에뤼만토스 산의 멧돼지\n이 멧돼지도 덩치가 어마어마하고 사람들을 괴롭히고 어쩌구 저쩌구\n근데 튀폰과 에키드나의 자식들인건 아니고 그냥 멧돼지임;\n근데 에우뤼스테우스는 지가 시켜놓고 헤라클레스가 잡아온 멧돼지를 보니까 너무 무서워서 청동 항아리 속에 숨었다고 함;;;;; (사실 다른 괴물들도 잡아올때마다 이랬음)\n5. 아우게이아스의 외양간 청소\n엘리스의 왕 아우게이아스는 엄청나게 많은 소떼가 있었는데 소떼가 지내는 외양간도 엄청나게 컸음\n소떼가 너무 많아서 외양간에 소똥이 가득했음\n근데 이걸 에우뤼스테우스가 하루만에 청소하라고 함;;;;\n근데 헤라클레스는 과업을 수행중이라고 안 하고 그냥 아우게이아스한테 나 그냥 놀러왔는데 여기 하루만에 청소할테니 소떼 1/10을 달라고 함 (왕국의 1/10을 달라고 했다는 기록도 있음)\n당연히 청소를 못 할거라고 생각한 아우게이아스는 수락함\n헤라클레스는 엘리스의 흐르는 두 개의 강 알페이오스와 페네이오스를 외양간 쪽으로 강이 흐르게 만들어서 강물로 하루만에 외양간을 씻겨버림\n근데 이것도 에우뤼스테우스가 니가 성공한건 알겠는데 죄를 지은 것에 대한 정화 의식을 치르는걸 대가를 받으면서 했냐면서 성공 못 한걸로 무효화시켜버림;;\n참고) 이렇게 두 개가 무효화돼서 신탁은 10개였지만 헤라클레스는 열두 과업을 수행하게 됨\n6. 스튐팔리스 호수의 괴조\n험악한 괴성을 질러대면서 사람들을 잡아먹는 식인 새들이 있었음\n그래서 가봤는데 새가 늪지대나 갈대 속에 숨어서 보이지가 않음;;;\n아테나 여신한테 물어보니까 새를 잡으려면 시끄럽게 해서 날라가게 만들어야 한다고 함\n그러면서 캐스터네츠같은 딱딱이를 줌\n이걸 가지고 딱딱딱딱하니까 괴성을 지르며 괴조들이 도망치려고 날아감\n이걸 화살로 싹 다 잡아버림\n여담\n지금까지 6개의 과업은 모두 펠로폰네소스 반도에서 이루어짐!!\n나중에 스파르타를 중심으로 펠로폰네소스 동맹이 만들어지는데,\n헤라클레스는 스파르타 근처로 오지도 않았지만 펠로폰네소스 반도의 모든 문제를 해결해준 스파르타의 영웅이라고 가져다써버림;;;\n그리고 막상 시켰던 에우뤼스테우스는 진짜 시킨거 다 해버리니까 무섭기도 하고\n안 그래도 원래 헤라클레스가 왕이여야 하는데 헤라빨로 운 좋게 왕 된건데\n사람들 사이에서 헤라클레스 위상이 높아지고 자기 위상이 흔들리는 것을 느낌\n그래서 헤라클레스가 제발 과업을 실패하거나 죽어버리길 바라면서 더 어려운 과업들을 주게 됨\n헤라클레스의 열두 과업(2)\n앞의 6개의 과업은 그냥 근처 펠로폰네소스 반도에서 이루어졌는데\n헤라클레스가 너무 빨리 끝내니까 실패하게 만들려면 일단 시간을 끌어야겠다는 생각으로 멀리 보냄;;;\n7. 크레타의 황소\n그래서 바다 건너 크레타 섬에 있는 황소를 잡아오라는 미션을 줌\n대충 미노아가 빼돌린 미친 황소 맞음\n근데 헤라클레스가 이것도 배타고 가서 잡아옴;;;\n깜짝 놀란 에우뤼스테우스는 헤라클레스가 그냥 못 들어오게 막아버림;;\n어차피 과업은 완수됐으니까 빡친 헤라클레스가 황소를 그냥 놔버림\n미친 황소가 펠로폰네소스를 이리저리 다니다가 어쩌다보니 본토쪽으로 가서 마라톤의 황소가 됨;;\n이걸 나중에 테세우스가 잡아오게 됨\n8. 트라케의 디오메데스의 식인 암말\n트라케는 디오메데스라는 사람이 살고 있는데 트로이아 전쟁의 뛰어난 전사 디오메데스는 아님;; (그냥 동명이인)\n디오메데스 왕은 전쟁의 신 아레스의 아들인데, 얘가 키우는 암말이 사람들을 잡아먹음;;\n그래서 이 암말들을 잡아오라는게 8번째 과업임 (참고 트라케는 더 멂;;;;)\n근데 당연하지만 디오메데스가 암말 잡는걸 반대했고, 결국 디오메데스와도 싸우게 됨\n다행히 전쟁에서 승리를 거두고 식인 암말도 가져옴\n깜짝 놀란 에우뤼스테우스는 암말을 보지도 않고 그냥 가져가라고 함;;\n9. 아마존 족의 여왕 힙폴뤼테의 황금 허리띠\n그래도 아직까진 주변 지역 정도는 됐었는데 이젠 아예 흑해 끝부분까지 보내버림;;;\n여기는 아마존 전사들이 살고 있었음 (우리가 아는 아마존 쇼핑몰이 여기서 이름을 갖고온 것!!!)\n참고) 아마존의 복수형은 아마조네스\n젖꼭지(마존)가 없는(아) 전사들이라는 뜻으로, 전부 여자들만 살고 있다고 함\n근데 아이를 낳을 수가 없으니까 임시로 남자를 데려와서 애를 낳은 뒤 죽이거나 쫓아냄;;;;\n애도 여자아이면 키우고 남자아이면 버려버림;;;\n아마존 족의 여왕 힙폴뤼테는 전쟁의 신 아레스의 딸인데, 아레스가 딸한테 준 황금 허리띠가 있다고 함\n근데 이걸 가져오라고 함;;;;; (에우뤼스테우스의 딸이 갖고 싶어했다고 함;;;;)\n근데 힙폴뤼테랑 만나니까 서로 마음이 통했음\n그래서 의외로 서로 대판 싸우지는 않고 우선 대화를 하기로 함\n헤라클레스가 자신이 과업을 시작하게 된 계기와 황금 허리띠가 필요한 이유를 설명해줌\n그러면서 허리띠를 빌려주면 과업을 완수한 뒤에 다시 돌려주겠다고 함\n힙폴뤼테는 다행히도 허리띠를 빌려주기로 함\n근데 헤라가 보니까 너무 날먹을 하는거임;;;\n그래서 좀 더 어렵게 만들기 위해서 헤라가 직접 아마존 전사로 변신한 다음 헤라클레스가 힙폴뤼테를 죽이려고 한다는 헛소문을 퍼트림;;\n그래서 아마존 여전사들이 무장해서 헤라클레스를 죽이려고 했고, 헤라클레스도 결국 어쩔 수 없이 싸우게 됨\n헤라클레스는 결국 이겼고, 힙폴뤼테를 죽이고 허리띠를 가져옴\n10. 에뤼테이아의 게뤼오네스의 소떼\n지금까지 지중해 동쪽에서만 놀고 있었는데 이젠 아예 지중해 서쪽 끝까지 보내버림;;;\n에뤼테이아 섬에는 희안한 전사 게뤼오네스가 있었음\n일단 이름부터 희안한게 게뤼오네스는 게뤼온의 복수임\n근데 틀린 이름은 아닌게 허리는 하나인데 허리로부터 세 사람의 몸통이 튀어나와야있음;;\n머리도 3개 다리도 6개임;;; 그래서 단수로 보는 사람은 게뤼온, 복수로 보는 사람은 게뤼오네스로 부름\n어쨌든 이놈의 소떼를 몰고 오는게 과업임 (참고; 지중해 완전히 반대편에 있음)\n근데 소떼를 지키는 파수꾼인 개가 있었는데 머리가 두 개였음;;\n근데 어찌저찌 개도 잘 잡고 게뤼오네스도 잘 잡아서 소떼도 잘 몰고왔음\n이렇게 10개의 과업을 완수하는데 8년 1개월이 걸렸음\n근데 12년이나 줬는데 8년은 너무 쉽게 끝난다는 이유로 2개를 무효화하고, 결국 새로 과업 2개를 추가함;\n11. 헤스페리데스의 황금 사과\n다행히 얘는 펠로폰네소스 반도에 있었음\n헤스페리데스 정원의 황금 사과를 가져오라고 함\n근데 헤스페리데스 정원이 어딨는지 몰라서 지중해 서쪽 끝에 있는 카우카소스 산까지 감\n거기 프로메테우스가 벌받으며 묶여있었는데 프로메테우스는 앞을 내다보는 능력이 있으니까 정원 어딨는지 물어보려고 찾아간거임\n근데 가보니까 독수리가 프로메테우스 간 먹고 있어서 이것도 튀폰과 에키드나의 자식인 김에 헤라클레스가 화살로 독수리를 쏴서 죽이고 프로메테우스를 풀어줌\n그 대가로 프로메테우스는 헤스페리데스 정원의 위치를 알려주고, 그곳에 가면 프로메테우스의 형 아틀라스가 있는데, 하늘을 떠받치는 벌을 받고 있으니까 니가 대신 들어주고 도움을 청하면 아틀라스가 황금 사과를 줄 것이라고 함\n근데 사실 그냥 헤라클레스 정도면 황금사과는 직접 가져올만 함;;\n근데 일부러 아틀라스 대신 도와주라고 한건 제우스한테 묶여있었던게 빡쳐서 똑같이 제우스한테 반기를 들어줬다가 벌을 받던 아틀라스를 풀어주려고 했었음\n헤라클레스는 결국 하늘을 대신 떠받치게 되고 호구 아틀라스는 진짜 황금사과를 가져와 줌;;\n근데 눈 앞에서 약올리기만 하고 황금사과를 진짜 주진 않았음;;\n알고보니 아틀라스도 제우스 싫어했는데 마침 헤라클레스가 제우스 자식이니까 니 아빠 때문에 고생이 심했다고 대신 니가 하라고 함;;\n헤라클레스는 이러다가 평생 하늘을 짊어지게 될 뻔했지만 꾀를 냄\n그러면서 아틀라스 보고 니 말대로 하늘을 들어주긴 할텐데 처음에 자세를 잘못 잡아서 자세 고칠 때까지만 잠시 하늘을 대신 들어달라고 함\n아틀라스는 그 말을 그대로 믿고 하늘을 들어주고, 헤라클레스는 황금사과를 가지고 도망침\n12. 하데스(라코니아의 타이나론)의 케르베로스\n이것까지 성공하자 에우뤼스테우스는 그냥 끝장을 내야겠다고 생각함\n그래서 저승세계 하데스로 내려가서 파수꾼 케르베로스를 데려오라는 말도 안 되는 과업을 내림;;;\n근데 헤라클레스는 진짜 저승세계로 내려가서 하데스에게 과업 수행중이니 케르베로스를 잠시 빌려달라고 요청함\n하데스는 데려갈 수 있다면 데려가봐라라고 함 (=제발 데려가지 말아줘)\n근데 헤라클레스가 진짜 싸워서 초크를 쓰더니 케르베로스를 제압해버림;;\n그리고 진짜 에우뤼스테우스 앞으로 데려옴\n갑자기 저승세계 파수꾼이 눈앞에 나타난 에우뤼스테우스는 깜짝 놀라서 숨었고,\n이건 인정할 수밖에 없다고 생각했는지 헤라클레스가 10개의 과업을 수행한 것을 모두 인정하고 헤라클레스는 죄가 씻음받고 정화된 채 새로운 길을 가게 되었다고 함\n이 이후에도 기간토마키아에 참가하고, 트로이아를 정복하고, 마지막 전쟁에서 죽은 줄 알았는데 하늘로 올라가서 올륌포스의 신이 되기도 함;;;\n여담\n헤라클레스가 국내에서만 과업을 하다가 국외로 점점 과업을 수행하게 되는데,\n그냥 순전히 꼬와서 그런게 아니라 그리스가 암흑기에 있다가 점점 지중해를 정복하면서 영토가 커진 과정을 헤라클레스의 과업으로 표현한게 아니냐는 얘기가 있음\n즉 헤라클레스가 과업을 마친 곳은 전부 그리스 땅이 되었음!\n참고로 튀폰과 에키드나의 자식들이 많이 나옴\n\n1번째: 네메이아의 사자\n2번째: 레르나의 히드라\n10번째: 게뤼오네스의 소떼 (정확히는 소떼를 지키는 머리 둘 달린 개 오르트로스)\n11번째: 헤스페리데스의 황금사과 (사실 본 적은 없는데 황금사과를 지키는 뱀 라돈이 튀폰과 에키드나의 자식임)\n12번째: 하데스의 케르베로스\n\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제11강 테베와 오이디푸스","url":"/posts/25/","content":"카드모스의 테베 건설\n카드모스의 계보\nRecall) 아게노르는 포이니케 문명을 만들게 되고(사실 포이니케는 나중에 붙여진 이름이고 그 당시에는 튀로 문명으로 불림) 텔레팟사와 에우로페를 낳음.\n근데 제우스가 황소로 변신해 에우로페를 크레타로 납치하고 미노스를 낳고, 미노스는 미노아 문명을 만들게 됨\n근데 아게노르 입장에선 눈뜨고 제우스한테 딸을 납치당해버림;;\n그래서 아들 3명 카드모스, 킬릭스, 포이닉스한테 납치당한 누이 찾아서 데려오라고 내보냄\n근데 진짜 누이 못 찾아서 집에 안 옴;;;\n어쨌든 여기서 나오는 카드모스가 테베 문명을 건설하게 됨\n테베 문명, 미노아 문명, 뮈케네 문명이 전부 제우스와 이오의 후손으로부터 만들어짐!\n테베를 건설한 카드모스\n아게노르의 아들들은 그냥 에우로페 못 찾겠어서 킬릭스는 킬리키아, 포이닉스는 포이니케라는 나라를 건설하고 거기서 살게 됨\n카드모스는 딸 찾으러 온 자기 엄마 텔레팟사와 함께 떠났지만, 여정 도중에 텔레팟사는 죽게 됨\n혼자 남은 카드모스는 어떻게 할지 모르겠어서 델피로 가서 신탁을 받게 됨\n(그리스 사람들은 뭔지 잘 모르겠으면 그냥 신탁 받으러 갔음)\n그랬더니 신탁에서 &quot;나가면 암소 1마리를 몰게 될텐데, 암소가 지쳐 쓰려지는 곳에 도시를 건설하라&quot;고 함.\n???? 에우로페를 어떻게 찾냐니까 뭔 개소린가 싶지만 일단 나가보니 진짜 암소가 있어서 시키는 대로 함\n암소는 보이오티아(암소의 나라)를 지나 테베로 가서 지쳐 쓰려짐\n(테베랑 보이오티아는 사실 나중에 붙여진 이름; 원래 지명은 다름)\n신탁대로 지쳐 쓰러진 곳에 나라를 세운 카드모스는 처음에는 테베가 아니라 카드메아(카드모스의 나라)라고 했음\n하지만 나중에 테베라는 딸이 태어나자 나라의 이름을 테베로 바꿈\n신탁 덕에 나라를 세웠다고 생각한 카드모소는 암소를 아테나 여신에게 바침\n그 후 물이 필요해서 부하들에게 물을 찾아오라고 시켰는데 안 돌아옴\n답답해서 직접 가보니까 부하들이 물을 구하려고 동굴에 접근했는데 하필 동굴에 아레스의 용이 있어서 부하들을 다 죽여버림\n빡친 카드모스는 아레스의 용을 죽여버림\n이걸 보고 기립박수를 친 아테나는 죽은 부하들을 대체할 부하가 필요하지 않냐면서 용의 이빨을 뽑아서 땅에 뿌리라고 함\n그 말대로 했더니 땅에서 사람들이 솟아나옴; 얘들을 스파르토이라고 함.\n근데 너무 많이 나와서 아테나 여신이 돌을 던지라고 함\n던져보니까 누가 돌에 맞았다고 빡쳐서 옆사람이랑 싸움;;;\n이게 번지면서 패싸움이 되더니 5명만 살아남게 됨....\n이것도 역사적인 의미로 해석해보면 땅에서 태어난 사람들은 토착세력이고, 카드모스가(외래세력) 와서 토착세력과 힘을 합쳐 테베를 만들었다는 뜻이라고 함.\n헤파이스토스의 목걸이\n이후 신들이 하르모니아 여신을 카드모스의 짝으로 줌;\n하르모니아는 전쟁의 신 아레스와 아름다움과 사랑의 신 아프로디테의 딸임\n하르모니아는 Harmony, 조화라는 뜻의 그리스 말임\n이 결혼은 정말 천상의 결혼이라면서 신들이 축하해줬고, 헤파이스토스도 왔음 (참고: 아프로디테 남편암)\n근데 불륜을 눈앞에서 본 헤파이스토스는 놀랍게도 모욕감, 분노 등을 꾹 참고 목걸이를 하르모니아에게 선물해줌\n이 목걸이는 목에 차고 있는 한 젊음과 아름다움을 유지할 수 있다고 함 ㄷㄷㄷㄷㄷ (불사인지는 언급 안 됨)\n하지만 자기는 항상 아름다운 대신 대가로 가문에 상상도 못 할 재앙이 일어나게 된다고 함;;\n카드모스는 그냥 무시하고 꼈고 이 목걸이는 카드모스 가문(테베의 가문)의 여러 재앙의 원인이 됨;;;\n아마 자기만 젊고 아름다우려는 욕망이 공동체의 재앙이 된다는 메시지를 담고 있는 것 같음\n테베 왕가의 가문\n카드모스와 하르모니아는 4명의 딸과(아가우에, 아우토노에, 이노, 세멜레) 1명의 아들을(폴뤼도로스) 낳음\n그 뒤로 모두 행복하게 살았는데 카드모스는 인간이라 죽어버리고 하르모니아는 신이라 계속 살아있었음\n근데 제우스 등 신들이 카드모스는 훌륭하게 삶을 살았으니 좋은 곳으로 보내자며 엘리시온으로 보냄\n원래 죽은 혼백들은 하데스로 가게 되는데 영웅들이나 특별한 존재들은 엘리시온의 들판으로 가게 됨 (낙원같은 공간)\n카드모스는 엘리시온에서 뱀이 되는데, 하르모니아도 찾아와서 같이 뱀이 됐다고 함;\n5명의 스파르토이 중 가장 믿음직하고 힘이 쎈 자는 에키온이였는데, 카드모스는 큰딸 아가우에와 결혼시켜 아들 펜테우스를 얻게 됨\n근데 카드모스가 죽은 이후 폴뤼도로스가 원래 왕이 되는게 맞는데 3~4살 애기 상태라서 펜테우스가 테베의 2번째 왕이 됨\n이후 폴뤼도로스가 큰 이후에는 정상적으로 3번째 왕이 됐고,\n이후 폴뤼도로스의 아들 랍다코스가 4번째 왕이 됨 (사실 폴뤼도로스 죽었을 때 또 랍다코스가 너무 어려서 중간에 섭정 세력들이 다스리는 기간이 있었음)\n이후 랍다코스의 아들 라이오스가 5번째 왕이 됨 (얘도 어려서 섭정세력 있었음)\n펜테우스는 아들 오클라소스를, 오클라소스는 아들 메노이케오스를 낳음\n메노이케오스는 아들 크레온과 딸 이오카스테를 낳음\n근데 라이오스와 이오카스테가 결혼해서 아들 오이디푸스를 낳음\n스포) 신탁에서 라이오스와 이오카스테의 아들은 아버지를 죽이고 어머니와 결혼할 거라고 해서 버려지게 됨;;\n코린토스의 왕 폴뤼보스와 아내 메로페는 자식이 없어서 버려진 아기를 자기 아기인 것처럼 키우게 됨\n근데 나중에 신탁대로 오이디푸스는 라이오스를 죽이고 이오카스테와 결혼한 후, 2명의 아들과(에테오클레스, 폴뤼네이케스) 2명의 딸을(안티고네, 이스메네) 낳게 됨\n디오뉘소스의 탄생과 가문의 시련\n제우스는 이와중에 카드모스의 딸 세멜레랑 사랑을 나눠서 디오뉘소스를 낳음;;;\n이걸 또 헤라가 질투해서 유모로 변신해 세멜레한테 접근함\n세멜레는 제우스랑 사귀고 있다고 유모(헤라)한테 티배깅함 ㅋㅋㅋㅋ\n빡친 헤라는 지가 제우스니 포세이돈이니 하면서 속이면서 여자의 순결을 빼앗는 주갤러들이 많다고 분노하며 너도 제우스랑 사귀는게 아니라 제우스 이름을 팔면서 너를 욕보이려는 남자(일반인)일 것이라고 함\n겁난 세멜레는 유모(헤라)한테 어떻게 하면 좋을지 상담함\n헤라는 스틱스 강에 멩세를 걸고 소원을 들어달라고 한 다음에, 진짜 제우스라면 헤라에게 다가갔을 때 모습 그대로 와달라고 소원을 빌라고 함\n세멜레는 이 말대로 제우스한테 소원을 빌지만, 제우스는 보통 남자의 모습 대신 신의 모습을 보여주면 세멜레가 죽어버릴거라고 함\n오해한 세멜레는 제우스보고 가짜라면서 화냄;;\n답답한 제우스는 본모습을 보여주고, 세멜레는 제우스와 함께 등장한 번개와 광채에 타죽음\n근데 타죽은 세멜레 뱃속에 아이가 있었음;; (얘가 디오뉘소스)\n근데 아직 6달밖에 안 돼서 자기 허벅지 속에 넣어놓고 키우다가 허벅지가 뭔 자궁이여 나중에 제대로(?) 태어남\n그래서 디오뉘소스는 두 번 태어났다고도 함\n이스트미아 제전\n세멜레 죽여도 애가 기어코 살아나는 꼬라지를 본 헤라는 빡쳐서 디오뉘소스를 죽이려고 함\n헤라를 피하고자 이노(세멜레 언니)와 아타마스(이노 남편) 집에서 디오뉘소스를 여장시켜서 몰래 키움\n처음엔 헤라도 디오뉘소스 사라진 줄 알았는데 여장했다는 것을 눈치채고 이노와 아타마스 부부에게 저주를 내림\n이 부부는 두 아들 레아르코스와 멜리케르테스가 있었는데,\n아타마스는 큰아들 레아르코스를 사슴인줄 알고 화살로 쏴죽이고,\n이노는 작은아들 멜리케르테스를 펄펄 꿇는 솥에 집어넣어서 죽여버림\n다른 이야기로는 아타마스가 헤라 때문에 미쳐버려서 레아르코스를 불 속에 집어넣고,\n멜리케르테스도 불 속에 집어넣으려고 하는걸 이노가 보고 깜짝 놀라서 같이 도망침\n미친 아타마스는 계속 쫓아오고, 이노는 결국 절벽 끝에 도달하게 됨\n어쩔 수 없이 이노와 멜리케르테스는 절벽 아래 바다로 뛰어들게 되고, 아쉽게도 죽게 됨\n멜리케르테스 시체는 바다를 둥둥 떠다니다 이스트미아에 도달하게 됨\n이 아이의 죽음을 추모하기 위해서 운동경기 이스트미아 경기를 열게 됨\n하지만 절벽에서 떨어진 이노와 멜리케르테스가 불쌍하다고 생각했는지, 신들이 이노는 레우케테아 바다의 여신으로, 멜리케르테스는 팔라이몬 신으로 환생시킴\n다른 자매들\n아우토노에는 아리스타이오스와 아들 악타이온을 낳는데,\n악타이온은 아르테미스 여신이 목욕하는 것을 보다가 사슴이 되어버리고, 자신이 몰고다니던 사냥개에 뜯겨 먹히는 저주를 받음\n아가우에는 에키온과 아들 펜테우스를 낳는데,\n펜테우스가 2번째 왕이 된 이후 디오뉘소스가 고향으로 돌아오게 됨\n근데 돌아온 디오뉘소스를 사람들이 신으로 추앙하고 축제를 벌임\n펜테우스는 내 사촌동생이 무슨 신이냐면서 디오뉘소스와 신도들을 핍박함\n근데 아가우에가 디오뉘소스의 여신도라서 디오뉘소스가 저주를 걸음;;;\n그래서 아가우에가 펜테우스를 짐승이라 생각하고 찢어죽임\n결론) 디오뉘소스 엄마랑 이모들은 싹다 저주를 받음\n디오뉘소스가 태어났을 때 네 자매가 디오뉘소스를 돌봐줘서 헤라가 저주를 퍼부었다는 얘기가 있음\n또 다른 얘기로는 카드모스가 아레스의 용을 죽여버리니까 아레스가 빡쳐서 테베 가문에 저주를 퍼부었다는 얘기가 있음\n오이디푸스의 비극\n참고) 소포클레스는 오이디푸스를 주제로 걸작을 만든 비극작가임\n이것도 그 비극 내용\n테베에 닥친 역병과 오이디푸스\n오이디푸스는 테베의 왕이 되고, 아내 이오카스테와 네 자식들과 행복한 삶을 살고 있었음\n근데 테베에 역병이 닥치고 사람들이 오이디푸스보고 구해달라고 함\n오이디푸스는 델피에 가서 신탁을 받았는데, &quot;선왕 라이오스의 살인자가 도시에 숨어 있기 때문에 도시가 오염되어 있다. 오염된 도시 정화를 위해서는 살인자를 처벌하고 추방해야 함&quot;이라는 신탁을 받음\n오이디푸스는 범인을 찾아 도시를 정화하고 재앙을 몰아내겠다고 선언함\n그 후 테베에서 가장 지혜로운 예언자인 테이레시아스를 찾는데 테이레시아스가 절대로 말할 수 없다고 하는거임\n빡친 오이디푸스가 너가 살인에 연루되어있으니까 말 안 하는거냐고 협박하면서 넌 눈이 안 보이니까 직접 한건 아니고 공범이냐고 따짐\n개빡친 테이레시아스는 오이디푸스를 범인으로 지목함;;;\n물론 라이오스를 만나지도 못 한 오이디푸스는 테이레시아스한테 뭔 소리를 하는거냐며 믿지 않았지만, 테이레시아스는 당신은 지금 어떤 범죄를 저지르고 있는지도 모르고 누구와 살고 있는지도 모른채 끔찍한 일을 저지르고 있다고 하면서 떠남\n이렇게 사건이 끝나버리자 오이디푸스는 황당했음\n그러자 오이디푸스의 부인인 이오카스테는 신탁과 예언자를 믿지 말라고 충고함\n오이디푸스가 그걸 어떻게 무시하냐고 하자 이오카스테는 자기가 신탁 안 지켜지는거 봤다고 함;;\n그러면서 사실 자기가 라이오스 아내였는데 &quot;태어날 아이는 아버지를 죽이고 어머니와 결혼할 것이다&quot;라는 신탁을 받았지만 라이오스가 술먹고 날뛰어서(?) 애가 생겨버림 ㅠㅠ\n근데 애 인생이 너무 비참할거 같아서 발뒤꿈치에 구멍을 뚫어서 밧줄로 묶어서 키타이론 산에 버려서 죽여버렸고이게 더 비참한거 아닌가 라이오스는 포키스 삼거리에서 도적떼에게 죽었으니 신탁이 이루어지지 않았다고 함\n그런 착한 이유 때문이 아니라 아이가 발이 아파 찾아오지 못 하도록 이랬다는 얘기도 있음;;\n참고로 오이디푸스라는 이름은 발이(푸스) 부어 오른 자(오이디)라는 뜻임\n오이디푸스의 과거\n근데 오이디푸스가 이 말을 들어보니 자기 얘기라 충격을 먹음;;;\n오이디푸스가 코린토스의 왕자로 살고 있을 때 사실 코린토스의 왕 폴뤼보스는 자신의 친아버지가 아니라는 소문을 들음\n마음이 불안해진 오이디푸스는 델피로 가서 부모가 누구인지 알려달라고 신탁을 받음\n근데 갑자기 아버지를 죽이고 어머니랑 결혼한다는 신탁을 받게 됨;;;;\n이게 코린토스의 왕과 왕비 얘기인줄 알앗던 오이디푸스는 깜짝 놀라서 코린토스에서 도망치고 테베 쪽으로 감\n근데 테베로 가던 도중 포키스 삼거리에서 왕족 무리랑 시비가 걸려서 오이디푸스가 왕족 무리를 죽여버렸음;\n게다가 테이레시아스가 뜬금없이 자기를 지목한게 생각나서 테이레시아스가 맞았다고 생각하게 됨\n오이디푸스는 이후 테베의 길목을 지키는 스핑크스를 만나는데 지나가는 사람들에게 수수께끼를 내서 못 맞추면 잡아먹는 괴물임\n스핑크스는 오이디푸스에게 &quot;아침에는 네 발로, 점심에는 두 발로, 저녁에는 세 발로 걷는 것은? (이 짐승은 발이 적을수록 힘이 더 있다.)&quot;이라는 수수께끼를 냈지만 오이디푸스가 인간이라고 맞춰버림\n분노한 스핑크스는 날뛰다가 죽어버림...\n근데 라이오스가 누군가에 의해 죽은 탓에 왕의 자리가 비어있었는데, 누가 스핑크스를 잡아서 테베로 오니까 테베 사람들은 오이디푸스를 테베의 영웅으로 환영함\n마침 라이오스를 이은 임시 왕 크레온이 스핑크스를 물리친 자한테 왕의 자리를 주겠다고 약속한 뒤라 오이디푸스는 공석인 왕자에 등극하고, 이오카스테와 결혼하게 됨\n오이디푸스의 진실\n자신이 진짜 범인일지도 모른다는 불안에 휩싸인 오이디푸스를 코린토스의 목동이 찾아옴\n코린토스의 목동은 폴뤼보스가 사망했다면서 코린토스로 돌아가 왕위를 이으라고 함\n오이디푸스는 아버지가 자연사했으니 신탁이 안 이루어진줄 알고 안심했지만 아직 어머니랑 결혼한다는 신탁이 남아있으니까 못 돌아가겠다고 함\n그러니까 목동이 너는 애초에 폴뤼보스랑 메로페 자식이 아니니까 관련 없다고 함;;;\n이걸 지금까지 몰랐던 오이디푸스는 이게 무슨 소리냐면서 당황하지만 알고보니까 이 목동이 갓난아기인 오이디푸스를 폴뤼보스에게 준 장본인이였음;;\n오이디푸스는 그럼 갓난아기인 나를 너한테 준 사람은 누구냐고 물어보는데 이때 라이오스 살해 현장의 유일한 목격자인 테베의 목동이 찾아옴\n처음에 사건을 조사할 때 오이디푸스는 진짜 자기가 아닌 줄 알고 살인자를 찾으려 목격자를 소환했음;\n소환된 테베의 목동은 당시에는 도적 떼한테 라이오스가 죽었다고 했지만 사실 오이디푸스가 왕족 무리를 다 죽였다고 함;;\n근데 갑자기 이놈이 스핑크스 죽이고 왕이 되길래 발설하면 새 왕한테 죽어버릴 것 같아서 지금까지 숨어지냈다고 함...\n근데 이 목동이 라이오스가 자기 아들을 발뒤꿈치에 구멍을 뚫어서 밧줄로 묶어서 키타이론 산에 버리라고 애를 맡겼던 목동임\n목동은 키타이론 산에서 그 아이를 버리려고 했으나 애가 너무 불쌍해서 망설이던 도중에 마침 코린토스의 목동이 사정을 듣고 애를 달라고 하길래 그냥 줘버림\n그래서 죽어야 할 애가 살았고, 정말 신탁대로 아버지를 죽이고 어머니랑 결혼하게 됨\n이오카스테는 일이 돌아가는 꼴을 보다가 대충 눈치챈 다음에 오이디푸스한테 제발 범인을 찾으려고 하지 말고 당신이 누군지도 알려고 하지 말라고 함\n하지만 멍청한 오이디푸스는 백성들한테 약속했으니 반드시 범인을 찾아내겠다고 함\n답답한 이오카스테는 오이디푸스에게 당신은 당신이 누구인지 알게 되질 않길 빌면서 목을 매달고 자살함\n이제 신탁이 이루어졌다는 것을 깨달은 오이디푸스는 어머니(겸 아내)의 자살을 보고 어머니의 목에 있는 브로치를 뽑아서 더 이상 이 모든 것을 보고 싶지 않다면서 자신의 눈을 찔러서 자해함\n(라이오스를 살인한 범인을 처벌하겠다는 약속을 지키려는 의미였을 수도 있음)\n그 후 오이디푸스는 스스로 테베를 떠나면서 테베에서 추방됨\n이 때 오이디푸스의 딸(겸 여동생)인 안티고네가 오이디푸스를 인도해줌\n쫓겨난 오이디푸스는 이리저리 돌아다니다가 아테네 근교 콜로노스에 도착하게 되는데, 여기는 자비로운 여신들의 성지라 함부로 접근할 수 없었음\n하지만 그 당시 아테네를 다스리던 테세우스 왕의 배려로 콜로노스에서 생을 마감하도록 도와줌\n오이디푸스, 그 이후\n에테오클레스와 폴뤼네이케스의 분열\n오이디푸스가 추방되자 테베는 또 왕좌가 비어있게 됨\n그러자 오이디푸스의 아들 에테오클레스와 폴뤼네이케스는 왕권을 탐내면서 싸우게 됨\n이를 본 오이디푸스가 추방당한 아빠 걱정은 안 하고 왕권에만 눈이 멀어버린 두 아들들한테 저주를 내리면서 왕권을 두고 다투다가 서로를 죽이게 될 것이라고 함;\n참고) 에테오클레스는 영광이(클레스) 빛나는(에테오) 자, 폴뤼네이케스는 심하게(폴뤼) 다투는(네이케스) 자라는 뜻임\n오이디푸스의 저주가 두려웠던 두 아들들은 1년씩 교대로 왕권을 누리기로 함\n그러면서 1명이 왕이 되어있는 동안 다른 1명은 테베를 떠나있기로 함\n근데 에테오클레스가 권력을 잡자마자 약속을 어기고 계속 왕위를 이어감\n어쩔 수 없이 폴뤼네이케스는 망명하고 아르고스로 가게 됨\n폴뤼네이케스는 아르고스의 왕 아드라스토스의 딸 아르게이아와 결혼하고, 아드라스토스에게 부탁해 군대를 받고 테베 원정을 준비함\n아로그스의 예언자 암피아라오스는 전쟁을 떠나면 장군들은 다 죽고 전쟁에서 패배할 것이라면서 전쟁을 반대하지만, 폴뤼네이케스가 암피아라오스의 아내 에리퓔레를 하르모니아의 목걸이로 매수해 남편을 설득하게 만듦 (Recall: 영원히 젊음을 유지하는 목걸이)\n제1차 테베 전쟁\n아드라스토스는 폴뤼네이케스를 포함한 7명의 장수들과 함께 테베를 공격함\n테베가 문이 7개라 7명의 장수들이 같이 공격함\n원정 도중 네메아에서 너무 목이 말라서 네메아를 다스리는 뤼쿠르고스 왕과 에우뤼디케 왕비의 아들 오펠테스의 유모 휩시필레에게 물을 달라고 요청함\n유모는 물을 주려고 오펠테스를 잠시 땅에 내려놓는데, 얘는 혼자 힘으로 걸을 때까지 발이 땅에 땋으면 안 된다는 신탁이 있었음;;;\n결국 신탁대로 오펠테스가 갑자기 나타난 큰 뱀에게 물려 죽음...\n7명의 장수들은 뱀을 처치한 뒤, 억울하게 죽은 오펠테스를 위해 네메아 제전을 창설하게 됨\n테베에 도착한 폴뤼네이케스와 아르고스 군대는 테베를 공격하게 되는데 테베에서 가장 지혜로운 예언자인 테이레시아스는 테베가 승리하기 위해서는 크레온의(이오카스테의 오라비이자, 폴뤼네이케스와 에테오클레스의 외삼촌) 아들 메노이케우스를 아레스 신의 제물로 바쳐야 한다고 함\n이게 사실 아레스의 용 죽여서 저주를 받은거라 아레스를 달래려면 어쩔 수 없다고 함;;;\n크레온은 굳이 그래야 되나 싶어서 메노이케우스를 빼돌릴려고 했지만 메노이케우스는 테베를 위해서 자신을 바치겠다면서 낭떠러지에서 떨어져 죽음\n덕분에 테베는 6명의 장수들을 다 죽이게 되고, 마지막 남은 하나의 문에서는 폴뤼네이케스와 에테오클레스의 맞대결이 이루어지지만, 둘은 오이디푸스의 저주대로 서로를 죽이게 됨\n안티고네\n테베의 왕은 그나마 남은 왕족인 크레온이 이어가고, 크레온은 테베를 지키려고 한 에테오클레스에게는 성대한 장례식을 치르고, 테베를 침략하려 한 폴뤼네이케스는 들판에 버려서 개와 새의 먹이가 되도록 하라는 칙령을 내림(어기면 사형이라고 함)\n폴뤼네이케스의 여동생 안티고네는 이를 거역하고 폴뤼네이케스를 매장하려고 함\n하지만 안티고네는 현장에서 체포당하고, 크레온은 자신의 왕권을 지키고 전쟁 이후의 질서를 잡기 위해서 조카지만 사형집행을 하기로 함\n결국 안티고네는 무덤(을 감옥으로 만든 곳)에 산 채로 갇히게 됨\n안티고네와 약혼한 사이자 크레온의 아들인 하이몬은 크레온의 처사에 항의를 하고,(하이몬: 님 독재자임? 크레온: 님 여자한테 미쳐서 아빠한테 대드는거임?) 테이레시아스는 안티고네를 풀어줘야 한다고 권유함(죽은 자에게 예의를 표하지 않으면 신의 노여움을 사게 되고, 안티고네를 죽이는 것은 더 큰 죄다)\n하이몬은 아들따리라 씹어버렸지만 다 맞춘 예언자 테이레시아스는 무시할 수 없어서 크레온은 안티고네를 풀어주기로 결정함\n근데 무덤을 까보니까 안티고네가 자살을 해버렸음;;;\n크레온과 같이 왔던 하이몬은 죽은 안티고네를 보고 울면서 아빠가 내 약혼녀를 죽였다고 칼로 크레온을 찌르려고 함;;\n크레온은 일단 하이몬 칼을 피하고 도망쳤지만 나중에 하이몬이 안티고네 옆에서 자살을 했다는 소식을 듣게 됨\n크레온의 아내 에우뤼디케는 아들도 며느리도 자살을 했고 다른 아들은 전쟁 때 테베를 위해 희생하겠다면서 투신자살했다는 것을 알게 되자 에우뤼디케도 자살을 함\n교훈: 크레온처럼 고집스럽게 자신의 의지를 관철시키려다가 세 명의 식구들을 다 잃게 된다\n제2차 테베 전쟁\n아르고스 사람들은 계속해서 시신을 반송해달라고 했지만, 크레온은 우리도 많이 죽었는데 우리 백성들을 위로하기 위해서는 장수들을 돌려보낼 수 없다면서 시신 반송을 거부함\n아르고스의 왕 아드라스토스는 아테네의 테세우스에게 도움을 요청함\n테세우스는 군대를 몰고가 테베를 점령하고 시신을 강제로 반송시킴;;\n아르고스 사람들은 성대한 장례식을 치르고, 그 과정에서 카파네우스 장군의 아내 에우아드네는 남편이 죽었는데 왜 사냐면서 자기 남편을 화장하는 현장에 뛰어들어 타 죽음\n10년 뒤, 7명의 아르고스 장군들의 자식들이(에피고노이라고 함) 보복전쟁을 감행하고, 알크마이온을 사령관으로 선택함.\n폴뤼네이케스의 아들 테르산드로스는 암피아라오스의 아내이자 알크마이온의 엄마인 에리퓔레를 옷으로 매수해 알크마이온이 참전하게 함;\n알크마이온이 에테오클레스의 아들 라오다마스를 죽이자, 테베인들은 성을 버리고 도주하여 아르고스가 승리하게 됨.\n전쟁을 마친 알크마이온은 에리퓔레를 죽여 아버지의 복수를 완수함. (갑자기 왜 엄마를 죽이나 싶은데 사실 엄마가 목걸이에 정신팔려 아빠를 설득 안 했으면 아빠가 제1차 테베 전쟁을 안 나갔을 거고, 그랬으면 아빠가 죽지 않았을거임)\n그럼 에리퓔레 복수는 누가 해줌?? 아니 그걸 왜 함\n다행히(?) 복수의 여신들이 알크마이온을 쫓아가면서 계속 괴롭힘\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제12강 황금양털을 찾아 떠난 이아손","url":"/posts/26/","content":"그리스 문명사\n미노아 문명 BC 3000~1100\n뮈케네 문명 BC 1600~1100\n테라섬에서 화산폭발이 일어나서 쓰나미로 인해 미노아 문명이 멸망함\n그러다가 트로이아 전쟁으로 외부에서 적이 들어오고, 트로이아에서 전쟁하는 동안 그리스 쪽에서 내부 반란도 생기면서 뮈케네 문명도 멸망함\n그 이후 암흑기 BC 1100~800를 거침\n하지만 그 당시에도 도기 만들고 도기에 문양도 새겨서 이것도 문명이라면서 기하학의 시대라고 말하는 사람들도 있음\n그 이후 상고기 BC 800~480를 거침 (상고기는 고전기보다 앞선 시기라는 뜻)\n각 마을마다 도시국가 폴리스가 생김\n이 때 4대 범그리스 제전이 탄생함\n점점 정치적으로 안정되고 인구가 늘어나면서 새로운 문명으로의 도약을 준비하는 시기\n이때 인구에 비해서 땅이 부족하니까 막 확장을 하는데, 튀르키에 서부, 흑해, 튀르키에 남부, 이집트 동복부, 이탈리아 반도 남부, 시칠리아 섬의 동쪽, 프랑스, 에스페냐 등등이 그리스 땅이였음\n헤라클레스 12과업도 사실 상고기 때 그리스인들의 확장의 역사였던거임;;\n마침내 그리스가 찬란한 문명을 이룬 시기인 고전기 BC 480~323가 오게 됨\n모든 서양 문명들이 모델을 삼았던 시기임\n고전기는 페르시아 제국을 아테네와 스파르타가 함께 몰아낸 BC 480부터 시작됨\n이후 아테네와 스파르타는 번성하면서 서로 경쟁하다가 27년간 전쟁을 하다가\n알렉산드로스가 그리스를 통합하고 페르시아를 정복하고, 이집트와 인도 서부까지 진출하게 됨\nBC 323에서 알렉산드로스 대왕이 사망하면서 고전기가 끝남\n이후 헬레니즘 시대 BC 323~31가 옴\n알렉산드로스가 죽자 알렉산드로스가 정복했던 나라들이 4개로 쪼개졌다가 3개가 되면서 지중해버전 삼국지 시대가 열리게 되고, 이를 헬레니즘 시대라고 함\n이때 이탈리아를 기반으로 한 로마 제국이 그리스 본토, 페르시아, 이집트의 프톨레마이오스 왕조를 정복함\n참고) 프롤레마이오스 왕조는 엄밀히 말하면 이집트에 자리잡은 그리스 왕조로,\n마지막 왕/여왕은 클레오파트라인데 로마의 실력자 안토니우스랑 손을 잡고 권력을 잡으려고 하지만 옥타비아누스 장군이 맞서싸움\n클레오파트라&amp;안토니우스와 옥타비아누스의 전쟁에서 BC 31년 옥타비아누스가 승리하게 되는데, 이 때 이후로 그리스가 완전히 역사에서 사라지고 로마 제국의 시대로 넘어감\n상고기의 범그리스 4대 제전(Panhellenic Festivals)\n하여튼 상고기때 그리스인들은 우리가 뿔뿔히 흩어져서 살고 있지만 같은 문화, 같은 신을 섬기는 같은 그리스인임을 상기시키기 위해 4대 제전을 열었음\n참고로 아테네에서도 아테나 여신의 생일을 축하하기 위해 판아테나이제 제전을 4년마다 열고 전국 그리스인들을 초대했지만 아쉽게도 4대 제전으로 취급하진 않음\n\nBC 776년 올륌피아 (4년마다 열림) 제우스가 주신\nBC 583년 이스트미아 (2년 주기로 4/5월에 열림) 포세이돈이 주신\nBC 582년 퓌티아 (4년 주기로 델피에서 열림) 아폴론이 주신\nBC 573년 네메이아 (2년 주기로 7월에 열림) 제우스가 주신\n\n요약하면 4년마다 올륌피아-이스트미아/네메이아-퓌티아-이스트미아/네메이아 반복\n네메이아는 헤라클레스가 12과업 중 네메이아의 사자를 잡은 기념으로,\n올륌피아는 헤라클레스가 12과업을 마친 기념으로 열림\nQ: 근데 네메이아가 더 나중에 열리지 않음? 원래 신화와 역사는 다른거야 임마\n네메이아는 아르고스 전사들이 테베로 향하다가 오펠테스를 죽여버린 것을 추모하기 위해서 열렸다는 설도 있음\n이스트미아는 디오뉘소스 키워주다가 죽은 이노와 멜리케르테스를 추모하기 위해서 열림\n퓌티아는 아폴론이 열었는데, 아폴론이 태어나기 직전에 헤라가 아폴론의 어머니인 레토를 해치기 위해 괴물 퓌톤을 보내서 잡아먹으라고 했지만 제우스가 북풍의 신 보레아스를 보내서 레토를 오르튀기아로 보냄\n아폴론이 태어난 뒤 이 사실을 알게 되고 퓌톤을 화살로 죽여버리려고 함;\n퓌톤이 화살 맞고 파르나소스 산이 있는 델피까지 도망갔지만 결국 아폴론에 의해 죽음\n아폴론은 죽은 퓌톤을 땅에 묻고 그 위에 아폴론 신전을 세운 뒤 퓌톤을 물리친 것을 기념하기 위해 퓌티아를 열게 됨\n참고) 올륌피아 제전은 사실 월계관이 아니라 올리브관을 씌워줌\n사실 월계관을 씌워주는 제전은 퓌티아 제전!\n네메이아 제전은 야생 샐러리를 뜯어서 관을 만들어줬고,\n이스트미아 제전은 소나무 가지로 관을 만들어줬음\n그 지역의 가장 대표적인 것들로 관을 만들어준거라고 함;\n콜키스로 간 황금털 숫양\n프릭소스와 헬레\n간만에 이오유니버스에서 넘어가서 완전 새로운 족보에서 시작함\n아이올로스와 엔아레테의 아들 아타마스는 사실 처음에 네펠레와 결혼하여 프릭소스와 헬레를 낳음;;\n나중에 네펠레를 버리고 이노와 결혼하여 레아르코스와 멜리케르테스를 낳음\n네펠레는 근데 사실 구름으로 만든 여인임!\n악시온이라는 못된 왕이 있었는데 제우스가 좋게 봐줘서 올륌포스 궁전까지 초대도 함\n근데 악시온이 헤라를 보더니 욕정에 사로잡히고 헤라를 범하려고 함;;\n헤라가 제우스한테 일러바쳤지만 제우스가 인간 따위가 그렇게 미쳤겠냐면서 안 믿음\n그래도 혹시 몰라 제우스가 구름으로 헤라랑 똑같이 만들어서 악시온이 있는 방에 넣어줬더니 악시온이 구름야스하려고 함\n제우스가 빡쳐서 악시온을 하데스로 던져서 죽여버림\n악시온은 불타는 수레바퀴에 매달려 영원히 불타는 벌을 받게 되었다고 함;\n구름으로 만든 헤라짭은 계속 남아있어서 네펠레라는 이름을 붙여줌\n근데 이노 입장에선 아타마스의 부인이 돼서 와보니까 프릭소스랑 헬레가 있는거임;;;\n자기 자식들이 잘 되길 바란 이노는 프릭소스와 헬레를 죽이려고 음모를 꾸미지만, 네펠레는 이를 보고 제우스에게 자식들을 살려달라고 부탁함\n제우스는 헤르메스를 보내고 헤르메스는 황금털 숫양을 보내고 네펠레가 이걸 프릭소스와 헬레한테 줌\n프릭소스와 헬레는 황금털 숫양을 타고 하늘을 날아 동쪽으로 계속 가지만, 이스탄불 근처 즈음에서 헬레가 바다에 빠져 죽음;;\n나중에 알고보니 헬레를 포세이돈이 구해줬다는 이야기도 있음\n콜키스에 도착한 프릭소스와 황금털 숫양\n프릭소스는 황금털 숫양을 타고 동쪽으로 가다가 콜키스에 도착함\n콜키스는 태양신 헬리오스와 페르세(오케아노스와 테튀스의 딸)의 아들인 반인반신의 영웅 아이에테스가 왕으로 다스리고 있었음\n아이에테스는 두 형제 페르세스, 알로에우스와 두 누이 키르케, 파시파에가 있었음\nRecall) 파시파에는 미노스가 아니라 황소와 결혼함\n스포) 키르케는 트로이아 전쟁 때 다시 나옴\n아이에테스는 프릭소스가 하늘에서 숫양타고 내려오니까 신기하게 생각해서 환대해주고 자기 딸 칼키오페를 프릭소스와 결혼시킴\n프릭소스는 제우스 덕에 살았다고 생각하고 자기를 구해준 황금털 숫양을 제우스에게 제물로 바치고, 제사를 지내면서 황금양털가죽을 벗겨내 아이에테스에게 선물함\n황금양털가죽은 온전하게 매달려있으면 쳐들어오는 적들을 퇴치할 수 있는 능력을 가지고 있어서 콜키스의 수호 성물이 됨!\n아이에테스는 아레스의 숲에 황금양털가죽을 걸어두고 거대한 용이 지키도록 만듦\n콜키스로 간 이아손\n아이손과 펠리아스와 이아손\n아이올로스는 아타마스 말고도 또 다른 아들 크레테우스가 있었음\n크레테우스는 튀로와 아이손을 낳고, 아이손은 폴뤼메데(또는 알키메데)와 이아손을 낳음\n튀로는 크레테우스 말고도 포세이돈과 동침하여 펠리아스와 네레우스를 낳음\n중요!! 아이손의 아들이 이아손임;;\n크레테우스는 이올코스의 왕이였는데 아이손에게 왕위를 물려주려고 하고 있었음\n근데 권력욕이 강한 펠리아스는 아이손과 네레우스 등 모든 형제들을 내쫓고 왕권을 차지함\n아이손은 펠리아스가 뭔 짓을 할까봐 이아손을 케이론에게 맡김\n혼자서 왕이 된 펠리아스에게 너도 왕권을 빼앗길 수 있다며 &quot;샌들을 한쪽만 신은 사람을 조심하라&quot;는 신탁이 내려짐\n근데 포세이돈(펠리아스 아빠) 제사를 드리는 곳에 한쪽 샌들만 신은 이아손이 등장함;;\n왜 샌들을 한쪽만 신었나 했더니 강을 건널 때 노파가 부탁해서 노파를 업고 강을 건너고 있었는데 힘들어서 샌들 한쪽이 강에 떠내려감...\n근데 강 건너고 노파 내려놓은 다음 잠깐 몸을 추스리는 동안 노파가 사라짐!\n사실 노파는 헤라였고 이아손이 어떤 인물인지 시험하려고 한 것; 이후 헤라는 이아손의 수호신이 됨\n이아손은 펠리아스를 만나자 왕위를 다시 돌려달라고 함;;\n펠리아스는 왕위의 자격을 시험해봐야겠다며 콜키스의 황금양털가죽을 가져오라고 명령함\n물론 그냥 가져오다가 죽어버리라는 생각이겠지만 이아손은 정말로 갖고오게 됨\n아르고호의 모험\n이아손은 콜키스로 가기 위해서 배가 필요했고, 프릭소스의 아들 아르고스가 아테나의 조언에 따라 50명이 노를 저을 수 있는 50노선을 건조하고, 도도나의 말하는 참나무를 뱃머리에 붙여 위험을 극복할 수 있도록 함 (근데 왜 콜키스에 있을 프릭소스 아들이 여기 있는지는 모름...)\n이렇게 아르고호가 완성됐고, 이아손이 콜키스에서 황금양털가죽을 가져오겠다고 홍보를 하자 그리스 전역에서 오르페우스, 헤라클레스, 카스토르, 폴뤼데우케스, 테세우스, 암피아라오스, 라에르테스, 멜레아그로스, 텔라몬, 펠레우스 등 50여명의 영웅들이 참가함. (아틀란테를 제외하면 다 남자)\n카스토르와 폴뤼데우케스는 제우스와 레다의 자식인 쌍둥이 형제로, 트로이아 전쟁에서 깽판칠 예정\n암피아라오스는 아르고스의 최고 예언자임 (테베 원정의 암피아라오스 맞음)\n라에르테스는 트로이아 전쟁의 영웅 중 하나인 오디세우스의 아버지임\n멜레아그로스는 칼리돈의 멧돼지 사냥으로 유명한 영웅임\n텔라몬과 펠레우스도 쌍둥이 형제인데, 텔라몬은 트로아이 전쟁의 영웅 아이아스의 아버지고, 펠레우스는 아킬레우스의 아버지임\n계속 보면 알겠지만 50명의 영웅들은 트로이아 전쟁에서 활약하는 영웅들의 아버지인 경우가 많음!\n그리고 중간에 헤라클레스 친구가 자기 노가 부러져서 노를 대용할 수 있는 나무를 하러 가겠다며 섬에서 내렸다가 실종됨;;\n그래서 이 친구를 찾겠다고 헤라클레스가 섬을 뒤지는데 아르고호가 까먹고 그냥 출발해서 헤라클레스는 중간에 빠지게 됨;;\n나중에 헤라클레스가 없는걸 발견하고 돌아가자고 하지만 이아손이 여기까지 왔는데 그냥 돌아갈 수는 없고, 어차피 헤라클레스면 알아서 잘 할테니 그냥 가자고 함\n이아손과 메데이아의 비극\n아르고호의 모험\n이아손과 50명의 영웅들은 무사히 콜키스에 도착했고, 바로 왕궁으로 달려감\n그 당시 콜키스의 왕은 프릭소스를 환영해준 아이에테스였는데 갑자기 배에서 수많은 영웅들이 내려서 오니까 침략하려 온 줄 알고 겁에 질렸음\n그때 아이에테스의 딸 메데이아가 이아손한테 사랑에 빠짐\n결국 권력만 생각하던 이아손과 사랑만 생각하던 메데이아가 결합하게 되면서 비극이 시작됨\n이아손은 아이에테스에게 황금양털가죽을 되찾기 위해 왔다고 하면서 사정을 알려줌\n어쨌든 프릭소스의 친척이긴 하지만 황금양털가죽은 콜키스의 수호성물이기 때문에 막 줘버릴 수는 없었음\n그래서 아이에테스는 청동 발을 가진 황소들에게 멍에를 지우고 밭을 간 후, 카드모스가 죽였던 아레스의 용의 이빨을 뿌리라고 명령함\nRecall) 카드모스는 아레스의 용을 죽이고 이빨을 땅에 뿌렸더니 땅에서 스파르토이가 솟아남; 근데 용 이빨을 반만 뿌리고 반이 남아있었는데 그게 아이에테스한테 있었음\n근데 이 황소들이 청동 발을 가졌을 뿐만 아니라 콧구멍과 입에서 불을 뿜고 엄청난 힘과 괴력을 가지고 있었음\n일단 안 한다고 할 순 없으니까 한다고 했고, 메데이아가 도와줄테니 대신 당신의 아내로 삼아달라고 함\n권력만 잡는다면 결혼도 할 수 있는 이아손은 메데이아의 제안을 받아들이고, 메데이아는 약초들로 마법의 약을 만들 수 있는 능력으로 황소들을 잠재우는 약을 만듦\n이아손은 이 약으로 황소들을 순하게 만들고 밭을 갈은 뒤 용의 이빨을 뿌림\n그랬더니 똑같이 전사들이 땅에서 솟아났는데 이아손에게 달려듦;;\n근데 메데이아가 돌을 던지라고 조언을 해줘서 이아손은 돌을 던지고 예전처럼 전사들끼리 싸움이 일어남\n하여튼 아이에테스가 말한대로 다 했지만 아이에테스는 약속을 어겨버림;;\n실망한 이아손을 메데이아가 도와주는데, 황금양털가죽을 지키는 용을 잠재우는 약을 만들어줘 용을 재운 뒤 황금양털가죽을 가지고 도주함\n이 사실을 알게된 아이에테스는 급히 군대를 보내서 메데이아와 이아손을 쫓도록 함\n메데이아는 아이에테스를 따돌리기 위해 동생 압쉬르토스를 데려감\n그 후 압쉬르토스를 죽이고 자기 동생을 토막낸 다음에 던져버림;;;;;;\n아이에테스는 토막난 자기 아들을 보고 경악하고 급하게 시신을 수습한 뒤 장례식을 치룸\n이 덕분에 아이에테스는 추격을 멈추게 되고, 메데이아와 이아손은 달아날 수 있게 됨\n그 뒤에도 아이에테스의 추격을 따돌리기 위해서 육로 등 일부러 험한 길을 택함\n또한 메데이아가 미친 짓을 했기 때문에 이 죄를 씻으려고 키르케(메데이아의 고모 겸 아이에테스의 누이)의 섬에서 정화도 받았고,\n섬에서 나올 때 세이렌이 아름다운 노래를 부르는데 세이렌은 노래를 들은 선원들을 잠재운 다음 자기들의 섬으로 인도해서 잡아먹는 식인 괴물임;;\n하지만 오르페우스가 노래를 부르자 세이렌보다 아름다운 노래라 아르고호의 전사들이 세이렌 대신 전부 오르페우스한테 갔다고 함;;\n하여튼 각종 난관을 극복하고 이올코스에 도착하게 됨\n펠리아스 앞에 선 이아손은 황금양털가죽을 가져왔으니 약속대로 왕위를 돌려달라고 함\n물론 펠리아스는 안 들음;; 이를 보자 메데이아가 또 나서게 됨\n메데이아는 신기한 것을 보여주겠다면서 펠리아스의 딸들을 부르고, 솥에다가 물을 펄펄 끓인 다음에 노쇠한 양을 토막내서 집어넣음\n그리고 메데이아가 준비한 약을 뿌리고 뚜껑을 덮고 다시 열자 노쇠한 양이 어린 양이 돼서 툭 튀어나옴\n펠리아스의 딸들이 와 정말 신기하네 하면서 푹 빠지자 메데이아는 노쇠한 펠리아스도 똑같이 젊어지게 만들 수 있으니 가서 토막내오라고 함;;;;;;\n펠리아스의 딸들은 정말 되는줄 알고 자는 도중에 펠리아스를 토막낸 다음 메데이아한테 옴\n메데이아는 똑같이 솥에다가 펠리아스를 넣고 약을 뿌리고 뚜껑을 덮음\n근데 이 약은 아까 염소를 되살렸던 약이 아니라 그냥 아무 효과가 없는 약임;;\n뚜껑을 열어보니 펠리아스는 그냥 인간 수육이 되어버림;;;\n깜짝 놀란 펠리아스의 딸들은 비명을 지르고 아니 니네가 토막냈잖아 이올코스의 백성들은 격분하여 이아손과 메데이아를 내쫓게 됨\n메데이아의 복수와 이아손의 최후\n왕권을 되찾으려고 뭐든지 했던 이아손은 왕권을 되찾기는 커녕 오히려 이올코스에서 추방당하게 됨;;\n메데이아와 이아손은 코린토스로 망명하고 10년 동안 아들도 둘이나 낳고 행복하긴 했음\n근데 10년동안 지내면서 사실 이아손 저놈이 황금양털가죽을 가져온 씹에이스라는 소문이 퍼짐\n코린토스의 왕 크레온은 이아손이 탐이나 자신의 딸 글라우케 공주와 결혼시키려고 함(크레우사라는 설도 있음)\n이아손은 아내랑 아들도 둘이나 낳았지만 버려버리고 왕권에 눈이 팔려 글라우케 공주와 결혼함\n물론 이랬다가 토막살인 전문 메데이아한테 토막날 지경이니 이아손은 우리가 살기 위해서는 일단 이혼하고 자신이 글라우케 공주와 결혼해 권력과 부를 얻은 뒤 다 같이 풍족하게 살아야 한다고 설득함\n하지만 이아손 하나만 보고 수호성물 황금양털가죽도 도둑질하고 동생도 죽이고 펠리아스도 죽였던 메데이아는 당연히 받아들이지 않고 복수를 결심하게 됨\n에우리피데스라는 비극 작가가 이 복수 과정을 담은 &lt;메데이아&gt;라는 작품을 냄\n참고) 그리스의 3대 비극 작가 아이스퀼로스, 소포클레스(테베 이야기), 에우리피데스(메데이아 이야기)\n일단 메데이아는 이아손한테 글라우케랑 행복하게 살라고 축복한다고는 함\n그러면서 태양신 헬리오스가 준 신적인 물건 아름다운 화단과 아름다운 옷을 두 아들한테 넘겨주고,\n글라우케에게 결혼선물로 화단과 옷을 줄테니 두 아들을 친자식처럼 키워달라고 부탁함\n글라우케는 너무 기쁜 나머지 화단을 쓰고 옷을 걸치게 됨\n하지만 얼마 지나지 않아 화단에 불이 붙어 글라우케의 온 몸이 불길에 뒤덮이게 되고, 옷은 점점 글라우케를 조였다고 함\n글라우케가 비명을 지르며 살려달라고 하자 크레온이 허겁지겁 다가옴\n근데 글라우케가 살려달라면서 크레온을 잡자 크레온도 불이 옮겨붙어 둘 다 불타죽게 됨\n글라우케는 불을 끄려고 샘으로 뛰어들었지만 그래도 불이 꺼지지 않았고, 이 샘을 글라우케의 샘이라고 함\n이제 남은건 이아손인데 메데이아는 자기와 이아손 사이에 태어난 두 아들을 죽여서 이아손한테 복수함;;\n그리고 자기는 헬리오스가 보내줬던 용이 끄는 마차를 타고 하늘로 올라감\n쫓아온 이아손은 왜 자기가 아니라 아들을 죽였냐고 메데이아한테 묻는데, 메데이아는 이아손을 죽인다면 고통은 그 순간뿐이지만 두 아들을 죽이면 이아손은 모든 것을 잃게 되는 고통을 겪게 될 것이라고 함;;\n이후 메데이아는 아테네로 가게 되고, 아테네에 있던 아이게우스와 메데이아가 결혼하게 됨\n테세우스 이야기에서 메데이아가 희대의 악녀라고 했던 이유가 이거임;;;;\n혼자 남은 이아손은 절망에 빠져서 이리저리 떠돌다가 아르고호를 만나게 됨\n자기를 태워줬던 아르고호 밑에서 잠을 자다가 갑자기 아르고호가 무너져서 깔려 죽었다고 함...\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제14강 로마건국신화와 다양한 이야기","url":"/posts/28/","content":"그리스 신화와 로마 신화\n로마인들은 기원전 3세기 이후에 그리스 인들을 만나고 그리스 문화를 보고 깜짝 놀람\n그래서 그리스 문명을 받아들여서 로마를 개선하려고 했고, 이때부터 그리스를 모방하기 시작함\n그러면서 그리스 신화를 가져와서 로마 신화로 만들어버림;;;\n물론 학술적으로는 다른 부분이 있지만 그리스 신화랑 로마 신화는 거의 비슷\nbut 로마 신화에만 있는 이야기도 있다!\nex) 그리스는 딱히 건국 신화가 없음\nbut 로마는 자기들이 주인공인 신화를 만들어내고 싶어서 건국 신화를 만들어냄\n로마의 탄생과 신화\n그리스 역사와 로마의 역사\n그리스 문명, 더 나아가 유럽 문명 전체의 시작점은 미노아 문명! (BC 3000~1100)\n이후 뮈케네 문명 (BC 1600~1100)이 생기고 문명 끝자락에 트로이아 전쟁이 발발함\n긴 전쟁으로 인한 반란과 외부 세력의 침략으로 암흑기에 돌입함 (BC 1100~800)\n이후 상고기(BC 800~480), 고전기(BC 480~323)를 거침\n이후 알렉산드로스 대왕에게 정복당하고 알렉산드로스 대왕에게 정복당한 땅들끼리 경쟁하는 헬레니즘 시대(BC 323~31)가 옴\n로마는 BC 753년에 건국되어 7명의 왕이 다스리는 왕정으로 시작함\n하지만 7번째 왕이 아주 포악하고 그 아들이 귀족의 부인을 범하는 등 사건이 발생해서 BC 509년부터 로마 공화정이 시작됨 (참고: 아테네 민주정은 BC 508년)\n이후 라티움 전쟁, 삼니움 전쟁을 거쳐(BC 340~338) 영토를 이탈리아 반도 주변으로 확장한 후, 포에니 전쟁과 마케도니아 전쟁을 통해 서쪽의 카르타고, 동쪽의 그리스(알렉산드로스 대왕 시절 다스린 지역)을 정복함\n제1차 삼두정치\n다스리는 지역이 너무 많아지자 삼두정치를 시행하는데, 카이사르, 크라수스, 폼페이우스가 로마 영토를 나누고 싸우게 됨\n왼쪽을 폼페이우스, 오른쪽을 크라수스, 중앙을 카이사르가 다스렸고, 세 사람이 서로 내전을 벌이게 됨\n이후 카이사르가 승리한 후 황제가 강력한 권한을 가지는 제국을 만들기로 함\n하지만 공화정의 우두머리 브루투스에 의해 제거됨\n제2차 삼두정치\n이후 또 수많은 내전을 거치다가 겨우 옥타비아누스, 안토니우스, 레피두스 3명으로 다시 줄어듦\n레피두스는 아프리카 북쪽, 안토니우스는 그리스쪽 동쪽, 옥타비아누스는 서쪽을 다스리게 됨\n레피두스가 패배하자 안토니우스는 마지막 그리스 왕조 클레오파트라와 동맹을 맺게 됨\n하지만 BC 31년 악티움 해전에서 옥타비아누스가 승자가 되고, BC 27년 로마제국을 성립하게 됨\n아우구스투스 황제의 로마 제국\n옥타비아누스는 자신을 아우구스투스라고 부르라고 하고 제국을 세움\n이렇게 로마제국은 지중해 주변을 전부 로마 영토로 만들었고, 지중해를 Mare Nostrum(우리 바다)라고 부르게 됨\n레아 실비아와 강물에 버려진 아이들\n이탈리아 반도 중부에 도시국가 알바 롱가가 있었는데, 누미토르 왕이 대를 이어서 통치하고 있었는데 동생 아물리우스가 형을 몰아냄\n이후 싹을 없애겠다고 형의 아들들을 죽이고 딸 레아 실비아를 베스타 신전의 여사제로 만듦\n베스타 신전은 그리스의 헤스티아 신의 신전인데 여기 여사제는 독신으로 살아야 했음\n근데 마르스(그리스에선 아레스)가 찾아와서 레아 실비아와 사랑을 나누고 쌍둥이 아들 로물루스와 레무스가 탄생함\n이 이야기를 들은 아물리우스는 독신이라 괜찮을 줄 알았는데 또 아들이 나오자 레아 실비아와 두 아이를 티베리누스 강물에 던져버림\n근데 티베리누스 강의 신인 티베리누스가 레아 실비아를 자신의 아내로 삼고, 나중에 레아 실비아는 불멸의 신이 됨\n로마의 탄생\n티베리누스는 두 쌍둥이 아이를 뭍에 올려놓았고, 늑대가 데리고 가서 쌍둥이는 늑대 젖을 먹고 자람\n그 때 왕의 목자 파우스툴루스가 늑대 젖을 먹는 두 아이들을 보고 깜짝 놀라서 거두어 키움\n성장한 로물루스와 레무스는 아물리우스를 죽이고 복수한 뒤, 누미토르를 다시 왕위로 올림\n이후 로물루스와 레무스는 알바 롱가 가까운 곳, 티베리누스 강이 흐르는 일곱 언덕에 나라를 세움\n로물루스는 팔라티누스 언덕에 나라를 세우고, 레무스는 팔라티누스 언덕을 마주보고 있는 아벤티누스 언덕에 나라를 세움\n근데 둘이 다투다가 로물루스가 레무스를 죽이고, 두 나라를 합쳐서 로마를 세움\n베르길리우스의 건국신화\n아우구스투스 황제는 권력을 확고히 다지기 위해 로마제국이 신들이 인정한 제국이라는 이야기를 만들기로 함\n바티칸 교황의 궁전에 4개의 방이 있는데, 북쪽의 서명의 방에는 파르나소스 산이 그려져 있음\n근데 여기 신전의 주인 아폴론, 뮤즈 여신들이 그려져있음!\n그 외에도 베르길리우스(그리스 최고의 시인), 호메로스(로마 최고의 시인), 단테 등 여러 시인들이 그려져 있음\n로마의 건국신화를 쓰는 시인, 베르길리우스\n아우구스투스는 베르길리우스에게 황제 등극과 로마 제국의 신화적 정당화를 위해 서사시를 집필해달라고 함.\n베르길리우스는 호메로스의 '일리아스'와 '오뒷세이아'를 모방하여 '아이네이스'를 쓰게 됨\n일리아스는 트로이아 전쟁을 다룸\n오뒷세이아는 집으로 돌아가는 오뒷세우스의 모험을 다룸\n아이네이스는 총 12권인데, 앞 1~6권은 아이네아스가 트로이아를 떠나 새로운 땅을 찾는 모험 이야기임 (오뒷세이아 표절)\n후반부 7~12권은 새로운 땅에 정착하며 원주민들과 벌이는 전쟁 이야기임 (일리아스 표절)\n아이네아스는 베누스(=아프로디테)와 트로이아의 왕족 앙키세스의 아들인데 트로이의 목마로 인해 멸망하던 도중 적들을 끝까지 막으려고 갑옷을 입고 있었는데 헥토르의 혼백이 나타나 그러지 말고 남아있는 사람들을 데리고 새로운 트로이아를 건설하라고 함;;\n물론 이 새로운 트로이아가 로물루스, 로마가 됨\n왜 베르길리우스는 로마를 만든 로물루스가 개쩐다고 하지 않고 로물루스를 거슬러가면 아이네아스가 있고, 아이네아스는 트로이아의 왕자라는 식으로 이야기를 했을까?\n로마는 그리스를 정복했지만 늑대 젖 뭐시기 하니까 출신이 좀 비천해보임;; 그래서 그리스한테 문화적 열등감을 갖고 있었음\n또한 족보가 윱피테르(=제우스)와 엘렉트라 -&gt; 다르다누스 -&gt; 엘렉토니우스 -&gt; 트로스 -&gt; 일루스와 앗사라쿠스와 가뉘메데스 -&gt; 카퓌스 -&gt; 앙키세스와 베누스 -&gt; 아이네아스라서 최고신 제우스로부터 로마가 나온 것이라고 주장할 수 있음\n새로운 트로이아를 건설하는 아이네아스\n새로운 트로이아 건설을 위한 모험을 떠난 아이네아스는 유노(=헤라)의 분노로 고생함\n황금 사과를 아프로디테한테 뺏겨서 아들한테 화풀이하는 중임;;\n하지만 윱피테르(=제우스)가 아이네아스가 이탈리아 반도에 나라를 세우고, 그 나라가 로마가 될 것이고, 로마가 세계를 지배하리라고 예언함\n그리고 아이네아스 후손 중에는 카이사르랑 아우구스투스도 나온다고 함;;\n아이네아스는 여정을 하다 카르타고에서 디도를 만나게 되고 사랑에 빠짐\n근데 윱피테르가 뭔 뻘짓 하냐면서 메르쿠리우스(=헤르메스)를 보내서 카르타고를 떠나라고 함\n파리스와 달리 사랑 대신 조국을 택했다는 것을 보여주는 장면!\n하지만 디도는 자손들이 보복할 것이라고 하고 자살함;; 포에니 전쟁을 신화적으로 표현한 것\n아이네아스에서 아우구스투스로\n이탈리아 반도에 도착한 아이네아스는 베누스로부터 불카누스(=헤파이스토스)가 만든 무장을 입고 싸우게 됨\n이것도 아킬레우스 짝퉁;; 심지어 투르누스라는 적장을 물리친 것까지 똑같음\n이 후 새로운 트로이아가 건설되고, 그게 로마가 된다!\n오비디우스의 '변신이야기'\n베르길리우스는 아우구스투스한테 신의 선택을 받았으니 인간의 욕망대로 움직이지 말고 신의 뜻대로 움직이라고 빨아줌\n하지만 오비디우스는 약간 다른 시를 아우구스투스에게 바침\n오비디우스를 추방한 아우구스투스\n오비디우스는 원래 사랑의 시를 쓰는 사람임\n그러면서 모든 종류의 사랑은 용서될 수 있다, 사랑은 인간을 행복하게 만든다 같은 시를 썼음\n그러면서 사교계의 고위층 여성들한테 시를 읽어주고 연애감정을 칭찬하고 찬양했음\n근데 미풍양속을 지키는 아름다운 나라 로마를 만들고 싶은 아우구스투스 눈에 거슬렸음;;\n마침 아우구스투스의 딸이 불륜 스캔들에 휩싸이는데 이게 오비디우스 때문이라고 판단한 아우구스투스는 흑해 끝쪽으로 추방시킴\n그러면서 오비디우스가 썼던 여러 책들을 금서로 지정함\nㅈ된 오비디우스는 황제를 찬양하는 '변신이야기'를 써서 바침\n변신이야기\n세상은 카오스에서 코스모스로 변신함 (사실 변신이야기에는 변신만 300개가 나옴;;)\n헤시오도스는 카오스를 텅 빈 공간이라고 했지만, 오비디우스는 카오스를 혼란, 혼돈이라고 함\n그러면서 막 뒤섞여있던 세상이 점차 질서(코스모스)를 잡아갔다고 함\n근데 이 질서가 누구냐? 하늘에서는 윱피테르지만 땅에서는 아우구스투스라고 함\n아우구스투스는 내심 좋았지만 불륜충은 그래도 쫓아내야된다고 생각해서 추방함 (사실 시를 받고 같은 해에 추방시켰음)\n그냥 아무 근거 없었던건 아니고 그와중에도 카오스에서 코스모스로 가게 된 원동력이 Amor(사랑)이라고 시를 썼음;\n그리고 아래 내용처럼 시 안에 아우구스투스 돌려까기가 너무 많아서 추방할만 했음\n아폴로와 쿠피도\n데우칼리온과 퓌르라의 홍수 이후, 땅에서 수많은 괴물들이 탄생함\n그 가운데 하나가 퓌톤이었고, 아폴로(=아폴론)가 화살로 퓌톤을 물리침. 이를 기념하기 위해 퓌티아 제전을 개최함\n이 아폴로가 아우구스투스가 로마의 내전을 진정시킨 모습과 닮았다고 또 빨아줌;\n근데 퓌톤을 물리친 아폴로가 너무 당당해진 나머지 화살을 갖고 다니는 쿠피도(=에로스)보고 화살은 위험하니까 애는 저리 치우고 아폴로 같은 검증된 사람만 쓸 수 있다고 조롱함;;\n쿠피도는 복수를 위해 기회를 엿보다가 아폴로한테 황금 화살을 쏨; 황금 화살은 처음 본 사람한테 사랑에 빠지게 만드는데 마침 옆에 아름다운 요정 다프네가 있었음\n근데 쿠피도가 다프네한테 납 화살을 쏨; 납 화살은 처음 본 사람을 혐오하게 만듦\n사랑에 빠진 아폴로와 혐오에 시달리는 다프네가 추격전을 벌이다가 아폴로가 다프네를 잡음\n다프네는 아버지였던 강물의 신에게 도와달라고 요청하고 아버지는 다프네를 월계수로 변신시킴\n월계수로 변신한 다프네를 잊지 못 한 아폴로는 퓌티아 제전에서 승리자에게 월계수로 만든 관을 주기로 함 (원래는 떡갈나무 가지였음)\n근데 아폴로가 아우구스투스잖음; 사실 쿠피도는 오비디우스를 상징하는 인물\n아우구스투스가 권력을 가지고 세계를 평정했지만 사랑의 시를 쓰는 자기 자신을 무시하지 말라고 돌려까고 있었던거임;;;;\n아라크네와 미네르바\n미네르바(=아테나)는 여성들에게는 베 짜는 기술 등 솜씨의 신이기도 함\n그런데 베 짜는 솜씨가 너무 좋은 아라크네가 있었음\n사람들이 미네르바의 은총을 입었다고 칭찬하자 아라크네가 그런 은총 받은 적 없다면서 순수한 실력으로 미네르바 이길 수 있다고 주장함;;\n미네르바가 이 이야기를 듣고 노파의 모습으로 나타나 아라크네한테 미네르바한테 그런 식으로 얘기하는건 무례하니 사과하라고 함\n아라크네는 미네르바를 정말 이길 수 있다면서 사과하지 않겠다고 함;\n미네르바는 자신의 본모습으로 돌아오고 진짜 베 짜기 대결을 벌임\n미네르바는 인간이 신에게 오만하게 굴면 벌을 받는 아름다운(?) 장면들을 그려넣음\n아라크네는 권력을 가진 신들이 인간을 유린하고 괴롭히는 그림들을 그려넣음\n미네르바가 이걸 보고 그림의 내용에 대해 괘씸해서 화가 나고, 진짜 해보니까 아라크네가 더 베를 잘 짜길래 화가 남;;\n결국 미네르바는 추하게 베를 짜는 베틀로 아라크네를 때리고 아라크네가 짜던 베를 다 찢음\n아라크네가 자살하려고 하니까 넌 여기서 죽으면 안 된다면서 저주를 걸고 거미로 만들어버림\n근데 이게 왜 변신이야기에 있을까?\n아라크네는 글 솜씨가 좋은 오비디우스고 미네르바는 아우구스투스였음;; 돌려까기 2트\n사실 실력이 있으면 신들한테 나대지 말고 겸손하자는 교훈을 주는 이야기가 아니라 님못그인데 왜 인정 못 하냐는 이야기였음...\n참고) 베를 짠다, 거미가 거미줄을 짠다는 라틴어는 텍스투라라고 함\n바쿠스와 미다스\n어느 날 포도주의 신 바쿠스(=디오니소스)가 스승 실레노스를 잃어버림\n그런데 미다스가 실레노스를 찾아서 잘 대접해줌\n바쿠스가 미다스는 휼륭하다면서 소원을 하나 들어주겠다고 함\n미다스가 제 몸이 닿는 모든 곳이 황금이 되게 해달라고 함\n바쿠스가 후회할 것이라고 했지만 미다스가 해달라고 해서 그냥 해줌\n그랬더니 돌도 금덩어리, 궁전도 황금궁전이 돼서 신났음\n그런데 식사를 하려고 했더니 사과도 황금사과, 포도주도 금물이 됨\n신하한테 대신 먹여달라고 했는데 입에 닿아도 금이 되어버림;\n그러다 신하를 만졌더니 신하가 금덩어리가 돼서 모두들 도망감\n결국 딸까지 황금덩어리로 만들어버리게 되자 미다스가 후회하며 바쿠스를 찾아가서 이 능력을 없애달라고 함\n다행히 강물에 가서 씻으래서 씼었더니 정상으로 돌아옴\n그 후 권력이고 황금이고 다 싫다면서 숲 속에 살게 되었고, 숲을 지배하는 판 신을(신 이름이 판임) 팬파이프에 반해서 따라다녔다고 함\n그러면서 미다스가 또 정신을 잃어버렸는지 판과 아폴론이 겨뤄도 이길 것 같다고 칭찬해줌\n이걸 또 들어버린 아폴론이 직접 나타나서 판과 겨룸\n주변의 모든 심판관들은 아폴른의 음악이 더 좋았다고 했지만 미다스만 판의 음악이 더 좋았다고 고집을 부림\n아폴론이 너의 귀는 잘못됐다고 하면서 미다스의 귀를 쭉 늘려 당나귀 귀로 만들어버림\n미다스가 창피해서 몰래 숨기고 있었는데 이발사한테까진 못 숨김;;\n이발사한테 아무 말도 하지 말라고 명령했지만 당연히 씹고 갈대숲에서 임금님 귀는 당나귀 귀라고 외침\n근데 이것도 아우구스투스 돌려까기임;;\n황금 이야기는 그렇게 권력과 힘을 가져봤자 주변에 아무도 안 남는다는 경고\n아폴론 이야기는 너가 잘난건 알겠는데 모든 사람이 그렇게 생각하는 건 아니고 다른 사람은 판의 음악이 더 좋다고 생각할 수도 있는데 이런 다양성을 인정하지 않고 당나귀 귀로 만들고 있다는 풍자\n교훈을 주는 이야기인줄 알았던 로마신화가 싹다 아우구스투스 돌려까기였음\n신화는 영원하다\n아우구스투스는 끝까지 오비디우스를 추방하고 다시 부르지 않음\n삐진 오비디우스는 책의 마지막 부분에 이런 구절을 넣음\n\n내 작품은 윱피테르의 분노도, 불도, 강철 칼도, 걸신들린 노령도 없애지 못할 것이다. 내 몸에, 내 인생에 마침표를 찍는다고 해도 내 영혼은 별들 위로 실려 가고, 내 이름은 불멸하리라. 로마의 힘이 정복된 나라들로 펼쳐질 때, 그 어디든, 사람들 입으로 내 작품은 읽혀지리니, 나는 온 세대를 지속하는 명성으로 살 것이다.\n\n하지만 아우구스투스가 훨씬 유명해져서 인기조차 발림\n오비디우스가 뭔데 씹덕아~~\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"제13강 트로이아 전쟁 신화","url":"/posts/27/","content":"전쟁의 원인, 헬레네\n불화의 황금사과\n트로이아 전쟁에 대해 알아보려면 그리스 신화 초창기까지 거슬러가야 함;\n신들의 세계에서 권력을 잡은 제우스는 프로메테우스에게 &quot;어떤 결혼에 의해 태어난 아이에게 권력을 빼앗길 것이다&quot;라는 불길한 신탁을 전해 들음\n수소문한 끝에 알아낸 제우스가 가까이 해서 안 될 여신은 바다의 여신 테티스였음\n제우스는 인간 펠레우스와 테티스를 결혼시켜, 파멸의 싹을 자르려고 함.\n제우스는 설마 내가 신쓰레기도 아니고 유부녀랑 결혼하려 들진 않겠지 하면서 저주를 피한 것이 너무 기쁜 나머지 모든 신들을 초대해 성대한 결혼식을 열게 됨\n근데 에리스는 불화의 여신이라 초대하지 않았음;;;\n에리스는 자신을 따돌린 것에 화가 나서 결혼식에 찾아와 황금 사과를 던짐\n황금 사과가 번쩍번쩍해서 모든 신들이 쳐다보는 와중에 황금 사과에 테 칼리스테(가장 아름다운 여신에게)라는 말이 적혀있는걸 발견함\n그러자 세 여신이 당연히 자기꺼라면서 싸우게 되고 불화와 경쟁이 생김\n파리스의 선택\n세 여신이 서로 싸우면서 양보를 안 하니까 트로이아의 왕자 파리스에게 세 여신 중 가장 아름다운 여신을 선택하라고 함\n파리스는 ㅈ됨을 감지하고 한 신한테 줬다간 큰일날 것 같아서 가만히 있었음\n그러자 세 여신이 파리스를 매수하기 시작함;\n헤라 여신은 권력의 여신이니까 트로이아와 소아시아를 넘어 세계를 지배할 수 있는 권력을 주겠다고 함\n아테나 여신은 지혜와 지략의 여신이니까 어떤 적과 싸워도 이길 수 있는 지략을 주겠다고 함\n아프로디테 여신은 아름다움의 여신이니까 이 세상에서 가장 아름다운 여인과 사랑하고 결혼하게 해주겠다고 함\n파리스는 아프로디테 여신에게 황금 사과를 줌\n이 세상에서 가장 가치있는건 진정한 사랑이라고 생각하여 권력과 명예보다 진정한 사랑이 진정한 행복을 준다 생각하여 아프로디테 여신을 선택함\n또한 애초에 아프로디테 여신이 아름다움의 여신이긴 함;;\n하지만 헤라와 아테나는 이것 때문에 파리스와 트로이아를 미워하게 됨\n트로이아 전쟁의 시작\n아프로디테는 약속대로 파리스와 헬레네를 결혼시키기로 하고, 헬레네한테 파리스와 결혼하라고 설득함\n근데 헬레네는 이미 스파르타의 메넬라오스 왕과 결혼했었음;;; 파리스는 스파르타에 와서 메넬라오스와 얘기도 나누고 축제도 즐기다 몰래 헬레네와 도주함\n분노한 메넬라오스는 뮈케네의 왕이였던 자기 형 아가멤논과 연합군을 구성함\n당시 뮈케네는 미노아 문명을 잇는 그리스 문화의 최고의 패권을 갖고 있던 나라였음\n아가멤논이 그리스 전역에 사절을 보내서 연합군을 구성하게 됨;\n당시 트로이아는 동양과 서양을 잇는 길목에 자리잡은 나라라 굉장히 부강한 나라였다고 함\n그래서 헤라랑 아테나가 준다는 권력과 지략이 딱히 필요 없어서 안 받았다는 설도 있음;;\n하여튼 트로이아는 생각보다 강력한 나라였기 때문에 메넬라오스만 나가는게 아니라 아가멤논도 부탁하고 전 그리스의 도움을 받아서 트로이아를 정복하려고 함\n또는 신화적인 해석으로는 헬레네랑 결혼할 사람을 모집했는데 그리스 전사들이 너무 많이 와버렸다는 이야기도 있음;;\n그래서 선택받지 못한 그리스 전사들이 단합해서 스파르타를 멸망시킬까봐 걱정하고 있었는데 마침 오디세우스가 등장함\n오디세우스도 헬레네랑 결혼하려고 왔는데 헬레네의 사촌인 페넬로페가 너무 정숙하고 지혜롭고 아름다워서 왕을 찾아감\n그러면서 헬레네의 구혼자들이 반란을 일으키지 못 하도록 하는 묘안을 알려줄테니 페넬로페랑 결혼하게 해달라고 함\n그 묘안이 구혼자들에게 누가 헬레네의 남편이 되든 승복하겠다는 승복의 멩세를 하고, 헬레네나 남편이 위험에 빠지면 도와주겠다는 조건에 합의를 하도록 만드는 것이었음\n어차피 내가 될거라고 생각한 구혼자들은 다 맹세에 응했다고 하고, 최종적으로 메넬라오스가 헬레나의 남편이 됨\n하지만 맹세는 유효해서 그리스 전사들이 헬레네와 메넬라오스를 돕기 위해 참전했다고 함...\n아킬레우스의 참전\n그런데 연합국을 구성하니까 테티스와 펠레우스의 아들 아킬레우스가 전쟁에 참전해야만 이길 수 있다는 신탁이 내려짐\n사람들은 아킬레우스를 참전시키려고 했지만 아킬레우스는 헬레네 구혼자가 아니였기 때문에 딱히 맹세를 안 했고 참전을 할 필요가 없었음 (사실 오디세우스도 참전하기 싫어서 미친 척을 했지만 너도 페넬로페랑 결혼했지만 헬레네 구혼자1 아니냐면서 맹세한걸로 치고 끌려옴;;)\n테티스 여신은 참전하면 내 아들이 죽을게 틀림없다면서 아킬레우스를 빼돌리려고 함\n마침 스퀴로스 섬의 뤼코메데스 왕한테 50여명의 딸이 있었는데, 테티스는 아킬레우스를 여장시켜 뤼코메데스의 공주들 사이에 숨겨놓았음\n근데 아킬레우스가 뤼코메데스의 딸 중 한 명인 데이다미아와 눈이 맞아서 사랑을 나누고 네오프톨레모스라는 아들이 태어남\n한편 징집당해서 빡친 오디세우스가 스퀴로스 섬에 아킬레우스가 있는 것 같아서 꾀를 냄\n오디세우스는 여러 물건들을 파는 박물장수로 변해 여자들이 좋아할만한 화장품, 장신구 등등을 가져와서 팔기 시작함\n그 후 오디세우스는 뤼코메데스의 공주들이 혹해서 장신구에 눈이 팔려있는 와중에 아주 멋진 칼을 내놓음\n아킬레우스는 화장품이나 장신구에는 전혀 관심이 없다가 칼보고 눈이 돌아갔고, 이걸 보고 오디세우스가 너 아킬레우스지 하고 참전시킴;\n이피게네이아의 희생\n하여튼 그리스 전사들이 다 같이 모여서 전쟁을 위해 아울리스 항구에 모이게 됨\n근데 아가멤논이 심심해서 사냥을 나간 도중에 실수로 아르테미스 여신의 사슴을 죽이게 됨\n아르테미스는 진노해서 그리스 연합군이 출항하지 못 하도록 바람을 묶어둠\n이때 아가멤논이 사슴을 죽여서 일어난 일이니까 아가멤논의 딸 이피게네이아를 아르테미스 여신의 제물로 바쳐야 여신이 분노를 풀 것이라는 신탁이 내려짐;\n아가멤논은 어쩔 수 없이 전쟁을 위해 딸을 제물로 바치게 됨\n근데 당연히 딸은 참전하지 않았기 때문에 딸을 전쟁터로 불러야 했음\n물론 실수로 사슴을 죽였는데 너가 제물이 되면 전쟁할 수 있다고 하면 올리가 없음\n그리고 아가멤논의 부인인 클리타임네스트라가 당연히 웬 전쟁 때문에 딸을 희생시켜야 한다고 하면 반대할게 뻔함\n참고로 클리타임네스트라는 헬레네의 쌍둥이 자매로, 전설에 의하면 제우스가 스파르타의 레다가 너무 이뻐서 찾아가려고 했는데 그냥 다가가면 안 되니까 백조로 변신해서 다가갔다고 함\n백조가 울고 있으니까 레다가 백조한테 다가갔는데 갑자기 백조가 레다를 덮치고, 이후 열 달 뒤 레다가 알을 두 개 낳음\n한 알에서는 남자 쌍둥이 카스토르와 폴뤼데우케스가 나오고, 다른 알에서는 여자 쌍둥이 클리타임네스트라랑 헬레네가 나옴\n근데 헬레네가 진짜 제우스랑 레다의 딸이고, 클리타임네스트라는 스파르타의 왕 튄다레오스와 레다의 딸이라는 이야기도 있음\n하여튼 아가멤논은 딸을 전쟁터로 부르기 위해 부인보고 이피게네이아와 아킬레우스와 결혼시킬거니까 딸을 데려오라고 함\n그래서 부인도 오고 딸도 오고 아킬레우스도 왔는데 딸이 제물로 바쳐짐;;\n아가멤논한테 속았다는 것을 안 사람들은 전부 아킬레우스한테 분노하게 됨\n하여튼 이피게네이아는 아르테미스 여신한테 산제물로 바쳐지게 됨\n아르테미스는 아가멤논 얘가 진짜로 딸을 바치니까 딸이 불쌍해져서 이피게네이아를 구하고 사슴 한 마리를 대신 놓음\n아가멤논을 조금 긍정적으로 보려는 측에서는 딸을 몰래 빼돌린 다음 사슴을 올려놓고 사람들한테 아르테미스 여신이 딸을 데려갔다고 말했다는 이야기도 있음\n아니면 이피게네이아가 사슴으로 변신해서 바쳐졌다는 이야기도 있음;;\n아킬레우스의 분노\n아킬레우스의 분노와 제우스의 뜻\n이렇게 1184척의 배를 타고 10만명이 넘는 병사들이 트로이아로 진격하고 트로이아 전쟁이 시작됨\n전쟁한지 10년이 지나고, 그리스 인들은 주변의 여러 도시들을 정복하지만 트로이아는 정복하지 못 하고 10년째 대치중임\n일단 주변의 여러 도시들을 정복하면서 얻은 전리품들을 나누고 있었는데 전리품 중 하나가 크리세스의 딸 크리세이스였음\n크리세스는 마차에 값진 보물을 싣고 아가멤논을 찾아와 크리세이스를 돌려달라고 하지만 아가멤논은 전리품은 명예의 상징이라면서 거절하고, 한 번만 더 내 눈에 보이면 네 목숨은 없다고 협박함\n근데 크리세스는 아폴론의 사제였기 때문에 아폴론에게 기도하고 아폴론의 사제를 모욕하는 것은 아폴론을 모욕하는 것과 같다고 하면서 당신과 나의 모욕을 씻어달라고 함\n아폴론은 진노하고 올륌포스에서 내려와 화살로 그리스인들을 쏘아대는데 사실 아폴론의 화살은 역병의 상징이라서 역병이 돌면서 그리스인들이 죽어가게 됨\n아킬레우스는 회의를 소집하고 아가멤논한테 크리세이스를 돌려줘야 우리가 역병의 재앙에서 벗어날 수 있다고 함\n근데 아가멤논은 전리품을 이렇게 빼앗기면 자기 명예가 없어진다면서 크리세이스를 줄테니 대신 아킬레우스의 전리품이였던 브리세이스를 내놓으라고 함\n빡친 아킬레우스는 칼을 뽑아서 아가멤논을 죽이려고 했지만 헤라가 아테나 여신을 보내서 아킬레우스를 말림\n하지만 아가멤논을 죽이지만 않았지 빡친건 그대로라 아가멤논이 자신을 모욕한 것, 그리고 그리스 사람들이 모욕당하는 것을 그대로 보고만 있었던 것을 후회할 때까지 전쟁에 참여하지 않겠다고 선언함\n아가멤논은 오히려 브리세이스를 강탈하면서 너 없어도 된다면서 도발함\n아킬레우스는 이걸 보고 울부짖고, 아킬레우스 어머니인 테티스 여신이 이걸 보고 아킬레우스를 찾아옴\n개빡친 아킬레우스는 자기 어머니인 테티스 여신한테 제우스한테 좀 말을 해서 그리스 인들이 후회할 때까지 그리스 인들이 패배하게 해달라고 함\n제우스는 테티스의 말을 받아들였고, 아킬레우스의 명예 회복을 위해 트로이아가 계속 승리하고 그리스 연합군이 계속 패배하도록 만듦\n이때 트로이아의 최고 전사 헥토르가 가장 눈부신 활약을 보임\n아킬레우스의 분노와 헥토르의 죽음\n아킬레우스의 친구 파트로클로스는 그리스 인들이 계속 죽어가는데 가만히 있으면 욕을 먹지 않겠냐고 하지만 끝까지 참전을 거부함\n파트로클로스는 차라리 자신이 나가겠다고 하자 아킬레우스는 그건 된다고 하며 자기가 입고 있던 모든 갑옷 등 모든 무장을 벗어서 줌\n근데 그냥 단순한 갑옷이 아니라 아킬레우스 부모님이 결혼할 때 헤파이스토스가 만들어서 선물한 것임;;\n이걸 입고 있는 동안에는 지지 않을 것이라고 하고 실제로도 트로이아 군대를 밀어내지만 결국 헥토르와 1대1 대결에서 죽게 됨\n아킬레우스한테 파트로클로스는 자기 자신보다 더 사랑하는 친구였는데, 그 친구가 죽어버리자 아킬레우스가 또 울부짖음\n또 바다 속에서 테티스 여신이 올라오니까 아킬레우스가 자신의 똥고집으로 친구가 죽어버렸다고 이제 참전하겠다고 함\n테티스 여신이 드디어 애가 사람이 됐구나 생각하면서 좋아했는데 생각해보니까 얘가 무장을 다 빌려줘버렸음\n그래서 테티스 여신이 헤파이스토스를 찾아가서 갑옷 좀 만들어달라고 함;;\n아킬레우스는 테티스 여신한테서 헤파이스토스의 갑옷을 받아서 입고 전쟁에 참전함\n당연히 아킬레우스는 헥토르를 잡아 족치려고 함\n근데 알고보니 헥토르도 헤파이스토스의 갑옷이 좋아보여서 파트로클로스가 입고 있던 아킬레우스의 갑옷을 다 벗겨서 지가 입고 있었음;;\n하지만 아킬레우스가 가볍게 이기고 헥토르가 감히 어울리지도 않는 갑옷을 입는다며 다 벗기고 알몸으로 만듦\n그 후 알몸 헥토르의 발을 자기 마차 뒤에 꽁꽁 묶은 다음 트로이아 성을 세바퀴 돌게 됨\n아킬레우스는 무려 12일동안 이 짓을 반복함;;\n신들은 이를 보고 분노하게 되는데, 남들도 아버지를, 형제를, 아들을 잃었는데 아킬레우스는 지가 잃은 것만 생각하면서 시신을 과도하게 모독했다고 아킬레우스는 사람 새끼가 아니라고 함\n특히 아폴론이 제일 분노했고, 제우스는 이 일을 해결하기 위해 테티스를 또 부름;; 테티스는 아킬레우스 때문에 바다에서 하늘까지 3번 왕복함\n제우스의 말을 들은 테티스는 아킬레우스한테 신들의 뜻을 전달해줌\n그래도 아킬레우스는 엄마 말은 잘 들어서 헥토르의 시신을 돌려주기로 마음 먹음\n한편 헥토르의 아버지이자 트로이아의 왕 프리아모스는 헥토르가 모욕당하는 것을 보고 누가 가서 구하라고 함\n근데 아무도 안 가자 혼자서 한밤중에 직접 아킬레우스를 찾아옴;;\n갑옷을 벗고 쉬고 있는 아킬레우스는 전사의 모습을 벗어던지고 인간 대 인간으로 프리아모스를 만나게 됨\n프리아모스는 이제는 아들의 시신을 돌려달라면서 아킬레우스의 손에 입을 맞춤\n다른 사람들도 가장 사랑하는 사람을 잃었다는 슬픔을 겪는다는 사실을 깨달은 아킬레우스는 시신을 돌려주고 서로 껴안고 엉엉 욺\n이후 아킬레우스는 파리스가 쏜 화살에 유일한 약점인 뒤꿈치를 맞고 사망하게 됨\n참고) 테티스는 아킬레우스를 불멸의 몸으로 만들기 위해 스튁스 강물에 넣고 뺐는데 뒤꿈치를 잡고 넣어서 뒤꿈치는 스튁스 강물이 묻지 않아 유일한 약점이 됨\n네오프톨레모스, 필록테테스, 오뒷세우스의 활약\n아킬레우스도 죽고 헥토르도 죽었지만 전쟁은 끝나지 않았음;;\n신탁을 또 받아보니 최고의 명사수 필록테테스, 아킬레우스의 아들 네오프톨레모스가 참전해야 한다고 해서 오뒷세우스가 얘들을 또 참전시킴\n나중에 결정적인 트로이아의 목마 작전을 통해 트로이아를 함락시키고 그리스 인들이 승리하게 됨\n트로이아 전사들은 거의 대부분 죽고, 그리스 전사들은 전리품을 챙겨서 집으로 돌아감\n무사히 돌아간 사람도 있지만 고생하면서 10년만에 집으로 돌아간 오뒷세우스도 있음;;\n아트레우스 가문의 비극\n참고) 아가멤논의 아버지가 아트레우스임\n전쟁을 이끌던 총사령관 아가멤논은 막대한 전리품을 챙겨서 집으로 돌아감\n근데 집에서는 클리타임네스트라가 복수의 칼날을 갈고 있었음;; (사실 도끼로 죽이긴 함)\n전쟁 때문에 딸을 제물로 바친 아가멤논에 대해서 앙심을 품고 있었던거임\n아이기스토스는 아트레우스의 조카 쯤인데, 아트레우스와 아가멤논에 의해 아버지를 잃어서 복수를 위해 뮈케네에 와있었음\n그래서 마침 아가멤논을 죽이려던 클리타임네스트라와 같이 협력해 아가멤논을 죽임\n그러자 아가멤논의 아들 오레스테스와 딸 엘렉트라가 또 복수의 칼날을 갈았음\n결국 오레스테스가 클리타임네스트라를 죽였지만, 복수의 여신이 오레스테스를 괴롭히게 됨\n뭔 개판이여\n사실 훨씬 개판인데 요약한거라고 함;;\n오뒷세우스의 모험\n페넬로페와 텔레마코스\n전쟁이 10년째 지속되자, 페넬로페 주변에 시정잡배들이 꼬임\n그러면서 당신 남편이 죽었으니 우리 중 한 명이랑 결혼하라고 함\n물론 페넬로페가 중요한게 아니라 오뒷세우스의 빈 자리를 차지해 왕국을 먹어버리려고 한거임\n근데 페넬로페한테는 오뒷세우스의 아들 텔레마코스도 있었음\n오뒷세우스가 떠날 때는 갓난아기였지만, 전쟁 10년 + 오뒷세우스 뻘짓 10년으로 벌써 20살이 됨\n자꾸 시정잡배들이 꼬이니까 그냥 아빠를 찾아와야겠다면서 집을 나가게 됨;\n근데 메넬라오스와 헬레네는 7년만에 집에 돌아와서 잘 지내고 있었음\n텔레마코스는 메넬라오스와 헬레네에게 오뒷세우스의 위치를 물어봤고, 어디 섬에 잡혀있다는 얘기를 듣고 죽진 않았으니 잘 살아있겠네 하고 다시 집으로 돌아옴(????)\n근데 페넬로페의 구혼자들은 텔레마코스가 자꾸 아빠 찾으려고 해서 방해도 되고 죽이면 결혼도 쉽고 왕국을 차지하기도 쉬워서 텔레마코스를 죽이려고 벼르고 있었음;;\n오귀기아 섬의 칼륍소와 오뒷세우스\n한편 오뒷세우스는 오귀기아 섬에서 칼륍소라는 요정한테 7년동안 잡혀있었음 사실 고생은 3년만 함\n칼륍소는 가리는 자라는 뜻인데 오뒷세우스가 너무 마음에 들어서 남편으로 삼으려고 함\n하지만 오뒷세우스는 낮에 축제를 열면 참석하고 같이 즐기지만, 밤에는 항상 밖으로 나와서 집으로 가기를 열망했음\n신들이 이 모습을 보고 불쌍해서 오뒷세우스를 귀향시키기로 함\n칼륍소는 신들이 지맘대로 오뒷세우스를 귀향시키니까 기분이 나빴음\n그래서 오뒷세우스한테 자기와 같이 살면 영원한 젊음과 아름다움을 지키면서 영생을 살 수 있다면서 어떻게 할거냐고 물어봄\n하지만 오뒷세우스는 집에 돌아가겠다고 하면서 떠남\n나우시카 공주와 알키노오스 왕과의 만남\n오뒷세우스는 떠나자마자 개고생을 하는데, 폭풍우를 만나 배가 난파되고 죽을 뻔하다가 어떤 섬에 도착하게 됨\n그 섬의 공주였던 나우시카 공주가 아테나 여신의 지령을 받아 오뒷세우스를 만나 자신의 왕국에 초대함\n알키노오스 왕은 오뒷세우스를 환대하고 듣고싶은 노래가 있냐고 하니까 트로이아 목마 얘기를 해달라고 함 자기 얘기 해달라고 한거임\n근데 오뒷세우스가 이걸 들으면서 엉엉 울고 있으니까 알키노오스 왕이 뭐가 수상하다고 눈치를 챔\n그래서 정체를 캐묻자 오뒷세우스가 자신이 트로이아 전쟁에 참전한 오뒷세우스임을 밝히고 어떻게 여기까지 오게 됐는지 알려줌\n사실 이건 오뒷세우스 모험의 극후반이고 여러 우여곡절이 있었음;;\n식인거신 퀴클롭스 앞에서 '아무도 안'인 오뒷세우스\n시칠리 섬에서 괜히 동굴에 들어갔다가 퀴클롭스한테 동료들이 잡아먹히는 신세가 됨\n퀴클롭스는 나갈 때 동굴의 입구를 돌로 막아서 못 빠져나가게 함\n오뒷세우스는 퀴클롭스한테 포도주를 먹여 취하게 만들고 재운 다음 눈을 찔러버리고 탈출함\n빡친 퀴클롭스가 누구냐고 하니까 오뒷세우스는 자기는 '아무도 안'이라고 답해줌\n이후 퀴클롭스는 친구들한테 아무도 아닌 사람이 자기 눈을 찔렀다고 하니까 친구들은 그럼 신의 저주라도 받았나보네 하면서 가버림;;\n이 틈을 타서 탈출한 오뒷세우스는 열받으라고 사실 자기는 오뒷세우스라고 알려줌\n근데 알고보니 퀴클롭스는 포세이돈의 아들이였음;;;\n포세이돈은 자기 아들을 괴롭힌 오뒷세우스를 괴롭히기 위해 오귀기아 섬을 빠져나오자마자 배가 난파되도록 만듦 (칼륍소한테서 벗어나자마자 배가 난파된 이유...)\n키르케와 오뒷세우스\nRecall) 키르케는 메데이아의 고모\n키르케가 오뒷세우스의 동료들을 마법의 지팡이로 때렸더니 전부 돼지들이 됨\n오뒷세우스가 키르케를 사로잡고 마법을 풀게 함\n그런게 키르케가 오뒷세우스를 보고 자신의 남편으로 삼아서 1년 동안 키르케에 머물게 됨\n세이렌과 오뒷세우스\nRecall) 세이렌은 이아손과 아르고호 모험에도 나옴\n근데 오뒷세우스가 이 이야기를 듣고 세이렌 노래를 꼭 듣고 싶다고 생각함;;;\n그래서 자기 동료들은 귀에다가 밀랍을 넣어서 아무것도 안 들리게 만들고 자기만 귀 뚫어놓은 채로 돛대에 묶여 노래를 들음\n이아손은 세이렌에게 현혹되어 제발 풀어달라고 하지만 동료들은 아무것도 못 듣고 그대로 순항함;;;\n근데 독일의 철학자들은 이게 부르주아들의 문화생활을 상징적으로 나타낸 것이라고 함;;;\n노동자 계급은 아무것도 못 들은 채 노동만 하고 부르주아들은 혼자서 세이렌 노래를 다 듣는 모습이라고 함...\n구혼자들을 처단하는 오뒷세우스\n마침내 집으로 돌아온 오뒷세우스는 108명의 구혼자들이 자기 아내를 노린다는 사실을 알아냄\n자기가 오뒷세우스라고 하면 108명한테 맞아죽을까봐 일단 자기 정체를 숨기고 아들과 만남\n이후 작전을 짜서 구혼자들을 전부 응징(활로 쏨)하고 페넬로페와 행복하게 살았습니다~\n일줄 알았는데 나중에 또 집을 떠나게 되고, 다시 돌아오니까 이번엔 자기 아들의 손에 죽을 운명이라길래\n왕국에 들어가지 못 하고 바깥에서 가축을 치며 지내고 있었는데 웬 젊은이랑 시비가 걸림\n결국 젊은이의 손에 죽게 되는데 알고보니 젊은이는 키르케에 있었을 때 생긴 아들임;;\n","categories":["SNU","4-0","그리스 로마 신화"]},{"title":"Introduction","url":"/posts/33/","content":"Principle of Design\n\nIdentify needs\nInterview people, etc.\nCreate/imagine possible solutions\nBrainstorming and prototyping\nSelect and implement solution\nEvaluate the result\nWe call this HCI experiments\n\nWe need to understand human information processing (HIP) and cognitive limitations.\nWhat is HCI?\nTraditional CS courses is about technology, effiency, effectiveness, speed, and accuracy.\nHuman-computer interaction (HCI) is a dicipline concerned with the design, evaluation and implementation of interactive computing systems for human use.\nHCI has higher level goal - optimal user experience (UX).\n\nUsefulness (유용성 - 도움이 되는가)\nUsability (사용성 - 쓰기 쉬운가)\nAffection (감성 - 의외로 되게 중요한 영역)\n\nHow do we measure it?\n\nspeed of performance\nrate of errors by users\nretention over time\ntime to learn\nsubjective satisfaction (We can measure this with likert scale!)\n\nHCI Research Framework\n\nObservation\nCreate new theories/models\nIterative Design Process (Repeat this!)\n\nAcceptance\nAnalysis\nDesign/Definition\nIdeation\nIdea Selection\nPrototyping/Implementation\nEvaluation\n\n\nUse new theories/models\n\nCan used as a guide for practitioners\nCan used as an advice for evaluators\n\n\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"HCI and User Experience","url":"/posts/37/","content":"Moore's law\nComputer abilities are doubled each 2 years. But human abilities are almost in place...\nAI has nearly caught up with human performance!\nHCI\nHuman-Computer Interaction is a discipline concerned with the analysis, design, implementation, and evaluation of interactive computing systems for human use and with the study of major phenomena surrounding them.\nThe field of HCI is very diverse!\ne.g. focus on children, seniors, animals???\nContent of HCI\n\nN The Nature of HCI\n\nN1 (Meta-)Models of HCI\n\n\nU Use and Context of Computers\n\nU1 Human Social Organization and Work\nU2 Application Areas\nU3 Human–Machine Fit and Adaptation\n\n\nH Human Characteristics\n\nH1 Human Information Processing\nH2 Language, Communication, Interaction\nH3 Ergonomics\n\n\nC Computer System and Interface Architecture\n\nC1 Input and Output Devices\nC2 Dialogue Techniques\nC3 Dialogue Genre\nC4 Computer Graphics\nC5 Dialogue Architecture\n\n\nD Development Process\n\nD1 Design Approaches\nD2 Implementation Techniques\nD3 Evaluation Techniques\nD4 Example Systems and Case Studies\n\n\nP Project Presentations and Examinations\n\nGrowing importance of HCI\n\nUsers\n\nFor heterogeneous &amp; diverse users\nFor novice users with less technical expertise (Use as tools, not for goals)\nFor almost every task in everyday life\n\n\nTechnology\n\nMobile (e.g. smartphone, tabs, notes)\nUbiquitous (Need faster, reliable, smaller, cheaper technology)\nAmbient (Much bigger and more complicated technology)\n\n\n\nTerms of HCI Research\n\nUser Experience (UX)\nUser Experience is a person's perceptions and responses that result from the use or anticipated use of a product, system or service.\nHow does people feel about it?\nWe cannot design a user experience, we can only design for a ser experience.\nUX is shaped by the user and the context in which they interact with a product, which can't fully controlled by designers.\nGoal of HCI is optimal User Experience!\nCharacteristics of UX\na.k.a. why UX design is hard...\n\nHumane BUT Soft: UX should be considerate of human needs and behaviors but can sometimes be perceived as lacking in hard, measurable outcomes.\nStrategic BUT Abstract: UX design is often based on high-level strategic thinking to guide product development, but these strategies can be abstract and may not always translate directly into tangible design decisions.\nContextual BUT Uncertain: UX design should be contextual, taking into account the specific situations and environments in which a product will be used. However, because contexts can change and are often complex, this can lead to uncertainty in the design process.\n\nDesigning UX\nWe need proper theories and methodologies!\nTheories and models explains &quot;WHY?&quot;\nMethodologies explains &quot;HOW?&quot;\nHCI is a field that provides theories and methodologies for optimal UX!\nKey factors of optimal UX\nUsefulness\nUsefulness helps users achieve their goals effectively.\nRelated to functions!\nUsability\nUsability is the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use.\nWe need specified users, goals, context!\nYou can't just design for all humans.\nUsability Criteria\n\nspeed of performance (efficiency)\nrate of human error (effectiveness)\ntime to learn (learnability)\nhuman retention over time (memorability)\nsubjective satisfaction\n\nWe can use these criteria to test and measure usability!\nAffection\nAffection is about emotional deisgn; How can we persuade people?\nLevels of Cognitive Processing (Perception of Design): Design of objects are perceived on three levels!\n\nVisceral: First Impressions &amp; Instincts\n\nReact to visual (sensory) aspects e.g. good, bad, safe, dangerous.\n\n\nBehavioral: Usability &amp; Functionality\n\nSimple everyday behaviors\nMajority of human behaviors\n\n\nReflective levels: Meaning &amp; Personal Connection\n\nConscious consideration and reflection on past experiences\n\n\n\nMeasuring affection???\nMeasuring affection is really hard...\nSome attempts:\n\nBiometric measurement: Heart rate, blood flow, etc.\nBrain measurement: Electroencephalogram, fMRI, etc.\n\nBut biometric measurement have too much noise...\nMost research interview participants.\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Historical Perspective of HCI","url":"/posts/40/","content":"History of Computer\nEarly Example: Astrolabe\nIn middle age, astrolabe can calculate people the time of the day, sun, star, season, latitude, etc.\nEasier to use, but need to learn how to use astrolabe.\nBabbage Difference Engine\n\nFirst mechanical calculator\nCan solve 7th-order polynomial equations!\n\nZ1 (1936)\n\nFirst programmable computer!\nCan add &amp; subtract 22-bit floating-point numbers\nCan multiple and divide numbers\nUsed punch tape to input program - we can now use input and output!\nYou have to observe the results inside the machine\nStill mechanical - human should run the &quot;computer&quot; themself\n\nENIAC (1943)\n\nFirst electronic numerical integrator and computer\nUsed for ballistic calculations, weather predictions, and nuclear research\nCan program, but humans should physically rewire ENIAC\nENIAC output was read through punch cards or lights\nThe first programmers were women!\n\nHarvard Mark 1 (1944)\n\nUsed paper tape as input\nUsed teletype to enter data and program\nOutput was printed on an electromechanical typewriter\nInput/output is becoming easier!\nStill the first programmers were women\n\nOne of the first programmers, Grace Hopper later contributed to compiler development\n\n\n\nHistory of HCI\nAs We Many Think (1945)\nComputers like ENIAC and Harvard Mark 1 was used for World War II.\nVannevar Bush proposed collective memory machine, a computer that can collect and share informations.\nNow computers are used for people, not just for complex calculations!\nMemex (1945)\nVannevar Bush proposed Memex as an example of collective memory machine.\nThis is not a real computer!!!\nHowever it was a precursor to hypertext, web (internet), digital libraries, etc.\n\nread a large library, and add or follow associative trails of links and notes\ncompresses and stores all of books, records, and communications\nuser can edit real-time! they can add memo to books, etc.\n\nSketchpad (1963)\nFirst graphical user interface!!\nIvan E Sutherland had to invent the graphical tool!\n\nInteractive problem-solving: Instead of coding programs for each problem, just solve the problem\nInstead of coding circle, just draw circle!\nReal-time interaction: User doesn't have to pre-plan everything, they can just solve the problem as they are formulating it.\nImmediate feedback: No coding -&gt; input -&gt; wait output\nTwo-handed interactions: Used light pen &amp; keypad\nConstraint satisfaction problem\nMagnetic snap: You could snap objects to grid - early smart object alignment\n\nDirect manipulation\nSketchpad was a precursor to modern UI interactions.\nThe term 'direct manipulation' was later coined by Ben Shneiderman (1983).\n\nVisibility of objects\nIncremental action and rapid feedback\nReversibility\nExploration\nSyntactic correctness of all actions\nReplacing language with action\n\nMouse (1963)\nFirst mouse was invented by Doug Engelbart.\nNLS (1968)\noNLine System - The first collaborative computing system.\nOffice workers can edit text and collaborate in real-time!\n90 minute live demo was really successful, so it was called the mother of all demos.\n\nFirst demonstration of mouse\nFirst hypertext\nFirst real-time text editing &amp; collaboration\nDemonstrated GUI elements\nFoldable outline system (Outline can be folded like folder structure)\n\nFirst HCI user study (1967)\nCompare mouse, joystick, lightpen, grafacon, knee-controlled lever\nExperiment Setup\n\n13 participants\nMake participants choose middle of 3x3 'X'\nIndependent variable: input method\nDependent variable: task completion time, eror rate\nWithin-subjects design: all participants used all input methods\nCounterbalanced: each participants used input methods in different order\n\nResults\nActually knee-controlled lever was the fastest method, but mouse had lower error rate!\nAugment (1977)\nThe modernization of the NSL.\nMouse, Keyboard, Foldable outline system, Hypertext, Hyperlink, Shared-screen collaboration, etc.\nSuccess of Apple\nApple referenced lots of HCI studies. actually they just went to Xerox lab and copied their works\nApple II (1977) released, and it was a great success.\nAfter that, Apple started Machintosh project. Its goal was producing a low-cost easy-to-use computer for the average consumer.\nAlto (1973)\nXerox made GUI components, such as menus, windows, pointing, dragging, etc.\nIt was a decade before mass market GUI machines arose, but Apple just copied to make Macintosh.\n\nWYSIWYG document systems\nemail tool\nvector graphics editor\nbitmap editor\ngraphical editor\nsmalltalk, interlisp\nnetwork-based multi-person video games\n\nNearly everything was started from Alto!\nHobbyist Computer\nComputer become cheaper, now we can see computers outside companies and labs!\nLots of developers like bill gates made software for computers.\nMost computers' case were made with wood.\nApple II (1977~1993)\n\nauto-repeat function - hold key to repeat keyyyyy\nfull ASCII character set\ninput/display lowercase letters (previously computers only supported uppercase letters)\n\nVisiCalc (1979)\nFirst spreadsheet\nMain reason for the success of the Apple II\nStar (1981)\nAnother computer from Xerox!\n\nUsed mouse and cursor keys\nFirst concept of windows, icons, window management, etc.\nIntegrated networked document environment\nWYSIWYG (scrollbar, icons, etc.)\n\nApple Lisa (1983), Macintosh (1984)\nApple literally copied Xerox Star.\nLisa was too expensive, but Machintosh was affordable($2500).\nIBM PC (1981)\nIBM also made PC using CLI.\nWindows 1.0 (1985)\nWindows copied Apple and made their own PC.\nApple sued Microsoft for using overlapping windows, but judge couldn't understand the importance of it.....\nBirth of HCI\nFirst ACM SIGCHI conference started at 1983.\nThe Psychology of Human-Computer Interaction was published at 1983.\n(Apple Machintosh was announced at 1983 and launched at 1984.)\nExample of an early research topic: breadth vs. depth in menu design\nResearch precedes products\n\nTwo-finger gestures (1978) -&gt; Apple iPhone (2007)\nAcceleration-sensing (1998) -&gt; Nintendo Wiimote (2005)\nWheel mouse (1993) -&gt; Microsoft Intellimouse (1996)\nSingle-stroke text input (1993) -&gt; Palm's Graffiti (1995)\n\nThere are lots of other examples: HyperText, Text Editing, Mouse, Windows, Gesture Recognition etc.\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Design Thinking","url":"/posts/44/","content":"Perpetual intermediates\nMost people are perepetually intermediate - only few beginners and experts\nDesign should target intermediates!\nInteraction Design\n\nThe design of spaces for human communication and interaction. (Winograd, 1997)\nDesigning interactive products to support the way people communicate and interact\nin their everyday and working lives. (Sharp, Rogers and Preece, 2011)\n\nDesign Thinking (Visual Thinking)\nTo understand them, the systems had to be constructed, and their behaviour observed. (Herbert A. Simon, 1969)\nWe need rapid prototyping and testing through observation!\nHowever, this is just an opinion, and there are people who think otherwise.\n\nDesign research is great for improving existing product categories, but essentially useless for new, innovative breakthroughs.\nConceptual inventions happen because technology\nhas finally made them possible.\nTechnology first → Invention second → Needs last\n\nDesign-thinking process\n\nEmpathize\nDefine\nIdeate\nPrototype\nTest\nImplement\n\nWe might have to turn back, or lop multiple times!\nFundamental Design Principles\n\nGeneralizable abstractions for thinking about different aspects of design\nThe dos and don'ts of interaction design\nKey considerations in interface design: what to include and what to avoid\n\nAffordance\nObjects have both perceived and actual properties.\nWhen an object's perceived and actual properties align, it has right affordances!\nWell-designed objects can be used effortlessly - without words, symbols, or trial and error.\nEx) A chair affords sitting.\nDifferent users have different affordances.\n\nKids: need large buttons, simplified interfaces\nElderly: reduced mobility, slower reactions\nDisabled: struggle with vision, mobility, and interaction limitations\n\nMapping\nControls should correspond logically to their effects.\nGood/natural mapping make the relationship between controls and actions obvious.\nMinimize the need for labels or legnends - users should instinctively know what to do without instructions.\nConstraints\n\nVisible Constraints: limit the possible actions by appearance\nPhysical Constraints: physical limitations constrain possible operations\nSemantic Constraints: depending on our meaning of situation\nCultural Constraints: allowable actions for social situations (e.g. Is stop sign red or blue?)\nLogical Constraints: use spatial layout of components (only one piece left, only one possible place to go)s\n\nConceptual model\nMental model of how things work.\nMental models are formed by:\n\nAffordances: what an object suggests it can do\nConstraints: what limits an action\nMappings: how controls correspond to effects\nExperience: what users have seen before\nTraining &amp; Instruction: learned behavior\n\nBut there are three aspects of mental models!!!\n\nDesign model: how the designer envisions the system working.\nUser's model: how the user believes the system works.\nSystem image: the actual system representation that the user interacts with.\n\nGreat design aligns three mental models! Bad design doesn't\nMake things visible\nImportant functions should be immediately noticeable.\nFunctions don't have to be seen; e.g. it can be touched.\nFeedback\nGood design should provide user with clear visual, tactile, auditory feedback.\nUser need confirmation that their actions have been recognized.\nGood feedback improves interaction and prevents user errors.\nCausality\nBe responsive - users need to recognize causality!\nPerceptual fusion: people assume that the thing happens right after an action be caused by that action. (Feedback should be within 100ms!)\nIf the task takes too long, you need feedback such as hourglass.\nConsistency\nSimilar elements for achieving similar tasks\nOther Design Constraints\n\nCost: real design has limited cost\nSpace: real design has limited space\nDiscoverability: if some controls are hidden, how do people find them?\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Design Process","url":"/posts/46/","content":"Design Process\nSystem-centered Design\nA Technology-First approach\nStill necessary if we have legal or security concerns\n\nWhat can be built easily on this platform?\nWhat can I create from the available tools?\nWhat do I as a programmer find interesting to work on?\n\nDesign Thinker: Designers who create\n\nTechnical Implementer: Implement as specified\nMothodical Engineer: Solve problems, follow a methodology\nInnovative Designer: Think beyond obvious, Create beyond the problem\n\nDesigner-centered Design\nExpert designers dictate solutions\n\nDesigners leverage expertise to anticipate user needs without listening to users\nUsers can't see past what they know - they rely on familiar experiences so they can't produce innovation\n\nUser-centered Design\nPutting users first\n\nPrioritize real user needs and abilities\nPrioritize their work environment and context\nPrioritize the tasks they aim to complete\n\nUser-centered System Design\nUser-centered system design begins with understanding the domain of work or play where people interact with computers.\nThe goal is to facilitate human action by designing systems that support engagement and usability.\nThree key assumptions:\n\nGood design directly satisfies user needs\nDesign is a collaborative process between designers and customers. It evolves based on changing user concerns, with specifications as a natural byproduct.\nContinuous communication between customers and designers is essential.\n\nDifferent roles of users\n\nUser: Uses the final product and provides feedback based on real-world experience\nTester: Evaluates early-stage prototypes to identify usability issues\nInformant: Shares insights and feedback on existing technologies or prototypes\nDesign Partner: Collaborates actively with designers, contributing ideas and refining solutions\n\nIDEO Human-Centered Design Process\n\nUnderstand the problem area\n\nwhy do we need a new design and how to come up with one\nproduce a small set of key ideas, general orientation\n\n\nObserve potential users and customers\n\nfictitious character maps\nknow the potential users → create persona\n\n\nVisualize and Predict (Ideate)\n\nbrainstorm, sketching, prototyping\ndetailed scenarios or storyboards\ndepict the interactions between users and the new device (design)\n\n\nEvaluate and Refine\n\nuser testing, feedback\niterative, agile approach\n\n\nImplementation\n\nDevelop the final solution based on validated insights\n\n\n\nUnderstand and Observe\nContextual Inquiry\nUnderstand user behavior in real environment with actual field study\nInvolves in-depth observation and interviews of a small sample of users to gain a robust understanding of work practices and behaviors\nFocus on real people &amp; constraints using notes, documentation, recording, screen capture, eye-tracking, etc.\n\nContext: Observe users in their natural environment, not in artificial lab settings\nPartnership: Treat users as co-researchers, allowing natural discussions rather than\ncontrolling the session\nInterpretation: Develop a shared understanding of the work through user feedback\nFocus: Maintain clear research goals to guide observations and discussions\n\nInterviews\n\nStakeholders: align goals and business constraints (budget, schedule, etc.)\nSubject matter experts: develop an ongoing relationship with experts\nUsers and customers: define goals, problems, etc.\n\nListening to user is important!\nBut there are many pitfalls of user interviews...\n\nUser may struggle to express their needs\n\nFocus on goals first, tasks second (don't divide into specific action)\nUsers are neither designers, nor experts (don't ask what is possible)\n\n\nDesigners might influence users\n\nDon't push a cool idea\nDon't push what you have been working on\n\n\nEncourage storytelling\n&quot;Could you show me?&quot; Ask users to demonstrate their process\nPerpetual intermediates - most users stay at an intermediate level\n\nMarketers design for beginners\nProgrammers design for experts\n\n\n\nDon't ask questions like these!\n\nLeading question: don't lead the respondent toward a postiive response\nBinary &amp; vague question: don't ask a yes or no question; it can't explore why\nFalse dichotomy: don't force a choice between two groups\nUnclear &amp; overly broad question: make users to focus on specific factors\nAmbiguous question: specify criteria for selection\nBiased question: don't assume anything\n\nSketching and storyboarding\nWe've observed user workflows, we need a way to organize this.\nSketching/Storyboarding visualize key steps in a user's journey through key frames.\nIt illustrate interactions in context - it shows how users interact with their environment or the system.\nWe can present and refine storyboard: play storyboards to gather feedback from users and designers.\nVisualize and Predict\nBrainstorming\n\nOne conversation at a time\nStay focused on the topic\nEncourage wild ideas\nDefer judgement\nBuild on the ideas of others\nBe visual\nGo for quality\n\nEach time the lifecycle is completed, the number of ideas gradually decreases.\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Persona and Goals","url":"/posts/48/","content":"Design Phase: Definition\nUsing data collected in the analysis phase,\n\nIdentify and name key persona: A fictitious user representing a class of users\nIdentify and name key goals:\n\nWhat do users want to accomplish?\nHow do different goals relate to each other?\nGoals are not Tasks! (i.e. not related to technology)\n\n\n\nElastic user\nElastic user can mean anyone and thus no one.\nVauge audience leads to unfocussed design.\nTrying to design for everyone often means designing for no one!\nTo avoid this:\n\nClearly define user personas before designing\nFocus on specific user needs &amp; behaviors\nEnsure design decisions are driven by real user insights\n\nFeature Overload: The Pitfall of Elastic Users\nFeature Overload: The interface tries to serve everyone but helps no one.\n\nToo many functionalities can cause cognitive load\nComplex interfaces increase navigational overhead\nDesign becomes unintuitive and overwhelming\nUsers struggle to find what they need\n\nPersonas: Who are we designing for?\nPersona is a descriptive models of users (not just single user!) that represents specific types of individuals with specific needs.\nPersonas help us define real users, not assumptions!\n\nPersona is a composite archetypes based upon observed behavior patterns\nPersona represents broad cross-section of users\n\nWhy Identify and Name Key &quot;Personas&quot;?\n\nHelps alignment (consensus and commitment)\nDrives design to desired product\nActs as a communication tool with stakeholders/developers/designers\nHelps later in usability studies to target the right users\nHelps marketing &amp; sales plans\n\nWhy use personas in design?\nPersonas provide a structured way to understand users and design effectively.\nA clear set of personas has a well articulated set of goals.\nPersonas help us:\n\nIdentify behaviour patterns in different user groups\nUnderstand user motivations beyond surface-level needs\nGain insight into how users think &amp; make decisions\nAlign design with what users want to achieve (goals)\nUncover the reasons behind user actions\n\nWithin the design team, personas help:\n\nProvide a shared understanding of the target user\nEncourage empathy-driven design decisions\nDefine who the product is for and who it isn't for\nDetermine what the product should and shouldn't do\nProvide a stable reference point throughout the design process\nSserve as a stand-in for real users\nTest design via walkthroughs\n\nWhat makes persona?\n\nPersona is derived from user research and data.\nPersona represent archetypes as individuals, not real people\nPersona act as composite models of multiple users - it covers a range of archetypes and behaviours\n\nHow to Identify personas?\n\nConduct user interview and observations to understand real users and their behaviors\nIdentify major patterns and clusters from stakeholders &amp; user interviews\nSynthesize their goals from collected data\nValidate personas for completeness and representativeness\nTry them out by developing narratives\n\nHow personas influence design?\n\nTap into universal patterns that users recognize instinctively (unconsciously)\nGuide branding &amp; product identity through emotional association\nHelp users quickly understand the product's personality &amp; values\nStrengthen user engagement by aligning with familiar narratives\n\nSteps for Constructing Personas\n\nIdentify behavioral variables\n\nConsider activity, attitude, aptitude, motivation, and skills\n\n\nMap interview subjects to behavioral variables\n\nUse character mapping techniques (e.g., from The Designer's Stance)\n\n\nIdentify significant behavior patterns\n\nRecognize key roles and recurring user behaviors\n\n\nSynthesize characteristics and relevant goals\n\nCraft goal-oriented interview questions to capture user intent\n\n\nValidate &amp; Expand Persona Attributes\n\nCheck for completeness and redundancy\nExpand description of attributes and behaviors\n\n\nDesignate persona types\n\nprimary &gt; secondary &gt; supplemental &gt; customer &gt; served &gt; negative\n\n\n\nCharacter Map\nCreate a table with personas as columns and characteristics as rows.\nCharacteristics can be:\n\nDemographics: Location, Age, ...\nRelationships: Parents, children, ...\nLife Context: Familiy situation, ...\nLifestyles &amp; Hobbies: Hobby, Job, Car, Income, ...\nPersonality &amp; Behavior: Personality, ...\nMiscellaneous Factors\n\nWhat makes a strong persona?\n\nspecific narrative\nspecific usage pattern\nembodied in a specific fictional user\nby means of text and images\nbased on data\nincludes specific name, photo, goals that matches persona\nvarious information (e.g. key characteristics, motivations, context, activities, narrative story, representative quotes, pain points)\n\nPersona Categories\nPrimary persona\nPrimary persona is a primary target for the design of an interface.\nCan be only one primary persona per interface for a product.\nA primary persona will not be satisfied by a design targeted at any other persona, but if the primary persona is the target, all other personas will not, at least, be dissatisfied.\ne.g. young people using a general user interface\nSecondary persona\nSecondary persona is mostly satisfied with the primary persona's interface.\nIt has specific additional needs that can be accommodated without upsetting the product's ability to serve the primary persona.\nDesigners should design for the primary, and then adjust the design to accommodate the secondary.\ne.g. senior citizens adapting to a general user interface\nSupplementary persona\nSupplementary persona is other personas that are not primary or secondary.\nTheir needs are mostly covered by a combination of primary and secondary personas.\nIt is often included due to stakeholder considerations or accessibility needs (political persona)\ne.g. visually impaired users requiring additional accommodations\nCustomer persona\nCustomer persona is the person who purchases the product but may not use it directly.\ne.g. kids (primary persona) use toys but parents (customer persona) buy toys.\nServed persona\nServed persona is directly affected by the use of the product.\nIt is often treated like a secondary persona.\ne.g. radiology technologist (primary persona) uses CT scanner, but patients (served persona) are the one who are being used.\nNegative persona\nNegative persona is a specific types of users that the product is not being built to serve.\nIt is used to set boundaries for product focus.\ne.g. Anti-personas (users who intentionally misuse a system)\ne.g. spam bots using SNS\nUser goals: Why did they do that?\n\nGoal: Deeper reasons behind user actions\nTasks: Specific actions or steps required to achieve goals\nSolution: Method to complete tasks\n\nUnderstanding User Goals\nSimilar to level of cognitive processing (perception of design)!\n\nExperience goals (Visceral level): how users want to feel\nEnd goals (Behavioral level): what a user wants to do\nLife goals (Reflective level): what a user wants to be\n\nWe might have to consider non-user goals, but we should align them with user goals.\nCustomer goals, Corporate goals, etc.\nHierarchy of Needs\nWe should serve low-level needs first, then high-level needs.\n\nFunctionality: working system\nReliability: stable &amp; consistent performance\nUsability: easy and forgiving design\nProficiency: do things better than before\nCreativity: interacting in innovative way\n\nc.f. Maslow's Hierarchy of Needs (Physilogical - Safety - Love/belonging - Esteem - Self-actualization)\nPeople tries to fulfill low-level needs first, then high-level needs.\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Prototyping","url":"/posts/52/","content":"Prototypes\nWe need to test our ideas!\nA prototype is a simplified and incomplete models of a design used to:\n\nexplore ideas\nelaborate requirements\nrefine specifications\nvalidate functionality\n\nWhy prototypes?\n\nDesigners can understand real-world design requirements\nDesigners can visualize, test, and improve design ideas early in the process\nEarly usability testing - we can catch issues before investing in full development\nInvesting early prevents expensive rework! (Pay less now or more later)\nPrototypes are inexpensive, fast, flexible, so you can do more of them\nMore prototypes = More ideas = Better ideas (Quantity breeds quality!)\n\nIdea Selection: Prioritizing ideas for prototyping\n\nDefine each idea's importance\n\nThink about real-world requirements\nUser preference and target user population\nFeasibility (available hardware, available software, cost)\nTeam skills and resources\n\n\nRank ideas\nPick the tops\n\nNumber of ideas to pick depends on resources and stage of the project\nRemember: prototyping is about learning, not perfection\n\n\n\nPrototyping methods\n\nPhase I - Early Exploration\n\nGoal: Understand user needs, test rough ideas\nRapid low-fidelity implementation\nWalk-throughs, storyboards\nPaper prototypes (sketches, printouts)\n\n\nPhase II – Concept Validation\n\nGoal: Test with real users in more realistic settings\nRapid prototyping with:\n\nDigital mockups (e.g. Figma, Sketch, Adobe XD)\nWizard of Oz simulations\nClickable or partially interactive UIs\n\n\n\n\nPhase III – User Evaluation with Working Systems\n\nGoal: Test in more realistic settings\nToolkit-based implementation\nTest with real users in more realistic settings\nBuilt with actual UI libraries\nLarger and larger group of users using the real interface\n\n\nPhase IV – Final Product\n\nGoal: Validate and polish\nFully functional system\nPerformance, edge case, and full usability testing\n\n\n\nLow-Fidelity Prototype\n\nPaper/plastic-based interface simulation...\n\nUser interacts with the paper interface\nA person who act as a computer (usually the designer) manually changes interface elements)\nObserver watches and records actions\n\nPros and Cons\n\n\nInexpensive: Easy and fast to create\n\n\nProvide high-level feedback: Useful for testing overall flow, structure, and dynamic of the interface\n\n\nTrigger user reactions: Help uncover user expectations and issues\n\n\nMight be inaccurate: Does not reflect actual system speed or responsiveness\n\n\nHuman error: Human-as-computer may unintentionally interfere with the user's experience\n\n\nDesign changes are cumbersome: repetitive earsing and drawing\n\n\nDoesn't scale well: Humans struggle to manually keep up with fast transitions across hundreds of screens\n\n\nSketches\n\nDrawing of the outward appearance of the intended system.\nShould be rough and crude! Crudity helps people concentrate on high level concepts.\nHarder to test complex flows or interactive behaviors.\n\nSketching is not about drawing!\nSketching is part of a design process:\n\nidea generation\ndesign elaboration\ndesign choices\nengineering\n\nSketching can help you:\n\nexpress and develop ideas\nvisualize and think through ideas\nrecord and archive ideas that you come across\ncommunicate (reflect, share, critique, decide) ideas\n\nGetting the right design vs. getting the design right\n\nGetting the Right Design\n\nGenerate many ideas\nFocus on what to solve\nExplore different directions\nThink broadly and creatively\n\n\nGetting the Design Right\n\nPolish a chosen idea\nFocus on how to solve it\nImprove usability and aesthetics\nTest, refine, and finalize\n\n\n\n\nElaborate(Getting the right design) - generate many ideas and variations\nReduce(Getting the design right) - decide on the ones worth pursuing\nRepeat until you get the best idea!\n\nWizard of OZ (originally OZ Paradigm)\nExperimenter (the wizard) simulates the behavior of a theoreticla intelligent computer application in a laboratory without the system actually being built.\nWe can test a system that does not exist!\nUsers don't know that someone is faking all the responses.\nIn an iterative design process, the wizard can be replaced step-by-step with real algorithms.\nMedium-Fidelity Prototypes\nUse prototyping tools! (e.g. Figma, Sketch, Javascript)\n\nVertical prototype: provide answer about a specific question\n\nincludes in-depth functionality for only a few selected features\nGood for performance comparison or usability tests\n\n\nHorizontal prototype: provide full interface without the functionality\n\nSimulates the entire interface layout, but with no real functionality\nGreat for testing structure or layout\n\n\nScenario prototype: pre-scripted walkthrough for specific task\n\nRigid: no deviation allowed\nUseful for simulating a user journey\n\n\n\nPros and Cons\n\nTime consuming: requires more effort than low-fidelity sketches or paper prototypes.\n\nHowever CSE students may think that coding is better than papers...\n\n\nBeware of misaligned expectations\n\nDeveloper might resist changes once effort has been spent\nManagement or stakeholders might mistake the prototype for the final product\n\n\nStay focused on function over form\n\nDo not get distracted by too small detail\ne.g. Don't waste time debating fonts or colors at this stage\n\n\n\nHigh-Fidelity Prototypes\n\n\nPiecewise prototype\n\nBuilt in horizontal, vertical, or scenario-based slices.\nCan be tested in a controlled, lab-like setting.\n\n\n\nAlpha and Beta releases\n\nDistributed to small groups for real-world testing\nGather feedback before full-scale release\n\n\n\nRequires monitoring help lines or sales/support teams for post-deployment feedback\n\n\nExpensive to change: mistakes at this stage are costly!\n\n\nProblem can be deeply rooted in the software architecture.\n\n\nAlternative classification\n\nConcept prototyping: develop and evaluate preliminary design ideas\n\nConcept sketches and storyboards\nArtificial reality problem: Designs by a good artist or modeler look like they will work. You should be terrible at drawing!!\n\n\nRapid(throw-it-away) prototyping: explore and test functionalities and performances\n\ne.g., New automobile design in wind tunnels\nScaling fallacy problem: Success in rapid prototype doesn't always mean success in full-scale model.\n\n\nEvolutionary prototyping (Incremental prototyping): used when design specs are uncertain or changing\n\nThe average programmer's day in the life\nOften glorified as an iterative process (design → evaluation → refine)\nSoftware developers using the facilities for actual product development\nDesigners tend to get tunnel vision, not exploring design alternatives\n\n\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Models of the Human","url":"/posts/58/","content":"Models of the Human\nDescriptive models help us think more clearly about human behavior.\nModel helps us design better systems that match how people actually think and act.\nThere are many models of the human, but we begin with two useful models,\nNewell's Time Scale of Human Action, and Human Factors Model.\nNewell's Time Scale of Human Action\nHuman actions happen at different time scales.\n\n100μs ~ 10ms: Biological Band, happens in neuron, cells, etc.\n100ms ~ 10s: Cognitive Band, Unit task, Operations, Reactions\n100s ~ 10^5s (hours): Rational Band, Complex task\n10^6s (days) ~ 10^8s (months): Social Band\n\nTime is the most common dependent variable in experimental HCI research!\ne.g. The time for a user to do a task.\nThis model shows how HCI spans biology to social behavior.\nLarger scale need qualitative and non-experimental research methods.\nSmaller scale need quantitative (experimental) and empirical research methods.\n\nSocial Band: Workplace habits, privacy, social networking, ...\nRational Band: Web navigation, collaborative computing, ...\nCognitive Band: Menu design, gestural input, ...\nBiological Band: Not widely used, probably brain wave?\n\nHuman Factors Model\n\nSensors\n\nVision (sight)\nHearing (audition)\nTouch (tactition)\nSmell (olfaction)\nTaste (gustation)\n\nVision\nPeople obtain about 80% of their information through vision.\nFovea image is the sharpest central vision.\nIt covers 1° of the visual angle - only 1% of the retina, but 50% of the visual cortex!\nVisual Stimulus\nPhysical properties of light: frequency and intensity (luminance)\ncreate subjective properties of vision: colour and brightness\nBrightness is determined by luminance contrast! (not luminance values)\nScan Paths\nFovea image covers only small area of the vision.\nSolution: eyes actual moves to get an idea of the whole vision!\nFixation: Eyes stop and take in visual detail from the environment (at least 200ms)\nSaccade: Rapid repositioning of the eye to fixate on a new location (about 120ms)\nScan path is the visual depicition of saccades and fixations (using eye trackers).\nSaccades are shown as straight lines and fixations are shown as circles.\nThe diameter of the circle is proportional to the duration of fixation.\nScan path is used in user behaviour research (e.g. reading patterns) or marketing research (e.g. ad placement).\nHearing\nSound is a cyclic fluctuations of pressure in a medium, such as air.\nSound is created when physical objects are moved or vibrated.\nPhysical properties of sound (frequency, intensity) create subjective properties of hearing. (pitch, loudness)\nBut there are other properties of sounds: timbre and attack!\nTimbre (richness, brightness, 음색)\nTimbre results from harmonic structure of sound.\ne.g. a musical note of 200Hz has harmonics at 400Hz, 600Hz, 800Hz, etc.\nNotes of the same frequency from different instruments are distinguished due to harmonic structure (i.e. timbre)\nAttack (envelop, 음의 발생)\nAttack results from the way a note and its harmonics build up and transition in time - from silent, to audible, to silent\nTouch (Tactition)\nPart of somatosensory system.\nReceptors are in skin, muscles, joints, bones.\nThey sense touch, pain, temperature, position, shape, texture, resistance, etc.\nFlavour\nSmell (olfaction) is ability to perceive odours through sensory cells in nasal cavity.\nTaste (gustation) is chemical reception of sweet, salty, bitter, and sour sensations.\nFlavour is a perceptual process that combines smell and taste.\nResponders\nHumans control their environment through responders.\ne.g. finger, feet, eyebrow, torso, ...\nMotor homunculus\nPenfield showed that human responders are dedicated to each relative area of the motor cortex.\nHandedness\nSome users are left-handed, others are right-handed.\nBut handedness exists by degree. (e.g. 80% left-handed, 20% right-handed)\nEdinburgh Handedness Inventory is used to measure handedness.\nHuman Voice\nHuman vocal cords are responders.\nSounds are created through combination of movement in the larynx and pulmonary pressure in the lungs.\nThere are two kinds of vocalized sounds: speech and non-speech.\nBoth have potential for computer control.\ne.g. NVVI (non-verbal voice interaction) uses signal detection such as frequency, duration, loudness, etc.\nEye as a responder\nAs a controller, the eye is called upon to do double duty:\n\nSense and perceive the environment/computer\nAct as a controller via saccades and fixations\n\ne.g. Eye Typing - use eye tracker to type on on-screen keyboard\nHuman Factors Model can be modified?\n\nThe Brain\nBrain is the most complex biological structure known!\nSensors (human inputs) and responders (human outputs) are connected by the brain.\nHuman Uniqueness\nPeople excel at perception, at creativity, at the ability to go beyond the information given, making sense of otherwise chaotic events. We often\nhave to interpret events far beyond the information available, and our ability to do this efficiently and effortlessly, usually without even being aware that we are doing so, greatly adds to our ability to function. (from The design of everyday things)\nThis is a very difficult thing for AI to do!\nPerception\n1st stage of processing for sensory input\nForms associations - e.g.\n\nAuditory stimulus -&gt; harmonious, discordant\nVisual stimulus -&gt; familiar, strange\nTactile stimulus -&gt; warm, hot\nSmell stimulus -&gt; pleasurable, abhorrent\nTaste stimulus -&gt; sweet, sour\n\nPhychophysics\nStudy relationship between human perception and physical phenomena\nExperimental method:\n\nPresent subject with two stimuli, one after the other\nRandomly vary the difference\nDetermine threshold below which the subject deems the two stimuli &quot;the smae&quot; (JND, just noticeable different)\n\nAmbiguity, Illusion\nPerception may vary!\nIn addition to vision, other senses such as tactile and auditory senses can evoke illusion.\nCognition\nCognition is the human process of conscious intellectual activity. i.e. decision making.\nIt's impossible to directly measure the time for a human to &quot;make a decision&quot;.\nWe assume sensory stimulus and motor response brackets the decision.\n(i.e. cognition time(decision time) is time between sensory stimulus and motor response)\nSimple Decisions\ne.g. press CALL in response to an incoming call\nThese are called the reaction time tasks.\nWhat about a more involved decision?\ne.g. should I hit in blackjack?\nMemory\n\nLong-term memory\n\nDeclarative/Explicit area: information about events in time and objects in the external world (data segment)\nProcedural/Implicit area: information about how to use objects and how to do things (code segment)\n\n\nShort-term memory (working memory)\n\nInformation is active and readily available for access\nAmount of working memory is small, about 7 ± 2 units or chunks\n\n\n\nShort Term Memory Experiments\n\nRandom sequences of digits is recited to subjects\nSequences vary from 4 to 13 digits\nAfter recitation, subjects copy sequence from memory to a sheet of paper\nTranscriptions on sheets is scored\n\nHuman record short term memory in chunks!\nLanguage\nThe mental faculty that allows humans to communicate.\nSpeaking is available to all humans without effort.\nWriting is only available with considerable effort.\nHCI is primarily interested in writing!\nCorpus\nLarge collection of representative text samples.\nA corpus may be reduced to a word-frequency list.\nPart-of-speech (POS) Tagging\nEach word is tagged by its category (e.g. noun, verb, adjective).\nUsed in word prediction to narrow search space.\nStatistics and Language\nNative speakers intuitively understand the statistical nature of their language.\nWe can fill in missing parts based on statistics!\nRedundancy and Language\nSince humans can fill in missing parts, perhaps the missing parts can be eliminated.\ne.g. remove vowels, replace words/characters with acronyms or homophone\nEntropy in Language\nRedundancy is what we know, entropy is what we don't know.\nEntropy is the uncertainty about forthcoming letters, words, phrases, ideas, etc.\nShannon's letter guessing experiment\n\nHide every letters\nSubject guesses letters one at a time\nShow '-' when correct, show correct letter when incorrect\n\nErrors are most common at beginning of words.\nErrors are less common as a wrod progresses.\nErrors are even less common as a phrase progresses.\nWith a good statistical model, the original text could be obtained from the reduced text.\nEntropy of Printed English\nSingle letter frequency's entropy is 4.25 bits per letter.\n100 letters frequency's entropy is about 1 bit per letter.\nRedundancy is about 75%!\nHuman Performance\nHumans uses their sensors, brain, and responders to do things.\nWhen these three work together to achieve a goal, human performance arises.\nSpeed-accuracy Trade-off\nIf human go faster, errors increase.\nIf human slow down, accuracy improves.\nHuman Diversity\nHuman performance is highly complex; Humans differ in age, gender, skills, motivation, environmental conditions, secondary tasks, etc.\nHuman diversity and human performance is shown in a distribution. (Often n normal distribution)\nReaction Time\nSimple reaction time: The delay between the occurrence of a single fixed stimulus and the initiation of a response assigned to it\nDelay time varies by type of sensory stimuli.\n\nAuditory -&gt; 150ms\nVisual -&gt; 200ms\nSmell -&gt; 300ms\nPain -&gt; 700ms\n\nComplex task have longer delay. e.g. visual search, name matching\nSkilled Behaviour\nFor many tasks, human performance improves considerably and continously with practice.\nHowever, in the simple reaction time tasks, human performance improves very little.\nAttention\nHuman can text while driving!\nAttention is complex:\n\nDivided attention: Attention can be divided to secondary tasks\nSelected attention: Attending to one task can exclude attention to other tasks\n\nHuman Error\nAn error is a discrete event where the outcome deviated from the desired outcome.\nBut in practice, tasks that are performed in error are often at least partly correct.\nAccidents\nHuman error cause accidents.\nBut design induced error might cause accidents too.\nInteraction errors are likely caused by design induced error.\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Predictive Models","url":"/posts/66/","content":"Predictive Models\nA predictive model is a mathematical formula used to estimate outcomes.\nIt predicts the result for a criterion variable (aka dependent variable) based on one ore more predictor variables. (aka independent variables)\nPredictor variables should be numeric or encoded in a way that the model can use.\nRatio-scale is ideal, but ordinal and categorical predictors can be used with proper encoding.\nPredictive models, like descriptive models, help us explore how users behave.\nHowever, unlike descriptive models, predictive models deal with numbers, not just ideas or concepts.\nLevels of Measurements\n\nNominal: Attributes are only named\nOrdinal: Attributes can be ordered\nInterval: Distance is meaningful\nRatio: Ratio is meaningful, attributes have a absolute zero point.\n\nWhy use predictive models?\nGood predictive models can predict human response!\n\nThe need for some experiments might be obviated. Theoretical account of results eliminates the need for further experiments.\nWays of improving pointing performance might be suggested. The equation can tell us how human response changes.\n\nLinear Prediction Equation\nWith linear regression, we can find the coefficients for the line that minimizes the squared distances. (least squares)\nResults are shown as scatterplot with 95% confidnence interval.\nFitt's Law\nModel for rapid aimed movements!\nWidely used to predict and compare design alternatives.\n\nFitts(1954): T=IMlog⁡2(2DS)T = I_M\\log_2 \\left( \\frac{2D}{S} \\right)T=IM​log2​(S2D​)\nWelford(1968): T=IMlog⁡2(DS+0.5)T = I_M\\log_2 \\left( \\frac{D}{S} + 0.5 \\right)T=IM​log2​(SD​+0.5)\nMacKenzie(1980s): T=a+blog⁡2(DS+1)T = a + b \\log_2 \\left( \\frac{D}{S} + 1 \\right)T=a+blog2​(SD​+1)\n\nIndex of Difficulty (ID)\nID=log⁡2(AW+1)ID = \\log_2 \\left( \\frac{A}{W} + 1 \\right)\nID=log2​(WA​+1)\nWhere A is a distance to the target, and W is a width of the target.\nFitts hypothesized that movement time increases linearly with ID.\nID represents difficulty of task, the unit of ID is bits!\nBut in real tasks, users may not hit the target precisely.\nWe use the effective index of difficulty to account for actual behavior.\nIDIDID captures what a participant was asked to do, IDeID_eIDe​ captures what a participant actually did.\nIDe=log⁡2(AWe+1)ID_e = \\log_2 \\left( \\frac{A}{W_e} + 1 \\right)\nIDe​=log2​(We​A​+1)\nWe=4.133⋅SDxW_e = 4.133 \\cdot SD_xWe​=4.133⋅SDx​, where SDxSD_xSDx​ is the standard deviation in click locations.\nChoice reaction time\nWhen n stimuli are each linked to a different response, the time it takes to respond is called choice reaction time.\nRT=a+blog⁡2(n+1)RT = a + b \\log_2(n+1)\nRT=a+blog2​(n+1)\nWhere a≈200msa \\approx 200msa≈200ms, b≈150ms/bitb \\approx 150ms/bitb≈150ms/bit.\nHick-Hyman Law\nH=∑ipilog⁡2(1pi+1)RT=a+bH\\begin{align*}\nH &amp;= \\sum_i p_i \\log_2 \\left( \\frac{1}{p_i} + 1 \\right) \\\\\nRT &amp;= a + bH\n\\end{align*}HRT​=i∑​pi​log2​(pi​1​+1)=a+bH​\nWhere pip_ipi​ is a probability of occurrence of the ith item in the set.\nH is an information-theoretic entropy of the decision, it is an uncertainty about whether to respond or not.\ne.g. Is a one-level menu with eight items is better, or is a two-level menu with four items at the top level is better?\nWe can solve this by comparing log⁡2(8+1)\\log_2(8+1)log2​(8+1) and log⁡2(4+1)+log⁡2(2+1)\\log_2(4+1) + \\log_2(2+1)log2​(4+1)+log2​(2+1).\nSkill Acquisition\nWhen learning a skill, we begin as novices.\nWith pratice, performance improves: faster, more accurate, more efficient.\nPower Law of Learning\nTn=T1×n−αT_n = T_1 \\times n^{-\\alpha}\nTn​=T1​×n−α\nWhere TiT_iTi​ is a time to do the task on the ith trial, and α\\alphaα is constant between 0.2 ~ 0.6.\nWe can also use speed as a dependent variable.\nSn=S1×nαS_n = S_1 \\times n^{\\alpha}\nSn​=S1​×nα\nStage of Skill Acquisition\n\nCognitive: Verbal representation of knowledge, learn through problem-solving\nAssociative: Proceduralization, from rehearsal to recognition\nAutonomous: No cognitive involvement, become difficult to verbally describe the skill. Only motor system is used!\n\nIn the cognitive stage, users often face novel problems.\nThey must figure out how to perform the task before they can perform it efficiently.\nOperator Subgoaling\nDivide-and-conquer method: Problem solving involves creating subgoals to reduce the difference between the current state and the goal state using available operators.\n\nSubgoals Creation: The larger problem is divided into smaller, more manageable parts, each representing a milestone on the way to solving the entire problem.\nOperator Application: For each subgoal, specific actions or operators are applied. These are the steps or procedures used to achieve the subgoal.\nIterative Process: This process may be iterative, as solving one subgoal may reveal new challenges or even new subgoals, which then require their own specific operators.\n\nBackward Planning\nSometimes it's easier to start with the goal and work in reverse.\nThis approach is used in chess, programming, and even in AI.\nProduction Rules\nProduction rules are a set of rule-based instructions used to control a system or predict an outcome, often formatted as if-then statements.\nSubgoaling is used to solve unfamiliar problems, production rules are used to solve familiar problems.\n\nProduction rules enable structured control over complex tasks.\nProduction rules reduce memory load through proceduralization.\nProduction rules allow fast, direct action without cognition.\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Human Information Processor","url":"/posts/59/","content":"The Human Information Processor (HIP) Model\nCombines several cognitive psychology concepts into a simplified, unified framework.\nIt's a simplified approximation of human information processing for computer scientists.\nView human mind as an information processing system.\nEach system has memory and processor.\n\nMemory has capacity, decay time, code type (physical, acoustic, visual, ...).\nProcessor operates on a fixed cycle time. (Perceptual processor cycle τP\\tau_PτP​, Cognitive processor cycle τC\\tau_CτC​, Motor processor cycle τM\\tau_MτM​)\n\nHuman Performance\nOf course, parameters of Model Human Processor has uncertainties. (i.e. is ranged)\ne.g. τC\\tau_CτC​ is between 25ms and 170ms.\nWe use three versions of the MHP model.\n\nSlowman: worst performance\nFastman: best performance\nMiddleman: nominal performance\n\nPerceptual system\n\nDetects input through visual and auditory sensors.\nStores data in sensory memory that is short-lived. (as visual image and auditory image)\nConverts sensory data into symbolic codes and stores in working memory.\n\nEye\n\nCentral vision: fovea\nPeripheral vision: retina\nHead movement: If target is more than 30° away from fovea, head should be moved.\n\nVisual Image Store (iconic memory)\n\n\nDuration: Less than 1 seond (200ms)\n\n\nCapacity: 17 letters\n\n\nType: Physical\n\n\nPassive system (remember everything)\n\n\nHolds physically coded raw visual info\n\n\nPre-attentive (no conscious effort)\n\n\nInfo is sensory and unprocessed\n\n\nRapid decay if not attended to\n\n\nWorking Memory (WM)\n\n\nDuration: few seconds\n\n\nCapacity: 7 ± 2 chunks (working memory is accessed in chunks!)\n\n\nType: Visual or acoustic\n\n\nActive system (remember on demand)\n\n\nHolds and processes attended info\n\n\nRequires attention and effort\n\n\nInfo is abstract and manipuable\n\n\nSupports comparison, manipuation, decisions\n\n\nDecay rate of WM\nWorking memory decays very fast!\nHalf-life for 3 chunks is about 7 sec. (i.e. After 7 sec, you forget half of the working memory)\nIf only 1 chunk is presented, working memory last for about 73 sec.\nEffect of interference: if you try to remember something, long term memory kicks in.\nChunks of WM\nChunks are unit of memory or perception.\nIt depends on presentation and what you already know.\ne.g. you can remember phone number as 8 digit number, or 2 chunks of 4 digit number.\nChunks can be related to other chunks.\nActivation spread in long term memory, and interfere with old ones.\nBut we have limited amount of activation resource -&gt; decay happens!\nCapacity of WM\nPure capacity of working memory is about 3 chunks.\nThis is the number of immediately preceding digits recallable from a long series when the series unexpectedly stops.\nEffective capacity of working memory is about 7 ± 2 chunks.\nAttention helps to remember more chunks.\nLong Term Memory (LTM)\n\n\nLTM is a network of related chunks, accessed associatively from the WM.\n\n\nHas very large capacity, more than enough for a person to remember everything in their life.\n\nWe'll just say that LTM has infinite capacity.\n\n\n\nInformation is stored in semantic encoding, not in perceptual encoding.\n\n\nFast read (70ms): can be accessed by pattern matching during each processing cycle.\n\n\nExpensive write (10s): noisy (can be interfered), need several rehearsal and/or recall to remember correctly.\n\n\nContext at the time of acquisition is the key for the retrieval.\n\n\nLTM can fail\nLTM has unlimited capacity, little decay, and no erasure!\nThen why do we forget?\nRetrieval could fail if:\n\nNo effective associations exists\nSimilar associations interferes\n\nTo remember something later:\n\nAssociate it with items already in LTM in novel ways.\nElaborative Rehearsal: associate with information already known.\nMaintenance Rehearsal: repetition of memorization.\n\nPerceptual Processor\n\n\nPhysical store raw sensory input from our senses (see/hear/smell).\n\n\nTransform physical perception into abstract concepts\n\nunidentified non-symbolic into recognized symbols (visually coded representation)\ne.g. transform color, shape, orientation, brightness, movement into circle\n\n\n\nProgressive decoding: Decode sensory input incrementally rather than all at once (about 10ms/letter)\n\n\nSelective decoding: Focused on spatial cues or pre-attentive features (hue, motion, etc.)\n\n\nVisual Memory experiment\n\nWhole-Report Procedure: Present a matrix of letters for 50ms\nPartial-Report Procedure: Present a matrix of letters with tone or arrows, participant only need to remember a specific cued row.\n\nRecall is much higher in the partial report procedure!\nVariable Perceptual Processor Rate Principle\nCycle Time of Perceptual Processor: τP\\tau_PτP​ = 100ms\nTime that takes before human claims to see it after impulse (unit impulse response)\nBut it may vary according to stimulus intensity.\nHigh-intensity / High-contrast stimuli have shorter cycle time!\nQuantum experience: within perceptual cycle τP\\tau_PτP​, stimuli can be\n\nFused together (Perceptual Fusion)\nPerceived as causally linked (Causality)\n\nBloch's Law\nR=ltR = ltR=lt is constant!\n\nR: response (perceived brightness)\nl: intensity of the stimulus\nt: lasting time of the stimulus (should be smaller than perceptual cycle)\n\nYour brain can't distinguish pulse of 10ms at intensity 50 and pulse of 20ms at intensity 25!\nStimuli are fused together into one perception.\nPerceptual Causality\nWhen two stimuli are fused, the first event appears to cause the second.\nWhen fram rate is higher than 10 fps (1/τP\\tau_PτP​ = 10fps), moving picture is perceived as a smooth motion, not separate images.\nIf UI feedback happens faster than cycle time, it feels instantaneous to the user!\nPre-attentive variables\nSome differences are pre-attentive - it can perceived before attention.\ne.g. hue (color), orientation, shape, size, number, curvature (curved/straight), motion (Very strong!!!)\nPre-attentive tasks can be performed in less than 250ms.\nIt requires only a single glance at the image being displayed!\ne.g. target detection, boundary detection, ...\nIf variables are mixed, task cannot be done pre-attentively.\ne.g. We can't determine if a red circle is present inside red square, blue circle, blue square.\nSome variables has priority!\ne.g. Hue is more important than shape, brightness is more important than hue.\nCognitive system\n\nRetrieves symbolic input from working memory.\nSearches for matching patterns in long-term memory.\nMake decisions about how to respond based on learned knowledge.\n\nCognitive Processor\nThe basic principle of operation of the Model Human Processor is the Recognize-Act Cycle of the Cognitive Processor.\n\nRecognize: The contents of WM initiate actions associatively linked to them in LTM.\nAct: These actions in turn modify the contents of WM.\n\nRecognize-Act Cycle Time\n(Recognize-Act) Cycle time is τC\\tau_CτC​ = 70ms.\nMatching time can be different. e.g. digits takes 33ms, colors takes 38ms, geometry takes 50ms, ...\nVariable Cognitive Processor Rate Principle: cycle time is shorter when greater effort is induced, or with sufficient practice.\nAction phase is serial\nCognitive Processor works in parallel in recognition phase, serial in action phase!\ni.e. it can be aware of many things at once, but cannot do more than one at a time.\nYou can do multiple things at once by silled intermittent allocation of control actions to each task.\nCognitive processor is interrupt-driven time-sharing system. (similar to context-switching)\n&quot;Stay in the Flow&quot;: Good interface design should make users focus only one at a time.\nWe should divide your interface and what they are doing.\nGoal is to make user spend all their time on what they are doing - interface have to leave as little a cognitive footprint as possible.\nMotor system\n\nReceive input from the cognitive processor.\nExecutes physical actions in response.\ne.g. typing, clicking, ...\n\nMotor programs are not step-by-step, and muscle action is no-return.\nPart of the learning process is to transfer from cognitive to muscle memory.\nClosed Loop vs. Open Loop\nClosed Loop (250ms?): Feedback from perception through cognitive to motor.\nCorrection happens and muscle is fine-tuned.\ne.g. Go back and forth with the pencil as much as you can without going over the line.\nOpen Loop (74ms/reversal?): Control is planned in advance and motor executes without perception or cognition (of the output of motor system).\nThere is no feedback or correction.\ne.g. archery\nτM\\tau_MτM​ = 70ms\nFitts' Law\nMT=a+blog⁡2(2DW)MT = a + b \\log_2 \\left( \\frac{2D}{W} \\right)\nMT=a+blog2​(W2D​)\nPredictive model of human movement!\nFitts' measured time for a human to move pen from start point to target.\n\n\nMT: time required\n\n\nD: distant to target\n\n\nW: width of target\n\n\nID (index of difficulty): log⁡2(2DW)\\log_2 \\left( \\frac{2D}{W} \\right)log2​(W2D​) difficulty of the task.\n\n\nIP (index of performance): IDT\\frac{ID}{T}TID​ speed of the task. About 63ms / bit?\n\n\nExplaining Fitts' Law\nLet's assume that the time to move the hand to the target is T=n(τP+τC+τM)T = n(\\tau_P + \\tau_C + \\tau_M)T=n(τP​+τC​+τM​).\n\nτP\\tau_PτP​: observe the hand\nτC\\tau_CτC​: decide on the correction\nτM\\tau_MτM​: do the correction\n\nLet XiX_iXi​ be the distance remaining to the target after the i-th correction, and X0=DX_0 = DX0​=D.\nLet's assume the relative accuracy of movemnt ϵ=XiXi−1\\epsilon = \\frac{X_i}{X_{i-1}}ϵ=Xi−1​Xi​​ is constant. (about 0.7)\nThe hand will stop when it reaches the target. (i.e. remaining distance is less than W2\\frac{W}{2}2W​)\nX1=ϵX0=ϵDXn=ϵXn−1=ϵnD=W2n=log⁡ϵW2D=−log⁡22DWlog⁡2ϵ∴T=−τP+τC+τMlog⁡2ϵlog⁡22DW=IMlog⁡22DWIM=−τP+τC+τMlog⁡2ϵ≈63ms/bit\\begin{align*}\nX_1 &amp;= \\epsilon X_0 = \\epsilon D \\\\\nX_n &amp;= \\epsilon X_{n-1} = \\epsilon^n D = \\frac{W}{2} \\\\\nn &amp;= \\log_\\epsilon{\\frac{W}{2D}} = -\\frac{\\log_2{\\frac{2D}{W}}}{\\log_2{\\epsilon}} \\\\\n\\therefore T &amp;= -\\frac{\\tau_P + \\tau_C + \\tau_M}{\\log_2{\\epsilon}}\\log_2{\\frac{2D}{W}} = I_M \\log_2{\\frac{2D}{W}} \\\\\nI_M &amp;= -\\frac{\\tau_P + \\tau_C + \\tau_M}{\\log_2{\\epsilon}} \\approx 63 \\text{ms/bit}\n\\end{align*}X1​Xn​n∴TIM​​=ϵX0​=ϵD=ϵXn−1​=ϵnD=2W​=logϵ​2DW​=−log2​ϵlog2​W2D​​=−log2​ϵτP​+τC​+τM​​log2​W2D​=IM​log2​W2D​=−log2​ϵτP​+τC​+τM​​≈63ms/bit​\nImplication of Fitts' Law\nFitts' Law relies on &quot;Closed Loop&quot; control of Motor System.\n\nLarger/Closer targets are easire to click\nMacintosh menu bar is faster to use. (no correction time - since menu bar is at top of the window, we can assume width is infinite)\nPie menu is faster than popup menu (less distance)\n\nPower Law of Practice\nTn=T1n−αT_n = T_1 n^{-\\alpha}\nTn​=T1​n−α\nThe time to do a task decreases with practice.\nThe rate of decrease is proportional to a power of the amount of practice!\nTypical values for α\\alphaα is between 0.2 and 0.6.\nPut it together: HIP experiments\nName matching\nDo two letters have same name?\n\nPerceive second letter τP\\tau_PτP​\nRecognize letter τC\\tau_CτC​\nMatch τC\\tau_CτC​\nInitiate response τC\\tau_CτC​\nRespond τM\\tau_MτM​\n\nTotal time is τP+3τC+τM\\tau_P + 3\\tau_C + \\tau_MτP​+3τC​+τM​!\nPerceiving calendar\nHow many days are in April, 2025?\n\nPerceive calendar τP\\tau_PτP​\nRecognize last day τC\\tau_CτC​\nDecide to move eye to last day τC\\tau_CτC​\nMove eye to last number τM\\tau_MτM​\nPerceive number τP\\tau_PτP​\nRecognize number τC\\tau_CτC​\nInitiate response τC\\tau_CτC​\nRespond τM\\tau_MτM​\n\nTotal time is 2τP+4τC+2τM2\\tau_P + 4\\tau_C + 2\\tau_M2τP​+4τC​+2τM​!\nEye Movement\nEye movement time (saccade + fixation) is about 230ms.\nSaccade takes about 120ms.\nIf we assume one eye movement per phrase, and one phrase is about 2.5 words, we can read 652 words/minute.\nBut speed readers read 2500 words/minute!\nThey don't see all words.\nWe can use RSVP (Rapid Serial Visual Presentation) reader.\nRSVP reader move words, so user can focus on middle of the screen. (no eye movement!)\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"High Level Model of Human Behavior","url":"/posts/74/","content":"GOMS\nModel Human Processor (HIP) is good for modeling short, isolated tasks, but it doesn't scale to complex, routine tasks.\ne.g. matching a symbol to memory, determining the fastest speed to type on two different keyboards\nCNM-GOMS (Card, Newell, and Moran) is a higher-level model that models skilled behavior using Goals, Operators, Methods, and Selection rules.\nIt predicts the performance time of experienced workers to perform a task with an interface design.\nOther model like CPM-GOMS (Critial-Path Method), NGOMSL (Natrual GOMS Language) address learning and/or parallel behvaior.\n\nGoals: Desired outcomes or tasks\nOperators: Elementary perceptual, motor or cognitive actions\nMethods: Sequences of sub-goals and operators that can accomplish a goal\nSelection rules: Choose between alternative methods available for a given goal\n\nIn programmers view, goal is a funtion name, method is a body of a function, selection rules is a program.\nCNM-GOMS example\nTop-level goal\nEdit manuscript, or, more specifically, move &quot;quick brown&quot; to before &quot;fox&quot;\nSubgoal\nHighlight text\nOperators\n\nMove-mouse\nClick mouse button\nType characters (keyboard shortcuts)\n\nMethods\n\n\nFor the editing goal:\n\nDelete-word-and-retype (retype method)\nCut-and-paste-using-keyboard-shortcuts (shortcuts method)\nCut-and-paste-using-menus (menus method)\n\n\n\nFor the highlighting subgoal:\n\nDrag across text (dragging method)\nDouble-click first; shift-click last (all-clicking method)\n\n\n\nSelection rules\n\n\nFor the editing goal:\n\nIf the text to be moved is one or two characters long, use retype method\nElse, if remember shortcuts, use shortcuts method\nElse, use menus method\n\n\n\nFor the highlighting subgoal:\n\nIf the text to be moved is not whole words, use dragging method\nElse, use all-clicking method\n\n\n\nKLM (Keystroke Level Model)\nKLM is a simplified version of CNM-GOMS.\nKLM doesn't have selection rules, and have only 5 operators:\n\nK: Key/Button press/release, tK=0.2st_K = 0.2stK​=0.2s\nP: Pointing with mouse, tP=1.1st_P = 1.1stP​=1.1s\nH: Homing between devices (e.g. from keyboard to mouse), tH=0.4st_H = 0.4stH​=0.4s\nM: Mental preparation, tM=1.35st_M = 1.35stM​=1.35s\nR(t): System response time, tR=tt_R = ttR​=t\n\nOperator time can be debated, but it is a measurement from the user. We can change the operator time as we wish.\nUsing KLM\n\nEncode the task using all physical operators (K, P, H, R(t))\nInsert M using KLM heuristics\nM indicates routine thinking - not problem solving (e.g. remembering a filename)\nAdjust R(t) (system response time) followed by an M\nSince computer and user can work at the same time, R(t)=max⁡(t−tM,0)R(t) = \\max(t - t_M, 0)R(t)=max(t−tM​,0)\nCompute the total time by adding all operator times\nResult can be used to predict time for an expert user!\n\nProblem: KLM heuristics is actually very complex...\nSimple summary: Insert M in front of all K and Ps, then remove M to leave only 1 M for 1 cognition unit.\nKLM variation\nAdd operator B: Seperate operator for mouse button press/release\nInstead R, use W: Represent wait time if system response is involved\nMore detailed K operator: Pressing SHIFT/CONTROL is a seperate keystroke, use type operator T(n) for a series of Ks\nCriticism of KLM model\n\nLearnability: Only assumes skilled users, doesn't deal with skill acquisition\nAccuracy: Even skilled users make mistakes and errors.\nCognitive load: Less finger movement doesn't always mean better UI; M is different for each UI\n\nCPM-GOMS (Cognitive-Perceptual-Motor GOMS)\n\nCPM-GOMS models parallel operations.\nEach processor (Perceptual, Cognitive, Motor) is serial, different processors run in parallel. (Each motor processors run in parallel, e.g. left hand, right hand, etc.)\nCPM-GOMS can explain whether removed keystrokes are on the critical path. i.e. It can determine bottleneck.\nNGOMSL (Natural GOMS Langauge)\n\nTop-down breadth-first task decomposition\n\nStart with the user's top-level goals\nWrite a step-by-step procedure for accomplishing each goal in terms of subgoals or keystroke-level operators.\nRecursively write a method for each subgoal until all methods contain only keystroke-level operators\nWrite a selection rule to specify which method to use if more than one for a goal\n\n\nCount the number of statements in methods to predict learning time\n\nIf similar methods or re-used submethods exists, (i.e. consistency) learning time is reduced.\n\n\nFor a specific task scenario, count the number of statements and operators executed to predict execution time.\n\nGeneral version of CNM-GOMS?\nValue of GOMS\n\nProvide reason for high value decisions - not &quot;I think so&quot;\nDesigners can develop an intuition about what works and what doesn't and the impact of design decisions on speed.\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Conceptual Frameworks","url":"/posts/76/","content":"Conceptual models and mental models\nConceptual frameworks help us explain and predict user behavior based on theories of cognition.\nConceptual frameworks help us:\n\nUnderstand how users approach a task\nPredict where confusion or failure may occur\nImprove interface design by addressing these breakdowns.\n\nTo design effective interfaces, we must align three models:\n\nDesign Model (Conceptual model): The system concept as intended by the designer\nUser's Model (Mental model): The user's mental understanding of the system\nSystem Image: What the system presents to the user\nthrough interface and behavior\n\nDesigner creates a conceptual model and communicates it via the system image.\nA good conceptual model bridges the gap between design and user understanding via the system image.\nNorman's Seven Stages of Action Model\n\n\nForming the goal\n\n&quot;What do I want to achieve?&quot;\nThe desired outcome or state the user wants to reach\nDesign question: &quot;Can user determine the function of the device?&quot;\n\n\nForming the intention\n\n&quot;What approach will I use to achieve it?&quot;\nA chosen strategy or plan to fulfill the goal\nDesign question: &quot;Can user tell what actions are possible?&quot;\n\n\nSpecifying the action\n\n&quot;What steps must I take?&quot;\nMapping the plan to specific actions supported by the system\nDesign question: &quot;Can user determine the mapping from intention to physical movement?&quot;\n\n\nExecuting the action\n\nDo the steps physically using the system\nPhysically performs the action specified in the previous stage\nDesign question: &quot;Can user perform the action?&quot;\n\n\nPerceiving the state\n\n&quot;What does the system do in response?&quot;\nNoticing the system's response\nDesign question: &quot;Can user tell what state the system is in?&quot;\n\n\nInterpreting the feedback\n\n&quot;What does the feedback mean?&quot;\nUnderstanding what the response mean\nDesign question: &quot;Can user determine the mapping from system state to interpretation?&quot;\n\n\nEvaluating the outcome\n\n&quot;Did I succeed in achieving my goal?&quot;\nComparing the outcome with the original goal\nDesign question: &quot;Can user tell if the system is in the desired state?&quot;\n\n\n\nCognitive Engineering\nCognitive Engineering applies cognitive psychology and human factors to the design of interactive system.\nGoals:\n\nUnderstand how people think, perceive, and act\nReduce cognitive load in complex interactions\nMatch human capabilities and mental models\n\nDesign Emphasis:\n\nSupport user-centered design, not just system functions\nPromote direct manipulation and intuitive interfaces\nEnable engagement, clarity, and error resistance\n\nCognitive Engineering helps explain why users fail, and how better design can prevent it.\nGulf of execution and gulf of evaluation\nCognitive engineering highlights two key breakdown points in user interaction.\nGulf of execution: Can the user figure out how to take the intended action?\nGulf of evaluation: Can the user understand what the system did?\nNeither gulf is under the control of the designer!\nIt depends on each individual's the cultural convention or technical knowledge.\nGulf of execution\nThe gulf of execution is the gap between the intention (what the user wants to do) and the allowable actions (what the system supports).\nIt is related to functionality and usability.\n\nCan users easily figure out what actions are possible?\nCan they map their intentions to those actions?\nCan they execute the actions without unnecessary effort?\n\nTo reduce the gap, designers should make the commands and mechanisms of the system match the thoughts and goals of the users.\n\nUse clear affordances\nMake actions visible and intuitive\nMinimize the number of steps needed to perform a task\n\nGulf of evaluation\nThe gulf of evaluation is the gap between the physical representation (what the system does) and the user's interpretation (what users understand).\nIt is related to feedback and visibility.\n\nCan the user perceive the system state after taking an action?\nCan they interpret what happened and why?\nCan they determine whether their goal was achieved?\n\nTo reduce the gap, designers should present a clear and consistent conceptual model through intuitive feedback.\n\nProvide immediate and meaningful feedback\nMake system state visible and understandable\nAvoid ambiguous indicators\n\nDistance in meaning and form of expression\n\nSemantic distance: Distance between goal and the meaning of expression\nArticulatory distance: Distance between meaning of expression and the form of expression\nDirect manipulation\nDirect manipulation is an interaction style where users directly act on visible objects using intuitive physical actions, rather than issuing abstract commands.\nIn other words, object should be understood by their visual characteristics\nThree principles of direct manipulation:\n\nContinuous representation of the objects and actions using meaningful visual metaphors\nUse of physical actions (e.g. clicking, dragging) instead of complex command syntax\nRapid, incremental, reversible actions with immediate visible feedback\n\nCentral ideas of direct manipulation\nObject is understood by their visual characteristics. (using good affordances, good conceptual model, and convincing metaphors)\nActions is understood in term of their effects on the screen. Action should be rapid and incremental, with immediate visual feedback, and easily reversible.\nThis enables WYSIWYG interface, with direct engagement.\nProblems with direct manipuplation\n\nConsumes valuable screen space\nMust learn the meaning of visual representations\nVisual representation can be misleading\nNeed to consider blid/vision-impaired users\nMight not work well in small screens\nToo repetitive and low accuracy for experts\n\nInterface language\nObject-Action (Noun-Verb)\nThe user select an object first, then perform an action on it.\n\nTypically modeless\nActions always occur within the context of the object\ne.g. double-click on a file to open it, select and delete, drag and drop\n\nAction-Object (Verb-Noun)\nThe user chooses an action (or task) first, then specifies the object.\n\nTypically modal, requires the user to stay in the correct mode\nOften more efficient for repetitive tasks\ne.g. use menu to open a file, pick a tool to use it\n\nInterface metaphors\nMetaphor is using one kind of (familiar) object or idea in place of another (unfamiliar) one to suggest a likeness or analogy between them, creating a mental bridge between the two.\n\nLeverages users' prior knowledge of familiar, concrete real-world objects/experiences\nTransfer this knowledge to abstract computer and task concepts\nReduces the learning curve and improves usability\n\ne.g. Desktop, files, folders, trash can is a metaphor of office workspace.\nPaintbrush in a painting program is a metaphor of real-world art.\nTwo types of interface metaphors\n\nConversation metaphor (common in CLI)\n\nThe interface is a language medium to express assumed implicit objects\nUsers think of the interface as an intermediary to a world that is not explicitly represented\nUsers interact by providing the intermediary with linguistic descriptions of actions to be accomplished\n\n\nModel world metaphor (Common in GUIs, simulations and direct manipulation systems)\n\nThe interface is itself a world where the user can act and get response\nThe world is explicitly depicted, the represented objects behave as if they were the things they refer to\nUsers manipulate objects directly rather than describe actions\n\n\n\nMetaphors Caveats\n\nToo limited: The metaphor restricts interface possibility\nToo powerful: The metaphor makes believe that the system can do things it can't\nToo literal or cute: Overly detailed metaphors can be tedious, confusing, or gimmicky\nMismatched: The metaphor doesn't align with the task or user mental model, leading to confusion\nWYSIAYG (What You See Is All You Get): limits access to hidden or abstract capabilities, can discourage exploration beyond visible options\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Graphic Design","url":"/posts/78/","content":"Graphic Design\nGraphic Design is a fundamental aspect of interface design. It helps users to:\n\nFollow the expected sequence of interactions\nUnderstand the organization of data, functions, and tasks\nBuild a consistent mental model for efficient use\nBenefit from visual consistency to become more proficient over time\nBrand recognition: Graphic design can provide a distinctive look and feel\n\nComponents of the Visual language\n\nLayout: How the content is structured on the display\nTypography: Typefaces &amp; typesetting\nImagery: Visual identity, icons\nSequencing: How interactions unfold\n\nGood Graphic Design\nGood graphic design uses clear, repeated visual elements to guide the viewer.\nIt relies on modular structures. e.g. Same text column widths and placements.\n\nCreate a unified visual identity\nWork across various presentation formats\nHelp users navigate and understand information quickly\n\nGestält Principles\nThe whole is different from the sum of its parts!\nSmaller objects are grouped to form larger ones.\nLaw of Prägnanz (Law of simplicity, Law of good figure): We tend to order our experience in a manner that is regular, orderly, symmetric, and simple. Complex images is perceived in the simplest possibility.\n\nEmergence: The whole is emerged as a sum of its parts. We can perceive the whole even if we have noise or insufficient information.\n\nReification: The experienced perception contains more information than the sensory stimulus. i.e. We fill the missing part.\n\nMultistability Perception: The experienced perception can be different even if the sensory stimulus is same.\n\nInvariance: Perception are same even when the properties of an object change, such as its size, orientation, distance, or lighting.\n\nGrouping:\n\nProximity: Tendency of elements to be associated with nearby elements\nSimilarity: Tendency of elements to be associated with similar elements\nContinuity: Preference for continuous, unbroken contours with the simplest possible physical explanation\n\n\nPerception of Forms:\n\nClosure: Complete, closed figures\nFigure-Ground: Smaller one as figure, larger one as ground\nSymmetry: Symmetrical components will tend to group together\n\nPrinciples of Graphic Design\nCRAP: Contrast, Repetition, Alignment, and Proximity\nContrast\n\nGuides the user's attention to the key elements of a design\nMaintain the distinction between similar elements in a design\nHelps preattentive processing\n\nRepetition\n\nImproves unity and harmony\nCreates visual consistency in page designs\n\ne.g. Use same style of headlines, same style of initial capitals, or repeat same basic layout from one page to another\nAlignment\n\nThe act of keeping design objects in line\nCreates a visual relationships between elements such as images, shapes, or blocks of texts\nEnhance boundaries and grouping\n\nEffective Visual Channel\n\nMagnitude channels are used for ordered attributes. They determine how much the amount is.\nIdentity channels are used for categorical attributes. They determine what/where the value is.\nFalse alignment\n\nAlmost but not quite aligned\nFree standing objects not to be aligned with other objects\nOptical adjustment\n\nCenter point looks different depending on the shape of the elements (e.g. sharp, round-ended, rectangular)\nFrom largest to smallest, straight edges, curved edges, and sharp edges appear larger.\nIt look misaligned even if it's actually perfectly aligned!\n\n\n\nRelating Structure\n\nGrouping: Spatial logic is more powerful; Avoid explicit grouping\nHierarchy: Privde a hierarchical context for each piece of information\nRelationship: Visual scanning order should match the logical information\nBalance: Harmonious global arrangement, usually using symmetrical objects\n\nProximity\nDesign elements near each other are perceived as related, while elements spaced apart are perceived as belonging to separate groups.\nNegative space\nEmpty space can be meaningful!\n\nProvide the ground on which the design appears\nEnhance/Enforce the structure of the display\nReduce the use of explicit borders\nUse negative space as a design\n\nConsistency\n\nInternal Level: inside an application\nExternal level: platform and interface guideline conventions\n\nConsistency can help users from reliable expectations and avoid costly mistakes.\nColor\n\nCross-Cultural color naming\n\nPrimary color terms are consistent across cultures (e.g. white, black, red)\n\n\nColor Categories\n\nOnly 8 hues are named out of 210 colors!\nWe only need small number of labels (or colors)\n\n\nColor coding\n\nLow saturation is enough for large areas\nHigh saturation is needed for small areas\nWe can use borders to create high saturation\n\n\nColor Deficiency\n\nThere are lots of color-deficiency simulators\nDistinguish by more than hue alone, e.g. saturation, brightness\n\n\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Information VIsualization","url":"/posts/79/","content":"Visualization\n\nScientific Visualization: Render actual objects\nInformation Visualization: Visualize abstract data, also called InfoVis\n\nBenefits of visualization\n\nExternal representation acts as an artificial memory that best supports our natural means of perception.\nProblem solving can proceed through a smooth traversal of the diagram.\nVisualizations can reveal structures, while summarization such as mean, variance lose information.\n\nInfoVis Reference Model\n\nInfoVis is interdisciplinary!\n\nGraphics: Drawing in real time, &lt;100 ms\nCognitive psychology: Need appropriate representation\nHCI: Users and tasks to guide design and evaluation\n\nConsiderations for designing InfoVis\n\nRelative percption: e.g. Luminance contrast: Luminance perception is based on relative judgements\nExpressiveness: Vis idiom should express all of, and only, the information in the dataset attributes\nEffectiveness: Most important attributes should be encoded with the most effective channels\n\nSteven's Power Law\np=kaα,∴p1p2=(a1a2)αp = ka^\\alpha, \\therefore \\frac{p_1}{p_2} = \\left( \\frac{a_1}{a_2} \\right)^\\alpha\np=kaα,∴p2​p1​​=(a2​a1​​)α\n\np is perceived magnitude\na is actual magnitude\nα\\alphaα varies across tasks\n\ne.g. α≈1\\alpha \\approx 1α≈1 if judging length, α&lt;1\\alpha &lt; 1α&lt;1 if judging area, α≪1\\alpha \\ll 1α≪1 if judging volume.\nIn layman's terms, we perceive length as it is, but we perceive volume to be much smaller than it actually is.\nPreattentive Processing\nPreattentive processing is a cognitive operation that are done preattentively, without the need for focused attention.\nIt takes the minimum amount of time to initiate eye movement, which is less than 200~250ms. (c.f. eye movement takes 200ms)\nIt only involves information available in a single glance.\n\nPopout effects: Different features pop out.\nSegmentation effects: Visualization is divided into different features. (Boundary detection)\n\nPreattentive tasks\nVisual features that are detected very rapidly by low-level, fast-acting visual processes.\nPrecedes focused attention, occuring within a single fixation.\nIt is easily detected (pop out) regardless of the number of distractors.\ne.g. finding different color, orientation, shape, size, convex/concave, curved/straight\ne.g. boundary detection (segmentation), region tracking (can a moving group be traced?), counting (how many elements of a certain type are present?)\nLaws of Preattentive display\n\nMust stand out on some simple dimension. e.g. color, shape, motion, depth\nSome dimension is more accurate than others.\ne.g. Position, length is more accurate than volume, color.\nOnly few should be highlighted.\ne.g. If you have a mixture of circles of different colors, you can't just find the red circle at once.\n\nDesign Principles/Guidelines\n\nVisual presentation of query components and results\nRapid, incremental and reversible actions\nImmediate and continuous feedback\nSelection by pointing\n\nMultidimensional Projection\nSometimes, we have to visualize data with higher dimension, especially in machine learning.\nCommonly we represent data as a 2D scatterplot.\nHowever, multidimensional projections suffer due to distortions!\ni.e. The features of high-dimensional data (e.g. area, volume) are highly distoreted in the projection.\nMeasuring misrepresentation\nVisual attribute value should be directly proportional to data attribute value.\nLie factor=Size of effect shown in graphicSize of effect in data\\text{Lie factor} = \\frac{\\text{Size of effect shown in graphic}}{\\text{Size of effect in data}}\nLie factor=Size of effect in dataSize of effect shown in graphic​\nDistortions in projeciton\n\nPoint-point relationships\n\nMissing Neighbors: Neighbors in the original space are no more neighbors in the projection\nFalse Neighbors: Neighbors in the projection are actually not neighbors in the original space\n\n\nCluster-cluster relationship\n\nMissing Groups: Clusters in the original space are missing in the projection\nFalse Groups: Clusters in the projection are actually not clusters in the original space\n\n\n\nAvoid chartjunk\nAvoid chartjunk, that are all visual elements (in charts and graphs) that are not necessary to comprehend the information represented on the graph, or that distract the viewer from the information.\nData-ink ratio=Data inkTotal ink used in graphic\\text{Data-ink ratio} = \\frac{\\text{Data ink}}{\\text{Total ink used in graphic}}\nData-ink ratio=Total ink used in graphicData ink​\nMaximizing data-ink ratio can reduce chartjunk.\nUse small multiples\nRepeat visually similar graphic elements nearby rather than spreading far apart.\n\nLearn once and compare (Invite users to compare)\nReveal, all at once, a scope of alternatives, and a range of options (i.e. Act as a overview)\n\nNegative Space\nSpace can deliver messages!\ne.g. 화학공/정신/기술/연구소 or 화학공정 신기술연구소...?\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Evaluation without users","url":"/posts/81/","content":"Design Heuristics\nGoal: Evaluate the evolving design when no users are present.\nWhy? Using users is the best, but it's not always possible.\n\nCognitive Walkthroughs: Path through interface with pre-determined tasks and actions\nAction Analysis: Predict the time for an exper to perform a task\nHeuristic Analysis: Interface oriented, Path through interface (without pre-determined tasks)\n\nCognitive Walkthrough\nCognitive walkthrough is a formalized way of imagining people's thoughts and actions when they use an interface for the first time.\nRequirements:\n\nDescription or prototype of interface (or a prototype)\nTask description\nComplete, written sequence of actions for completing the task\nUser background (or profile)\nUsually only one analyzer is required\n\nGoal:\n\nWill users know how to perform the action? (Will they try to do it?)\nWill users see the control?\nWill users know if the control does what they want?\nWill users understand the feedback?\n\nMethod: Select prototype and a task, then try to tell a believable story about each action a user has to take to do the task.\nTo make the story believable, you have to motivate each of the user's actions, relying on the user's general knowledge and on the prompts and feedback provided by the interface.\nIf you can't tell a believable story about an action, then we need to fix the interface.\nAction Analysis\nAction analysis allows a designer to predict the time for an expert to perform a task.\ne.g. GOMS models\nIt forces the designer to take a detailed look at the interface, and forces the designer to decide what physical and mental steps a user will perform to complete tasks.\nBy analyzing steps, we can look for problems such as:\n\nDoes it take too many steps to perform a simple task?\nDoes it take too long to perform the task?\nDoes it take too long to learn about the interface?\n\nHeuristic Analysis/Evaluation\nHeuristic evaluation is a usability inspection method where experts evaluate a user interface (UI) against a set of predefined heuristics or principles.\nUsually several analyzers are needed.\nWhy? Best analyzers can miss easy problems, while worst analyzers might discover hard problems.\nUsing serveral analyzers can find both easy and hard problems.\nUsability Engineering\nIntroduced by Jakob Nielsen. He involved a structured process that integrates usability principles and methods throughout the entire product development lifecycle.\nRequires a small set of evaluators to examine the UI. 3-5 of evaluators are proven to have highest ratio of benefits to costst.\nNielsen's evaluation phases\n\nPre-evaluation training: Provide the evaluator with domain knowledge.\nEvaluation: First get a feel for flow and scope, then focus on specific elements.\nSeverity rating: Establishes a ranking between problems, reflecting frequency, impact and persistence.\nDebriefing: Discuss outcome and potential solutions with design team.\n\nNielsen's heuristics\n\nSimple and natural dialog: Visibility of system status\nSpeak the users' language: Match the real world\nMinimize user memory load: User control and freedom\nConsistency: Consistency and standards\nFeedback: Error prevention\nClearly marked exits: Recognition rather than recall\nShortcuts: Flexibility and efficiency of use\nPrevent errors: Aesthetic and minimalist design\nGood error messages: Help users recognize, diagnose and recover from error\nProvide help and documentation: Help and documentation\n\nSimple and natural dialog\n\nMinimalist design: UI should be simplified as much as possible. It reduces learning effort &amp; possibility of errors.\nPresent information in a natural order. (logical flow)\nRemove or hide irrelevant or rarely needed information. (e.g. dynamic menus: some menus are hidden under other menus)\nUse windows frugally to avoid complex window management.\n\nSpeak the users' language\n\nUse a language compatible with users' conceptual model (e.g. DO NOT use error log directly)\nUse meaningful mnemonics, icons and abbreviations.\n\nMinimize user memory load\n\nUse recognition rather than recall. i.e. Let user recognize thing previously experienced, instead of recall the things from long-term memory.\ne.g. Combo box require recognition, while textbox require recall.\nMake things visible.\nFamiliar option is selected over unfamiliar options.\nDescribe expected input clearly, and don't allow incorrect input.\nCreate orthogonal command systems. i.e. Commands shouldn't overlap or interfere with one another, therefore they can be combined in any order without unexpected side-effects.\nUse generic commands such as open, save, cut, copy, and paste.\n\nConsistency\nBe consistent in:\n\nCommand design: Same action, same effect in same situations.\nGraphic design: Same input, output format.\nFlow disign: Similar tasks are handled in similar ways.\n\nFeedback\nSemantic: System status should be visible, but it shouldn't overburden users.\ne.g. Make cursor same as selected tool\nTime: Different feedback should be used based on time scales.\n\nUnder 100ms: Causality.\n1s: Delay but user's flow of thought is uninterrupted. -&gt; Use hourglass cursor!\nOver 10s: Difficult to stay focused, user will switch to another task while waiting. -&gt; Use estimate of time left, always use overestimate!\n\nClearly marked exits\nUsers don't like to be trapped!\ne.g. Always responsive cancel button, universal undo/redo\nShortcuts\n\nProvides flexibility and efficiency of use\nLet expert users perform operations rapidly\n\ne.g. shortcuts, toolbars and tool palettes (trading screen real estate for rapid access), macros, navigation jumps (history systems)\nPrevent errors\nAn ounce of prevention is worth more than a pound of cure!\n\nMistakes: Conscious decision with unforeseen consequences\n\nWhen users don't know the system well, they cannot formulate the right goal\nRadical redesign or improved training is necessary.\n\n\nSlips: Users know what and how to do, but fail to do it correctly\n\nError in the mental model!!!!\ne.g. Capture errors, description errors, loss of activation, mode errors\n\n\n\nCapture Error\nFrequently done activity takes charge instead of one intended.\nWe can minimize it by making actions undoable instead of confirmation, or allowing reconsideration of action by user. (e.g. Recycle bin in Windows)\nDescription Error\nIntended action is similar to others that are possible.\nUsually occurs when right and wrong objects are physically near each other.\nWe can minimize it by rich feedback, undo, and checking for reasonable input.\nLoss of activation\nForget what the goal is while undergoing the sequence of actions.\nWe can minimize it by allowing person to see path taken, or making goal explicit if the system knows it. (How????? AI can make it possible!)\nIf you are the user... just continue action then you might remember the goal!\nMode errors\nPeople do actions in one mode thinking they are in another\nWe can minimize it by having as few modes as possible, or making modes highly visible.\nForcing functions\n\nLockin mechanisms\n\nKeep users in a state until conditions are met\nProcess continues unless user removes constraints before stopping it\n\n\nLockout mechanisms\n\nPreevnt users from entering a state until conditions are met\nProcess won't occur unless user removes constraints before starting it\n\n\nInterlock mechanisms\n\nMakes the state of two mechanisms or functions mutually dependent.\ne.g. The door of a microwave oven is locked while the magnetron is on, and the magnetron is prevented from operating while the door is open\n\n\n\nGood error messages\n\nBe phrased in clear language, avoid obscure codes\nBe precise rather than vague or general\nConstructively help the user solve the problem\nBe polite, and not intimidate the user\n(i.e. don't put the blame on users)\nProvide meaningful error messages, explain the problem in terms of the user conceptual model\n\nProvide help and documentation\nNo need of docs is always better, but providing help is not wrong!\nMost users will stay at the intermediate level.\nAlso, they don't like to read manuals. They would rather learn by making progress towards their goals.\nTypes of help\n\nTutorial and/or getting started manuals\n\nProvide a clear learning path\nPresents conceptual model of the system\nDemonstrates basic featrues with online tours and demos\n\n\nReference manuals (only for experts!)\nReminders (e.g. short reference cards, keyboard templates, tooltips)\nWizards (e.g. Windows Installer wizard)\n\nWalks user through typical tasks\nUsers might feel they are losing control\n\n\nTips\n\nMigration path to learning new features\nCan be boring and tedious\n\n\nContext sensitive help e.g. balloon help\n\nSystem provides help on the interface component the user is currently working with\n\n\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Evaluation with users","url":"/posts/84/","content":"User Testing\nTest the interface with real users!\nProblem: Users are human beings with feelings and rights...\nThe ethics and responsibility of evaluators are necessary.\n\nQualitative/Naturalistic\nQuantitative/Experimental\nField Study (usually long term and qulitative)\n\nMilgram's Obedience Experiment\nMeasured the willingness of study participants to obey an authority figure who instructed them to perform acts that conflicted with their personal conscience.\ne.g. Martial law by president Yoon?\nThe experimenter told the participants that we will test how shocks might improve recall of word associations;\nThe experimenter encouraged the participants to continue raising the voltage of the shocks (up to 450V) whenever the learner gave wrong answers.\nHowever, this was a fake experiment! The real purpose of the experiment was to see if the participants would follow orders.\nThe experimenter played pre-recorded sounds for each shock level to make participants believe.\nMilgram discoverd that over seventy percent of participants delivered lethal shocks to an innocent person!\nEthics of the milgram experiment\nThe participant was given incorrect information about the experiment.\nThey were also under more pressure than many believe was necessary. (Was it necessary to watch a person receiving electric shocks?)\n\nWas it useful?: Did we learn anything that can be broadly applied?\nWas it ethical?: Could we have gathered this knowledge by other means?\n\nTreating Subjects with Respect\n\nFollow human subject protocols\n\nIndividual test results should be kept confidential\nUsers can stop the test at any time\nUsers are aware and understand the monitoring technique\nTheir performance will not have implication on their life\nRecords will be anonymous and must be explicitly approved\n\n\nUse standard informed consent form\n\nEspecially for quantitative tests\nBe aware of legal requirements\n\n\nSpecial protocol for children\n\nConsent from their parents\n\n\n\nThere are many legal ways to force the experimenters to treat subjects with respect!\nConducting the Experiment\n\nBefore the experiment\n\nHave them read and sign the consent form\nExplain the goal of the experiment\n\nIn a way accessible to users\nAnswer questions\n\n\n\n\nDuring the experiment\n\nStay neutral\nNever indicate displeasure with users' performance\n\n\nAfter the experiment\n\nDebrief users\nInform users about the goal of the experiment\nAnswer any questions they have\n\n\n\nManaging Subjects\n\nDon't waste users' time\n\nUse pilot tests to debug experiments, questionnaires, etc.\nHave everything ready before users show up\nIf an error occurs during the experiment, stop it and find other participants.\n\n\nMake users comfortable\n\nKeep a relaxed atmosphere\nAllow for breaks\nPace tasks correctly\nStop the test if it becomes too unpleasant\n\n\nCompensation\n\nPay participants whether they complete the study or not\n\n\n\nConcerns of User Testing\n\nInternal validity\n\nobserved results caused by the independent variables? (i.e. Are controlled variable actually controlled?)\nconfidence in our explanation of experimental results\nusually good in experimental settings\nwatch for &quot;confounding&quot; variables\n\n\nExternal validity\n\ngeneralizability of observed results\nconfidence that results applies to real situations\nusually good in natural settings\n\n\nReliability\n\nWould the same results be achieved if the test were repeated?\n\n\n\nConsiderations on internal validity\n\nOrdering effects\n\nInterface X first or Y first?\nLearning effect: Learning may influence performance\nGet tried or bored over time\n\n\nSelection Bias\n\nAssigning pre-existing groups to different levels of an independent variable can skew results\nUse random assignment to ensure fairness and balance\n\n\nExperimenter bias\n\nTendency to favor expected or desired results\nMitigate with a rigid standardized experiment protocol\n\n\nHow to counter-balance\n\nRandomization, double-blind experiment (both experimenter &amp; subjects unaware of conditions)\n\n\n\nConsiderations on external validity\n\nPopulation validity: How well does the sample reflect the target population?\nEcological validity: How similar is the testing environment to the real world?\nTraining validity: Does the training provided simulate realistic learning conditions?\nTask validity: Are the experimental tasks representative of real world activities?\n\nQualitative Evaluation\n\nThe raw data is text, picture, or artifacts (not numbers)\n\nObservations, video\nOpen-ended interviews and questionnaires\nCollections of work samples, artifacts\nNarrative, textual descriptions\n\n\nKey Characteristics\n\nFocus on richness and depth of data, not reduction to numbers\nCaptures context, meaning, and complexity of user experiences\n\n\nGrounded Theory Approach\n\na data-driven method for building theory from qualitative data\nAim: to generate new theories grounded in the data itself\n\n\n\nUsability Study - Qualitative approach\n\nPurpose\n\nUnderstand the user's perception of the interaction\nFocus on the perceived quality of the experience\nEmphasize the user's ability to use the system, rather than just how much they like it\n\n\nQualitative Methods\n\nIntrospection\n\nCognitive Walkthroughs (conducted by designers)\n\n\nDirect observation\n\nSimple observation\nThink aloud protocols\nConstructive interaction (e.g., co-discovery)\n\n\nInterviews, questionnaires and surveys\n\nOpen-ended questions to explore user experience in depth\n\n\n\n\n\nPrepare Experiment\n\nSelect the correct population\nSet objectives and tasks\n\nRealistic\nInformative\n\n\nApply for the IRB (irb.snu.ac.kr)\nHardware e.g. Computer, video equipment\nSoftware e.g. Up and running, properly debugged\nFacilitator (Experimenter)\n\nUsing a checklist might be useful\nPractice and practice!\n\n\n\nCreate tasks\n\nDescribe in terms of end goals\nSpecific and realistic\nDoable\nNot too long (about 3 minutes each)\n\nDirect Observation\n\n\nObserving (and recording) users interacting with the system\n\nIn lab or in the field\nFor a set of pre-determined tasks or through normal duties\n\n\n\nExcellent at identifying gross design/interface problems\n\n\nObservation methods:\n\nsimple observation\nthink-aloud\nconstructive interaction (question-suggestion protocol)\ninterview and questionnaire\n\n\n\nRecording Observations\n\nWhy keep a record?\n\nEasy to forget details\nFurther analysis (post-analysis)\nUseful for justifying findings in discussions\n\n\nTechniques\n\nPaper and pencil\n\nSimple to set up\nNeed coding scheme (e.g. tabular?)\nMight be biased\n\n\nAudio/Video recording\n\nMore accurate\nTime consuming to analysis\n\n\n\n\n\nSimple Observation Method\nEvaluator observes interacting users.\nEasy, but we get no insight into the user's decision process or attitude.\nThink-aloud Method\n\nSubjects are asked to say what they are thinking/doing\n\nWhat they believe is happening\nWhat they are trying to do\nWhy they took an action\n\n\nWidely used in industry\nDrawbacks\n\nAwkward/uncomfortable for subject (thinking aloud is not natural!)\n&quot;Thinking&quot; about it may alter the way people perform their task\nHard to talk when they are concentrating on problem\n\n\n\nConstructive Interaction Method\n\nTwo people work together on a task\n\nNormal conversation between the two users is monitored (less distortion)\nremoves awkwardness of think-aloud\n\n\nVariant: Co-discovery learning (question-suggestion protocol)\n\nUse semi-knowledgeable &quot;coach&quot; and naive subject together\nMake naive subject use the interface\n\n\nDrawback: Need a good team\n\nInterviews\n\nMethod\n\nPick the right population (Individual or group discussion)\nPlan a set of central questions\nProbe more deeply on interesting issues as they arise\n\nFocus on goals not technology\nFind the root of the problem\n\n\n\n\nPros and cons\n\nVery good at directing next design phase\nProvide many constructive suggestions\nSubjective (e.g. Leading questions should not be asked)\nTime consuming\n\n\n\nDebriefing\n\nPost-observation interviews\n\nQuestions from your notes\nQuestions from users' diary\nQuestions from a video footage\n\n\nPros and Cons\n\nAvoids erroneous reconstruction\nProvide many constructive suggestions\nTime consuming\nBut extremely valuable\n\n\n\nQuestionnaires and Surveys\n\nMethod\n\nPick the population e.g. Between 50 and 1000 subjects\nEstablish the purpose of the questionnaire\n\nWhat information is sought?\nHow would you analyze the results?\n\n\nEstablish the means of deliver/collection\n\nOn-line\nDirect interaction with users\n\nWalking in the street\nPost-user testing\n\n\nSurface mail\n\nincluding a pre-addressed reply envelope gives far better response\n\n\n\n\nDesign the questionnaire\n\nDon't forget to debug it!\n\n\nDeliver\nCollect and analyze the data\nEstablish the main finding\n\n\nPros and cons\n\nPreparation is expensive\nNeed to design and debug the questionnaire\nCan reach a large population, but often a low return rate\nAs good as the questions asked\nData collection can be tedious\n\nUse automatic forms for large volume\nGoogle doc forms, survey monkey\n\n\n\n\n\nType of questions\n\nClosed Question\n\nScalar, e.g. 1~5\n\nBe sure to pick odd numbers of choice, so user can pick middle.\n\n\nMulti-choice\n\nChoices can be mutually exclusive, or not (inclusive)\n\n\nRanked chocie\n\nHelpful to understand user's preference\n\n\n\n\nOpen Ended Questions\n\nGood for general information\nDifficult to analyze\nCan complement closed questions\n\n\n\nQualitative Approaches Outcome\n\nHigh level effects\n\nTask flow problems\nTask description problems\nContextual findings\n\ne.g. Conflict with social pattern\nTwo hands needed but only one available\n\n\n\n\nPros and Cons\n\nApply to a real situation\nGood external validity, but poor internal validity\nPoor control of independent variables\nOften subjective data\n\n\n\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Quantitative Evaluation","url":"/posts/87/","content":"Qualitative vs Quantitative\n\nQualitative\n\nDevelop understanding of human experience\nEmphasis on preserving the richness of the data\nBetter external validity\n\n\nQuantitative\n\nObjectively measure human performance\nMore narrowly focused\nBetter internal validity\n\n\n\nQualitative approach\nQualitative approaches focus on high-level effects!\n\nIdentifies task flow problems\nHighlights ambiguities in task description\nRevel contextual and situational insights e.g. conflict with social norms or expectations, usability mismatches\n\nIt can reflect a real-world usage and context, leads to strong external validity.\nBut it have lower internal validity, and relies on subjective interpretations of data.\nQuantitative approach\nQuatitative approaches focus on low-level effects!\n\nCaptures patterns of use\n\nIt provides objective measurements, leads to strong internal validity.\nBut it might be difficult to interpret real world implications, and small differences may lack practical significance (even if they are statistically significant).\ne.g. Is difference between 3.05s and 3.00s really meaningful?\nQuantitative evaluation\n\nMore scientific approach?\nSpecify users and tasks\nPredict and measure...\n\ntime to learn\nspeed of performance\nrate of human errors\nhuman retention over time\nsubjective satisfaction\n\n\nby using...\n\nUser events collection (e.g. mouse clicks, key presses, mouse moving distance)\nAnalytics tool (e.g. Google Analytics)\nControlled experiments\n\nLucid/testable hypothesis\nIndependent variables\nDependent variables\nShould be able to reproduced by others\n\n\n\n\nAccomodate individual differences\nConsider social, organizational, and cultural context\n\ne.g. Google SktechUp found that undo and erase events can detect over 90% of the severe usability problems.\nControlled Experiments\nBased on practical problem and existing theory!\nIt can be:\n\na guidance for practitioners\nan advice for experimenters\na refined theory\n\n\nState a lucid, testable hypothesis\n\ne.g. With a proper acceleration function, a scroll-wheel based system can be faster than a ScrollPoint\n\n\nIdentify independent and dependent variables\nDesign the experimental protocol\nChoose the user population\nApply for human subjects protocol review\nRun a couple of pilots\nRun the experiment\nRun statistical analysis\nDraw conclusions\n\nNotes for running the experiment\n\nAlways run pilots first!\n\nThere are always unexpected problem!\nWhen the experiment has started you cannot change\n\n\nUse a check-list so that all subjects follow the same steps (i.e. specific protocol)\nDon't forget the informed consent form\nDon't forget to debrief each subject\n\nAlways ask these questions:\n\nIs it ethical?\nIs it useful?\nIs it reliable?\n\nDoes repeating the experiment yield the same result?\n\n\nIs it valid?\n\n(internal) Does the experiment consider variations between subjects?\n\nNeed for testing an enough samples of subjects\n\n\n(internal) Was the experience biased?\n(external) Does the experiment reflect target use?\n\nWere users typical?\nWere tasks typical?\nWas the setting realistic?\n\n\n\n\n\nIndependent/Dependent variables\nIndependent variables are the things you manipulate independent of a subject's behavior.\nIt determines a modification to the conditions the subjects undergo.\nDependent variables are the things you set out to quantitatively measure.\nIt depends on the subject's behavior or reaction to the independent variable.\nControl variables\nControl variables are circumstances that is kept constant while testing the effect of an independent variables to dependent variables.\nNumber of control variables should be small!!\nMore control means the experiment is less generalizable.\nRandom variables\nRandom variables are circumstances that is allowed to vary randomly.\nMore variability is introduced by random variables in the measures, but the results are more generalizable.\ni.e. Internal validity is bad, but external validity is good.\nConfounding variables\nConfounding variables is an external factor that can affect the results of the experiment or user study, making it difficult to determine the true effect of the independent variable on the dependent variable.\nIt varies systematically with independent variables (and dependent variables)!\nConfounding variables cause Type I errors (false positive), thus hurting internal validity.\nWe should find it, and control/randomize it to avoid misleading results!\ne.g. Consider a study comparing the target selection performance of a mouse and a gamepad where all participants are mouse experts, but gamepad novices.\nConfounding variable &quot;Prior experience&quot; affects experiment.\nControlling confounding variable\n\nCase-Control Study: A study where subjects with a particular outcome or behavior (cases) are compared to those without it (controls) a identify factors that may be associated with the outcome\nCohort Study: A study that follows a group of people who share a common characteristic or experience over time.\nBlocking: Grouping experimental units that are similar to each other to control for confounding variables.\n\nBlocking Factor: a source of variability that is not of primary interest to the experimenter.\nRandomize within each block, then measure differences within the block.\ne.g. First divide participants into male and female groups (blocks), then randomly assign half of each group to a different condition.\nBlock what you can, randomize what you cannot!\n\n\nRandomization: Randomly assigning subjects to different groups to ensure that confounding variables are evenly distributed\n\nExperimental Protocol\n\nBetween subjects: each subjects runs one condition\n\nCompare observed results between independent groups\nCan eliminate ordering and learning effects\nDifference between subjects might introduce a bias\nNeed more subjects\nMight allow confounding variables to affect results\n\n\nWithin subjects: each subjects runs several conditions\n\nCompare observed results within each subject (e.g. paired t-Test)\nCan eliminate variation due to subject differences\nNeed fewer subjects\nMight suffer from ordering and learning effects (reduce it by randomizing the treatment order)\n\n\nVery important for the statistical analysis phase!\n\nIRB (Institutional Review Board) Review\nIRB is a committee to approve, monitor, and review research involving human subjects with the aim to protect the rights and welfare of the research subjects.\nIRB review is required for research involving human subjects.\nBut lazy people don't do this...\nUser population\nPick a well-balanced samples considering novices, experts, age group, and gender.\nPopulation group may be one of the independent variable.\nStatistical Analysis\n\nThere are various methods to perform statistical analysis...\nConsider:\n\nProperties of our population (e.g. mean, variance)\nHow different data sets relate to each other\nProbability that our claims are correct\n\nConfidence level: 95% or 99%?\nUsually use the level of statistical significance, tested with p-value. (e.g. p &lt; 0.05)\n\n\n\nAnalysis of Variance (ANOVA)\nAnalysis of variance is the most widely used statistical test for hypothesis testing in factorial experiments.\nGoal: Determine if an independent variable has a significant effect on a dependent variable.\ne.g. Is the time to complete a task less using method A than using method B?\nANOVA example\nThe mean task completion time for Method A was 4.5s. This was 20.1% less than the mean of 5.5s observed for Method B. The difference was statistically significant (F1,9 = 9.80, p &lt; .05)\nThe mean task completion time were 4.5s for Method A and 5.5s for Method B.\nAs there was substantial variation in the observations across participants,\nthe difference was not statistically significant as revealed in an analysis of variance (F1,9 = 0.626, ns)\nns means not significant. Use ns if F &lt; 1.0, or use p &gt; .05 if F &gt; 1.0.\n","categories":["SNU","4-1","인간컴퓨터상호작용"]},{"title":"Introduction","url":"/posts/32/","content":"What is Computer Graphics?\nComputer graphics is the use of computers to synthesize and manipulate sensory(usually visual) information from digital information.\nSound, touch, taste, smell is also part of computer graphics!\n3D printing even allow us to turn digital information into physical matter.\nWe can easily watch state-of-art technologies from the SIGGRAPH Technical Paper Trailer.\n\nEntertainment (movies, games)\nArt and design\nIndustrial design\nComputer Aided Engineering (CAE) (What happens when two car collide?)\nArchitecture\nScientific/Mathematical Visualization\nMedical visualization and simulation\nNavigation\nCommunication (zoom, Codec Avatars etc.)\n\nExtremely broad!!!!!\nEverything is mixed - physics(optics), mathematics(mathematical optimization), HCI, etc.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Affine Geometry","url":"/posts/35/","content":"2D/3D Geometry Representation\nBasic elements: vertices, edges\nA vector represents an edge, and a point represents a vertex.\nIn computer view, vectors and points are represented as tuple.\nBut they are totally different!!!\nPoints are actually vector from origin.\nIf we change origin, sum of two points will be different.\nAdding points is coordinate-dependent!\nWe should use only coordinate-free operations.\nRecall) Vector space\nVector space consists of\n\nset of vectors\ntwo operations\n\nvector addition\nscalar multiplication\n\n\n\nc.f. Vector space + metric (norm, etc.) = Euclidean space\nAffine space\nVector space + points!\nAffine space consists of\n\nset of points\nassociated vector space\nadditional operations on points\n\ndifference between two points\naddition of a vector to a point\nscalar multiplication of point\n\n\n\nCoordinate-Invariant Geometric Operations\nAddition\nVector added by vector is a vector.\nPoint added by vector is a point.\nSubtraction\nVector subtracted by vector is a vector.\nPoint subtracted by vector is a point.\nPoint subtracted by point is a vector.\nScalar multiplication\nScalar multiplication of vector is a vector.\nPoint multiplied by 1 is a point.\nPoint multiplied by 0 is a zero vector.\nPoint multiplied by any other scalar is undefined.\nLinear combination\nLinear combination of vector.\nv=∑i=0Ncivi\\mathbf{v} = \\sum_{i=0}^{N} c_i\\mathbf{v}_i\nv=i=0∑N​ci​vi​\nAffine combination\nLinear combination of points????\nIt is even possible??\n∑i=0Ncipi=(∑i=0Nci)p0+∑i=0Nci(pi−p0)\\sum_{i=0}^{N} c_i\\mathbf{p}_i = \\left(\\sum_{i=0}^{N} c_i\\right)\\mathbf{p}_0 + \\sum_{i=0}^{N} c_i(\\mathbf{p}_i-\\mathbf{p}_0)\ni=0∑N​ci​pi​=(i=0∑N​ci​)p0​+i=0∑N​ci​(pi​−p0​)\n∑i=0Nci(pi−p0)\\sum_{i=0}^{N} c_i(\\mathbf{p}_i-\\mathbf{p}_0)∑i=0N​ci​(pi​−p0​) is actually a vector.\nIf ∑i=0Nci\\sum_{i=0}^{N} c_i∑i=0N​ci​ is 0, this is just a linear combination of vectors.\nIf ∑i=0Nci\\sum_{i=0}^{N} c_i∑i=0N​ci​ is 1, this is a point, and we call this an affine combination.\nOtherwise, this is undefined.\nAffine Frame\nA frame is defined as a basis of a vector space and a point o (called origin).\nAny point p can be written as p=o+c1v1+⋯+cNvN\\mathbf{p} = \\mathbf{o} + c_1\\mathbf{v}_1 + \\cdots + c_N\\mathbf{v}_Np=o+c1​v1​+⋯+cN​vN​.\nAny vector v can be written as v=c1v1+⋯+cNvN\\mathbf{v} = c_1\\mathbf{v}_1 + \\cdots + c_N\\mathbf{v}_Nv=c1​v1​+⋯+cN​vN​.\nRecall) A coordinate in a vector space is defined as a coefficients of linear combination on given basis.\nCoordinate system of a point\nHomogeneous Coordinates\nUse an extra dimension!\nPoints are represented as (x, y, 1).\nVectors are represented as (x, y, 0).\nNow we can just add and multiply like vectors!\nIf extra dimension of a result of operation is not 0 or 1, this operation is undefined.\nBarycentric Coordinate System\nA barycentric coordinate system is a coordinate system in which the location of a point is specified by reference to a simplex.\nA simplex is a simplest shape in given dimension.\nIn 2D space, it is a triangle.\nIn 3D space, it is a tetrahedron.\nFor example, let p1,p2,p3\\mathbf{p}_1, \\mathbf{p}_2, \\mathbf{p}_3p1​,p2​,p3​ is points of a triangle.\nThen any point q can be represented as\nq=p3+c1(p1−p3)+c2(p2−p3)=c1p1+c2p2+(1−c1−c2)p3=w1p1+w2p2+w3p3\\begin{align*}\n\\mathbf{q} &amp;= \\mathbf{p}_3 + c_1(\\mathbf{p}_1 - \\mathbf{p}_3) + c_2(\\mathbf{p}_2 - \\mathbf{p}_3) \\\\\n&amp;= c_1\\mathbf{p}_1 + c_2\\mathbf{p}_2 + (1 - c_1 - c_2)\\mathbf{p}_3 \\\\\n&amp;= w_1\\mathbf{p}_1 + w_2\\mathbf{p}_2 + w_3\\mathbf{p}_3\n\\end{align*}q​=p3​+c1​(p1​−p3​)+c2​(p2​−p3​)=c1​p1​+c2​p2​+(1−c1​−c2​)p3​=w1​p1​+w2​p2​+w3​p3​​\nBarycentric coordinates must satisfy ∑i=1Nwi=1\\sum_{i=1}^{N} w_i = 1∑i=1N​wi​=1!\nIf and only if ∀wi,0≤wi≤1\\forall w_i, 0 \\leq w_i \\leq 1∀wi​,0≤wi​≤1, q is located inside given simplex.\nc.f. q=∑i=1Nwipi\\mathbf{q} = \\sum_{i=1}^{N} w_i\\mathbf{p}_iq=∑i=1N​wi​pi​ is called a convex combination if ∀wi,0≤wi≤1\\forall w_i, 0 \\leq w_i \\leq 1∀wi​,0≤wi​≤1.\nBarycentric coordinates can be computed!\nq=w1p1+w2p2+(1−w1−w2)p3=p3+w1(p1−p3)+w2(p2−p3)=p3+w1v1+w2v2u≔q−p3=[v1v2][w1w2]∴[w1w2]=[v1v2]−1u\\begin{align*}\n\\mathbf{q} &amp;= w_1\\mathbf{p}_1 + w_2\\mathbf{p}_2 + (1 - w_1 - w_2)\\mathbf{p}_3 \\\\\n&amp;= \\mathbf{p}_3 + w_1(\\mathbf{p}_1 - \\mathbf{p}_3) + w_2(\\mathbf{p}_2 - \\mathbf{p}_3) \\\\\n&amp;= \\mathbf{p}_3 + w_1\\mathbf{v}_1 + w_2\\mathbf{v}_2 \\\\\n\\mathbf{u} &amp;\\coloneqq q - \\mathbf{p}_3 = \\begin{bmatrix}\\mathbf{v}_1 &amp; \\mathbf{v}_2\\end{bmatrix}\\begin{bmatrix}w_1 \\\\ w_2\\end{bmatrix} \\\\\n\\therefore \\begin{bmatrix}w_1 \\\\ w_2\\end{bmatrix} &amp;= \\begin{bmatrix}\\mathbf{v}_1 &amp; \\mathbf{v}_2\\end{bmatrix}^{-1}\\mathbf{u}\n\\end{align*}qu∴[w1​w2​​]​=w1​p1​+w2​p2​+(1−w1​−w2​)p3​=p3​+w1​(p1​−p3​)+w2​(p2​−p3​)=p3​+w1​v1​+w2​v2​:=q−p3​=[v1​​v2​​][w1​w2​​]=[v1​​v2​​]−1u​\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Spatial Transformations","url":"/posts/36/","content":"Recall) Linear transformation\nLinear transformation TLT_LTL​ is a mapping between vector spaces.\nLinear combination is invariant under TLT_LTL​.\nTL(∑i=0Ncivi)=c0TL(v0)+⋯+cNTL(vN)T_L\\left(\\sum_{i=0}^{N}c_i\\mathbf{v}_i\\right) = c_0T_L(\\mathbf{v}_0) + \\cdots + c_NT_L(\\mathbf{v}_N)\nTL​(i=0∑N​ci​vi​)=c0​TL​(v0​)+⋯+cN​TL​(vN​)\nIn 3D space, TLT_LTL​ can be represented by a 3x3 matrix AAA.\nTL(v)=A3×3v3×1T_L(\\mathbf{v}) = A_{3 \\times 3}\\mathbf{v}_{3 \\times 1}\nTL​(v)=A3×3​v3×1​\nExamples of linear transformation\n\n2D rotation [cos⁡θ−sin⁡θ sin⁡θcos⁡θ]\\begin{bmatrix}\\cos\\theta &amp; -\\sin\\theta\\ \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}[cosθsinθ​−sinθ cosθ​]\n2D scaling [sx00sy]\\begin{bmatrix}s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix}[sx​0​0sy​​]\n2D shear\n\nalong x-axis [1s01]\\begin{bmatrix}1 &amp; s \\\\ 0 &amp; 1 \\end{bmatrix}[10​s1​]\nalong y-axis [10s1]\\begin{bmatrix}1 &amp; 0 \\\\ s &amp; 1 \\end{bmatrix}[1s​01​]\n\n\n2D reflection\n\nalong x-axis [100−1]\\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}[10​0−1​]\nalong y-axis [−1001]\\begin{bmatrix}-1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}[−10​01​]\n\n\n3D rotation\n\nalong x-axis (pitch) [1000cos⁡θ−sin⁡θ0sin⁡θcos⁡θ]\\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\theta &amp; -\\sin\\theta \\\\ 0 &amp; \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}​100​0cosθsinθ​0−sinθcosθ​​\nalong y-axis (yaw) [cos⁡ψ0sin⁡ψ010−sin⁡ψ0cos⁡ψ]\\begin{bmatrix}\\cos\\psi &amp; 0 &amp; \\sin\\psi \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin\\psi &amp; 0 &amp; \\cos\\psi \\end{bmatrix}​cosψ0−sinψ​010​sinψ0cosψ​​\nalong z-axis (roll) [cos⁡ϕ−sin⁡ϕ0sin⁡ϕcos⁡ϕ0001]\\begin{bmatrix}\\cos\\phi &amp; -\\sin\\phi &amp; 0 \\\\ \\sin\\phi &amp; \\cos\\phi &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}​cosϕsinϕ0​−sinϕcosϕ0​001​​\n\n\n\nAny linear transformation between 3D spaces can be represented as a combination of rotation, shear, and scaling.\nIn fact, rotation can be represented as a combination of scaling and two-axes shear, or as a combination of two one-axis shear transformation!\nThis can be efficient than computing cosine and sine values.\nLinear transformation as a change of basis\nArray is usually written as column-major Representation.\nAv=[a1a2a3][xyz] \\bm{A}\\mathbf{v} = \\begin{bmatrix}\n\\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\mathbf{a}_3\n\\end{bmatrix} \\begin{bmatrix}\nx \\\\ y \\\\ z\n\\end{bmatrix}Av=[a1​​a2​​a3​​]​xyz​​\nIn fact, each column is a linear tranform of basis.\na1=A[100],a2=A[010],a3=A[001]\\mathbf{a}_1 = \\bm{A}\\begin{bmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{bmatrix}, \\mathbf{a}_2 = \\bm{A}\\begin{bmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{bmatrix}, \\mathbf{a}_3 = \\bm{A}\\begin{bmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{bmatrix}a1​=A​100​​,a2​=A​010​​,a3​=A​001​​\n\nInstead of viewing linear transformation as transforming vector, we can view as changing basis.\nxv0+yv1=x′v0′+y′v1′[v0v1][xy]=[v0′v1′][x′y′][v0′v1′]−1[v0v1][xy]=A[xy]=[x′y′]\\begin{align*}\nx\\mathbf{v}_0 + y\\mathbf{v}_1 &amp;= x&#x27;\\mathbf{v}&#x27;_0 + y&#x27;\\mathbf{v}&#x27;_1 \\\\\n\\begin{bmatrix} \\mathbf{v}_0 &amp; \\mathbf{v}_1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} &amp;= \\begin{bmatrix} \\mathbf{v}&#x27;_0 &amp; \\mathbf{v}&#x27;_1 \\end{bmatrix} \\begin{bmatrix} x&#x27; \\\\ y&#x27; \\end{bmatrix} \\\\\n\\begin{bmatrix} \\mathbf{v}&#x27;_0 &amp; \\mathbf{v}&#x27;_1 \\end{bmatrix}^{-1} \\begin{bmatrix} \\mathbf{v}_0 &amp; \\mathbf{v}_1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} &amp;= A\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x&#x27; \\\\ y&#x27; \\end{bmatrix}\n\\end{align*}xv0​+yv1​[v0​​v1​​][xy​][v0′​​v1′​​]−1[v0​​v1​​][xy​]​=x′v0′​+y′v1′​=[v0′​​v1′​​][x′y′​]=A[xy​]=[x′y′​]​\nAffine transformation\nAffine transformation TAT_ATA​ is a mapping between affine spaces.\nTAT_ATA​ maps vectors to vectors, and points to points.\nTAT_ATA​ is a linear transformation on vectors.\nAffine combination is invariant under TAT_ATA​.\nTA(∑i=0Ncipi)=c0TA(p0)+⋯+cNTA(pN)T_A\\left(\\sum_{i=0}^{N}c_i\\mathbf{p}_i\\right) = c_0T_A(\\mathbf{p}_0) + \\cdots + c_NT_A(\\mathbf{p}_N)\nTA​(i=0∑N​ci​pi​)=c0​TA​(p0​)+⋯+cN​TA​(pN​)\nIn 3D space, TAT_ATA​ can be represented by a 3x3 matrix AAA with a 3x1 translation vector.\nTA(p)=A3×3p3×1+t3×1T_A(\\mathbf{p}) = A_{3 \\times 3}\\mathbf{p}_{3 \\times 1} + \\mathbf{t}_{3 \\times 1}\nTA​(p)=A3×3​p3×1​+t3×1​\nHomogeneous Coordinates\nWith homogeneous coordinates, any affine transformation between 3D spaces can also be represented by a 4x4 matrix.\nTA(p)=[A3×3t3×101][p3×11]=[A3×3p3×1+t3×11]T_A(\\mathbf{p}) = \\begin{bmatrix}\nA_{3 \\times 3} &amp; \\mathbf{t}_{3 \\times 1} \\\\\n0 &amp; 1\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{p}_{3 \\times 1} \\\\ 1\n\\end{bmatrix} = \\begin{bmatrix}\nA_{3 \\times 3}\\mathbf{p}_{3 \\times 1} + \\mathbf{t}_{3 \\times 1} \\\\\n1\n\\end{bmatrix}TA​(p)=[A3×3​0​t3×1​1​][p3×1​1​]=[A3×3​p3×1​+t3×1​1​]\nTA(v)=[A3×3t3×101][v3×10]=[A3×3v3×10]T_A(\\mathbf{v}) = \\begin{bmatrix}\nA_{3 \\times 3} &amp; \\mathbf{t}_{3 \\times 1} \\\\\n0 &amp; 1\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{v}_{3 \\times 1} \\\\ 0\n\\end{bmatrix} = \\begin{bmatrix}\nA_{3 \\times 3}\\mathbf{v}_{3 \\times 1}\\\\\n0\n\\end{bmatrix}TA​(v)=[A3×3​0​t3×1​1​][v3×1​0​]=[A3×3​v3×1​0​]\nExamples of affine transformation\nObviously linear transformations (rotation, scaling, shear, reflection) are affine transformations\n\n2D translation [10tx01ty001]\\begin{bmatrix}1 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; t_y \\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}​100​010​tx​ty​1​​\n\nComposite transformation\n\n2D translation T(t2)⋅T(t1)=T(t2+t1)T(\\mathbf{t}_2) \\cdot T(\\mathbf{t}_1) = T(\\mathbf{t}_2 + \\mathbf{t}_1)T(t2​)⋅T(t1​)=T(t2​+t1​)\n2D scaling S(s2)⋅S(s1)=S(s2⊙s1)S(\\mathbf{s}_2) \\cdot S(\\mathbf{s}_1) = S(\\mathbf{s}_2 \\odot \\mathbf{s}_1)S(s2​)⋅S(s1​)=S(s2​⊙s1​) (⊙\\odot⊙ is hadamard product, i.e. element-wise multiplication)\n2D rotation R(θ2)⋅R(θ1)=R(θ2+θ1)R(\\theta_2) \\cdot R(\\theta_1) = R(\\theta_2 + \\theta_1)R(θ2​)⋅R(θ1​)=R(θ2​+θ1​)\n\nOrder of matrix multiplication\nLet T(p)=Tx(3)⋅R(−90°)⋅pT(\\mathbf{p}) = T_x(3) \\cdot R(-90\\degree) \\cdot \\mathbf{p}T(p)=Tx​(3)⋅R(−90°)⋅p.\n\nR to L multiplication interpret operations with respect to world coordinates.\n\nL to R multiplication interpret operations with respect to moving local coordinates.\nUsing translation\n\nPivot-point rotation T(p)⋅R(θ)⋅T(−p)T(\\mathbf{p}) \\cdot R(\\theta) \\cdot T(-\\mathbf{p})T(p)⋅R(θ)⋅T(−p)\nFixed-point scaling T(p)⋅S(s)⋅T(−p)T(\\mathbf{p}) \\cdot S(\\mathbf{s}) \\cdot T(-\\mathbf{p})T(p)⋅S(s)⋅T(−p)\nScaling along arbitrary axis R(−θ)⋅S(s)⋅R(θ)R(-\\theta) \\cdot S(\\mathbf{s}) \\cdot R(\\theta)R(−θ)⋅S(s)⋅R(θ)\n\nProperties of affine transformations\nAny affine transformation between 3D spaces can be represented as a combination of linear transformation followed by translation.\n\nMaps parallel lines to parallel lines\nPreserves ratios of distance along a line\nDoes not preserve absolute distances and angles\nDoes not preserve the origin\n\nAffine transformation as a change of frame\nxv0+yv1+o=x′v0′+y′v1′+o′[v0v1o][xy1]=[v0′v1′o′][x′y′1][v0′v1′o′]−1[v0v1o][xy1]=[x′y′1]\\begin{align*}\nx\\mathbf{v}_0 + y\\mathbf{v}_1 + \\mathbf{o} &amp;= x&#x27;\\mathbf{v}&#x27;_0 + y&#x27;\\mathbf{v}&#x27;_1 + \\mathbf{o}&#x27; \\\\\n\\begin{bmatrix} \\mathbf{v}_0 &amp; \\mathbf{v}_1 &amp; \\mathbf{o} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1\\end{bmatrix} &amp;= \\begin{bmatrix} \\mathbf{v}&#x27;_0 &amp; \\mathbf{v}&#x27;_1 &amp; \\mathbf{o}&#x27; \\end{bmatrix} \\begin{bmatrix} x&#x27; \\\\ y&#x27; \\\\ 1 \\end{bmatrix} \\\\\n\\begin{bmatrix} \\mathbf{v}&#x27;_0 &amp; \\mathbf{v}&#x27;_1 &amp; \\mathbf{o}&#x27; \\end{bmatrix}^{-1} \\begin{bmatrix} \\mathbf{v}_0 &amp; \\mathbf{v}_1 &amp; \\mathbf{o} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} &amp;= \\begin{bmatrix} x&#x27; \\\\ y&#x27; \\\\ 1 \\end{bmatrix}\n\\end{align*}xv0​+yv1​+o[v0​​v1​​o​]​xy1​​[v0′​​v1′​​o′​]−1[v0​​v1​​o​]​xy1​​​=x′v0′​+y′v1′​+o′=[v0′​​v1′​​o′​]​x′y′1​​=​x′y′1​​​\nRigid transformation\nRigid transformation TRT_RTR​ is a special case of affine transformation that consists of rotation and translation.\n\nTRT_RTR​ preserves distances between all points. (Therefore preserves angles as well - SSS Congruence)\nTRT_RTR​ preserves cross product for all vectors. (i.e. TRT_RTR​ is not reflection)\n\nIn 3D space, TRT_RTR​ can be represented by a 3x3 rotation matrix RRR with a 3x1 translation vector.\nTR(p)=R3×3p3×1+t3×1T_R(\\mathbf{p}) = R_{3 \\times 3}\\mathbf{p}_{3 \\times 1} + \\mathbf{t}_{3 \\times 1}\nTR​(p)=R3×3​p3×1​+t3×1​\nRotation matrix\nRotation matrix rotates basis, so column vectors should be normalized and orthogonal.\nTherefore RTR=IR^TR = IRTR=I, i.e. R−1=RTR^{-1} = R^TR−1=RT.\nAlso, we only choose matrices with a determinant of 1. Determinant of -1 means reflection!\nIn mathematics, rotation matrices in 3D space are called SO(3) (Special orthogonal group).\nEuler angles\nAny rotation matrix can be decomposed as a product of three elemental rotation matrices!\nR=Rx(θ)Ry(ψ)Rz(ϕ)R = R_x(\\theta)R_y(\\psi)R_z(\\phi)\nR=Rx​(θ)Ry​(ψ)Rz​(ϕ)\nc.f. In 2D sapce, degree of rotation is 1.\nIn 4D space, degree of rotation is 6???\nActually, 12 different combinations exists. Just don't rotate along same axis twice!\nXYZ, XYX, XZY, XZX, YZX, YZY, YXZ, YXY, ZXY, ZXZ, ZYX, ZYZ\nProblems of Euler angles\nGimbal Lock: In certain alignments of the axes, we lose degree of freedom.\nNot unique: Two different euler angles can represent the same rotation.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Hierarchical Modeling","url":"/posts/42/","content":"Hierarchical modeling\nModel complex objects by combinig simple primitives. (e.g. triangles, box, cylinders, spheres)\nStatic Hierarchical Model\nRelative transformations do not change. (e.g. car)\nDynamic Hierarchical Model\nRelative transformations may change on the fly. (e.g. human)\nThe movement between connected components are often described by joints.\nKinematics\nKinematics is the study of motion of systems of bodies without considering the physical forces acting on them.\nc.f. If you consider physical forces, it's called Dynamics.\nTree structure of joints and bodies (links) is used, but base link (root) can be chosen arbitrarily.\nJoint\nA joint is an area of connection where two or more bodies meet, allowing relative movement.\nMathematically, joints are constraints that reduce the degrees of freedom (DoF) of a system.\n\nfixed (weld) joint: does not allow any relative movement (0-DoF)\nrevolute (hinge) joint: allows only rotation about a fixed axis (1-DoF)\nprismatic (sliding) joint: allows only translation along a fixed axis (1-DoF)\nuniversal joint: allows roation around two fixed axes (2-DoFs)\nball-and-socket joint: allows rotation around an arbitrary axis (3-DoFs)\nfree joint: can rotate and translate freely (6-DoFs)\n\nThere are many joints - bolts and nuts, crank, etc.\nLocal vs. Global configuration\nAlso called minimal vs maximal representation.\n\nLocal (joint) configuration: Use only relationship between joints. (e.g. joint angle)\nGlobal configuration: Use rotations and translations of each body with respect to base.\n\nForward vs. Inverse Kinematics\n\nForward Kinematics (FK): Compute global configuration given local configuration.\nInverse Kinematics (IK): Compute local configuration given global configuration. (Much harder!!!)\n\nForward kinematics map\nLocal configuration doesn't have origin.\nForward kinematics map transforms the origin to the position and orientation of the end effector.\nGenerally, a FK map can be represented as T=TbaseTL1TJ1TL2TJ2⋯T = T_{base}T_{L1}T_{J1}T_{L2}T_{J2}\\cdotsT=Tbase​TL1​TJ1​TL2​TJ2​⋯.\n\nTbaseT_{base}Tbase​ transforms origin to the position and orientation of base.\nLink transformations defines a frame relative to its parent. (often only fixed)\nJoint transformations represents joint movement. (often set by variables)\n\nAny transformation can be represented as a translation followed by a rotation.\nT=Tx(tx)Ty(ty)Tz(tz)Rx(θ)Ry(ψ)Rz(ϕ)T=T_x(t_x)T_y(t_y)T_z(t_z)R_x(\\theta)R_y(\\psi)R_z(\\phi)\nT=Tx​(tx​)Ty​(ty​)Tz​(tz​)Rx​(θ)Ry​(ψ)Rz​(ϕ)\nKinematic Trees\nEach node may include:\n\nlink transformation with respect to its parent\njoint transformation with respect to link transformation\na list of child joints\na list of visual node\n\ngeometry\ngeometry transformation with respect to joint transformation\ncolor\ntexture\n\n\n\nTo process a specific node, just DFS!\nWhen visiting each node, apply a new state. (a.k.a. multiply matrix)\nWhen returning back to its parent, restore its state to the previous state. (a.k.a. revert multiplying matrix)\nNEVER multiply inverse of matrix!!!!!\nInstead use a state stack - push the current state (multiplied matrix), and pop the current state when returning back\nScene Graphs\ne.g. OpenUSD (Open Universal Scene Description)\nWe can also use tree idea to the entire scene!\nInstead of link and joint, use group and entities.\ne.g. car -&gt; engine -&gt; its parts\nUnfortunately, it's usually a directed acyclic graph.\nScene graph and its group is not enforced strictly, it's just for convenience.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Viewing Pipeline","url":"/posts/45/","content":"Viewing Pipeline\n\nModeling Coordinates: Objects are created by their own coordinates.\nWorld Coordinates: The objects are placed in the same world coordinate.\nViewing Coordinates: Objects are now expressed relative to camera.\nNormalized Coordinates: Everything visible to the camera is mapped to unit cube for easy clipping.\nDevice Coordinates: Normalized coordinate (unit-cube) is mapped to the device coordinate.\nRasterization: 2D primitives turns into pixels.\n\nWC to VC (Viewing Transformation)\nIf the position and orientation of camera are given, everything is done.\nqwc=[Rp01]qvc\\mathbf{q}_{wc} = \\begin{bmatrix}\nR &amp; \\mathbf{p} \\\\\n\\mathbf{0} &amp; 1\n\\end{bmatrix}\\mathbf{q}_{vc}qwc​=[R0​p1​]qvc​ where RRR is camera's rotation matrix and p\\mathbf{p}p is camera's position.\n∴qvc=[Rp01]−1qwc=[RT−RTp01]qwc∵qwc=Rqvc+pqwc−p=RqvcRTqwc−RTp=qvc\\begin{align*}\n\\therefore \\mathbf{q}_{vc} = \\begin{bmatrix}\nR &amp; \\mathbf{p} \\\\\n\\mathbf{0} &amp; 1\n\\end{bmatrix}^{-1}\\mathbf{q}_{wc} &amp;= \\begin{bmatrix}\nR^T &amp; -R^T\\mathbf{p} \\\\\n\\mathbf{0} &amp; 1\n\\end{bmatrix}\\mathbf{q}_{wc}\\\\\n\\because  \\mathbf{q}_{wc} &amp;= R\\mathbf{q}_{vc} + \\mathbf{p} \\\\\n\\mathbf{q}_{wc} - \\mathbf{p} &amp;= R\\mathbf{q}_{vc} \\\\\nR^T\\mathbf{q}_{wc} - R^T\\mathbf{p} &amp;= \\mathbf{q}_{vc} \\\\\n\\end{align*}∴qvc​=[R0​p1​]−1qwc​∵qwc​qwc​−pRTqwc​−RTp​=[RT0​−RTp1​]qwc​=Rqvc​+p=Rqvc​=qvc​​\nBut usually we only use a look-at direction or a reference position in the scene.\nThis can reduce number of parameters so we don't have to send the whole rotation matrix.\nzvc=−w∣w∣ or zvc=p−pref∣p−pref∣xvc=vup×zvc∣vup∣yvc=zvc×xvc∴R=[xvcyvczvc]\\begin{align*}\n\\mathbf{z}_{vc} = -\\frac{\\mathbf{w}}{|\\mathbf{w}|} \\text{ or } \\mathbf{z}_{vc} &amp;= \\frac{\\mathbf{p}-\\mathbf{p}_{ref}}{|\\mathbf{p}-\\mathbf{p}_{ref}|} \\\\\n\\mathbf{x}_{vc} &amp;= \\frac{\\mathbf{v}_{up} \\times \\mathbf{z}_{vc}}{|\\mathbf{v}_{up}|} \\\\\n\\mathbf{y}_{vc} &amp;= \\mathbf{z}_{vc} \\times \\mathbf{x}_{vc} \\\\\n\\therefore R &amp; = \\begin{bmatrix}\n\\mathbf{x}_{vc} &amp; \\mathbf{y}_{vc} &amp; \\mathbf{z}_{vc}\n\\end{bmatrix}\n\\end{align*}\nzvc​=−∣w∣w​ or zvc​xvc​yvc​∴R​=∣p−pref​∣p−pref​​=∣vup​∣vup​×zvc​​=zvc​×xvc​=[xvc​​yvc​​zvc​​]​\nvup\\mathbf{v}_{up}vup​ can be any vector other than one parellel to zvc\\mathbf{z}_{vc}zvc​, usually chosen as (0,1,0).\nVC to NC\nMvc,nc=Mnorm⋅MprojM_{vc,nc} = M_{norm} \\cdot M_{proj}\nMvc,nc​=Mnorm​⋅Mproj​\nProjection Transformation\nWe want to project all object positions onto view plane (or projection plane).\nProjection only applies to x, y coordinates, while z (depth) values are kept.\nProjecting from 3D to 2D all at once is not a good idea!\nDepth information will be lost, and computing objects that are far distant is inefficient.\n\nInstead of rendering every object, we introduce view frustrum.\nOnly objects that are inside view frustrum will be rendered, so we have to only care about a finit viewing volume.\nNormalizing Transformation\nAfter projection, view frustrum becomes a cuboid.\nNormalizing transform maps a cuboid to unit cube [−1,1]3[-1, 1]^3[−1,1]3 (a.k.a. canonical view volume).\n\nIt makes clippling easier because you can just discard objects outside the cube.\nIt makes manufacturing easier because all manufacturers can assume the normalized coordinates are given as input.\n\nTo normalize view frustrum, move the center to the origin, and scale height, width, and depth to 2.\nIf the coordinates are given as l,r,t,b,n,fl, r, t, b, n, fl,r,t,b,n,f, (left, right, top, bottom, near, far) normalizing transformation is:\nMnorm=[2r−l00−r+lr−l02t−b0−t+bt−b002n−f−n+fn−f0001]M_{norm} = \\begin{bmatrix}\n\\frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\\frac{r+l}{r-l} \\\\\n0 &amp; \\frac{2}{t-b} &amp; 0 &amp; -\\frac{t+b}{t-b} \\\\\n0 &amp; 0 &amp; \\frac{2}{n-f} &amp; -\\frac{n+f}{n-f} \\\\\n0 &amp; 0 &amp; 0 &amp; 1 \\\\\n\\end{bmatrix}Mnorm​=​r−l2​000​0t−b2​00​00n−f2​0​−r−lr+l​−t−bt+b​−n−fn+f​1​​\nTaxonomy of Geometric Projections\nWe have multiple choices for MprojM_{proj}Mproj​.\n\nPerspective projection\n\nOne-point perspective (Single-point perspective)\nTwo-point perspective\nThree-point perspective\n\n\nParallel projection\n\nOrthographic projection\n\nMultiview projection\nAxonometric projection\n\nTrimetric projection\nDimetric projection\nIsometric projection\n\n\n\n\nOblique projection\n\nCavalier projection\nCabinet projection\n\n\n\n\n\nPerspective Projection\n\nObjects look smaller as they get further away\nParallel lines (that are not parallel to a view plane) converge at the vanishing point on the horizon\n\nNumber of vanishing point can be 1, 2, or 3. (a.k.a. One-point, Two-point, Three-point)\n\n\n\nPinhole Camera Model\n\nA pinhole camera creates an image by controlling only the light that passes through a pinhole.\n\nUnlike pinhole camera, the view plane in our camera model is located in front of the pinhole. (We don't need to make upside down image!)\nPinhole camera model maps coordinate (x,y,z)(x, y, z)(x,y,z) into (−dxz,−dyz,−d)\\left( -d\\frac{x}{z}, -d\\frac{y}{z}, -d \\right)(−dzx​,−dzy​,−d)\nLook carefully! d&gt;0d &gt; 0d&gt;0 and our projection plane is -d!\nThis mapping is not linear, but it can be represented by a matrix using homogeneous coordinate representation!\n[10000100001000−1d0][xyz1]=[xyz−zd]⇒4th coord.divide by[−dxz−dyz−d1] \\begin{bmatrix}\n1 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; -\\frac{1}{d} &amp; 0\n\\end{bmatrix}\\begin{bmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{bmatrix} = \\begin{bmatrix}\nx \\\\ y \\\\ z \\\\ -\\frac{z}{d}\n\\end{bmatrix} \\xRightarrow[\\text{4th coord.}]{\\text{divide by}} \\begin{bmatrix}\n-d\\frac{x}{z} \\\\ -d\\frac{y}{z} \\\\ -d \\\\ 1\n\\end{bmatrix} ​1000​0100​001−d1​​0000​​​xyz1​​=​xyz−dz​​​divide by4th coord.​​−dzx​−dzy​−d1​​\nIf we assume that the projection plane is the same as the near plane, full perspective matrix looks like this:\nMproj=[2dr−l0r+lr−l002dt−bt+bt−b000f+nn−f2dfn−f00−1d0] M_{proj} = \\begin{bmatrix}\n\\frac{2d}{r-l} &amp; 0 &amp; \\frac{r+l}{r-l} &amp; 0 \\\\\n0 &amp; \\frac{2d}{t-b} &amp; \\frac{t+b}{t-b} &amp; 0 \\\\\n0 &amp; 0 &amp; \\frac{f+n}{n-f} &amp; \\frac{2df}{n-f} \\\\\n0 &amp; 0 &amp; -\\frac{1}{d} &amp; 0\n\\end{bmatrix} Mproj​=​r−l2d​000​0t−b2d​00​r−lr+l​t−bt+b​n−ff+n​−d1​​00n−f2df​0​​\nParallel Projection\n\nObjects are projected along the specific direction\nObject's size does not change in the direction parallel to the view plane\nYou can think as the center of proection (pinhole) is at infinity\n\nMultiview Projection\n\n\nPreserves relative size\nThe direction of projection is parallel to a principle axis of objects\nThe projection plane is perpendicular to the direction of projection\n\nAxonometric Projection\n\n\nCan display more than one face of an object\nThe direction of projection isn't parallel to a principle axis of objects\nBut the projection plane is still perpendicular to the direction of projection\n\nOblique Projection\n\n\nThe projection plane is aligned with one face of the object\nBut the direction of projection isn't perpendicular to the projection plane\nTypically we choose 30° or 45°\n\n\nLet the direction of projection is Vp≔(Vpx,Vpy,Vpz)V_p \\coloneqq (V_{px}, V_{py}, V_{pz})Vp​:=(Vpx​,Vpy​,Vpz​), and the view plane is z=zvpz = z_{vp}z=zvp​.\nxp−xzvp−z=VpxVpzxp=x+VpxVpz(zvp−z)=x−VpxVpzz+zvpVpxVpz∴Mproj=[10−VpxVpzzvpVpxVpz01−VpyVpzzvpVpyVpz000zvp0001]\\begin{align*}\n\\frac{x_p-x}{z_{vp}-z} &amp;= \\frac{V_{px}}{V_{pz}} \\\\\nx_p &amp;= x + \\frac{V_{px}}{V_{pz}}(z_{vp}-z) \\\\\n&amp;= x - \\frac{V_{px}}{V_{pz}}z + z_{vp}\\frac{V_{px}}{V_{pz}}\\\\\n\\therefore M_{proj} &amp;= \\begin{bmatrix}\n1 &amp; 0 &amp; -\\frac{V_{px}}{V_{pz}} &amp; z_{vp}\\frac{V_{px}}{V_{pz}} \\\\\n0 &amp; 1 &amp; -\\frac{V_{py}}{V_{pz}} &amp; z_{vp}\\frac{V_{py}}{V_{pz}} \\\\\n0 &amp; 0 &amp; 0 &amp; z_{vp} \\\\\n0 &amp; 0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\end{align*}zvp​−zxp​−x​xp​∴Mproj​​=Vpz​Vpx​​=x+Vpz​Vpx​​(zvp​−z)=x−Vpz​Vpx​​z+zvp​Vpz​Vpx​​=​1000​0100​−Vpz​Vpx​​−Vpz​Vpy​​00​zvp​Vpz​Vpx​​zvp​Vpz​Vpy​​zvp​1​​​\n\nCavalier projection: tan⁡α=1\\tan \\alpha = 1tanα=1\nCabinet projection: tan⁡α=2\\tan \\alpha = 2tanα=2\n\nNC to DC (Screen Transformation)\nNow we need to transform unit cube into W x H image (screen).\nOpenGL has lower-left origin, while Vulkan and Direct3D has upper-left origin with flipped y-aixs.\nTrackball\nA trackball translates 2D mouse movements into 3D rotations.\nThis is done by projecting the position of the mouse on to an imaginary sphere behind the viewport.\nTo implement this, the camera rotates around an axis perpendicular to the mouse movement.\nvaxis=vold×vnewθ=arccos⁡(vold∣vold∣⋅vnew∣vnew∣)\\begin{align*}\nv_{axis} &amp;= v_{old} \\times v_{new} \\\\\n\\theta &amp;= \\arccos \\left( \\frac{v_{old}}{|v_{old}|} \\cdot \\frac{v_{new}}{|v_{new}|} \\right)\n\\end{align*}vaxis​θ​=vold​×vnew​=arccos(∣vold​∣vold​​⋅∣vnew​∣vnew​​)​\nBut... We don't actually use arccos.\n\narccos doesn't know direction\narccos isn't used in computing because of stability (absolute value of input might be over 1)\n\nSolution: use sin and cos values!\n\nUse atan2: θ=atan2⁡(∣A×B∣,A⋅B)\\theta = \\operatorname{atan2} (|A \\times B|, A \\cdot B)θ=atan2(∣A×B∣,A⋅B)\n\natan2's input is not limited, and it has better numerical stability - e.g. can avoid division by zero\n\n\nConstruct rotation matrix directly from sin and cos values\n\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Rotation and Orientation","url":"/posts/61/","content":"What is Rotation?\nRotation is very nonintuitive;\nIn geometry, rotation is a movement that keeps at least one point fixed.\nBut in physics, rotation is extremely complex!\nIntermediate axis theorem: Rotation around major and minor axis is stable, but rotation around intermediate axis is unstable!\nThe rotation axis keeps chaning, and the object can even be flipped!\nOrientation vs. Rotation\nOrientation is the state of being oriented.\nGiven a coordinate system, the orientation of an object can be represented as a rotation relative to a reference pose (similar to origin)\nRotation is a circular movement.\n2D Orientation\nJust use angle, right?\nProblem: Object has same orientation if it makes a full turn.\ne.g. What is the middle orientation between π2\\frac{\\pi}{2}2π​ and 5π2\\frac{5\\pi}{2}25π​?\nWe can't just average the numbers!\nSolution: Add extra parameters and constraints.\nWe represent 2D rotation as a complex number a+bia + bia+bi, but it's constrained to the unit circle a2+b2=1a^2 + b^2 = 1a2+b2=1.\nMultiplying rotation is equal to a composition of rotation!!\nIn fact, it's actually eiθ=cos⁡θ+isin⁡θe^{i\\theta} = \\cos\\theta + i \\sin\\thetaeiθ=cosθ+isinθ.\n2D Rotation\nWe can't use complex number for rotation;\nWe have to distinguish between 1 full turn and 2 full turns.\nWe'll just use angle to represent 2D rotations. (as we've been doing)\nUnlike point and vectors, we have to choose different representation for rotation and orientation.\n3D Rotation\nEuler's rotation theorem: The general displacement of a rigid body with one point fixed is a rotation about some axis.\ni.e. any 3D rotation can be represented as a one rotation around an axis.\n\nAxis-angle: Use 4 parameters. (θ,x,y,z)(\\theta, x, y, z)(θ,x,y,z)\nRotation vector: Use only 3 parameters! v=θv^\\mathbf{v} = \\theta\\hat{\\mathbf{v}}v=θv^\nVector's direction represents axis, and vector's magnitude represents angle.\n\nRotation vector is better!\n3D Orientation\nEuler parameters\nEuler angle has gimbal lock problem.\nWe can use 4 parameters to solve gimbal lock problem,\neo=cos⁡θ2,(e1,e2,e3)=v^sin⁡θ2e_o = \\cos\\frac{\\theta}{2}, (e_1, e_2, e_3) = \\hat{\\mathbf{v}}\\sin\\frac{\\theta}{2}\neo​=cos2θ​,(e1​,e2​,e3​)=v^sin2θ​\nwhere θ\\thetaθ is rotation angle and v^\\hat{\\mathbf{v}}v^ is rotation axis.\nBut how do we use this?\nQuaternions\nw+ix+jy+kzw + ix + jy + kzw+ix+jy+kz where i2=j2=k2=ijk=−1,ij=k,jk=i,ki=j,ji=−k,kj=−i,ik=−ji^2 = j^2 = k^2 = ijk = -1, ij = k, jk = i, ki = j, ji = -k, kj = -i, ik = -ji2=j2=k2=ijk=−1,ij=k,jk=i,ki=j,ji=−k,kj=−i,ik=−j\nc.f. Trinion(?) can't exist! It's proven that you can't define multiplication in tirion.\nWe can represent quaternions as a scalar and a vector!\nq=w+xi+yj+zk=(w,x,y,z)=(w,v)\\begin{align*}\n\\mathbf{q} &amp;= w + xi + yj + zk \\\\\n&amp;= (w, x, y, z) \\\\\n&amp;= (w, \\mathbf{v})\n\\end{align*}q​=w+xi+yj+zk=(w,x,y,z)=(w,v)​\nRotation as Unit Quaternion\nWe use unit quaternions, i.e. w2+x2+y2+z2=1w^2+x^2+y^2+z^2 = 1w2+x2+y2+z2=1.\nq1q2=(a,u)(b,v)=(ab−u⋅v,av+bu+u×v)\\begin{align*}\n\\mathbf{q}_1 \\mathbf{q}_2 &amp;= (a, \\mathbf{u}) (b, \\mathbf{v}) \\\\\n&amp;= (ab - \\mathbf{u}\\cdot\\mathbf{v}, a\\mathbf{v} + b\\mathbf{u} + \\mathbf{u} \\times \\mathbf{v})\n\\end{align*}q1​q2​​=(a,u)(b,v)=(ab−u⋅v,av+bu+u×v)​\n(w,x,y,z)−1=(w,−x,−y,−z)=(−w,x,y,z)(w, x, y, z)^{-1} = (w, -x, -y, -z) = (-w, x, y, z) \n(w,x,y,z)−1=(w,−x,−y,−z)=(−w,x,y,z)\nWe use euler parameters as a quaternion. Rotating θ\\thetaθ along axis v^\\hat{\\mathbf{v}}v^ is represented as (cos⁡θ2,v^sin⁡θ2)(\\cos\\frac{\\theta}{2}, \\hat{\\mathbf{v}}\\sin\\frac{\\theta}{2})(cos2θ​,v^sin2θ​).\nWe represent 3D vectors (x,y,z)(x, y, z)(x,y,z) as a pure imaginary Quaternions (0,x,y,z)(0, x, y, z)(0,x,y,z).\nThen rotating vector x\\mathbf{x}x about rotation q\\mathbf{q}q is represented as x′=qxq−1\\mathbf{x}&#x27; = \\mathbf{q}\\mathbf{x}\\mathbf{q}^{-1}x′=qxq−1.\nAntipodal Equivalence\nq\\mathbf{q}q and −q-\\mathbf{q}−q represents same rotation.\nMathmatically saying, this is a 2-to-1 mapping between S3S^3S3 (surface of unit sphere in 4D space) and SO(3).\nThat's why we have to use θ2\\frac{\\theta}{2}2θ​ instead of θ\\thetaθ!\nWhy not use rotation matrix?\nEvery representation have a conversion between two.\nBut it's challenging due to differences in conventions.\nUnit quaternion is the most efficient method!\nIt's very similar to rotation matrix, and has fewer paramers.\nAlso, normalizing matrix is very hard (Gram-Schmidt Process), while normalizing quaternion is easy.\ne.g. Quaternion to Rotation matrix\nR=[q02+qx2−qy2−qz22qxqy−2q0qz2qxqz+2q0qy02qxqy+2q0qzq02−qx2+qy2−qz22qyqz−2q0qx02qxqz−2q0qy2qyqz+2q0qxq02−qx2−qy2+qz200001]R = \\begin{bmatrix}\nq_0^2 + q_x^2 - q_y^2 - q_z^2 &amp; 2q_xq_y - 2q_0q_z &amp; 2q_xq_z + 2q_0q_y &amp; 0 \\\\\n2q_xq_y + 2q_0q_z &amp; q_0^2 - q_x^2 + q_y^2 - q_z^2 &amp; 2q_yq_z - 2q_0q_x &amp; 0 \\\\\n2q_xq_z - 2q_0q_y &amp; 2q_yq_z + 2q_0q_x &amp; q_0^2 - q_x^2 - q_y^2 + q_z^2 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 1\n\\end{bmatrix}R=​q02​+qx2​−qy2​−qz2​2qx​qy​+2q0​qz​2qx​qz​−2q0​qy​0​2qx​qy​−2q0​qz​q02​−qx2​+qy2​−qz2​2qy​qz​+2q0​qx​0​2qx​qz​+2q0​qy​2qy​qz​−2q0​qx​q02​−qx2​−qy2​+qz2​0​0001​​\nTangent Vector\nTangent space of a point q\\mathbf{q}q on S3S^3S3 is three-dimensional space of quaternions, TqS3T_{\\mathbf{q}} S^3Tq​S3.\nIf a point q\\mathbf{q}q is given, we apply q−1\\mathbf{q}^{-1}q−1 and use tangent space TIS3T_I S^3TI​S3. (I=(1,0,0,0)I = (1,0,0,0)I=(1,0,0,0))\nEvery vector in TIS3T_I S^3TI​S3 is purely imaginary quaternion - i.e. real part should be 0 such as (0, x, y, z).\nWe call tangent vector q˙\\dot{\\mathbf{q}}q˙​ is a vector from I to the projection of q\\mathbf{q}q onto TIS3T_I S^3TI​S3.\nq˙\\dot{\\mathbf{q}}q˙​ isn't related to velocity, but it is related to angular velocity. ω=2q−1q˙\\omega = 2\\mathbf{q}^{-1}\\dot{\\mathbf{q}}ω=2q−1q˙​\nExp and Log\nWe define log as a map from q\\mathbf{q}q to q˙\\dot{\\mathbf{q}}q˙​, and exp as a map from to q˙\\dot{\\mathbf{q}}q˙​ to q\\mathbf{q}q.\ni.e. log is a map from S3S^3S3 to TIS3T_I S^3TI​S3, following the shortest path on the sphere, a.k.a. the great arc.\nExp maps 3D vector (in form of purely imaginary quaternion) to quaternion.\ni.e. it maps 3D rotation vector to a corresponding rotation quaternion.\nexp⁡(v)=exp⁡(θv^)≔(cos⁡θ,v^sin⁡θ)\\exp(\\mathbf{v}) = \\exp(\\theta\\hat{\\mathbf{v}}) \\coloneqq (\\cos\\theta, \\hat{\\mathbf{v}}\\sin\\theta)\nexp(v)=exp(θv^):=(cosθ,v^sinθ)\nNote that rotation quaternion of rotation vector θv^\\theta\\hat{\\mathbf{v}}θv^ is exp⁡(θ2v^)\\exp(\\frac{\\theta}{2}\\hat{\\mathbf{v}})exp(2θ​v^)!!!\nRotation Vector\nLet's say if you rotate orientation q1\\mathbf{q}_1q1​ by the rotation vector v\\mathbf{v}v, you get orientaiton q2\\mathbf{q}_2q2​. i.e. q2=q1exp⁡(v)\\mathbf{q}_2 = \\mathbf{q}_1\\exp(\\mathbf{v})q2​=q1​exp(v).\nIf we apply q1−1\\mathbf{q}_1^{-1}q1−1​, rotation vector v\\mathbf{v}v is a rotation from III to q1−1q2\\mathbf{q}_1^{-1}\\mathbf{q}_2q1−1​q2​.\nTherefore, v\\mathbf{v}v is a projection of q1−1q2\\mathbf{q}_1^{-1}\\mathbf{q}_2q1−1​q2​, i.e. v=log⁡(q1−1q2)\\mathbf{v} = \\log(\\mathbf{q}_1^{-1}\\mathbf{q}_2)v=log(q1−1​q2​)\nNow we get rotation quaternion from q1\\mathbf{q}_1q1​ to q2\\mathbf{q}_2q2​ is exp⁡(log⁡(q1−1q2))\\exp(\\log(\\mathbf{q}_1^{-1}\\mathbf{q}_2))exp(log(q1−1​q2​)).\ni.e. q2=q1exp⁡(log⁡(q1−1q2))\\mathbf{q}_2 = \\mathbf{q}_1\\exp(\\log(\\mathbf{q}_1^{-1}\\mathbf{q}_2))q2​=q1​exp(log(q1−1​q2​)).\nFinite rotation\ne.g. Angular displacement\nWe can't just add rotation vectors in 3D!\neuev≠eu+v≠eveue^u e^v \\neq e^{u+v} \\neq e^v e^u\neuev=eu+v=eveu\nInfinitesimal rotation\ne.g. Instantaneous angular velocity\nAddition of angular velocity is possible.\nSpherical Linear Interpolation (SLERP)\nLinear interpolation of two orientations!\nWe interpolate great arc between two orientations.\nRegarding radians, we can think of this as an interpolation of angle.\n\nslerp⁡t(q1,q2)=q1exp⁡(t⋅log⁡(q1−1q2))=q1(q1−1q2)t\\begin{align*}\n\\operatorname{slerp}_t(q_1, q_2) &amp;= q_1\\exp(t \\cdot \\log (q_1^{-1}q_2)) \\\\\n&amp;= q_1(q_1^{-1}q_2)^t\n\\end{align*}slerpt​(q1​,q2​)​=q1​exp(t⋅log(q1−1​q2​))=q1​(q1−1​q2​)t​\nUnlike interpolating with Euler angles or rotation vectors, slerp interpolates along the shortest route (i.e. straight line path).\nCoordinate-Invariant Operations\n\n\norientation is quaternion representing orientation.\nexp of rotation vector is quaternion representing rotation.\nq1−1q2q_1^{-1}q_2q1−1​q2​ is quaternion representing rotation.\npower of quaternion representing rotation is well defined. (q1−1q2)t=exp⁡(t⋅log⁡(q1−1q2))(q_1^{-1}q_2)^t = \\exp(t \\cdot \\log (q_1^{-1}q_2))(q1−1​q2​)t=exp(t⋅log(q1−1​q2​))\npower of quaternion representing orientation is only defined when power is 0 or 1.\n\nAdding rotation vectors\nQuaternion multiplication is non‑commutative.\neveu≠eueve^ve^u \\neq e^ue^v\neveu=euev\nIf we divide rotation into half, it's still non-commutative, but the difference has decreased.\neu2ev2eu2ev2≠ev2eu2ev2eu2e^{\\frac{u}{2}}e^{\\frac{v}{2}}e^{\\frac{u}{2}}e^{\\frac{v}{2}} \\neq e^{\\frac{v}{2}}e^{\\frac{u}{2}}e^{\\frac{v}{2}}e^{\\frac{u}{2}}\ne2u​e2v​e2u​e2v​=e2v​e2u​e2v​e2u​\nIf we divide rotation infinitely, we can swap the rotations!\nWe define this as the addition of the rotation vectors.\neu+v≔lim⁡n→∞(eunevn)n=lim⁡n→∞(evneun)ne^{u+v} \\coloneqq \\lim_{n\\to\\infty} (e^{\\frac{u}{n}}e^{\\frac{v}{n}})^n = \\lim_{n\\to\\infty} (e^{\\frac{v}{n}}e^{\\frac{u}{n}})^n\neu+v:=n→∞lim​(enu​env​)n=n→∞lim​(env​enu​)n\nAffine combination of orientations\nThere are some definitions about affine combination of orientations.\nBut it's very complex, and each definitions results different orientations.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Introduction to Geometry","url":"/posts/56/","content":"Geometry in graphics\nScene is an assembly of one or more objects.\nAn object contains transformation, geometry, material, and lighting.\nImplicit representations\nPros:\n\nDescription can be very compact\nEasy to determine if a point is inside or outside\nOther queries may also be easy (e.g. distance to surface)\nSimple shapes have exact description and no sampling error\n\nCons:\n\nVery hard to find all points in the shape\nVery difficult to model complex shapes\n\nAlgebraic surfaces\nSurface is zero set of a polynomial in x, y, z.\ni.e. f(x,y,z)=0f(x, y, z) = 0f(x,y,z)=0\nIt's impossible to draw complicated shapes...\ne.g. how do we draw cow?\nConstructive Solid Geometry\nBuild complicated shapes via boolean operations of basic primitives.\nHow do we do this? Boolean operations of geomery is extremely hard...\nSolution: draw a line from a given point, then do boolean operations of segments!\nBlobby surfaces\nBoolean operations makes surface too sharp...\nInstead blend surfaces together!\ne.g. in 2D, f(x)=e−∣x−p∣2+e−∣x−q∣2f(x) = e^{-|x-p|^2} + e^{-|x-q|^2}f(x)=e−∣x−p∣2+e−∣x−q∣2\nUsed in fluid, human body, etc.\nLevel Set Methods\nWe don't know the shape, but we know the distance to the surface from each pixel in the grid.\nFractals\nObviously implicit...\nUsed for describing natural phenomena - e.g. tree, leaf, flower.\nc.f. Fractal dimension: Some fractal has infinite perimeter - these are not 1D! maybe 1.67D...?\nc.f. Space-filling curve: 1D and 2D has one-to-one mapping!\nMandelbrot set\nFor each point in the plane:\n\nDouble the angle\nSquare the magnitude\nAdd the original point\nRepeat\n\nIf magnitude remains bounded, it's in the Mandelbrot set.\nExplicit Representations\nParametric space\nf:R2→R3;(u,v)→(x,y,z)f: R^2 \\rightarrow R^3; (u, v) \\rightarrow (x, y, z) \nf:R2→R3;(u,v)→(x,y,z)\ne.g. for 0≤u≤2π,0≤v≤π0 \\leq u \\leq 2\\pi, 0 \\leq v \\leq \\pi0≤u≤2π,0≤v≤π, points on sphere are (cos⁡usin⁡v,sin⁡usin⁡v,cos⁡v)(\\cos u \\sin v, \\sin u \\sin v, \\cos v)(cosusinv,sinusinv,cosv)\nPoint Cloud\nList of points (x, y, z)\nCan represent any kind of geometry, but hard to modify (e.g. processing, simulation, ...)\nPolygon Mesh\nStore vertices and polygons\nEasier to modify, but need more complicated data structures to verify polygon mesh.\nPolygon mesh can be verified by v-e+f = 2\nTriangle Mesh\nNormally we use only triangles.\nQuads are not defined when vertices are skewed, triangles are always well defined!\nNormal vector is also well defined!\nBarycentric interpolation can define points inside triangles.\nManifold\nEarth is sphere, but we use 2D map at the surface.\nSimiliary, we can make 3D surface into 2D grid.\nHowever, not every shape is manifold.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Splines","url":"/posts/62/","content":"Motivation\nImplementing curves and surfaces with lots of points is inefficient.\nWe want higher-level representation of curves and surfaces.\nSpecifically, we would like to find a smooth curve/surface with a minimal number of control points.\nParametric Geometry\np(t)=(x(t),y(t))p(t) = (x(t), y(t))\np(t)=(x(t),y(t))\np(u,v)=(x(u,v),y(u,v),z(u,v))p(u, v) = (x(u, v), y(u, v), z(u, v))\np(u,v)=(x(u,v),y(u,v),z(u,v))\ne.g. parameterized line passing through (t0,p0),(t1,p1)(t_0, p_0), (t_1, p_1)(t0​,p0​),(t1​,p1​) is\np(t)=t1−tt1−t0p0+t−t0t1−t0p1p(t) = \\frac{t_1-t}{t_1-t_0}p_0 + \\frac{t-t_0}{t_1-t_0}p_1\np(t)=t1​−t0​t1​−t​p0​+t1​−t0​t−t0​​p1​\nWhy parameteric?\nIt matches the intrinsic dimension of objects we want to manipulate.\nCurves are 1D, surfaces are 2D.\nIt may exist in higher dimensions, but it doesn't matter!\nAlso, once a curve/surface is parameterized, sampling process is easy, so our graphics hardware can render easily.\nWe can sample like P(0),P(0.1),P(0.2),…P(0), P(0.1), P(0.2), \\ldotsP(0),P(0.1),P(0.2),…\nDifferential Properties of Parameteric Curves\nVelocity\nInstantaneous positional change: P′(t)P&#x27;(t)P′(t)\nTangent\nNormalized velocity vector: T(t)≔P′(t)∥P′(t)∥T(t) \\coloneqq \\frac{P&#x27;(t)}{\\|{P&#x27;(t)}\\|}T(t):=∥P′(t)∥P′(t)​\nCurvature\nInstantaneous tangential change: K(T)≔T′(t)K(T) \\coloneqq T&#x27;(t)K(T):=T′(t)\nCurvature is always orthogonal to tangent, K(t)⋅T(t)=0K(t) \\cdot T(t) = 0K(t)⋅T(t)=0\nIn geometry, 1∥K(t)∥\\frac{1}{\\|K(t)\\|}∥K(t)∥1​ is same as the radius of the circle that touches P(t)P(t)P(t) at ttt.\nCurve Normal\nNormalized curvature vector: N(t)≔K(t)∥K(t)∥N(t) \\coloneqq \\frac{K(t)}{\\|{K(t)}\\|}N(t):=∥K(t)∥K(t)​\nIn 3D, there is also a binormal vector: B(t)≔T(t)×N(t)B(t) \\coloneqq T(t) \\times N(t)B(t):=T(t)×N(t)\nTangent vector, Curve normal vector, Binormal vector form a coordinate system called Frenet frame.\nHowever, it is not well defined (curvature can be 0).\n1D Interpolation\nIf n+1 points (0,x0),(t1,x1),…(tn−1,xn−1),(1,xn)(0, x_0), (t_1, x_1), \\ldots (t_{n-1}, x_{n-1}), (1, x_n)(0,x0​),(t1​,x1​),…(tn−1​,xn−1​),(1,xn​) are given, degree is n, and order is n + 1.\nx(t)=hntn+⋯h1t+h0x(t) = h_nt^n + \\cdots h_1t + h_0\nx(t)=hn​tn+⋯h1​t+h0​\nHow do we solve this linear system???\nLagrange Polynomial\nLk(t)≔(t−t0)⋯(t−tk−1)(t−tk+1⋯(t−tn)(tk−t0)⋯(tk−tk−1)(tk−tk+1⋯(tk−tn)L_k(t) \\coloneqq \\frac{(t-t_0)\\cdots(t-t_{k-1})(t-t_{k+1} \\cdots (t-t_n)}{(t_k-t_0)\\cdots(t_k-t_{k-1})(t_k-t_{k+1} \\cdots (t_k-t_n)}\nLk​(t):=(tk​−t0​)⋯(tk​−tk−1​)(tk​−tk+1​⋯(tk​−tn​)(t−t0​)⋯(t−tk−1​)(t−tk+1​⋯(t−tn​)​\nThen Lk(t)L_k(t)Lk​(t) is 1 if k=ik=ik=i, or it is 0 otherwise.\n∴x(t)=L0(t)x0+L1(t)x1+⋯Ln(t)xn\\therefore x(t) = L_0(t)x_0 + L_1(t)x_1 + \\cdots L_n(t)x_n\n∴x(t)=L0​(t)x0​+L1​(t)x1​+⋯Ln​(t)xn​\nCubic Hermite Interpolation\nNew idea: only two control points and their derivatives are given, intermediate parts are smoothly connected.\nAssume that x0,x0′,x1,x1′x_0, x&#x27;_0, x_1, x&#x27;_1x0​,x0′​,x1​,x1′​ is given.\nx(t)=at3+bt2+ct+dx′(t)=3at2+2bt+c∴x0=d,x1=a+b+c+d,x0′=c,x1′=3a+2b+ci.e.[x0x1x0′x1′]=[0001111100103210][abcd]∴[abcd]=[2−211−33−2−100101000][x0x1x0′x1′]\\begin{align*}\nx(t) &amp;= at^3 + bt^2 + ct + d \\\\\nx&#x27;(t) &amp;= 3at^2 + 2bt + c \\\\\n\\therefore x_0 &amp;= d, \\\\\nx_1 &amp;= a + b + c + d, \\\\\nx&#x27;_0 &amp;= c, \\\\\nx&#x27;_1 &amp;= 3a + 2b + c \\\\\ni.e. \\begin{bmatrix}\nx_0 \\\\\nx_1 \\\\\nx&#x27;_0 \\\\\nx&#x27;_1\n\\end{bmatrix} &amp;= \\begin{bmatrix}\n0 &amp; 0 &amp; 0 &amp; 1 \\\\\n1 &amp; 1 &amp; 1 &amp; 1 \\\\\n0 &amp; 0 &amp; 1 &amp; 0 \\\\\n3 &amp; 2 &amp; 1 &amp; 0\n\\end{bmatrix}\\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\nd\n\\end{bmatrix} \\\\\n\\therefore \\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\nd\n\\end{bmatrix} &amp;= \\begin{bmatrix}\n2 &amp; -2 &amp; 1 &amp; 1 \\\\\n-3 &amp; 3 &amp; -2 &amp; -1 \\\\\n0 &amp; 0 &amp; 1 &amp; 0 \\\\\n1 &amp; 0 &amp; 0 &amp; 0\n\\end{bmatrix}\\begin{bmatrix}\nx_0 \\\\\nx_1 \\\\\nx&#x27;_0 \\\\\nx&#x27;_1\n\\end{bmatrix}\n\\end{align*}x(t)x′(t)∴x0​x1​x0′​x1′​i.e.​x0​x1​x0′​x1′​​​∴​abcd​​​=at3+bt2+ct+d=3at2+2bt+c=d,=a+b+c+d,=c,=3a+2b+c=​0103​0102​0111​1100​​​abcd​​=​2−301​−2300​1−210​1−100​​​x0​x1​x0′​x1′​​​​\nHermite bases\nWe can think cubic polynomial as a 4D vector (a,b,c,d)(a, b, c, d)(a,b,c,d) of which bases are t3,t2,t,1t^3, t^2, t, 1t3,t2,t,1.\nWe can change bases to use independent bases instead of monomial bases.\nx(t)=[x0x1x0′x1′][10−32003−201−2100−11][1tt2t3]=x0(2t3−3t2+1)+x1(−2t3+3t2)+x0′(t3−2t2+t)+x1′(t3−t2)=x0H0(t)+x1H1(t)+x0′H2(t)+x1′H3(t)\\begin{align*}\nx(t) =&amp; \\begin{bmatrix}\nx_0 &amp; x_1 &amp; x&#x27;_0 &amp; x&#x27;_1\n\\end{bmatrix}\\begin{bmatrix}\n1 &amp; 0 &amp; -3 &amp; 2 \\\\\n0 &amp; 0 &amp; 3 &amp; -2 \\\\\n0 &amp; 1 &amp; -2 &amp; 1 \\\\\n0 &amp; 0 &amp; -1 &amp; 1\n\\end{bmatrix}\\begin{bmatrix}\n1 \\\\\nt \\\\\nt^2 \\\\\nt^3\n\\end{bmatrix} \\\\\n=&amp; x_0(2t^3 - 3t^2 + 1) + x_1(-2t^3 + 3t^2) \\\\\n&amp;+ x&#x27;_0(t^3 - 2t^2 + t) + x&#x27;_1(t^3 - t^2) \\\\\n=&amp; x_0H_0(t) + x_1H_1(t) + x&#x27;_0H_2(t) + x&#x27;_1H_3(t)\n\\end{align*}x(t)===​[x0​​x1​​x0′​​x1′​​]​1000​0010​−33−2−1​2−211​​​1tt2t3​​x0​(2t3−3t2+1)+x1​(−2t3+3t2)+x0′​(t3−2t2+t)+x1′​(t3−t2)x0​H0​(t)+x1​H1​(t)+x0′​H2​(t)+x1′​H3​(t)​\nThese four bases are called Hermite bases.\nCubic Bezeier Curve (Cubic Bernstein Polynomial)\nInstead of two control points and its derivatives, we use four control points.\nBernstein Bases are used, Bi(t)=(3i)(1−t)3−itiB_i(t) = \\binom{3}{i} (1 - t)^{3 - i} t^iBi​(t)=(i3​)(1−t)3−iti.\nPartition of Unity is hold, sum of bases is 1. ∑i=03Bi(t)=1\\sum_{i=0}^3 B_i(t) = 1∑i=03​Bi​(t)=1\nx(t)=x0B0(t)+x1B1(t)+x2B2(t)+x3B3(t)=x0⋅(1−t)3+x1⋅3t(1−t)2+x2⋅3t2(1−t)+x3⋅t3=[x0x1x2x3][1−33−103−63003−30001][1tt2t3]\\begin{align*}\nx(t) &amp;= x_0B_0(t) + x_1B_1(t) + x_2B_2(t) + x_3B_3(t) \\\\\n&amp;= x_0 \\cdot (1-t)^3 + x_1 \\cdot 3t(1-t)^2 + x_2 \\cdot 3t^2(1-t) + x_3 \\cdot t^3\\\\\n&amp;= \\begin{bmatrix}\nx_0 &amp; x_1 &amp; x_2 &amp; x_3\n\\end{bmatrix}\\begin{bmatrix}\n1 &amp; -3 &amp; 3 &amp; -1\\\\\n0 &amp; 3 &amp; -6 &amp; 3\\\\\n0 &amp; 0 &amp; 3 &amp; -3\\\\\n0 &amp; 0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\nt \\\\\nt^2 \\\\\nt^3\n\\end{bmatrix}\n\\end{align*}x(t)​=x0​B0​(t)+x1​B1​(t)+x2​B2​(t)+x3​B3​(t)=x0​⋅(1−t)3+x1​⋅3t(1−t)2+x2​⋅3t2(1−t)+x3​⋅t3=[x0​​x1​​x2​​x3​​]​1000​−3300​3−630​−13−31​​​1tt2t3​​​\nProperties of Cubic Bezier Curve\n\nEnd-point interpolation. x(0)=x0,x(1)=x3x(0) = x_0, x(1) = x_3x(0)=x0​,x(1)=x3​\nDeritvatives are 3 times difference between control points. x′(0)=3(x1−x0),x′(1)=3(x3−x2)x&#x27;(0) = 3(x_1 - x_0), x&#x27;(1) = 3(x_3 - x_2)x′(0)=3(x1​−x0​),x′(1)=3(x3​−x2​)\nThe curve is contained in the convex hull of the control polygon.\nInvariance under affine transformation. i.e. Applying affine transformation to the curve is the same as applying affine transformation to the control points then evaluating the curve.\n\nDe Casteljau Algorithm\nx(t) can be evaluated by recursively interpolating control points with the ratio of (t, 1-t).\nx0,x1,x2,x3x_0, x_1, x_2, x_3\nx0​,x1​,x2​,x3​\nx0(1)(t)=(1−t)x0+tx1x1(1)(t)=(1−t)x1+tx2x2(1)(t)=(1−t)x2+tx3\\begin{align*}\nx_0^{(1)}(t) &amp;= (1 - t) x_0 + t x_1 \\\\\nx_1^{(1)}(t) &amp;= (1 - t) x_1 + t x_2 \\\\\nx_2^{(1)}(t) &amp;= (1 - t) x_2 + t x_3\n\\end{align*}x0(1)​(t)x1(1)​(t)x2(1)​(t)​=(1−t)x0​+tx1​=(1−t)x1​+tx2​=(1−t)x2​+tx3​​\nx0(2)(t)=(1−t)x0(1)(t)+tx1(1)(t)x1(2)(t)=(1−t)x1(1)(t)+tx2(1)(t)\\begin{align*}\nx_0^{(2)}(t) &amp;= (1 - t) x_0^{(1)}(t) + t x_1^{(1)}(t) \\\\\nx_1^{(2)}(t) &amp;= (1 - t) x_1^{(1)}(t) + t x_2^{(1)}(t)\n\\end{align*}x0(2)​(t)x1(2)​(t)​=(1−t)x0(1)​(t)+tx1(1)​(t)=(1−t)x1(1)​(t)+tx2(1)​(t)​\nx0(3)(t)=(1−t)x0(2)(t)+tx1(2)(t)=x(t)x_0^{(3)}(t) = (1 - t) x_0^{(2)}(t) + t x_1^{(2)}(t) = x(t)\nx0(3)​(t)=(1−t)x0(2)​(t)+tx1(2)​(t)=x(t)\n\nAfter De Casteljau algorithm, bezier curve can be divided into two parts with new control polygons!\nx0,x0(1)(t),x0(2)(t),x(t)x_0, x_0^{(1)}(t), x_0^{(2)}(t), x(t)x0​,x0(1)​(t),x0(2)​(t),x(t) and x(t),x1(2)(t),x2(1)(t),x3x(t), x_1^{(2)}(t), x_2^{(1)}(t), x_3x(t),x1(2)​(t),x2(1)​(t),x3​ forms two control polygons.\n2D, 3D Interpolation\nCurves in higher dimensions can be constructed by simply expanding coordinates.\nP(t)=(x(t),y(t),z(t)),P′(t)=(x′(t),y′(t),z′(t))P(t) = (x(t), y(t), z(t)), P&#x27;(t) = (x&#x27;(t), y&#x27;(t), z&#x27;(t))\nP(t)=(x(t),y(t),z(t)),P′(t)=(x′(t),y′(t),z′(t))\ne.g. 2D Bezier Curve is constructed from four control points:\nP(t)=[x0x1x2x3y0y1y2y3][1−33−103−63003−30001][1tt2t3]=[p0p1p2p3][1−33−103−63003−30001][1tt2t3]\\begin{align*}\nP(t) &amp;= \\begin{bmatrix}\nx_0 &amp; x_1 &amp; x_2 &amp; x_3 \\\\\ny_0 &amp; y_1 &amp; y_2 &amp; y_3\n\\end{bmatrix}\\begin{bmatrix}\n1 &amp; -3 &amp; 3 &amp; -1\\\\\n0 &amp; 3 &amp; -6 &amp; 3\\\\\n0 &amp; 0 &amp; 3 &amp; -3\\\\\n0 &amp; 0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\nt \\\\\nt^2 \\\\\nt^3\n\\end{bmatrix} \\\\\n&amp;= \\begin{bmatrix}\n\\mathbf{p}_0 &amp; \\mathbf{p}_1 &amp; \\mathbf{p}_2 &amp; \\mathbf{p}_3\n\\end{bmatrix}\\begin{bmatrix}\n1 &amp; -3 &amp; 3 &amp; -1\\\\\n0 &amp; 3 &amp; -6 &amp; 3\\\\\n0 &amp; 0 &amp; 3 &amp; -3\\\\\n0 &amp; 0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\nt \\\\\nt^2 \\\\\nt^3\n\\end{bmatrix}\n\\end{align*}P(t)​=[x0​y0​​x1​y1​​x2​y2​​x3​y3​​]​1000​−3300​3−630​−13−31​​​1tt2t3​​=[p0​​p1​​p2​​p3​​]​1000​−3300​3−630​−13−31​​​1tt2t3​​​\nSpline\nA piecewise polynomial with a high degree of smoothness where pieces meet.\nUse multiple curves to make complex curve!\ne.g. TrueType fonts use quadratic bezier curve, and PostScript fonts use cubic bezier curve.\nContinuity\nTo ensure a smooth transition from one section of a piecewise parametric spline to the next, we can impose various continuity conditions at the connection points.\n\nParametric continuity: Matching the parametric derivatives of adjoining curve sections at their common boundary\nGeometric continuity: Geometric smoothness independent of parametrization (i.e. parametrization can be different between curves!)\n\nOrders of Continuity\n\nC0C^0C0, G0G^0G0 continuity (positional continuity): The positions of common control points are the same\nG1G^1G1 continuity (tangency continuity): C0C^0C0 and the directions of derivates are the same\nC1C^1C1 continuity: C0C^0C0 and the derivatives are the same\nG2G^2G2 continuity (curvature continuity): G1G^1G1 and the second derivatives (or curvature) are the same\nC2C^2C2 continuity: C1C^1C1 and the second derivatives (or curvature) are the same\n\nG2G^2G2, C2C^2C2 continuity is extremely hard...\ne.g. When connecting cubic bezier curves,\n\nC0C^0C0 continuity is guaranteed if points are same.\nG1G^1G1 continuity is guaranteed if three control points are on a straight line.\nC1C^1C1 continuity is guaranteed if three control points are on a straight line, and the distance is the same. (recall: bezier curve's derivative is 3 times difference between control points.)\n\nc.f. Fonts someimtes need to be angled.\nSolution: Overlap control points!\nBy overlapping two control points, we lose C1C^1C1 continuity, but we get angled curve.\nBezier Splines with Tangent Conditions\n\nBezier curve is tedious - we need to position every control points carefully.\nInstead, just give points and tangent vectors of each points.\nThen we can poisition control points by moving 1/3 of the tangent vector.\nCatmull-Rom Splines\nGiving tangent vectors is tedious - just give points and calculate tangent vectors!\nFirst and last tangents are set to be zero, and other tangents are set to difference between adjacent points.\np0′=pn′=0,pi′=pi+1−pi−12\\mathbf{p}&#x27;_0 = \\mathbf{p}&#x27;_n = 0, \\mathbf{p}&#x27;_i = \\frac{\\mathbf{p}_{i+1} - \\mathbf{p}_{i - 1}}{2}\np0′​=pn′​=0,pi′​=2pi+1​−pi−1​​\nOnly points are needed to create C1C^1C1 continuous curve that passes through them!\nWe can either position control points and draw a Bezier spline, or draw a Hermite spline directly.\nNatural Cubic Splines\nIs C1C^1C1 enough? Car surface have to guarantee at least G3G^3G3 continuity.\nTheoretically, splines of degree n can guarantee Cn−1C^{n-1}Cn−1 continuity.\nGiven n + 1 control points, we want to find n connected Bezier curves passing through the points with C2C^2C2 continuity.\n\nWe have 4n unknowns (4 control points per each curve)\nWe have 4n-2 equations\n\n2n equations for end point interpolations\nn-1 equations for tangential continuity\nn-1 equations for second derivative continuity\n\n\n\nWe need 2 more equations! We need to specify boundary conditions.\n\nFor open curve, p′′(t0)=p′′(tn)=0p&#x27;&#x27;(t_0) = p&#x27;&#x27;(t_n) = 0p′′(t0​)=p′′(tn​)=0 (i.e., curve ends with a straight line)\nFor closed curve, p′(t0)=p′(tn)p&#x27;(t_0) = p&#x27;(t_n)p′(t0​)=p′(tn​) and p′′(t0)=p′′(tn)p&#x27;&#x27;(t_0) = p&#x27;&#x27;(t_n)p′′(t0​)=p′′(tn​)\n\nNatural Cubic Splines is affine-invariant!\nBut we lose local controllability, i.e. moving one point affects the entire curve.\nB-splines\nMotivation: We need C2C^2C2 continuous curve with local controllability.\nUse all four consecutive points as control points.\ne.g. p0,p1,p2,p3p_0, p_1, p_2, p_3p0​,p1​,p2​,p3​ forms control points, p1,p2,p3,p4p_1, p_2, p_3, p_4p1​,p2​,p3​,p4​ forms control points, ...\nWe get local controllability, but we lose interpolation, i.e. curves no longer pass through any points.\nUniform Cubic B-spline bases\n\nWhy this works? B-spline bases are C2C^2C2 continuous!\nB0(t)=16(1−t)3B1(t)=16(3t3−6t2+4)B2(t)=16(−3t3+3t2+3t+1)B3(t)=16t3P(t)=[p0p1p2p3]16[1−33−140−63133−30001][1tt2t3]\\begin{align*}\nB_0(t) &amp;= \\frac{1}{6}(1-t)^3 \\\\\nB_1(t) &amp;= \\frac{1}{6}(3t^3 - 6t^2 + 4) \\\\\nB_2(t) &amp;= \\frac{1}{6}(-3t^3 + 3t^2 + 3t + 1) \\\\\nB_3(t) &amp;= \\frac{1}{6} t^3 \\\\\nP(t) &amp;= \\begin{bmatrix}\n\\mathbf{p}_0 &amp; \\mathbf{p}_1 &amp; \\mathbf{p}_2 &amp; \\mathbf{p}_3\n\\end{bmatrix}\n\\frac{1}{6}\n\\begin{bmatrix}\n1 &amp; -3 &amp; 3 &amp; -1\\\\\n4 &amp; 0 &amp; -6 &amp; 3\\\\\n1 &amp; 3 &amp; 3 &amp; -3\\\\\n0 &amp; 0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\nt \\\\\nt^2 \\\\\nt^3\n\\end{bmatrix}\n\\end{align*}B0​(t)B1​(t)B2​(t)B3​(t)P(t)​=61​(1−t)3=61​(3t3−6t2+4)=61​(−3t3+3t2+3t+1)=61​t3=[p0​​p1​​p2​​p3​​]61​​1410​−3030​3−630​−13−31​​​1tt2t3​​​\nCurve segment is changed when ttt becomes an integer.\nSince curve segment is changed uniformly, (i.e. knots are equally spaced) this spline is called uniform cubic B-spline.\nProperties of B-splines\n\nConvex hull\nAffine invariance\nCn−1C^{n-1}Cn−1 continuity\nLocal controllability\nVariation diminishing: B-spline never crosses any given plane more often than its control polygon does.\n\nConversion between Bezier &amp; B-Spline\nP(t)=GBezier⋅BBezier⋅M(t)=GBezier⋅BBezier⋅BBspline−1⋅BBspline⋅M(t)=GBspline⋅BBspline⋅M(t)∴GBspline=GBezier⋅BBezier⋅BBspline−1\\begin{align*}\nP(t) &amp;= G_{Bezier} \\cdot B_{Bezier} \\cdot M(t) \\\\\n&amp;= G_{Bezier} \\cdot B_{Bezier} \\cdot B_{Bspline}^{-1} \\cdot B_{Bspline} \\cdot M(t) \\\\\n&amp;= G_{Bspline} \\cdot B_{Bspline} \\cdot M(t) \\\\\n\\therefore G_{Bspline} &amp;= G_{Bezier} \\cdot B_{Bezier} \\cdot B_{Bspline}^{-1}\n\\end{align*}P(t)∴GBspline​​=GBezier​⋅BBezier​⋅M(t)=GBezier​⋅BBezier​⋅BBspline−1​⋅BBspline​⋅M(t)=GBspline​⋅BBspline​⋅M(t)=GBezier​⋅BBezier​⋅BBspline−1​​\nWe can compute control points of the B-spline curve from control points of the Bezier curve.\nParametrization is different, but curves are same!\nNURBS (Non-Uniform Rational B-Spline)\np(t)=∑wjpjBj(t)∑wjBj(t)\\mathbf{p}(t) = \\frac{\\sum w_j\\mathbf{p}_jB_j(t)}{\\sum w_jB_j(t)}\np(t)=∑wj​Bj​(t)∑wj​pj​Bj​(t)​\nGeneralized version of B-Splines!\nWe use rational polynomial to represent conics (circle, ellipses, hyperbolics).\nRational polynomial is invariant under projective transformation.\nNon-uniform means we can use non-uniform location of knots.\nSurfaces\nBicubic Bezier Surfaces\nEach control vector is also on the bezier curve!\np(u,v)=∑i=03Bi(u)pi(v)=∑i=03Bi(u)[∑j=03pijBj(v)]=∑i=03∑j=03pijBij(u,v)\\begin{align*}\n\\mathbf{p}(u,v) &amp;= \\sum_{i=0}^3 B_i(u)\\mathbf{p}_i(v) \\\\\n&amp;= \\sum_{i=0}^3 B_i(u) \\left[ \\sum_{j=0}^3 \\mathbf{p}_{ij}B_j(v) \\right] \\\\\n&amp;= \\sum_{i=0}^3\\sum_{j=0}^3 \\mathbf{p}_{ij}B_{ij}(u, v)\n\\end{align*}p(u,v)​=i=0∑3​Bi​(u)pi​(v)=i=0∑3​Bi​(u)[j=0∑3​pij​Bj​(v)]=i=0∑3​j=0∑3​pij​Bij​(u,v)​\nIn matrix form,\nx(u,v)=∑i=03∑j=03xijBi(u)Bj(v)=M(u)T⋅BBezierT⋅Gx⋅BBezier⋅M(v)=[1uu2u3][1000−33003−630−13−31][x00x01x02x03x10x11x12x13x20x21x22x23x30x31x32x33][1−33−103−63003−30001][1vv2v3]\\begin{align*}\nx(u,v) &amp;= \\sum_{i=0}^3\\sum_{j=0}^3 x_{ij}B_i(u)B_j(v) \\\\\n&amp;= M(u)^T \\cdot B_{Bezier}^T \\cdot G^x \\cdot B_{Bezier} \\cdot M(v) \\\\\n&amp;= \\begin{bmatrix}\n1 &amp; u &amp; u^2 &amp; u^3\n\\end{bmatrix}\\begin{bmatrix}\n1 &amp; 0 &amp; 0 &amp; 0 \\\\\n-3 &amp; 3 &amp; 0 &amp; 0 \\\\\n3 &amp; -6 &amp; 3 &amp; 0 \\\\\n-1 &amp; 3 &amp; -3 &amp; 1\n\\end{bmatrix}\\begin{bmatrix}\nx_{00} &amp; x_{01} &amp; x_{02} &amp; x_{03} \\\\\nx_{10} &amp; x_{11} &amp; x_{12} &amp; x_{13} \\\\\nx_{20} &amp; x_{21} &amp; x_{22} &amp; x_{23} \\\\\nx_{30} &amp; x_{31} &amp; x_{32} &amp; x_{33}\n\\end{bmatrix}\\begin{bmatrix}\n1 &amp; -3 &amp; 3 &amp; -1 \\\\\n0 &amp; 3 &amp; -6 &amp; 3 \\\\\n0 &amp; 0 &amp; 3 &amp; -3 \\\\\n0 &amp; 0 &amp; 0 &amp; 1\n\\end{bmatrix}\\begin{bmatrix}\n1 \\\\\nv \\\\\nv^2 \\\\\nv^3\n\\end{bmatrix}\n\\end{align*}x(u,v)​=i=0∑3​j=0∑3​xij​Bi​(u)Bj​(v)=M(u)T⋅BBezierT​⋅Gx⋅BBezier​⋅M(v)=[1​u​u2​u3​]​1−33−1​03−63​003−3​0001​​​x00​x10​x20​x30​​x01​x11​x21​x31​​x02​x12​x22​x32​​x03​x13​x23​x33​​​​1000​−3300​3−630​−13−31​​​1vv2v3​​​\nWe need 4x4 control points and 4x4 2D basis functions to create a surface.\nEach sub-surface is called patch.\nAlso called bicubic tensor product surface because it's cubic bezier curve in two direction, and surface is made with tensor product of two bezier curves.\nTensor Product B-Spline Patches, Rational splines, NURBS is also possible!\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Subdivision Curves","url":"/posts/63/","content":"Subdivision Curves\nSubdivide polygonal mesh into larger number of polygons.\nControl points eventually converge to underlying smooth curve!\nBinary Curve Subdivision\n\nWe'll divide curve into left segment and right segment.\npL(t)=p(t2)=[p0p1p2p3]⋅B⋅[1t2t24t38]=[p0p1p2p3]⋅B⋅[1000012000014000018]⋅B−1⋅B⋅[1tt2t3]=[p0Lp1Lp2Lp3L]⋅B⋅[1tt2t3]\\begin{align*}\n\\mathbf{p}^L(t) &amp;= \\mathbf{p}\\left( \\frac{t}{2} \\right) \\\\\n&amp;= \\begin{bmatrix}\n\\mathbf{p}_0 &amp; \\mathbf{p}_1 &amp; \\mathbf{p}_2 &amp; \\mathbf{p}_3\n\\end{bmatrix} \\cdot B \\cdot \\begin{bmatrix}\n1 \\\\\n\\frac{t}{2} \\\\\n\\frac{t^2}{4} \\\\\n\\frac{t^3}{8} \\\\\n\\end{bmatrix} \\\\\n&amp;= \\begin{bmatrix}\n\\mathbf{p}_0 &amp; \\mathbf{p}_1 &amp; \\mathbf{p}_2 &amp; \\mathbf{p}_3\n\\end{bmatrix} \\cdot B \\cdot \\begin{bmatrix}\n1 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; \\frac{1}{4} &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; \\frac{1}{8}\n\\end{bmatrix} \\cdot B^{-1} \\cdot B \\cdot \\begin{bmatrix}\n1 \\\\\nt \\\\\nt^2 \\\\\nt^3\n\\end{bmatrix}\\\\\n&amp;= \\begin{bmatrix}\n\\mathbf{p}_0^L &amp; \\mathbf{p}_1^L &amp; \\mathbf{p}_2^L &amp; \\mathbf{p}_3^L\n\\end{bmatrix} \\cdot B \\cdot \\begin{bmatrix}\n1 \\\\\nt \\\\\nt^2 \\\\\nt^3\n\\end{bmatrix}\n\\end{align*}pL(t)​=p(2t​)=[p0​​p1​​p2​​p3​​]⋅B⋅​12t​4t2​8t3​​​=[p0​​p1​​p2​​p3​​]⋅B⋅​1000​021​00​0041​0​00081​​​⋅B−1⋅B⋅​1tt2t3​​=[p0L​​p1L​​p2L​​p3L​​]⋅B⋅​1tt2t3​​​\npR(t)\\mathbf{p}^R(t)pR(t) can be obtained from p(t+12)\\mathbf{p}\\left( \\frac{t+1}{2} \\right)p(2t+1​).\n[p0Lp1Lp2Lp3L]=[12120018681800121200186818][p0p1p2p3]\\begin{bmatrix}\n\\mathbf{p}^L_0 \\\\\n\\mathbf{p}^L_1 \\\\\n\\mathbf{p}^L_2 \\\\\n\\mathbf{p}^L_3\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{1}{2} &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\\n\\frac{1}{8} &amp; \\frac{6}{8} &amp; \\frac{1}{8} &amp; 0 \\\\\n0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; 0 \\\\\n0 &amp; \\frac{1}{8} &amp; \\frac{6}{8} &amp; \\frac{1}{8}\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{p}_0 \\\\\n\\mathbf{p}_1 \\\\\n\\mathbf{p}_2 \\\\\n\\mathbf{p}_3\n\\end{bmatrix}​p0L​p1L​p2L​p3L​​​=​21​81​00​21​86​21​81​​081​21​86​​00081​​​​p0​p1​p2​p3​​​\n[p0Rp1Rp2Rp3R]=[18681800121200186818001212][p0p1p2p3]\\begin{bmatrix}\n\\mathbf{p}^R_0 \\\\\n\\mathbf{p}^R_1 \\\\\n\\mathbf{p}^R_2 \\\\\n\\mathbf{p}^R_3\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{1}{8} &amp; \\frac{6}{8} &amp; \\frac{1}{8} &amp; 0 \\\\\n0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; 0 \\\\\n0 &amp; \\frac{1}{8} &amp; \\frac{6}{8} &amp; \\frac{1}{8} \\\\\n0 &amp; 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2}\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{p}_0 \\\\\n\\mathbf{p}_1 \\\\\n\\mathbf{p}_2 \\\\\n\\mathbf{p}_3\n\\end{bmatrix}​p0R​p1R​p2R​p3R​​​=​81​000​86​21​81​0​81​21​86​21​​0081​21​​​​p0​p1​p2​p3​​​\nSubdivision Rule for Cubic B-Splines\nThe new control polygon consist of:\n\nEdge points: the midpoint of each line segment\nVertex points: the weighted average (1:6:1) of the corresponding vertex and its two neighbors\n\nUsing subdivison rule recursively, we can converge the control polygon  to a cubic B-spline curve.\nThere are other subdivison rules, some converges to a non-polynomial curve, some converges to a curve with no closed form.\nChaikin's Algorithm\n\nCorner cutting algorithm!\nConverges to a quadradic B-Spline curve.\nQi=34Pi+14Pi+1Ri=14Pi+34Pi+1\\begin{align*}\n\\mathbf{Q}_i &amp;= \\frac{3}{4}\\mathbf{P}_i + \\frac{1}{4}\\mathbf{P}_{i + 1} \\\\\n\\mathbf{R}_i &amp;= \\frac{1}{4}\\mathbf{P}_i + \\frac{3}{4}\\mathbf{P}_{i + 1}\n\\end{align*}Qi​Ri​​=43​Pi​+41​Pi+1​=41​Pi​+43​Pi+1​​\nInterpolating Subdivision\n\nB-Splines, Chaikin's Algorithm is non-stationary.\nHow to make stationary (i.e. interpolating given control points) subdivision?\n\nVertex points: the original control points\nEdge points: the weighted average of the original points\n\np2i+1m+1=116(−pi−1m+9pim+9pi+1m−pi+2m)\\mathbf{p}_{2i + 1}^{m + 1} = \\frac{1}{16}(-\\mathbf{p}_{i-1}^m + 9\\mathbf{p}_i^m + 9\\mathbf{p}_{i + 1}^m - \\mathbf{p}_{i + 2}^m)\np2i+1m+1​=161​(−pi−1m​+9pim​+9pi+1m​−pi+2m​)\nInterpolating subdivision converges to a affine curve, but not convex curve. (i.e. curve goes outside the convex hull)\nSubdivision Surfaces\nLoop Subdivision\nOnly applicable to triangle meshes!\n\nStep 1: For each edge, add an edge point.\n\nIf edge is boundary edge, use midpoint of endpoints. (same as binary curve!)\nIf edge is interior edge, use weighted average (1:3:3:1) of neighbour vertices.\n\n\nStep 2: For each vertex, add an vertex point. (i.e. move vertex to new position)\n\n\nIf vertex is boundary vertex, use weighted average (1:6:1) of neighbour vertices. (same as binary curve!)\n\n\nIf vertex is interior vertex, use weighted average (β:⋯:1−kβ\\beta : \\cdots : 1 - k\\betaβ:⋯:1−kβ) of neighbour vertices.\n\n\nβ=1n(58−(38+14cos⁡2πn)2)2\\beta = \\frac{1}{n} \\left( \\frac{5}{8} - \\left( \\frac{3}{8} + \\frac{1}{4}\\cos\\frac{2\\pi}{n} \\right)^2 \\right)^2β=n1​(85​−(83​+41​cosn2π​)2)2, Loop 1987\n\n\nβ={316if n=338nif n&gt;3\\beta = \\begin{cases}\n\\frac{3}{16} &amp; \\text{if}\\ n = 3 \\\\\n\\frac{3}{8n} &amp; \\text{if}\\ n &gt; 3\n\\end{cases}β={163​8n3​​if n=3if n&gt;3​, Warren 1995\n\n\n\nStep 3: Connect edge/vertex points.\nConnect vertex point and adjacent edge points, then connect adjacent edge points.\nEach triangle is divided into four triangles.\nSubdivison rule of boundary is same as binary curve subdivison.\nTherefore, boundary will converge to a cubic B-spline curve.\nCatmull-Clark Subdivison\nApplicable to any meshes!\n\nStep 1: For each face, add an face point.\nFace point is a average of vertices.\n\nStep 2: For each edge, add an edge point.\n\nIf edge is boundary edge, use midpoint of endpoints.\nIf edge is interior edge, use average of face points and endpoints.\n\n\nStep 3: For each vertex, add an vertex point. (i.e. move vertex to new position)\n\nIf vertex is boundary vertex, use weighted average (1:6:1) of neighbour vertices.\nIf vertex is interior vertex, use weighted average (1:2:n−31 : 2 : n-31:2:n−3) of average of neighbour face points, average of midpoints of neighbour edges, and original vertex, where n is valence of vertex. (i.e. number of faces that meet at the vertex)\nDon't use edge points calculated in Step 2!\n\nP′=1n(1n∑Fi+2n∑P+Pi2+(n−3)P)=F+2R+(n−3)PnP&#x27; = \\frac{1}{n} \\left( \\frac{1}{n}\\sum F_i + \\frac{2}{n}\\sum \\frac{P + P_i}{2} + (n-3)P \\right) = \\frac{F + 2R + (n-3)P}{n}\nP′=n1​(n1​∑Fi​+n2​∑2P+Pi​​+(n−3)P)=nF+2R+(n−3)P​\n\nStep 4: Connect face/edge/vertex points.\nConnect vertex point and adjacent edge points, then connect face point and adjacent edge points.\nEach n-sided polygon is divided into n quadrilaterals (4-sided polygon).\nIf valence of vertex is not 4, that vertex is isolated, and subdivision converges to C1C^1C1 continuous surface at this vertex.\nOther parts converges to B-spline surface, which is C2C^2C2 continuous.\nc.f. This paper was ignored at 70s because people didn't think this was a real science, but computer changed everything.\nActually paper is only 9 pages long; This is not some kind of optimal subdivision.\nDisplacement Mapping\nLevel-of-Detail: We should control quality of rendering.\ne.g. Game should render real-time!\nInstead of using mesh with full detail, we use subdivision + displacement.\nFirst, we use base model with subdivision to make a rough model.\nThen details are implemented with height information of details.\nThis method can make detail model with less computing power!\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Sampling and Reconstruction","url":"/posts/65/","content":"Sampling 1D Signal\nHow do we sample and reconstruct continuous 1D signal? (e.g. audio)\n\nPiecewise constant approximation: f(x) is value of sample closest to x.\nPiecewise linear approximation: f(x) is linear interpolation between values of two closest samples to x.\n\nDenser sampling makes better reconstruction.\nSampling 2D Signal\nSame for image, use piecewise constant/bi-linear.\nFourier Transform\nAny signal can be expressed as a superposition of frequencies.\nHigh frequency components in image have strongest edges.\nAliasing\nAliasing: If sampling is not enough, high frequencies in the original signal masquerade as low frequencies after reconstruction.\nSpatial aliasing\n\ne.g. Image of sin⁡(x2+y2)\\sin(x^2 + y^2)sin(x2+y2)\nIn the middle, the ring appears at the limit of what we can represent with individual pixels.\nOn the right, aliasing happens, and the ring appears as if it were low frequency.\nTemporal aliasing\ne.g. Wagon Wheel Effect: If sampling rate is same as rotation speed, it appears to stand still.\nNyquist-Shannon Theorem\nIf a signal is band-limited signal, i.e. it has no frequencies above some threshold ω\\omegaω, the signal can be perfectly reconstructed if sample with frequency 2ω2\\omega2ω, and if interpolation is performed using $\\operatorname{sinc}(x) = \\frac{1}{\\pi x}\\sin(\\pi x) $\nBut why sinc?? Fourier transform of sinc is rectangular function.\nConvolution with sinc (i.e. multiplying rectangular function) limits frequency of sampled signal.\nSampling in computer graphics\nSampling always happens in computer graphics!\n\nSignals are often not band-limited. e.g. Fourier transform of straight line need infinite frequency.\nInfinite extent of sinc filter is impractical for efficient implementations.\nEverything is discrete - graphics are visualized as pixels, numbers have limited precision, etc.\n\nAliasing makes jaggies, roping, shimmering, moire patterns, etc.\nThere are several techniques to reduce aliasing - e.g. pre-filtering.\nFog effect remove high frequency components in advance.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Roller Coaster Physics","url":"/posts/64/","content":"Roller Coaster Physics\n\na point mass moving on a spline\nfrictionless with gravity\nenergy conservation law applies\n\nEK+EP=12m∥v∥2+mgh=mghmaxE_K + E_P = \\frac{1}{2} m \\| v \\|^2 + mgh = mgh_{max}\nEK​+EP​=21​m∥v∥2+mgh=mghmax​\nWe can always compute the magnitude of the velocity of any given point!\n∥v∥=2g(hmax−h)\\| v \\| = \\sqrt{2g(h_{max}-h)}\n∥v∥=2g(hmax​−h)​\nMotion on a spline\nProblem: our mass point moves on a parametric function P(u)=(x(u),y(u),z(u))P(u) = (x(u), y(u), z(u))P(u)=(x(u),y(u),z(u)) where u(t)u(t)u(t) is a function of time ttt.\nWe know the initial position of the point and its velocity v=v(u)v = v(u)v=v(u) at any time uuu.\nCan we compute the position of the point at arbitrary time ttt?\nArc-length Parameterization\nArc-length s(u)s(u)s(u) is the distance from the initial position along the curve.\nWe need to reparameterization to arc-length.\ns=LENGTH⁡(u,u0)=∫u0u(dx(u~)du~)2+(dy(u~)du~)2+(dz(u~)du~)2du~\\begin{align*}\ns &amp;= \\operatorname{LENGTH}(u, u_0) \\\\\n&amp;= \\int_{u_0}^{u} \\sqrt{ \\left( \\frac{dx(\\tilde{u})}{d\\tilde{u}} \\right)^2 + \\left( \\frac{dy(\\tilde{u})}{d\\tilde{u}} \\right)^2 + \\left( \\frac{dz(\\tilde{u})}{d\\tilde{u}} \\right)^2 } d\\tilde{u}\n\\end{align*}s​=LENGTH(u,u0​)=∫u0​u​(du~dx(u~)​)2+(du~dy(u~)​)2+(du~dz(u~)​)2​du~​\ns(u)s(u)s(u) is monotonically increasing!\nBut the integral cannot be evaluated analytically, not even for polynomials.\nWe need to compute integral numerically.\nChord Length Approximation\nSample the curve and estimate the arc length by computing the linear distance through the sequence of samples.\ns(u)=∑di=∑∥p(ui+1)−p(ui)∥s(u) = \\sum d_i = \\sum \\| p(u_{i+1}) - p(u_i) \\|\ns(u)=∑di​=∑∥p(ui+1​)−p(ui​)∥\nIf curvature is high, the error between the chord and the curve can become too large.\nSolution: Adaptive sampling!\nIf adding a new sample at the midpoint changes total length above give tolerance, add it.\nRepeat until there is no more point to add.\nGeneral Numerical Integration Methods\nWe use error rate to evaluate integration methods.\nIf an error rate is O(n2)O(n^2)O(n2), it means that if a sampling interval is reduced to 1/2, the error rate will decrease to 1/4.\n\nTrapezoidal rule: piecewise linear, error rate O(n)O(n)O(n)\nSimson's rule: piecewse quadratic, error rate O(n2)O(n^2)O(n2)\nGaussian quadrature rule: used for prodcution\n\nAdaptive sampling can be also used!\nOnly polynomials up to the fourth order are used, higher order don't reduce the error rate.\nComputing inverse\nGiven arc-length sss, determine the original parameter uuu.\nSince s(u)s(u)s(u) is monotonically increasing, so is u(s)u(s)u(s).\nWe can formulate as a root finding problem.\nf(u)=s−LENGTH⁡(u,u0)=0f(u) = s - \\operatorname{LENGTH}(u, u_0) = 0\nf(u)=s−LENGTH(u,u0​)=0\nBisection algorithm may be used, (since s(u) is monotonically increasing) but Newton-Raphson iteration is faster.\nSimulating mass point on a spline\nAssume we know ∥v(u)∥\\| v(u) \\|∥v(u)∥ at any uuu, and initial point is u0,s0,t0u_0, s_0, t_0u0​,s0​,t0​.\nWe simulate the point with constant time step Δt\\Delta tΔt.\ns←s+∥v(u)∥Δtt←t+Δt\\begin{align*}\ns &amp;\\leftarrow s + \\| v(u) \\| \\Delta t \\\\\nt &amp;\\leftarrow t + \\Delta t\n\\end{align*}st​←s+∥v(u)∥Δt←t+Δt​\nuuu can be computed by root finding f(u)=0f(u) = 0f(u)=0.\nCamera movement along a Spline\nFrenet frame\nWell-defined and continuous as long as P′P&#x27;P′ and P′′P&#x27;&#x27;P′′ do not vanish.\n\ntangent vector T(u)=P′(u)∣P′(u)∣T(u) = \\frac{P&#x27;(u)}{|P&#x27;(u)|}T(u)=∣P′(u)∣P′(u)​\nbinormal vector B(u)=P′(u)×P′′(u)∣P′(u)×P′′(u)∣B(u) = \\frac{P&#x27;(u) \\times P&#x27;&#x27;(u)}{|P&#x27;(u) \\times P&#x27;&#x27;(u)|}B(u)=∣P′(u)×P′′(u)∣P′(u)×P′′(u)​\nnormal vector N(u)=B(u)×T(u)N(u) = B(u) \\times T(u)N(u)=B(u)×T(u)\n\nProblem: if spline is straight line, (i.e. curvature is 0) P′′P&#x27;&#x27;P′′ do vanish.\nContinuous Frenet Frame?\nWe can use this:\nTn+1=T(un+1)Nn+1=Bn×Tn+1Bn+1=Tn+1×Nn+1\\begin{align*}\nT_{n+1} &amp;= T(u_{n+1}) \\\\\nN_{n+1} &amp;= B_n \\times T_{n+1} \\\\\nB_{n+1} &amp;= T_{n+1} \\times N_{n+1}\n\\end{align*}Tn+1​Nn+1​Bn+1​​=T(un+1​)=Bn​×Tn+1​=Tn+1​×Nn+1​​\nThis is not frenet frame, and eventually frame will be disorted, but anyway it's continuous.\nUnit Quaternion Splines\nA curve is constructed as affine combination of control points.\nP(t)=∑p0B0(t)P(t) = \\sum \\mathbf{p}_0 B_0(t)\nP(t)=∑p0​B0​(t)\nSimiliary, we can construct a curve on unit quaternion space if an affine combination of unit quaternions is defined.\nRecall) we already know one affine combination: slerp!\nProblem: Quaternion doesn't match with tanget of the spline.\nSolution: Fix tangent vector and project to S3S^3S3, or make orthogonal matrix with modified vectors.\nAffine Combination in S3S^3S3\nProblems:\n\nAmbiguity\n\nAntipodal equivalence\nSpherical structure\n\n\nUnstable\n\ne.g. let's just average and projection to S3S^3S3. If average results near the center of the sphere, projection will vary a lot.\n\n\n\nSolution? We'll assume that we're computing affine combination only for points that are close to each other.\nRe-normalization\nJust treat unit quaternions as 4D vectors, then project the affine combination to S3S^3S3.\nq=w0q0+⋯+wnqn∥w0q0+⋯+wnqn∥\\mathbf{q} = \\frac{w_0 \\mathbf{q}_0 + \\cdots + w_n \\mathbf{q}_n}{\\| w_0 \\mathbf{q}_0 + \\cdots + w_n \\mathbf{q}_n \\|}\nq=∥w0​q0​+⋯+wn​qn​∥w0​q0​+⋯+wn​qn​​\nPros: simple, efficient\nCons: Linear precision doesn't holds. i.e. Re-normalization of f(qi)f(\\mathbf{q}_i)f(qi​) is not f(qi)f(\\mathbf{q}_i)f(qi​) for linear function fff.\nMulti-Linear Method\nEvaluate n-point weight sum as a sequence of slerps.\nRecall) slerp⁡t(q1,q2)=q1exp⁡(t⋅log⁡(q1−1q2))\n\\operatorname{slerp}_t(q_1, q_2) = q_1\\exp(t \\cdot \\log (q_1^{-1}q_2))slerpt​(q1​,q2​)=q1​exp(t⋅log(q1−1​q2​))\ne.g. Let's calculate c=12p1+14p2+14p3\\mathbf{c} = \\frac{1}{2}\\mathbf{p}_1 + \\frac{1}{4}\\mathbf{p}_2 + \\frac{1}{4}\\mathbf{p}_3c=21​p1​+41​p2​+41​p3​.\nc=12p1+14p2+14p3=34(23p1+13p2)+14p3=slerp⁡14(slerp⁡13(p1,p2),p3)\\begin{align*}\n\\mathbf{c} &amp;= \\frac{1}{2}\\mathbf{p}_1 + \\frac{1}{4}\\mathbf{p}_2 + \\frac{1}{4}\\mathbf{p}_3 \\\\\n&amp;= \\frac{3}{4} \\left( \\frac{2}{3}\\mathbf{p}_1 + \\frac{1}{3}\\mathbf{p}_2 \\right) + \\frac{1}{4}\\mathbf{p}_3 \\\\\n&amp;= \\operatorname{slerp}_\\frac{1}{4}\\left( \\operatorname{slerp}_\\frac{1}{3}(\\mathbf{p}_1, \\mathbf{p}_2), \\mathbf{p}_3 \\right)\n\\end{align*}c​=21​p1​+41​p2​+41​p3​=43​(32​p1​+31​p2​)+41​p3​=slerp41​​(slerp31​​(p1​,p2​),p3​)​\nQuaternion Bezier Curve\nRecall) De Casteljau Algorithm evaluates a point on a Bezier curve by recursively interpolating control points.\nWe use De Casteljau Algorithm on quaternions, but instead of interpolation, we use slerp.\nPros: Simple, intuitive, inherit good properties of slerp (e.g. coordinate-invariance)\nCons: Algebraically complicated, need ordering because slerp is not associative.\nLinearization by exp/log\nlog maps quaternion to vector!\nq=exp⁡(w0log⁡q0+w1log⁡q1+⋯+wnlog⁡qn)\\mathbf{q} = \\exp(w_0 \\log \\mathbf{q}_0 + w_1 \\log \\mathbf{q}_1 + \\cdots + w_n \\log \\mathbf{q}_n)\nq=exp(w0​logq0​+w1​logq1​+⋯+wn​logqn​)\nProblem: log are used with rotation, not ment to use with orientation..\nThis depends on the choice of the reference frame.\nFunctional Optimization\nView affine combination as a certain energy function?\ne(p‾)=12∑iwi∥p‾−pi∥2e(\\overline{\\mathbf{p}}) = \\frac{1}{2} \\sum_i w_i \\| \\overline{\\mathbf{p}} - \\mathbf{p}_i \\|^2\ne(p​)=21​i∑​wi​∥p​−pi​∥2\nTo minimize e(p‾)e(\\overline{\\mathbf{p}})e(p​), ddp‾e(p‾)=∑iwi(p‾−pi)=0\\frac{d}{d\\overline{\\mathbf{p}}} e(\\overline{\\mathbf{p}}) = \\sum_i w_i ( \\overline{\\mathbf{p}} - \\mathbf{p}_i ) = 0dp​d​e(p​)=∑i​wi​(p​−pi​)=0 should hold.\n∴p‾=w1p1+w2p2+⋯+wnpn\\therefore \\overline{\\mathbf{p}} = w_1 \\mathbf{p}_1 + w_2 \\mathbf{p}_2 + \\cdots + w_n \\mathbf{p}_n\n∴p​=w1​p1​+w2​p2​+⋯+wn​pn​\nIn unit quaternion space, we use spherical (geodesic) distance dist⁡(q1,q2)=∥log⁡(q1−1q2)∥\\operatorname{dist}(\\mathbf{q}_1, \\mathbf{q}_2) = \\| \\log(\\mathbf{q}_1^{-1} \\mathbf{q}_2) \\|dist(q1​,q2​)=∥log(q1−1​q2​)∥\nf(q‾)=12∑iwi∥log⁡(q‾−1qi)∥2f(\\overline{\\mathbf{q}}) = \\frac{1}{2} \\sum_i w_i \\| \\log(\\overline{\\mathbf{q}}^{-1} \\mathbf{q}_i) \\|^2\nf(q​)=21​i∑​wi​∥log(q​−1qi​)∥2\nNow we just have to find root of ∂∂qf(q‾)=(0,0,0,0)\\frac{\\partial}{\\partial \\mathbf{q}} f(\\overline{\\mathbf{q}}) = (0, 0, 0, 0)∂q∂​f(q​)=(0,0,0,0) with numerical methods!\nPros: COOL! Theoretically rigorous\nCons: Need numerical iterations, computationally demanding.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Rasterization","url":"/posts/68/","content":"Rasterization\nWe should determine which pixels should be turn on.\nFor each primitive, which pixels are light up?\nRasterization is extremely fast (billions of trangles per second on GPU!), but harder to achieve photorealism.\nc.f. Ray tracing: For each pixel, which primitives are seen?\nGenerally slower, but easier to get photorealism!\nLine rasterization\nDiamond rule\n\nModern GPUs light up pixel if line passes through diamond inside pixel.\nIncremental line rasterization\n\nLet's say a line is represented with integer endpoints: (u1,v1), (u2, v2).\nConsider an easy case: u1&lt;u2,v1&lt;v2u1 &lt; u2, v1 &lt; v2u1&lt;u2,v1&lt;v2 (line points toward upper-right) and 0&lt;v2−v1u2−u1&lt;10 &lt; \\frac{v2 - v1}{u2 - u1} &lt; 10&lt;u2−u1v2−v1​&lt;1. (slope is low, i.e. more change in x than y)\nv = v1;for (u = u1; u &lt;= u2; ++u) &#123;    v += (v2 - v1) / (u2 - u1);    draw(u, round(v));&#125;\nThere are 8 cases in total. You can handle them in a similar way!\nc.f. Bresenham's line algorithm\nBresenham's line algorithm is an efficient line rasterization in assembly level!\nMultiplication, division is avoided, and only integer addition, subtraction is used.\nBut modern GPUs are really fast!\nThese days, we just use division, round, etc.\nTriangle rasterization\nHow do we handle width of line?\nHow do we rasterize points, triangles, or other primitives?\nSolution: Convert everything into triangles!\nModern GPUs doesn't have line algorithm, point algorithm, etc.\nInstead, rasterization pipeline convert all primitives to triangels.\ne.g. line is interpreted as very narrow rectangle.\nWhy do we use triangle?\n\nIf we could convert everything to triangles, we could focus on making an extremely well-optimized pipeline for drawing triangles!\nTriangles can approximate any shape.\nTriangles are always planar, and have well-defined normal.\nTriangles are easy to interpolate data at corners.\nIf barycentric coordinates is (λ0,λ1,λ2)(\\lambda_0, \\lambda_1, \\lambda_2)(λ0​,λ1​,λ2​), we can interpolate data as λ0f0+λ1f1+λ2f2\\lambda_0 f_0 + \\lambda_1 f_1 + \\lambda_2 f_2λ0​f0​+λ1​f1​+λ2​f2​, where f0,f1,f2f_0, f_1, f_2f0​,f1​,f2​ are data at corners.\n\nPoint-in-triangle Test\nHalf-plane test\nChecking if a point is inside a half-plane is easy.\nCreate three half-planes from the edges of the triangle, and check if the point is contained in three half-planes.\nBarycentric coordinate test\nCompute the barycentric coordinate of the point, then check if all coefficients are within [0, 1].\nTraditional approach: Incremental Traversal\n\nGo up if you can, then go as far right as you can.\nGood memory coherence! (Neighbour memory access is faster)\nClever incremental scheme can reuse line algorithms.\nModern approach: Parallel coverage tests\nModern GPU has special-purpose hardware for point-in-triangle test.\nWe just test all pixels in bounding box in parallel.\nHybrid approach: Tiled triangle traversal\nProblem: Testing every pixel can be wasteful if traingle is pointed.\nSolution: Check if block intersect the triangle!\nIf block doesn't intersect the triangle, skip this block.\nIf block is contained inside the triangle, all pixels of the block are covered.\nOtherwise, use parallel coverage tests. (i.e. test every pixel of the block.)\nAntialiasing\nAliasing is an error induced by insufficient samples when compared to the frequency of its original signal.\nIn rasterization, jaggies (계단현상) are induced by insufficient pixels.\nAntialiasing adjusts color values by the coverage of the pixel.\ni.e. if triangle covers 10% of the pixel, then pixel should be 10% red.\nNew problem: How do we rasterize alpha channel?\nSupersampling\nInstead of calculating the exact coverage of the pixel, we can use supersampling.\nChoose samples (i.e. points) within a pixel, then calculate coverage of the samples.\n\nSamples can be taken as grid, (often called subpixels) but there are other ways to sample.\nWhichever method you use, more sample gives a better result.\nClipping\n\nClipping eliminates triangles not visible to the camera. (i.e. not in the view frustum)\nCut triangles outside the view frustum, and then divide partially clipped primitives into triangles.\nThere are lots of clipping algorithms...\n\nLine clipping\n\nCohen-Sutherland algorithm\nCyrus-Beck algorithm\n\n\nPolygon clipping\n\nSutherland-Hodgman algorithm\nWeiler-Atherton algorithm\n\n\n\nCulling (Back-Face Removal)\nIdea: Remove faces that do not point toward the camera. (i.e. view plane)\nVery simple! Compute inner product of traingle's normal vector and camera's look-at direction.\nIf inner product is non-positive, this face is not visible.\nHowever, culling works correctly only when objects are closed and convex.\nDepth Buffer (Z-Buffer)\nPainter's algorithm: Sort objects by depth (distance from eye), then draw objects from the farthest to the closest.\nProblem: Visibility is not total order... (only partial order)\nSolution: Depth Buffer!\nFor each pixel, depth buffer stores the depth of the closest primitive seen so far. (If primitive is not seen, far plane's depth is stored)\nIf primitive's depth is closer than depth buffer, update depth buffer and color of pixel.\nLots of advantages!\n\nOrder of drawing objects doesn't matter\nCan handle intersections\nConstant additional space per frame\nNot specific to triangles\n\nBut it was ignored at 1960s because people thought that allocating memory per pixel is impossible...\nc.f. double buffering: GPU stores current frame and next frame at the same time, then switch two frames to show smooth animation.\nWe're actually using 2 depth buffers for each pixel!\nOpacity as Alpha channel\nAn alpha is a value between 0 and 1 that describes the opacity of an object.\nOver Operator\nB over A composite image B with opacity αB\\alpha_BαB​ over image A with opacity αA\\alpha_AαA​.\n\nComposite color C=αB⋅B+(1−αB)⋅αA⋅AC = \\alpha_B \\cdot B + (1-\\alpha_B) \\cdot \\alpha_A \\cdot AC=αB​⋅B+(1−αB​)⋅αA​⋅A\nComposite alpha αC=αB+(1−αB)⋅αA\\alpha_C = \\alpha_B + (1-\\alpha_B) \\cdot \\alpha_AαC​=αB​+(1−αB​)⋅αA​\n\nUnfortunately, over operator is not commutative, so order of drawing is important even when using depth-buffer.\nFringing\n\nPoor treatment of color/alpha during composition can yield fringing.\nBackground color of the original image appears at a transparent boundary.\nFringing in upsampling\n\nIf alpha and color are upsampled, antialiasing makes blurred boundary, thus causing fringing.\nFringing in downsampling\n\nIf alpha and color are downsampled, boundary color and alpha are mixed, thus causing fringing.\nFringing in repeated over operator\nOver operator is not a closed operation!\nRepeated over operator changes original color.\ne.g. composite (1, 0, 0, 0.5) over (1, 0, 0, 0.5) is (0.75, 0, 0, 0.75).\nColor is darker than before!\nOver operator with premultiplied alpha\nModern GPU uses permultiplied alpha to solve fringing.\nBefore compositing image B (rB,gB,bB,αB)(r_B, g_B, b_B, \\alpha_B)(rB​,gB​,bB​,αB​) over image A (rA,gA,bA,αA)(r_A, g_A, b_A, \\alpha_A)(rA​,gA​,bA​,αA​), we multiply alpha to color.\nA′=(αArA,αAgA,αAbA,αA)B′=(αBrB,αBgB,αBbB,αB)C′=(rC,gC,bC,αC)≔B′+(1−αB)A′C≔(rCαC,gCαC,bCαC,αC)\\begin{align*}\nA&#x27; &amp;= (\\alpha_A r_A, \\alpha_A g_A, \\alpha_A b_A, \\alpha_A) \\\\\nB&#x27; &amp;= (\\alpha_B r_B, \\alpha_B g_B, \\alpha_B b_B, \\alpha_B) \\\\\nC&#x27; &amp;= (r_C, g_C, b_C, \\alpha_C) \\coloneqq B&#x27; + (1-\\alpha_B)A&#x27; \\\\\nC &amp;\\coloneqq \\left( \\frac{r_C}{\\alpha_C}, \\frac{g_C}{\\alpha_C}, \\frac{b_C}{\\alpha_C}, \\alpha_C \\right)\n\\end{align*}A′B′C′C​=(αA​rA​,αA​gA​,αA​bA​,αA​)=(αB​rB​,αB​gB​,αB​bB​,αB​)=(rC​,gC​,bC​,αC​):=B′+(1−αB​)A′:=(αC​rC​​,αC​gC​​,αC​bC​​,αC​)​\nWe premultiply alpha, composite images, then divide alpha to get back color.\nAdvantages:\n\nPremultiply removes background color -&gt; Significantly less fringing!\nSimilar with homogeneous coordinates in pinhole camera model -&gt; Works well with existing GPU hardwares!\nClosed under composition\nFewer arithmetic operations needed\n\nAccumulation Buffer (A-Buffer)\nStill, over operator depends on the order of the drawing objects.\nSolution: Store more information than depth buffer!\nFor each pixel, accumulation buffer stores the information of the 10 closest primitives, including depth, RGBA color, percent of area coverage, etc.\nIf new object is in the 10 closest primitives, recalculate color from that depth.\nWe ignore primitives after the 11th, assuming they are invisible or meaningless.\nVery memory intensive and expensive, and it's not widely used.\nBut it's essential for high quality rendering.\nc.f. We can check if an A-Buffer exists by checking if the rendering changes depending on the order of drawing primitives.\nPreviously it was implemented with linked list, but nowdays it's implemented with arrays.\nOpenGL lets you to reconfigure framebuffer, so we can define A-Buffer, texture memory usage, etc.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Rendering","url":"/posts/72/","content":"Photorealistic Rendering\nRendering or image synthesis is the process of generating a image from a 2D or 3D model by means of a computer program.\nThe goal is to create images from 2D or 3D models which are not distinguisible from what we've seen in the real world.\nIf we can make photorealistic rendering, non-photorealistic rendering can be easily achieved.\nAchieving photorealism in computer graphics requires:\n\nRealistic/detailed geometry of models\nAccurate representations of surface properties (e.g. material)\nGood physical descriptions of the lighting effects\nGood understanding of human perception (Human doesn't perceive absolute value... But they are good at perceiving relative value!)\n\nLocal Illumination Models\nOnly think about light source, eye, and object. (i.e. object illuminations are independent)\nEasier, but doesn't compute light between objects.\nNo light scattering between objects, no shadows, no reflection, no transmission (see through semi-transparant objects).\nGlobal Illumination Models\nA surface point can receive light from various sources such as light sources and other objects.\n\nRay tracing\nRadiosity\nPhoton mapping\n\nProperties of light sources\nLinearity\nLight source has linearity.\nWe can easily mix multiple light sources!\nI(sa+b)=sI(a)+I(b)I(sa+b) = sI(a) + I(b)\nI(sa+b)=sI(a)+I(b)\nBut human perceive differently; Human perceive intensity in log scale.\nFalloff (attenuation)\nAs radiant energy travels, its amplitude is attenuated by the factor 1d2\\frac{1}{d^2}d21​.\nWhy? The constant energy is emitted from a light source while the surface area gows with d2d^2d2.\nSometimes, more realistic attenuation effects can be obtained with an quadratic function of distance, while also preventing the problem of divergence when the distance is zero.\nffalloff(d)={1.0if light is very strong and located very far1a2d2+a1d+a0otherwisef_{falloff}(d) = \\begin{cases}\n1.0 &amp; \\text{if light is very strong and located very far}\\\\\n\\frac{1}{a_2d^2 + a_1d + a_0} &amp; \\text{otherwise}\n\\end{cases}ffalloff​(d)={1.0a2​d2+a1​d+a0​1​​if light is very strong and located very farotherwise​\nLamert's Cosine Law\nThe amount of light energy received by a surface depends on incoming angle.\nThis is the reason why we have summer and winter seasons.\nLight Sources\nWe use specific types of light sources in computer graphics.\nColor of light sources\nWe use 3D vector for light's intensity!\nEach element represents red's intensity, blue's intensity, and green's intensity.\nBut actually this is not enough.\nLinear combination of finite number of primary colors can't represent every color of light!\nWe'll just use RGB, but note that this is a approximation!\nPoint Lights\nPoint lights emit radiant energy from a single point.\nIntensity at a point x on the surface is:\nIin=1d2Ilightmax⁡(cos⁡θ,0)I_{in} = \\frac{1}{d^2} I_{light} \\max(\\cos\\theta, 0)\nIin​=d21​Ilight​max(cosθ,0)\nwhere N is the surface normal vector and L is the direction from x to the point light source.\nDirectional Lights\nDirectional lights are point lights that are infinitely far.\nDirectional lights don't have falloff.\nIin=Ilightmax⁡(cos⁡θ,0)I_{in} = I_{light} \\max(\\cos\\theta, 0)\nIin​=Ilight​max(cosθ,0)\nSpotlights\nSpotlights are point lights with non-uniform directional emission.\nWe assume symmetric about a central direction V with angular falloff, so spotlights outside of hotspot will have reduced intensity.\nIin=1d2Ilightcos⁡n(max⁡(α−12αhotspot,0))I_{in} = \\frac{1}{d^2} I_{light} \\cos^n \\left( \\max \\left( \\alpha - \\frac{1}{2}\\alpha_{hotspot}, 0 \\right) \\right)\nIin​=d21​Ilight​cosn(max(α−21​αhotspot​,0))\nα\\alphaα is an angle between L and V, and αhotspot\\alpha_{hotspot}αhotspot​ is an angle of hotspot.\nnnn is just a parameter that controls how quickly the falloff should occur.\nArea Lights\nArea lights emit light from their surface.\nIn general, computing radiometric quantities (e.g. intensity) related to area lights requires computing integrals over the surface.\nHowever, this can't be computed exactly, so we just approximate it as many point light sources.\nIin=∑p∈A1dp2Ilight(p)max⁡(cos⁡θp,0)I_{in} = \\sum_{p \\in A}\\frac{1}{d_p^2} I_{light}(p) \\max(\\cos\\theta_p, 0)\nIin​=p∈A∑​dp2​1​Ilight​(p)max(cosθp​,0)\nc.f. Point lights have sharp shadows, while area lights have soft shadows.\n\nUmbra: Every light source can't seen\nPenumbra: Some light source can't seen\n\nRendering point lights have too unrealistically sharp shadows, so we need area lights in practice.\nMeasuring reflection\nBRDF (Bidirectional Reflectance Distribution Function)\nThere are bidirection reflection, to the incoming direction and to the outgoing direction.\nReflection can be implemented as distribution function.\nGiven incoming direction ωi\\omega_iωi​ and outgoing direction ωo\\omega_oωo​, we define BRDF as fr(ωi,ωo)f_r(\\omega_i, \\omega_o)fr​(ωi​,ωo​).\nInstead of direction, we can use spherical coordinates (zenith and azimuth angles). fr(θi,ϕi,θo,ϕo)f_r(\\theta_i, \\phi_i, \\theta_o, \\phi_o)fr​(θi​,ϕi​,θo​,ϕo​)\ne.g. If frf_rfr​ is 1 at specific direction ωo\\omega_oωo​, (dirac delta) we can represent mirror.\nIn practice, reflection can also change with frequency, so we need 5 parameters. fr(θi,ϕi,θo,ϕo,f)f_r(\\theta_i, \\phi_i, \\theta_o, \\phi_o, f)fr​(θi​,ϕi​,θo​,ϕo​,f)\nWe can simplify this to computing BRDF only for RGBs.\nIout(ωo)=Iin(ωi)fr(ωi,ωo)=Ilightcos⁡θid2fr(ωi,ωo)I_{out}(\\omega_o) = I_{in}(\\omega_i) f_r(\\omega_i, \\omega_o) = \\frac{I_{light}\\cos\\theta_i}{d^2} f_r(\\omega_i, \\omega_o)\nIout​(ωo​)=Iin​(ωi​)fr​(ωi​,ωo​)=d2Ilight​cosθi​​fr​(ωi​,ωo​)\nIsotropic and anisotropic\nWhen ωi\\omega_iωi​ and ωo\\omega_oωo​ are fixed, material is called isotropic if rotation around the normal does not change the reflectance.\nOtherwise, the material is called anisotropic. Anisotropic material have strongly oriented microgeometry elements. (e.g. Aluminum)\nMeasuring BRDFs\n\nBRDF can be measured with devices.\ne.g. Gonioreflectometer measure reflection by changing ωi\\omega_iωi​ and ωo\\omega_oωo​.\nNote that we have specular reflection part (정반사) and diffuse reflection part (난반사).\nMaybe we can parameterize BRDF?\nParametric BRDFs\nActual BRDF need tabulated 5D data, which is too large.\nParametric BRDF models represent the relationship between incident and outgoing light by some mathmatical formula.\nIdeal Diffuse Reflectance\nIncident light is scattered with equal intensity in all directions.\nAt the microscopic level, an ideal diffuse surface is a very rough surface. (e.g. chalk, clay, ...)\nLight intensity depends on the angle of incidence according to Lambert's cosine law, but is independent of the angle of reflection.\nFor a single point light source,\nIout=kdIlightd2max⁡(N⋅L,0)I_{out} = k_d \\frac{I_{light}}{d^2} \\max(N \\cdot L, 0)\nIout​=kd​d2Ilight​​max(N⋅L,0)\nwhere kdk_dkd​ is diffuse coefficient, and IlightI_{light}Ilight​ is light source intensity.\nBRDF for ideal diffuse surface is a constant function!\nIdeal Specular Reflectance\nPerfect reflector reflects all rights to the direction where angle of reflection is identical to the angle of incidnece.\ni.e. This is a mirror!\nLight only reflects to the mirror direction.\nBRDF is a dirac delta function multiplied by a specular coefficient ksk_sks​.\nHowever, ideal specular BRDF is not useful for point light sources.\nYou can't see light source if you're outside of direction of reflection.\nNon-ideal Reflectors\nMost of the reflected light travels in the direction of the ideal mirror ray, while some of the light are also reflected around the reflected ray.\nIf more light is reflected around the ideal mirror ray, the surface becomes blurred.\nPhong specular model: For a single point light source,\nIout=ksIlightd2(R⋅V)nI_{out} = k_s \\frac{I_{light}}{d^2} (R \\cdot V)^n\nIout​=ks​d2Ilight​​(R⋅V)n\nwhere ksk_sks​ is specular coefficient, IlightI_{light}Ilight​ is light source intensity, RRR is reflected ray direction, VVV is viewing direction, and nnn is shininess parameter. (similar to cos⁡n\\cos^ncosn)\nIf n is large, surface is shiny. i.e. BRDF is more like dirac delta.\nSince R+L=2(L⋅N)NR+L = 2(L \\cdot N)NR+L=2(L⋅N)N, R=2(L⋅N)N−LR = 2(L \\cdot N)N - LR=2(L⋅N)N−L.\nAmbient Illumination\nIn the real world, we can see objects beneath the table even if there is no direct path between the objects and the light sources.\nThe ambient term represents the reflection of all indrect illumination.\nIout=kaIaI_{out} = k_aI_a\nIout​=ka​Ia​\nThe Complete Phong Illumination Model\nSum of three components:\n\nIdeal diffuse reflection\nNon-ideal specular reflection\nAmbient\n\nFor single light source,\nIout=kaIa+Ilightd2[kdmax⁡(N⋅L,0)+ks(R⋅V)n]I_{out} = k_aI_a + \\frac{I_{light}}{d^2} [k_d \\max(N \\cdot L, 0) + k_s (R \\cdot V)^n]\nIout​=ka​Ia​+d2Ilight​​[kd​max(N⋅L,0)+ks​(R⋅V)n]\nFor multiple light source with emission,\nIout=Iemit+kaIa+∑lIldl2[kdmax⁡(N⋅Ll,0)+ks(Rl⋅V)n]I_{out} = I_{emit} + k_aI_a + \\sum_l \\frac{I_l}{d_l^2} [k_d \\max(N \\cdot L_l, 0) + k_s (R_l \\cdot V)^n]\nIout​=Iemit​+ka​Ia​+l∑​dl2​Il​​[kd​max(N⋅Ll​,0)+ks​(Rl​⋅V)n]\nPhong Illumination Model is good for representing simple plastics and metals, but cannot represent correctly for other materials like clothes, brushed metals.\nPhong Illumination Model is not physically correct!\n\nDoes not even conserve energy, it can reflect more energy than what goes in.\nDoes not conform to the BRDF model directly. (cosine for diffuse, but not for specular)\nAmbient was a total hack, doesn't even related to actual light.\n\nBut it has been popularly used for real-time rendering (e.g. OpenGL) due to its compactness.\nIn general, specular reflection makes more use of the color of the light, and diffuse reflection makes more use of the color of the material.\nParameter Choosing Tips\n\nThe sum of reflectance coefficients is usually smaller than 1. ka+kd+ks≤1k_a + k_d + k_s \\leq 1ka​+kd​+ks​≤1\nTry nnn (shininess parameter) in the range [0, 100].\nUsually ambient term is small. ka≈0.1k_a \\approx 0.1ka​≈0.1\nThe material properties in the Phong model of the common material is already calculated.\n\nFresnel Reflection\nSpecularity is increased near grazing angles.\nBut most BRDF models doesn't account for this.\nBlinn-Phong variation\nUse the halfway vector H between L and V. H=V+L∣V+L∣H = \\frac{V+L}{|V+L|}H=∣V+L∣V+L​\nIout=ksIlightd2(N⋅H)nI_{out} = k_s \\frac{I_{light}}{d^2}(N \\cdot H)^n\nIout​=ks​d2Ilight​​(N⋅H)n\nIt is considered more accurate than Phong model, especially highlight is more pretty?????\nBSSRDF\n\nBidirectional Scattering Surface Reflectance Distribution Function\nsr(pi,ωi,po,ωo)s_r(p_i, \\omega_i, p_o, \\omega_o)\nsr​(pi​,ωi​,po​,ωo​)\n\nIn BRDF, light was reflected at the point where the light meets.\nIn BSSRDF, light can travel through objects, reflected inside objects, then go out in completely different directions.\n\nIf you're really ambitious, you can measure and use BSSRDF!\nSurface Rendering\nFlat shading\nEntire triangle has a solid color!\nProblem: Mach band effect\nEven when surface have a solid color, we perceive mach bands.\nGouraud shading\n\nCompute the average unit normal vector at each vertex. (i.e. average of the normal vectors of surfaces touching vertex)\nIllumination is calculated for each vertex\nValues are bi-linearly interpolated within a triangle.\n\nProblem:\n\nMach band effect still appears\nOrientation dependence (which two edges should we use?)\n\nNobody use this anymore....\nPhong shading\nSimilar to Gouraud shading, we compute normal vector at each vertex.\nHowever, instead of interpolating values, (color) we interpolate normal vector, then compute illumination again.\nTo solve orientation dependence, we use barycentric interpolation.\nBetter than Gouraud shading, especially Phong shading can render highlights inside surface!\nBecause Gouraud shading interpolate colors, highlight inside surface is ignored.\nProblems with interpolated shading\n\nPolygon sihouette (If number of polygon is too low, boundary appears)\nPerspective distortion (Not invariant to perspective projection)\nShared edges (How do we compute normal vector if edges are shared?)\n\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Texture","url":"/posts/75/","content":"Spatial Variation\nAll materials seen so far are the same everywhere.\ni.e. BRDF was independent of location on surface.\nWe will allow BRDF to vary over a surface!\nc.f. Why don't we divide the surface into lots of triangles with same BRDF?\nThis will make the mesh too complex.\nWe want to distinguish geometric complexity and material complexity.\nTexture Mapping\nIdea: use a mapping from a geometry to an image.\nf:(x,y,z)→(u,v)f: (x,y,z) \\rightarrow (u,v)\nf:(x,y,z)→(u,v)\nMapping is very general!\n\nAn object can have multiple images\nMany object can share a single image\nThe same image can be used repeatedly\n\nTexture Coordinates (UV Coordinates)\nEach vertex stores 2D (u,v) texture coordinates.\nThe values inside of triangles are interpolated by barycentric coordinates.\nTexture Interpolation\nUnfortunately, the interpolated UV coordinate is very likely to be positioned between pixels. (i.e. not integer)\n\nClosest: Grab value of the nearest texture pixel (texel).\nBilinear: Apply linear interpolation repeatedly.\n\n\ns=u−it=v−jCU,V=[1−tt][Ci,jCi+1,jCi,j+1Ci+1,j+1][1−ss]\\begin{align*}\ns &amp;= u-i \\\\\nt &amp;= v-j \\\\\nC_{U,V} &amp;= \\begin{bmatrix}\n1-t &amp; t\n\\end{bmatrix}\\begin{bmatrix}\nC_{i,j} &amp; C_{i+1,j} \\\\\nC_{i,j+1} &amp; C_{i+1, j+1}\n\\end{bmatrix}\\begin{bmatrix}\n1-s \\\\\ns\n\\end{bmatrix}\n\\end{align*}stCU,V​​=u−i=v−j=[1−t​t​][Ci,j​Ci,j+1​​Ci+1,j​Ci+1,j+1​​][1−ss​]​\nAliasing problem in texture\nTriangles are projected to 2D screen space.\nPixels in screen space will correspond to regions of varying size &amp; location in texture space.\nWe have a aliasing problem: sampling rate is too low.\nMagnification\nScreen image size is way larger than texture size.\ne.g. camera is very close to scene object.\nIf closest filtering is used, hard boundaries can be visible.\nInterpolation should applied to avoid it.\nMinification\nScreen image size is way smaller than texture size.\ne.g. camera is very far away from scene object.\nLets assume the triangle only takes up 1 pixel on the screen.\nIn our texture sampling algorithm, center of the texture space will be sampled instead of average color of the triangle.\nThis will incur aliasing!\nMipmap (Texture pre-filtering)\nRecall) Nyquist-Shannon Theorem: If signal has no frequencies above some threshold ω\\omegaω, the signal can be perfectly reconstructed if sampled with period T=12ωT = \\frac{1}{2\\omega}T=2ω1​.\nPre-filtering: If we remove signals that have a frequency higher than ω\\omegaω, we can perfectly reconstruct signal!\nIn image, downsampling removes high frequency.\nIdea: store prefiltered (downsampled) images at every possible scale, then choose an appropriate image on the fly.\ne.g. Mipmap level 0 is 128x128 (original texture),\nMipmap level 1 is 64x64,\nMipmap level 2 is 32x32, ...\nComputing mipmap level\nLx2=(dudx)2+(dvdx)2Ly2=(dudy)2+(dvdy)2d=log⁡2max⁡(Lx2,Ly2)\\begin{align*}\nL_x^2 &amp;= \\left( \\frac{du}{dx} \\right)^2 + \\left( \\frac{dv}{dx} \\right)^2 \\\\\nL_y^2 &amp;= \\left( \\frac{du}{dy} \\right)^2 + \\left( \\frac{dv}{dy} \\right)^2 \\\\\nd &amp;= \\log_2 \\sqrt{\\max\\left( L_x^2, L_y^2 \\right)}\n\\end{align*}Lx2​Ly2​d​=(dxdu​)2+(dxdv​)2=(dydu​)2+(dydv​)2=log2​max(Lx2​,Ly2​)​​\nwhere d is mipmap level, and Lx,LyL_x, L_yLx​,Ly​ is the differences in texture space at neighboring pixels in screen space.\nSince mipmap size is halved each time, we use log scale.\nContinuous mipmap level\nIf we just use the nearest level, we can get artifacts where level jumps.\nInstead of clamping the mipmap level to the closest integer, we can use continuous mipmap level.\n\nWe need a trilinear filtering between two mipmaps.\nWe perform bilinear interpolation independently in each level, then interpolate these two values.\nBilinear interpolation need four texel reads and 3 linear interpolations. (3 multiplication + 6 addition)\nTrilinear interpolation need eight texel reads and 7 linear interpolations. (7 multiplication + 14 addition)\nAnisotropic Filtering\n\nPerspective projection stretches the sample by different amounts along u and v (especially at grazing angle).\nSolution: Use more mipmap with different ratios!\nUnfortunately, it requires even more arithmetic and bandwidth than trilinear filtering.\nSpecifying the mapping function\nHow do we map to the texture coordinate?\nSome objects have natural parameterizations; e.g. sphere, cylinder.\nWe can use parametric surface as a texture surface.\nIf object is star shaped, we use the point where the rays from the center of the object meet with the outer sphere.\nThen, outer sphere is mapped with parameterization.\nTexture mapping can also be created by flattening surfaces.\nThe goal is to minimize distortion; i.e. polygon size shouldn't change a lot.\nTextures other than colors\nRecall) Phong Illumination Model Iout=kaIa+Ilightd2[kdmax⁡(N⋅L,0)+ks(R⋅V)n]I_{out} = k_aI_a + \\frac{I_{light}}{d^2} [k_d \\max(N \\cdot L, 0) + k_s (R \\cdot V)^n]Iout​=ka​Ia​+d2Ilight​​[kd​max(N⋅L,0)+ks​(R⋅V)n]\nInstead of storing just color kdk_dkd​, we can store other material properties, such as ka,ks,nk_a, k_s, nka​,ks​,n.\nBump mapping\n\nHow do we make bumpy surface?\n\nReal bump: Model the surface with many small polygons.\nFake bump: Replace normal vectors before the shading calculation.\n\nWe use bump map to store the amount of perturbation in the normal direction, (i.e. height variation) then compute normal vectors on the fly based on perturbed geometry.\nUnderlying object is still smooth, but shading looks like bumpy!\nNormal mapping\nCan't we just save a normal vector?\nSurface normals are directy saved in RGB channel of texture &quot;image&quot;.\nDisplacement mapping\nBump/Normal mapping doesn't change the actual geometry, so the deception can easily be broken.\ne.g. boundary line, shadow is too smooth\nInstead, we can just store actual displacement from underlying geometry into a texture.\nWe just move pixel directly, so rendering is accurate.\nProblem: If geometry resolution is too low, aliasing happens.\nEnvironment mapping\nGoal: How do we render reflection (without ray tracing)?\nWe compute the reflect vector (using the normal vector and camera view vector). Then, we use the 3D environment texture to determine which color should be shown based on the reflect vector.\nProcedural Textures\nWe only need a texture mapping f:(x,y,z)→colorf: (x,y,z) \\rightarrow \\text{color}f:(x,y,z)→color.\nDo we really need to store into a image?\nIdea: Directly store texture mapping as a function!\nPros: Easy to implement, more compact than texture maps, have infinite resolution (i.e. textures are not affected by image resolution)\nCons: Unintuitive, difficult to match existing texture\nPerlin Noise\nA pseudo random function with smoothness f:Rn→Rf: R^n \\rightarrow Rf:Rn→R.\nHollywood movies used perlin noise a lot! Perlin was awarded an Oscar (Academy Award for Technical Achievement) for his work.\nIdea: Generate random values at grid point, then interpolate smoothly between these values.\nIn 2D, interpolating random value is hard. Instead, we assign a random gradient and compute noise levels by interpolating them.\nPerlin noise texture\nInstead of using one noise, we can use a mixture of noise with different scales. (i.e. frequency) A scale is also called an octave.\n\n∑f1fkPNoise⁡(f,x,y)\\sum_f \\frac{1}{f^k} \\operatorname{PNoise}(f,x,y)∑f​fk1​PNoise(f,x,y) looks like cloud\nsin⁡(ax+by+∑f1fk∣PNoise⁡(f,x,y)∣)\\sin \\left( ax + by + \\sum_f \\frac{1}{f^k} \\left| \\operatorname{PNoise}(f,x,y) \\right| \\right)sin(ax+by+∑f​fk1​∣PNoise(f,x,y)∣) looks like marble\nsin⁡(a∣x2+y2∣+∑f1fk∣PNoise⁡(f,x,y)∣)\\sin \\left( a \\left| x^2 + y^2 \\right| + \\sum_f \\frac{1}{f^k} \\left| \\operatorname{PNoise}(f,x,y) \\right| \\right)sin(a​x2+y2​+∑f​fk1​∣PNoise(f,x,y)∣) looks like wood\n\nWhy? Nobody knows... These are just experimental values.\nReaction-Diffusion\nMake texture with diffsuion!\ne.g. Only prepare texture of head and foot of zebra, then use diffusion to fill rest of texture.\nTexture Synthesis\nUse 2D reference texture and 2D target texture to change 3D reference texture to 3D target texture.\nWe find point on the 2D reference texture, then use corresponding point on the 2D target texture.\nShadows\nShadows are essential for rendering photorealistic images.\nThere are many things to consider:\n\nUmbra and penumbra (How many light sources are watching a point?)\nSoft shadows and hard shadows (point light source at a distance vs. area light source nearby)\nObject-to-object shadows\n\nPoint light sources make sharp boundaries. Probably we should use sth like Gaussian blur?\nPlanar Projection Shadows\nProject object to the ground plane, and just draw black object at ground plane.\nProblem: If surface is not flat, computing projection is hard.\nShadow map\nRender twice!\n\nRender from the light source to compute Z-buffer.\nRender from the eye to compute Z-buffer.\nIf depth from light and eye is differnet, this point should be rendered as shadow.\n\nCan handle moving objects and lights, and multiple light sources.\nShadow volume\n\nCompute intersection of shadow volume with view frustum.\n\nCompute shadow volume: Invisible area behind polygon from point light source.\nUse stencil buffer to determine whether we are inside or outside of the intersection.\n+1 if we pass the front face of the intersection.\n-1 if we pass the back face of the intersection.\nIf stencil buffer is higher than 0, render as a shadow.\n\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Color","url":"/posts/77/","content":"Physical properties of color\n\nLights: Things that emit energy to surroundings\nMaterial: Things that absorb or reflect a portion of incident energy\n\nLight\nTypes of energy being transferred determines the color of light source.\nVisible spectrum: 400nm ~ 700nm\nWhite is defined as a color of the sunlight.\nIt is actually a mixture of all visible frequencies.\nEmission Spectrum\n\nLight sources have different emission spectrum.\nAbsorption Spectrum\n\nMaterials have absorption spectrum.\nReflectance spectrum is also possible. (1 - absorption)\nWhen a light hits a material, the energy existing for each wavelength will be absorbed proportionally to the material's reflectance spectrum. Φ(λ)=I(λ)R(λ)\\Phi(\\lambda) = I(\\lambda)R(\\lambda)Φ(λ)=I(λ)R(λ)\nColor mixing\n\nAdditive Color Mixing: Add emission spectra; e.g. multiple lights are added\nSubtractive(Multiplicative) Color Mixing: Multiply reflectance spectra; e.g. multiple paints(materials) are added\nSince any number multiplied by 0 is 0, only colors reflected by both materials will be shown.\n\nThe Human Visual/Perception System\nThe human eye is a camera! in fact it is opposite\n\n\nAperture: Pupil(동공/눈동자)\n\n\nLens: Lens(수정체)\n\n\nFilm/Sensor: Retina(망막)\n\n\nFocal point: Fovea centralis(중심와)\n\n\nMean of spectrum determines the hue (color).\n\n\nVariance of spectrum determines the saturation.\n\n\nArea of spectrum determines the brightness.\n\n\nRetina\n\nRods are primary receptors under dark viewing conditions. They can perceive brightness on a scale of 101010^101010.\nCones are the primary\nreceptors under high-light viewing conditions. They can perceive color.\n\nUnder dark viewing conditions, only rods work (Rod vision), so we can't perceive color very well.\nIf environment becomes brighter, cones can work (Cone vision), allowing us to perceive color.\nDistribution of Rods and Cones\n\nThere are 120 million rods and 6~7 milion cones in the human eye. Rods are much more common than cones!\nHowever, at fovea, cones are much more common than rods.\ne.g. Night sky have more stars off-center, because there are less rods at fovea.\ne.g. If object is far from fovea, we can't perceive color.\nSpectral Response of Cones\n\nThere are three types of cones: S, M, L.\nWe have more L-type cones than other types because longer wavelengths have lower energy.\nDespite the large number of L-type cones, the most sensitive cones are M-type.\nTristimulus values\nWe can define tristimulus values (S,M,L) with three types of cones!\nS=∫λI(λ) S(λ) dλM=∫λI(λ) M(λ) dλL=∫λI(λ) L(λ) dλ\\begin{align*}\nS &amp;= \\int_\\lambda I(\\lambda)\\,S(\\lambda)\\,d\\lambda\\\\\nM &amp;= \\int_\\lambda I(\\lambda)\\,M(\\lambda)\\,d\\lambda\\\\\nL &amp;= \\int_\\lambda I(\\lambda)\\,L(\\lambda)\\,d\\lambda\n\\end{align*}SML​=∫λ​I(λ)S(λ)dλ=∫λ​I(λ)M(λ)dλ=∫λ​I(λ)L(λ)dλ​\nThe brain receives light as a electric signals of tristimulus values.\nMetamers\nIf two different spectra have same tristimulus values, we perceive them as the same color.\ne.g. we can detect counterfeit currency by using special inks that appear to be the same color but have different appearance under UV light.\nProbably we can use metamers to reduce the difference between the display and the printed image?\nColor space\nRGB color space\n\nMotivation: color matching experiment!\nCan we match target light by mixing primary lights?\nIn fact, we can't!!! Sometimes, we need to add light to target light to match two colors.\nThis can be viewed as mixing negative amount of colors???\ne.g. To match 500nm light, we need to mix blue, green, and negative amount of red light.\nGrassmann's Law\nFor color matches,\n\nSymmetry: U=V⇔V=UU = V \\Leftrightarrow V = UU=V⇔V=U\nTransitivity U=V∧V=W⇔U=WU = V \\land V = W \\Leftrightarrow  U = WU=V∧V=W⇔U=W\nProportionality U=V⇔tU=tVU = V \\Leftrightarrow tU = tVU=V⇔tU=tV\nAdditivity U=V∧W=X⇒U+W=V+XU = V \\land W = X \\Rightarrow U + W = V + XU=V∧W=X⇒U+W=V+X\n\ni.e. we can view color matching as an vector space!\nXYZ color space\n\nUsing grassmann's law, we define three imaginary primary colors X, Y, Z satisfying:\n\nAll visible light can be made by mixing positive amount of X, Y, Z.\nY corresponds to the luminance (perceived brightness).\nX, Z correspond to the chromaticity (color).\n\n[XYZ]=[0.490000.310000.200000.176970.812400.010630.000000.010000.99000][RGB][RGB]=13400850[8041697−3049000−1591847−1752003485100030185317697−490003432153][XYZ]\\begin{gather*}\n\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} = \\begin{bmatrix}\n0.49000 &amp; 0.31000 &amp; 0.20000\\\\\n0.17697 &amp; 0.81240 &amp; 0.01063\\\\\n0.00000 &amp; 0.01000 &amp; 0.99000\n\\end{bmatrix} \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} \\\\\n\\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} = \\frac{1}{3400850} \\begin{bmatrix}\n8041697 &amp; -3049000 &amp; -1591847 \\\\\n-1752003 &amp; 4851000 &amp; 301853 \\\\\n17697 &amp; -49000 &amp; 3432153\n\\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}\n\\end{gather*}​XYZ​​=​0.490000.176970.00000​0.310000.812400.01000​0.200000.010630.99000​​​RGB​​​RGB​​=34008501​​8041697−175200317697​−30490004851000−49000​−15918473018533432153​​​XYZ​​​\nxyY color space\nx=XX+Y+Z,y=YX+Y+Zx = \\frac{X}{X+Y+Z}, y = \\frac{Y}{X+Y+Z}\nx=X+Y+ZX​,y=X+Y+ZY​\nInstead of X, Y, Z, we use normalized version x, y, and luminance Y.\nz is not used because it can easily obtained from x, y. (z=1−x−yz = 1-x-yz=1−x−y)\nWe can restore original X, Y, Z values from x, y.\nX=Yyx,Z=Yyz=Yy(1−x−y)X = \\frac{Y}{y}x, Z = \\frac{Y}{y}z = \\frac{Y}{y}(1-x-y)\nX=yY​x,Z=yY​z=yY​(1−x−y)\n\nBecause x, y is normalized, only Y contains luminance information, and only x, y contains chromaticity information.\nTherefore, we can use xy plane to illustrate every visible color.\nPerceptually Uniform Space\nDoes L2 norm (i.e. eclidean distance) in color space reflect perceived differences between colors?\nxyY color space doesn't satisfy perceptually uniform color space.\nMacAdam ellipses specify areas of unperceivable differences.\nPeople can't distinguish between hue in certain areas and saturation in others.\nIf we can map MacAdam ellipses into circles, we can have perceptually uniform space.\nHowever, we need non-linear mapping.\ne.g. CIE LAB Color Space use the color of the reference white (X0,Y0,Z0)(X_0, Y_0, Z_0)(X0​,Y0​,Z0​) to calculate L, a, b values.\nL represents lightness, and a, b represents color opponents.\nL=25(100YY0)13−16a=500[(XX0)13−(YY0)13]b=200[(YY0)13−(ZZ0)13]\\begin{align*}\nL &amp;= 25 \\left( 100\\frac{Y}{Y_0} \\right)^\\frac{1}{3} - 16 \\\\\na &amp;= 500 \\left[ \\left( \\frac{X}{X_0} \\right)^\\frac{1}{3} - \\left( \\frac{Y}{Y_0} \\right)^\\frac{1}{3} \\right] \\\\\nb &amp;= 200 \\left[ \\left( \\frac{Y}{Y_0} \\right)^\\frac{1}{3} - \\left( \\frac{Z}{Z_0} \\right)^\\frac{1}{3} \\right]\n\\end{align*}Lab​=25(100Y0​Y​)31​−16=500[(X0​X​)31​−(Y0​Y​)31​]=200[(Y0​Y​)31​−(Z0​Z​)31​]​\nColor Gamut\n\nReal devices can't use imaginary color like XYZ. We can only mix existing colors!\nIf we mix positive amount of n colors, we can represent every color inside the convex hull of the color positions.\nThis area - the range of colors that can be accurately represented - is called color gamut.\nsRGB is the most popular color gamut, using red, green and blue. But you can't represent pure red color in sRGB.\nCMYK is used in printers, and they use more than 3 colors because color gamut is too small if only 3 colors are used.\nColor gamut outside color space are not representable in devices.\nThey are used to define image format that can be converted to any other color gamut.\nReference white\nWhich color is &quot;white&quot;? We choose specific color as a pure white.\ne.g. D65: White similar to sunlight, color temperature is 6504K\nComplementary Colors\n\nDraw a straight line connecting the reference white and the color. The color on the other side is the complementary color.\nDominant Wavelength\n\nDraw a straight line connecting the reference white and the color. The point where the line intersects the spectral locus (the curve part) is the dominant wavelength of the color.\ni.e. The dominant wavelength is the hue of the color. (Pure color)\ni.e. This color can be made by mixing dominant wavelength and reference white.\nIf the line intersects the line of purples (the straight part), it is non-spectral. (i.e. no monochromatic (single frequency) light source can generate it)\nInstead, we define the intersection between the line and the spectral locus as a subtractive dominant wavelength (complementary wavelength) of the color.\ni.e. This color can be made by subtracting complementary wavelength from reference white.\nColor models\nColor space is the full range of colors we can choose from.\nColor model is the way a particular color in a color space is specified, which is relevant to color gamut.\n\nConvenience: User can easily choose the color they want\nColor compositing/processing: Color space matters when we interpolate/blend colors.\nEfficiency of encoding: We can use more of numberical range for perceptually significant colors.\n\nAdditive/Subtractive Color Models\n\nAdditive: Combines colored lights, e.g. RGB\nSubtractive: Combines paint colors, e.g. CMYK Actually multiplicative because we multiply colors\n\nRGB Color Model\n\n\nAdditive color model\nEasy for display devices\nVERY widely used\nNot perceptual (e.g. How do I make indigo?)\n\nRGB Color Encoding\nUsually RGB color is encoded in 8bpc (bits per color channel) hexadecimal values. e.g. #1B1F8A\nBut sRGB is already small, is 256 possible values per channel enough???\nHigh dynamic range (HDR) images use floating point number instead of 8-bit integer.\nDevices with high dynamic range are also available, but it is very expensive.\nCMYK Color Model\n\nSubtractive color model\nEasy for printing devices\nStill, not perceptual (e.g. When do we use black?)\n\nHSV Color Model\n\n\nAdditive color model\nIntuitive (Use Hue, Saturation (purity), Value (brightness) directly)\n\nActually made with RGB Color Model?\nWe can rotate the RGB cube to make it look like a hexagon, then turn it into a circle to create a HSV model.\nThe color around the perimeter of the circle is called the pure color.\nHSV Color Encoding\nHSV is also encoded in 8bpc.\nBut actually, RGB and HSV values are not 1:1 correspondences when divided by 256 equally spaced values!\nIn practice, we use certain algorithm to match RGB and HSV values.\nIntensity-based Color Models\nUse intensity (brightness) as Y, and use color as other two.\nWhy? Human are more sensitive to brightness.\nIf we use full resolution for intensity, and low resolution for colors, we can actually get similar image to the original!\n\n\nYIQ: Color model used by analog NTSC color TV systems, uses orange as I and magenta as Q.\nYCbCr: Color model used by digital videos and photography systems, uses blue as Cb and red as Cr.\n\nColor sensing device\nCamera\n\nAs light reflects, refracts, and passes through a prism, it splits into three branches.\nEach branch is headed toward the color sensor.\nColor sensor have cellophane-like color filter in front of it, so it can measure intensity of red, green, and blue.\nProfessional camera use this, but this make camera too large, and each sensor have to process 3 colors.\nBayer filter\n\nInstead, we normally use Bayer filter.\nIt divides single pixel into four subpixel, and each subpixel measure intensity of single color.\nWe use 1 red, 1 blue, and 2 green sensors because humans are more sensitive to the green color.\nThe color of pixel is estimated from neighboring 8 subpixel's value.\nThis algorithm... is actaully trade secret!\nBy using different algorithms in bayer filter, we can make various filters. e.g. food, portrait, night\nWhite balance\nChromatic adaptation: Most white lights are not white!\ne.g. sunlight is blue, fluorescent lamp is yellow\nBut human eye adjust color based on light color, then perceive white paper as a white.\nProblem: Devices doesn't do that... Real picture taken by camera is not white!\nWe have to adjust color based on light color too!\nFirst, we find pixel that should be white, then we adjust color based on that pixel.\nColor Temperature\nWe use temperature as a color of the light sources!\ne.g. Sunrise/sunset's color temperature is 2000K, and sunlight in day's color temperature is 5500~6500K.\nTo make precise camera, we should measure color temperature of the light, and use it in white balance.\nBut most people just believe auto white balance\nHuman Perception\nHumans are more sensitive to the difference of stimulus rather than its absolute value.\nWeber-Fechner law: The subjective sensation is proportional to the logarithm of the stimulus intensity.\np=kln⁡(SS0)p = k \\ln\\left( \\frac{S}{S_0} \\right)\np=kln(S0​S​)\nColor Quantization Problem\nWe want to encode intensity level. Because of Weber-Fechner law, linear encoding doesn't feel like linear...\nHuman perceive intensity is linear if relative differences is constant.\nI1I0=⋯=InIn−1=γ,Ik=γkI0\\frac{I_1}{I_0} = \\cdots = \\frac{I_n}{I_{n-1}} = \\gamma, I_k = \\gamma^k I_0\nI0​I1​​=⋯=In−1​In​​=γ,Ik​=γkI0​\nGamma Correction\n\nLinear encoding can't differenciate darker intensity, and waste too much bits at lighter intensity.\nSolution: we use gamma function! Iencoded=I1γI_{encoded} = I^\\frac{1}{\\gamma}Iencoded​=Iγ1​\nWe don't use log because log gets crazy around 0.\nMost images (especially sRGB images) are encoded using gamma correction!\nTo get actual intensity of the image, we should apply inverse gamma function. I=IencodedγI = I_{encoded}^\\gammaI=Iencodedγ​\n2.2 is normally used because of historical reasons such as CRTs.\nBut gamma can be any value higher than 1, or even any value lower than 1.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Ray Tracing","url":"/posts/80/","content":"Global Illumination Models\nA surface point can receive light from various sources such as light sources and other objects.\n\nRay tracing\nRadiosity\nPhoton mapping\n\nRecall) Local illumination model: A surface point receives light directly from light sources, no shadows, reflection, transmission.\nRay Tracing\nForward (Real World) Ray Tracing\n\nDefine rays as paths of photons in world space\nFollow photon from light sources to viewer\nMaybe... phton can reach the viewer?\n\nBackward Ray Tracing\n\nTrace rays backward from viewer to light sources!\nFor every pixel, we cast ray from camera to pixel\nWhenever ray hits objects, we always consider ray to light sources, then add ambient/diffuse/specular colors.\n\nChecking intersection between ray and objects is the most expensive operation! (about 95%)\n\nCompute plane containing triangle\nCompute the closest intersection between ray and plane\nCheck whether intersection is inside triangle\n\nWe need to do this for every pixel and every polygons...\nThis only computes local illumination model!\nWe can also compute shadow, reflection, and refraction.\nTo do this, we need to compute more rays, and more intersections..\nShadow Rays\nHow do we determine if light really hits the surface point?\nSolution: Cast shadow ray from surface point to every light sources!\nIf shadow ray hits opaque object, (i.e. have intersection) light source can't be seen.\nUnlike backward ray tracing, we don't need closest interaction, so we can stop at first intersection.\nPrecision problems\n&quot;Surface point&quot; may slightly beneath the surface due to numerical precision.\nThis causes the surface to cast a shadow on itself.\nSolution: Move the intersection point by epsilon along the surface normal to position it outside the object.\nReflection\n\nCast ray from surface point towards the reflection.\nMultiply color by reflection coefficient.\n\nR=V−2(V⋅N)NR = V - 2(V \\cdot N)N\nR=V−2(V⋅N)N\nTransparency (Refraction)\n\nCast ray from surface point towards the refraction.\nMultiply color by transparency coefficient.\n\nLooks same... but refraction is way harder than reflection.\nSnell's Law\nηisin⁡θi=ηrsin⁡θr\\eta_i \\sin\\theta_i = \\eta_r \\sin\\theta_r\nηi​sinθi​=ηr​sinθr​\nWhere ηi,ηj\\eta_i, \\eta_jηi​,ηj​ is the index of refraction of each material.\nWe define η=ηiηr=sin⁡θrsin⁡θi\\eta = \\frac{\\eta_i}{\\eta_r} = \\frac{\\sin\\theta_r}{\\sin\\theta_i}η=ηr​ηi​​=sinθi​sinθr​​\n\nM is an arbitrary tangential unit vector.\nL=Ncos⁡θi−Msin⁡θiM=Ncos⁡θi−Lsin⁡θiT=−Ncos⁡θr+Msin⁡θr=−Ncos⁡θr+(Ncos⁡θi−L)sin⁡θrsin⁡θi=(ηcos⁡θi−cos⁡θr)N−ηL=(ηcos⁡θi−1−sin⁡2θr)N−ηL=(ηcos⁡θi−1−η2sin⁡2θi)N−ηL=(ηcos⁡θi−1−η2(1−cos⁡2θi))N−ηL\\begin{align*}\nL &amp;= N \\cos\\theta_i - M \\sin\\theta_i \\\\\nM &amp;= \\frac{N \\cos\\theta_i - L}{\\sin\\theta_i} \\\\\nT &amp;= -N \\cos\\theta_r + M \\sin\\theta_r \\\\\n&amp;= -N \\cos\\theta_r + \\left( N \\cos\\theta_i - L \\right)\\frac{\\sin\\theta_r}{\\sin\\theta_i} \\\\\n&amp;= \\left( \\eta\\cos\\theta_i - \\cos\\theta_r \\right)N - \\eta L \\\\\n&amp;= \\left( \\eta\\cos\\theta_i - \\sqrt{1 - \\sin^2\\theta_r} \\right)N - \\eta L \\\\\n&amp;= \\left( \\eta\\cos\\theta_i - \\sqrt{1 - \\eta^2\\sin^2\\theta_i} \\right)N - \\eta L \\\\\n&amp;= \\left( \\eta\\cos\\theta_i - \\sqrt{1 - \\eta^2 \\left( 1 - \\cos^2\\theta_i \\right) } \\right)N - \\eta L \\\\\n\\end{align*}LMT​=Ncosθi​−Msinθi​=sinθi​Ncosθi​−L​=−Ncosθr​+Msinθr​=−Ncosθr​+(Ncosθi​−L)sinθi​sinθr​​=(ηcosθi​−cosθr​)N−ηL=(ηcosθi​−1−sin2θr​​)N−ηL=(ηcosθi​−1−η2sin2θi​​)N−ηL=(ηcosθi​−1−η2(1−cos2θi​)​)N−ηL​\nProperties of refraction\n\nSideness: Whether light enters or leaves the material is important\nTotal Internal Reflection: After the critical angle θr=90∘\\theta_r = 90^\\circθr​=90∘, entire lights are reflected, and no refraction occurs.\nWavelength-dependent: Refraction increases as the wavelength of light decreases. (The reason why rainbows happen)\nBut usually ignored in computer graphics...\nRay Tracing Algorithm\ndef check_nearest_intersection(ray, objects):  dist_min = INFINITY  hit_min = NULL  for obj in objects    hit = intersect(obj, ray)    d = dist(obj, hit)    if d &lt; dist_min:      d = dist_min      hit_min = hit  return hit_mindef trace_ray(ray, objects, lights):  hit = check_nearest_intersection(ray)  color = BLACK  if hit != NULL:    color = ambient * hit.diffuse_color() # add ambient color    for light in lights:      shadow_ray = Ray(hit, light) # ray from hit to light      # we don&#x27;t need nearest intersection, but we reuse algorithm      # For faster implementation, we can modify this function to early return on any intersection      hit2 = check_nearest_intersection(ray)      if dist(hit, hit2) == dist(hit, light):        # path to the light exists, add diffuse &amp; specular colors        color += local_shade(ray, hit, light, light.color())      # reflection      reflect = get_reflection(ray, hit)      reflect_ray = Ray(hit, reflect)      reflect_color = trace_ray(reflect_ray, objects, lights)      # refraction      refract = get_refraction(ray, hit)      refract_ray = Ray(hit, refract)      refract_color = trace_ray(refract_ray, objects, lights)      color += hit.reflect_factor * reflect_color + hit.refract_factor * refract_colordef ray_tracing(camera, objects, lights):  for pixel in pixels:    ray = Ray(camera, pixel) # ray from camera to pixel    pixel.color = trace_ray(ray, objects, lights)\nReflection and refraction can be impelemented in recursive function.\nStopping criteria can be recursion depth or ray contribution (when reflected/refracted contribution becomes too small)\nAdvanced Ray Tracing\nFresnel factor\nInstead of using constant coefficients for reflection/refraction, we can use fresnel factor.\nSchlick's approximation use angle between incoming ray and surface normal.\nkfresnel(θ)=kfresnel(0)+(1−kfresnel(0))(1−cos⁡θ)5k_{fresnel}(\\theta) = k_{fresnel}(0) + \\left( 1 - k_{fresnel}(0) \\right) (1 - \\cos\\theta)^5\nkfresnel​(θ)=kfresnel​(0)+(1−kfresnel​(0))(1−cosθ)5\nMonte Carlo Ray Tracing\nNo surface is a perfect mirror!\nNon-ideal reflection/refraction have many reflection/refraction rays.\nWe can use Monte Carlo method!\nInstead of 1 reflection/refraction ray, we use random reflection/refraction direction, and multiple reflection/refraction rays.\nWe can sum average of reflection/refraction colors.\nIf you're ambitious, you can use BRDF!\nSoft shadows\nPoint light source only have one shadow ray, making hard shadows.\nFor realistic shadows, we use area light source to make soft shadows with umbra and penumbra.\nMonte Carlo method again!\nInstead of 1 shadow ray, we use random shadow rays pointing to area light source.\nSupersampling\nSort of anti aliasing!\nInstead of 1 ray per pixel, we divide pixel into subpixels, then shoot 1 ray per subpixel.\nShooting randomly within each subpixel renders better than shooting at the center of the subpixel.\nMotion Blur\n\nActual camera behavior!\nTo implement this, we can sample objects temporally over time interval, then mix those images.\nDepth of Field\n\nAnother actual camera behavior!\nChanging the aperture size affects depth of field.\nA smaller aperature increases the range in which the object is approximately in focus.\nWe can implement this with ray tracing???\nJust simulate lens behavior...\nCaustics\n\nCaustics represents some of the most visually stiking patterns of light in nature.\nIt is formed by light that is reflected or transmitted by a number of specular surfaces before interacting with a diffuse surface.\nThis is difficult to implement because the probability of a ray interacting with multiple specular surfaces and then with a diffuse surface is very low...\nWhite noise, blue noise\nWhite noise randomly sample points. (Frequency is uniform)\nBlue noise trys to keep sample points a certain minimum distance from each other. (Certain frequencies are dominant)\nThere are many different colored noise!\nUsing different noise in Monte Carlo method yields different rendering results.\nMaybe blue noise can give better results than white noise?\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Spatial Data Structure","url":"/posts/83/","content":"Problem\nRay tracing is too slow. We need to accelerate it!\nTo make ray tracing faster, we need to solve this problem:\nGiven a scene defined by a set of NNN primitives and a ray r(t)\\mathbf{r}(t)r(t), find the closest point p\\mathbf{p}p of intersection of r(t)\\mathbf{r}(t)r(t) with the scene.\nBounding Volume\nIdea: Precompute conservative but smallest bounding volume that includes all primitives, then early reject if ray does not hit the box.\nProblem: In the worst case, we still need to examine all primitives.\nBounding volume should be tight to avoid false positives, and ray-volume intersection should be fast enough.\n\nAxis-aligned bounding box\nBounding box (Slower)\nBounding sphere\nBounding half-space (any convex polygon)\n\nAxis-aligned bounding box (AABB)\nBox can be represented as ((xmin,ymin,zmin),(xmax,ymax,zmax))\\left( \\left( x_{min}, y_{min}, z_{min} \\right), \\left( x_{max}, y_{max}, z_{max} \\right) \\right)((xmin​,ymin​,zmin​),(xmax​,ymax​,zmax​)).\nHow do we compute intersection with ray r(t)=o+td\\mathbf{r}(t) = \\mathbf{o} + t\\mathbf{d}r(t)=o+td?\nEfficient Ray-AABB Intersection\n\nFor all 6 faces, compute all intersections.\ne.g. Let's test intersection with x=xminx=x_{min}x=xmin​.\nxmin=nx⋅r(tmin,x)=nx⋅(o+tmin,xd)=(1,0,0)⋅(o+tmin,xd)∴tmin,x=xmin−oxdx\\begin{align*}\nx_{min} &amp;= \\mathbf{n}_x \\cdot \\mathbf{r}\\left( t_{min, x} \\right) \\\\\n&amp;= \\mathbf{n}_x \\cdot \\left( \\mathbf{o} + t_{min, x}\\mathbf{d} \\right) \\\\\n&amp;= (1, 0, 0) \\cdot \\left( \\mathbf{o} + t_{min, x}\\mathbf{d} \\right) \\\\\n\\therefore t_{min, x} &amp;= \\frac{x_{min} - \\mathbf{o}_x}{\\mathbf{d}_x}\n\\end{align*}xmin​∴tmin,x​​=nx​⋅r(tmin,x​)=nx​⋅(o+tmin,x​d)=(1,0,0)⋅(o+tmin,x​d)=dx​xmin​−ox​​​\nNow we have the interval [tmin,x,tmax,x]\\left[ t_{min, x}, t_{max, x}\\right][tmin,x​,tmax,x​] that overlaps the box along x axis.\nWe do the same for y and z axes, then we compute the intersection of these intervals.\n[t_enter, t_exit] = [-∞, +∞]for each dimension k:    invD  = 1 / d_k    t_min = (k_min - o_k) * invD    t_max = (k_max - o_k) * invD    if t_min &gt; t_max:        t_min, t_max = t_max, t_min # swap    t_enter = max(t_enter, t_min)    t_exit  = min(t_exit, t_max)if t_enter &gt; t_exit:    return -1 # box missedelif t_exit &lt; 0:    return -1 # box is behind the rayelif t_enter &lt; 0:    return t_exit # closest intersectionelse:    return t_enter # closest intersection\nBounding Volume Hierarchy (BVH)\n\nBuild a binary tree, then find the bounding box of each node.\nFinding AABB of AABBs is really easy! Just compute min and max coordinates.\nWe traversal node only if its bounding box intersects the ray.\nIf the bounding boxes of multiple child nodes intersect with the ray, we must traverse each of those nodes.\nSplitting objects\nThere is no one correct answer!\nDifferent strategies may work better for different types of geometry/different distributions of primitives.\nAlso, we cannot handle every worst case!\ne.g. All primitives shares same centriod, all primitives have same minimal AABB, ...\nWe use heuristics that can handle most cases.\n\nSplit at midpoint of the current volume\nSort objects, then put half of objects on each side.\nUse modeling hierarchy as a bounding volume hierarchy. (Actuall works well)\nUse surface area as a heuristic\n\nAssume rays are randomly distributed, and there are no occlusion. (i.e. Objects doesn't block other objects)\nThe probability that a ray will hit an object is proportional to the object's surface area.\n\n\n\nBVH Pros and Cons\n\nEasy to construct\nEasy to traverse\nUse simple data structure (binary tree)\nFinding a good split for a node is difficult\nPoor split may result in minimal spatial pruning\n\nK-D Tree\n\nRecursively partition space via axis-aligned partitioning planes.\nNodes can be traversed in front-to-back order!\n\nUnfortunately, traversing space in front-to-back order does not guarantee that primitives will be traversed in front-to-back order.\nIn general, we cannot early exit after first hit is found.\nHowever, if object-partition inclusion relationship is uniquely determined, (i.e. no object overlaps more than one partition) we can early exit!\nThis is the algorithm that is currently being used in hardware!\nMost ray tracing is done with the K-D Tree algorithm.\nK-D Tree Algorithm\nNode\n\nEach node has two child nodes, axis to split (e.g. dimSplit=0 means x aixs), split distance, and the list of primitives if the node is the leaf node.\nConstruction\n\nStart with axis-aligned bounding box of the scene.\nDecide which dimension to split (e.g. longest)\nDecide at which distance to split (e.g. half? not so easy to determine)\nDistribute primitives to each side. If a primitive overlaps split plane, assign it to both sides.\nRepeat until stopping criteria reached (e.g. minimum number of primitives in node)\n\nTraversal\n\nGet intersection from parent bounding box and the current spliting plane.\nIf the ray does not hit parent bounding boxes, ignore this node.\nIf the ray does hit parent bounding box,\n\nTraversal left child node if tenter&lt;texit&lt;tt_{enter} &lt; t_{exit} &lt; ttenter​&lt;texit​&lt;t.\nTraversal both child node if tenter&lt;t&lt;texitt_{enter} &lt; t &lt; t_{exit}tenter​&lt;t&lt;texit​.\nTraversal right child node if t&lt;tenter&lt;texitt &lt; t_{enter} &lt; t_{exit}t&lt;tenter​&lt;texit​.\n\nUniform Grid\n\nPartition space into equal sized volumes. (called volume-elements or voxels)\nEach grid cell contains primitives that overlap voxel.\n\nLike K-D Tree, we can traversal volume in front-to-back order.\nVery efficient because line rasterization algorithm can be used!\nConstructing is easy\nTeapot in a stadium problem: If primitive distribution is non-uniform, (e.g. high-resolution object such as teapot end up in one grid cell) uniform grid won't work.\n\nGrid size heuristic\nToo few grid cells degenerates to brute-force approach.\nToo many grid cells incur significant cost traversing through cells with empty space.\nWe can choose number of voxels proportional to the total number of primitives.\nAssuming uniform distrubtion, the intersection cost is O(n3)O(\\sqrt[3]{n})O(3n​).\nQuadtree/Octree\n\nQuadtree/Octree has greater ability to adapt to location of scene geometry than uniform grid in 2D/3D.\ni.e. It can handle some non-uniform primitive distribution.\nBut it has lower intersection performance than K-D tree.\ni.e. It has limited ability to adapt.\nEach node in quadtree/octree can have at most 4/8 child nodes, divided by the plane on each axis.\nIf node have enough primitives, recursively divide into 4/8 child nodes.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Animation","url":"/posts/90/","content":"History of Animation\nAncient Egypt, Leonardo da Vinci, ...\nLots of people drew movement as a series of frame!\nPhenakistoscope: Draw frames on the disk, then spin it to animate it!\nIn fact we have a 3D version of phenakistoscope, called zoetrope!\nThe first film captured movement of the horse!\nMultiple cameras were placed in the path of the horse.\nIn fact this was only a set of photos, not a real animation.\nThe first animation on film was made by hand!\nPhotos of chalkboard drawing were taken and animated. (about 15 min)\nThe first feature-length animation (about 1.5 hour) was made with pictures of paper.\nThe first hand-drawn feature-length animation was made by Disney. (Snow White)\nAlthough some people still use hand-drawn animation, most people use computer-generated animation nowadays.\nThe first computer-generated animation was made with math functions.\nIn 1970s, hand and face was animated using polygon mesh and Phong shading.\nThe first CG feature film was made by Pixar. (Toy Story)\nComputer animation is the standard for modern animation!\nComputer Animation\n\nArtist-directed (e.g. keyframing)\nData-driven (e.g. motion capture)\nProcedural (e.g. simulation)\nAI based?\n\nHistorically, hand-drawn animation was made by senior artists and apprentices.\nSenior artist draw important frames, and apprentice draws inbetweens.\nThe important frames is called keyframe!\nKeyframing\nSpecify important events only, computer fills in the rest via interpolation/approximation.\nEvents can be anything! e.g. position, color, materials, light intensity, camera zoom, ...\nHow do we interpolate? Use spline!\nCharacter Animation\nCharacter often have sophisticated rigs.\nKeyframe only set rigs, then character is made from rigs by computer.\nBlend shapes interpolate directly between surfaces instead of skeleton!\nFacial expressions are usually made with blend shapes.\nBlend shapes can be also made automatically with 4D scanning and machine learning.\nMotion Capture\nUse lots of calibrated cameras! (From 6~7 cameras to 200 cameras!)\nEach camera have light sources (e.g. visible red, near infra red) and filter that only allows that particular light.\nThen each camera can only see markers as a white dot!\nBy examining which camera saw which markers, we can reconstruct model and animation.\nIn fact, motion capture is rarely used...\nIt is only used when we have to create a new data.\nMostly we just gather data from internet and use machine learning.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Radiosity and Photon Mapping","url":"/posts/86/","content":"Radiometry\nRadiometry is a set of techniques for measuring electromagnetic radiation, including visible light.\n\nMany physcial processes convert energy into photons. (e.g. incandescent lightbulb turns heat into light)\nEach photon carries a small amount of energy.\nBrightness is determined by energy of photons hitting an object. We need this information to make accurate images!\n\nSolid angle\n\nAngle is a ratio of subtended arc length on circle to radius. θ=lr\\theta = \\frac{l}{r}θ=rl​\nCircle has 2π radians.\nSolid angle is a ratio of subtended area on sphere to radius squared. Ω=Ar2\\Omega = \\frac{A}{r^2}Ω=r2A​\nSphere has 4π steradians.\nDifferential solid angle\n\ndA=(rdθ)(rsin⁡θ dϕ)=r2sin⁡θ dθ dϕ∴dω=dAr2=sin⁡θ dθ dϕ∴Ω=∫S2dω=∫02π∫0πsin⁡θ dθ dϕ=4π\\begin{align*}\ndA &amp;= (r d\\theta)(r \\sin\\theta \\, d\\phi) \\\\\n&amp;= r^2 \\sin\\theta \\, d\\theta \\, d\\phi \\\\\n\\therefore d\\omega &amp;= \\frac{dA}{r^2} = \\sin\\theta \\, d\\theta \\, d\\phi \\\\\n\\therefore \\Omega &amp;= \\int_{S^2} d\\omega \\\\\n&amp;= \\int_0^{2\\pi} \\int_0^\\pi \\sin\\theta \\, d\\theta \\, d\\phi \\\\\n&amp;= 4\\pi\n\\end{align*}dA∴dω∴Ω​=(rdθ)(rsinθdϕ)=r2sinθdθdϕ=r2dA​=sinθdθdϕ=∫S2​dω=∫02π​∫0π​sinθdθdϕ=4π​\nNote that smaller θ makes smaller area!\nThat's why we have sin⁡θ\\sin\\thetasinθ term.\nWe'll use ω\\omegaω as a unit length direction vector, so ωdω\\omega d\\omegaωdω denotes differential solid angle in the direction of the direction vector.\nRadiant Energy\nRadiant energy is the total number of hits that occur anywhere in the scene, over the complete duration of the scene.\nEnergy of a signle photon is Q=hcλ[J]Q = \\frac{hc}{\\lambda} \\left[ J \\right]Q=λhc​[J].\nRadiant Flux\nRadiant flux is the total number of hits per second.\nΦ=dQdt[Js=W]\\Phi = \\frac{dQ}{dt} \\left[ \\frac{J}{s} = W \\right]\nΦ=dtdQ​[sJ​=W]\nOr we can think in other way: integral of flux is the total radiant energy.\nQ=∫t0t1Φ(t)dtQ = \\int_{t_0}^{t_1} \\Phi(t)dt\nQ=∫t0​t1​​Φ(t)dt\nRadiant energy density\nRadiant energy density is the total number of hits per unit area.\nu=dQdA[Jm2]u = \\frac{dQ}{dA} \\left[ \\frac{J}{m^2} \\right]\nu=dAdQ​[m2J​]\nIrradiance\nIrradiance is the total number of hits per second, per unit area.\nE(p)=dΦ(p)dA[Wm2]E(p) = \\frac{d\\Phi(p)}{dA} \\left[ \\frac{W}{m^2} \\right]\nE(p)=dAdΦ(p)​[m2W​]\nImage generation estimates the irradiance at each point of an image!\nTo be precise, we estimate the total radiant flux per pixel.\nSpectral Power Distribution\n\nSpectral power distribution describes the irradiance per unit wavelength.\nRadiance\nRadiance is the solid angle density of irradiance.\nL(p,ω)=dEω(p)dω[Wm2sr]L(p, \\omega) = \\frac{dE_\\omega(p)}{d\\omega} \\left[ \\frac{W}{m^2sr} \\right]\nL(p,ω)=dωdEω​(p)​[m2srW​]\nEω(p)E_\\omega(p)Eω​(p) is an irradiance when differential surface area is oriented to face in the direction ω\\omegaω.\nRadiance is the total number of hits per second, per unit area, per unit solid angle...!\nSurface Radiance\n\nNormally, surface is not oriented to ω\\omegaω...\nWe need cos⁡θ\\cos\\thetacosθ term!\nL(p,ω)=dEω(p)dω=dE(p)dωcos⁡θ=d2Φ(p)dA dωcos⁡θL(p, \\omega) = \\frac{dE_\\omega(p)}{d\\omega} = \\frac{dE(p)}{d\\omega\\cos\\theta} = \\frac{d^2\\Phi(p)}{dA \\, d\\omega\\cos\\theta}\nL(p,ω)=dωdEω​(p)​=dωcosθdE(p)​=dAdωcosθd2Φ(p)​\nSpectral Radiance\nThe total number of hits per second, per unit area, per unit solid angle, per unit wavelength??\nSpectral radiance describes the radiance per unit wavelength.\nIncident/Exitant radiance\n\nIn general, incident radiance and exitant radiance are different. We have to distinguish between them!\nLi(p,ω)≠Lo(p,ω)L_i(p, \\omega) \\neq L_o(p, \\omega)\nLi​(p,ω)=Lo​(p,ω)\nProperties of radiance\nRadiance is the energy along a ray defined by origin point p and direction ω\\omegaω.\nIt characterizes the distribution of light in an environment.\nRendering is all about radiance!\nPinhole camera model can measure radiance.\nRadiant Intensity\nRadiant Intensity is the power per solid angle emanating from a point source.\nI(ω)=dΦdω[Wsr]I(\\omega) = \\frac{d\\Phi}{d\\omega} \\left[ \\frac{W}{sr} \\right]\nI(ω)=dωdΦ​[srW​]\n\nInstead of constant intensity, we can use goniometric diagram that measures light intensity as a function of angle.\nComputing irradiance from radiance\nIrradiance can be computed from radiance.\nE(p)=∫H2Li(p,ω)cos⁡θ dωE(p) = \\int_{\\mathcal{H}^2} L_i(p, \\omega) \\cos\\theta \\, d\\omega\nE(p)=∫H2​Li​(p,ω)cosθdω\nFor most sources, the integral has already been computed.\nIf integral is not analytical, we should compute it numerically with Monte Carlo method.\nIrrandiance from a uniform hemispherical source\n\nE(p)=∫H2Li(p,ω)cos⁡θ dω=L∫02π∫0π2cos⁡θsin⁡θ dθ dϕ=Lπ\\begin{align*}\nE(p) &amp;= \\int_{\\mathcal{H}^2} L_i(p, \\omega) \\cos\\theta \\, d\\omega \\\\\n&amp;= L \\int_0^{2\\pi} \\int_0^{\\frac{\\pi}{2}} \\cos\\theta \\sin\\theta \\, d\\theta \\, d\\phi \\\\\n&amp;= L\\pi\n\\end{align*}E(p)​=∫H2​Li​(p,ω)cosθdω=L∫02π​∫02π​​cosθsinθdθdϕ=Lπ​\nIrrandiance from a uniform area source\n\nE(p)=∫H2Li(p,ω)cos⁡θ dω=L∫Ωcos⁡θ dω=LΩ⊥\\begin{align*}\nE(p) &amp;= \\int_{\\mathcal{H}^2} L_i(p, \\omega) \\cos\\theta \\, d\\omega \\\\\n&amp;= L \\int_\\Omega \\cos\\theta \\, d\\omega \\\\\n&amp;= L\\Omega^\\perp\n\\end{align*}E(p)​=∫H2​Li​(p,ω)cosθdω=L∫Ω​cosθdω=LΩ⊥​\nWhere Ω⊥\\Omega^\\perpΩ⊥ is a projected solid angle, an area of object (O) projected onto unit sphere (Ω\\OmegaΩ), then projected onto plane. (Ω⊥\\Omega^\\perpΩ⊥)\ndω⊥=∣cos⁡θ∣dωd\\omega^\\perp = \\left| \\cos\\theta \\right| d\\omega\ndω⊥=∣cosθ∣dω\nIrrandiance from a uniform disk source\n\nE(p)=LΩ⊥=L∫02π∫0αcos⁡θsin⁡θ dθ dϕ=Lπsin⁡2α\\begin{align*}\nE(p) &amp;= L\\Omega^\\perp \\\\\n&amp;= L \\int_0^{2\\pi} \\int_0^{\\alpha} \\cos\\theta \\sin\\theta \\, d\\theta \\, d\\phi \\\\\n&amp;= L\\pi \\sin^2\\alpha\n\\end{align*}E(p)​=LΩ⊥=L∫02π​∫0α​cosθsinθdθdϕ=Lπsin2α​\nAmbient Occlusion (AO)\n\nAssume spherical light source at infinity, then irradiance is now rotation, translation invariant.\nTherefore, we can pre-compute and bake irradiance into texture to enhance shading.\n\nEspecially we use AO in games!\nPrecomputing makes runtime efficient, so we can mimic global illumination in realtime!\nPhotometry\nRadiometry is about physical energy, photometry is about actual perception of light!\nPhotometry accounts for response of human visual system to electromagnetic radiation.\ne.g. Luminance integrate radiance over all wavelengths, weight by eye's luminuous efficacy curve. (sensitivity per frequency)\nY(p,ω)=∫0∞L(p,ω,λ)V(λ)dλY(p, \\omega) = \\int_0^\\infty L(p, \\omega, \\lambda)V(\\lambda) d\\lambda \nY(p,ω)=∫0∞​L(p,ω,λ)V(λ)dλ\nAll radiometric quantities have equivalents in photometry.\nUnfortunately, units of photometric quantities is not good...\n\n\n\nPhysics\nRadiometry\nPhotometry\nMKS\nCGS\nBritish\n\n\n\n\nEnergy\nRadiant Energy\nLuminous Energy\nTalbot\nTalbot\nTalbot\n\n\nFlux (Power)\nRadiant Power\nLuminous Power\nLumen\nLumen\nLumen\n\n\nFlux Density\nIrradiance (incoming)Radiosity (outgoing)\nIlluminance (incoming)Luminosity (outgoing)\nLux\nPhot\nFootcandle\n\n\nAngular Flux Density\nRadiance\nLuminance\nNit / Apostilb / Blondel\nStilb / Lambert\nFootlambert\n\n\nIntensity\nRadiant Intensity\nLuminous Intensity\nCandela\nCandela\nCandela\n\n\n\nnt=lxsr=cdm2=lmm2srnt = \\frac{lx}{sr} = \\frac{cd}{m^2} = \\frac{lm}{m^2sr}\nnt=srlx​=m2cd​=m2srlm​\nThe Rendering Equation\nRay tracing is not enough!\nIt miss out on small-scale effects, (e.g. diffraction, iridescence)\nand also miss out on large-scale effects. (e.g. bending of light due to gravity)\nRendering equation can compute radiance at a given point p, in a given direction ωo\\omega_oωo​.\nLo(p,ωo)=Le(p,ωo)+∫H2fr(p,ωi,ωo)Li(p,ωi)cos⁡θ dωiL_o(p, \\omega_o) = L_e(p, \\omega_o) + \\int_{\\mathcal{H}^2} f_r(p, \\omega_i, \\omega_o) L_i(p, \\omega_i) \\cos\\theta \\, d\\omega_i\nLo​(p,ωo​)=Le​(p,ωo​)+∫H2​fr​(p,ωi​,ωo​)Li​(p,ωi​)cosθdωi​\n\nLo(p,ωo)L_o(p, \\omega_o)Lo​(p,ωo​) is an outgoing radiance.\nLe(p,ωo)L_e(p, \\omega_o)Le​(p,ωo​) is an emitted radiance.\nfr(p,ωi,ωo)f_r(p, \\omega_i, \\omega_o)fr​(p,ωi​,ωo​) is a scattering function. (e.g. BRDF)\nLi(p,ωi)L_i(p, \\omega_i)Li​(p,ωi​) is an incoming radiance.\nθ\\thetaθ is an angle between incoming direction (ωi\\omega_iωi​) and surface normal\n\nProblem: To compute incoming radiance, you need to use rendering equation again. This is a recursive function!\nEstimating the rendering equation\nSolve the rendering equation with assumptions!\n\nAll surfaces are small, opaque, ideal diffuse reflectors.\nThe system is closed. (No extra energy can be in/out)\n\nLk=Ek+ρk∑i=1nLjFjkL_k = E_k + \\rho_k \\sum_{i=1}^n L_j F_{jk}\nLk​=Ek​+ρk​i=1∑n​Lj​Fjk​\nWe can assume radiance and scattering function differ only by surface.\nρk\\rho_kρk​ is the reflectivity factor for surface k, and FjkF_{jk}Fjk​ is the fractional amount of radiant energy from surface j that reaches surface k. (called the form factor)\nSince rendering equation is now a linear system, we can solve it!\nLk=Ek+ρk∑i=1nLjFjk(1−ρkFkk)Lk−ρk∑j≠knLjFjk=Ek[1−ρ1F11−ρ2F21⋯−ρnFn1−ρ2F121−ρ2F22⋯−ρnFn2⋮⋮⋱⋮−ρnF1n−ρnF2n⋯1−ρnFnn][L1L2⋮Ln]=[E1E2⋮En]\\begin{gather*}\nL_k = E_k + \\rho_k \\sum_{i=1}^n L_j F_{jk} \\\\\n(1 - \\rho_k F_{kk}) L_k - \\rho_k \\sum_{j \\neq k}^n L_j F_{jk} = E_k \\\\\n\\begin{bmatrix}\n1 - \\rho_{1}F_{11}   &amp; -\\rho_{2}F_{21}   &amp; \\cdots &amp; -\\rho_{n}F_{n1} \\\\[1ex]\n-\\rho_{2}F_{12}      &amp; 1 - \\rho_{2}F_{22} &amp; \\cdots &amp; -\\rho_{n}F_{n2} \\\\[1ex]\n\\vdots               &amp; \\vdots             &amp; \\ddots &amp; \\vdots         \\\\[1ex]\n-\\rho_{n}F_{1n}      &amp; -\\rho_{n}F_{2n}    &amp; \\cdots &amp; 1 - \\rho_{n}F_{nn}\n\\end{bmatrix} \\begin{bmatrix}\nL_{1} \\\\ L_{2} \\\\ \\vdots \\\\ L_{n}\n\\end{bmatrix} = \\begin{bmatrix}\nE_{1} \\\\ E_{2} \\\\ \\vdots \\\\ E_{n}\n\\end{bmatrix}\n\\end{gather*}Lk​=Ek​+ρk​i=1∑n​Lj​Fjk​(1−ρk​Fkk​)Lk​−ρk​j=k∑n​Lj​Fjk​=Ek​​1−ρ1​F11​−ρ2​F12​⋮−ρn​F1n​​−ρ2​F21​1−ρ2​F22​⋮−ρn​F2n​​⋯⋯⋱⋯​−ρn​Fn1​−ρn​Fn2​⋮1−ρn​Fnn​​​​L1​L2​⋮Ln​​​=​E1​E2​⋮En​​​​\nForm factor\nThe form factor can be calculated before solving the equation.\nFjk=1Aj∫surfj∫surfkcos⁡ϕjcos⁡ϕkπr2 dAk dAjF_{jk} = \\frac{1}{A_{j}} \\int_{surf_{j}} \\int_{surf_{k}} \\frac{\\cos\\phi_j \\cos\\phi_k}{\\pi r^2} \\, dA_{k} \\, dA_{j}\nFjk​=Aj​1​∫surfj​​∫surfk​​πr2cosϕj​cosϕk​​dAk​dAj​\nAjA_jAj​ is the area of the surface j.\nAgain, most cases in which the integral is analytical have already been computed.\nNote that this equation satisfies some properties!\n\nConservation of energy, ∀j,∑k=1nFjk=1\\forall j, \\sum_{k=1}^n F_{jk} = 1∀j,∑k=1n​Fjk​=1\nUniform light reflection, AjFjk=AkFkjA_jF_{jk} = A_kF_{kj}Aj​Fjk​=Ak​Fkj​\nOnly plane or convex patches, ∀j,Fjj=0\\forall j, F_{jj} = 0∀j,Fjj​=0\n\nWe can avoid computing integral by... rendering?\nRender from each surface j, then the fractional amount of pixel of surface k is FjkF_{jk}Fjk​!\nDifference between radiosity and ray tracing\nRay tracing is also sort of the estimate of the rendering equation!\nRay tracing tends to render sharp parts, while radiosity tends to render soft.\nAlso, radiosity renders surface less shiny.\nRadiosity is used in construction industry, especially making indoor footage.\nHowever, you can always use both method.\nPhoton mapping\n\nVarious effects such as caustics, BSSRDF is hard to implement using ray tracing or radiosity.\nWhy not simulate the light's behavior?\nPhoton mapping is two-pass global illumination algorithm.\nIt can effectively make various effects!\nFirst pass\nEmit photons randomly from the light sources throughout the scene.\nThe emitted photons are traced through the scene until they strike an object.\nBased upon a probability distribution for that object, phtons are reflected, transmitted or absorbed by the object.\nAt each intersection with an object, information about the photon's incident direction, incoming power, and the intersection point are stored into the photon map.\nSecond pass\nRender image using the photon map!\nCast rays into the scene, and estimate the radiance from the intersection points along the ray. (e.g. use k-nearest algorithm)\nThe estimated radiance is used in the calculation of the pixel color.\nPros and cons\nBasically modified version of ray tracing! If ray tracing is implemented, photon mapping is easy.\nHowever even millions of photon has a lot of aliasing.\nWe need to shoot billions of photon.... or use a good anti-aliasing methods.\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Dynamics","url":"/posts/97/","content":"Dynamics\nSimulation is the most popular method to create computer animation procedurally.\nUnlike kinematics, dynamics concerns forces and their effect on motion.\nAnimation Equation\nThe basic equation: F=maF=maF=ma??\n\nAny system has a configuration q(t)q(t)q(t)\nIt also has a velocity q˙=dqdt\\dot{q} = \\frac{dq}{dt}q˙​=dtdq​\nUsually have some constraints g(q,q˙,t)=0g(q, \\dot{q}, t) = 0g(q,q˙​,t)=0\n\n(q,q˙)(q, \\dot{q})(q,q˙​) is called the state of the system!\nTo change, we need to apply forces! q¨=Fm\\ddot{q} = \\frac{F}{m}q¨​=mF​.\nGeneralized Coordinates\nOften we describe systems with many moving pieces.\nqqq is the collection of the position of the objects.\nq=(x0,x1,…,xn)q = (x_0, x_1, \\ldots, x_n)\nq=(x0​,x1​,…,xn​)\nThen, we can think of qqq as a single point moving along a trajectory in RnR^nRn!\nGeneralized Velocity\nSimilarily, q˙\\dot{q}q˙​ is the collection of the velocity of the objects.\nq˙=(x˙0,x˙1,…,x˙n)\\dot{q} = (\\dot{x}_0, \\dot{x}_1, \\ldots, \\dot{x}_n)\nq˙​=(x˙0​,x˙1​,…,x˙n​)\nOrdinary Differential Equations (ODE)\nMany dynamical systems can be described via an ODE in generalized coordinates.\nOrdinary means only derivatives in time (but not space) is used.\nddtq=f(q,q˙,t)\\frac{d}{dt}q = f(q, \\dot{q}, t)\ndtd​q=f(q,q˙​,t)\nHigher order ODEs\nIn face, newton's law is second order ODE!\nq¨=Fm\\ddot{q} = \\frac{F}{m}\nq¨​=mF​\nHigher order ODEs can be written as first order ODE by introducing dummy variables.\nq˙=vv˙=Fm\\begin{align*}\n\\dot{q} &amp;= v \\\\\n\\dot{v} &amp;= \\frac{F}{m}\n\\end{align*}q˙​v˙​=v=mF​​\nTherefore, we can only focus on first order ODEs.\nLagrangian Mechanics\nLagrangian is defined as kinetic energy subtracted by potential energy. L=K−U\\mathcal{L} = K-UL=K−U\nDynamics can be given by Euler-Lagrange equation.\nddt∂L∂q˙=∂L∂q\\frac{d}{dt}\\frac{\\partial\\mathcal{L}}{\\partial{\\dot{q}}} = \\frac{\\partial\\mathcal{L}}{\\partial{q}}\ndtd​∂q˙​∂L​=∂q∂L​\nThis is a extreme way to write F=maF=maF=ma... Why do we even use this?\nNewton's law use coordinates, but lagrangian equation use energy.\nTherefore, it can be used for systems where it is difficult to define coordinates. e.g. Optics.\nExample - Pendulum\nUse angle as a coordinate! q=θq = \\thetaq=θ\nLet mmm is the mass, then we can calculate kinetic and potential energy.\nK=12Iω2=12m(Lθ˙)2U=mgh=−mgLcos⁡θ∴L=m(12L2θ˙2+gLcos⁡θ)\\begin{align*}\nK &amp;= \\frac{1}{2}I\\omega^2 = \\frac{1}{2}m(L\\dot{\\theta})^2 \\\\\nU &amp;= mgh = -mgL\\cos\\theta \\\\\n\\therefore \\mathcal{L} &amp;= m \\left( \\frac{1}{2}L^2\\dot{\\theta}^2 + gL\\cos\\theta \\right)\n\\end{align*}KU∴L​=21​Iω2=21​m(Lθ˙)2=mgh=−mgLcosθ=m(21​L2θ˙2+gLcosθ)​\nBy solving lagrangian equation, we get θ¨=−gLsin⁡θ\\ddot{\\theta} = -\\frac{g}{L}\\sin\\thetaθ¨=−Lg​sinθ!\nProblem: we don't have a closed form solution...\nChaotic systems\nIf we perturb input, output will change widely.\nEven Lagrangian equation can't help us...\nOnly numerical solution exists!\ne.g. Double pendulum\nPendulum swings from pendulum!\ne.g. n-Body problem\nIf n≥3n \\geq 3n≥3 objects interact with each other gravitationally, how does the objects move?\ne.g. Rigid body\nWe were assuming point mass so far!!!\nWhat happens if we drop the rigid body?\nEven with a small orientation difference, object will bounce in all directions.\nParticle Systems\nModel phenomena as a large collection of particles.\nEach particle has a behavior described by forces.\nThen particles are numerically computed!\nNormally, forces are only applied by k-nearest neighbors.\nFast hierarchical data structure (kd-tree, BVH) can be used!\ne.g. crowds, particle-based fluids, granular materials (such as snow), molecular dynamics, mass-spring system (clothes can be simulated with 2D spring!!!), hair, ...\nExtremely common in graphics/games!\nSimple equation + easy to scale up/down!\nExample - Flocking\nEach bird is a particle.\nUse simple forces:\n\nAttraction to center of neighbors\nRepulsion from individual neighbors\nAlignment toward average trajectory of neighbors\n\nTo simulate flocking, solve large system of ODEs!\n","categories":["SNU","4-1","컴퓨터그래픽스"]},{"title":"Overview","url":"/posts/34/","content":"What is internet?\nNuts and Bolts view\n\nBillions of connected computing devices (hosts)\nPacket switches that forawrds packets (routers, switches)\nCommunication links (fiber, copper, radio, satellite)\nNetworks: Collection of devices, routers, links that is managed by an organization\n\nInternet is a network of networks - interconnected Internet Service Providers (ISPs)\n\nProtocols that defines format and order of the messages sent and received\ne.g. HTTP, TCP, IP, Ethernet, etc.\nInternet standards\n\nRFC: Request for Comments\nIETF: Internet Engineering Task Force\n\n\n\n&quot;Services&quot; view\nInternet is an infrastructure that provides services to applications.\ne.g. web, streaming video, email, games, etc.\nInternet provides programming interface to distributed applications.\nA closer look at Internet\n\nNetwork edge: an end of network\nAlso called hosts: clients and servers\nAccess networks, physical media: communication links, either wired or wireless\nNetwork core: interconnected routers\nThis makes network of networks!\n\nAccess networks\nWired: uses cable, actually sometimes use telephone line\nUsually asymmetric! Download bandwidth is higher than upload bandwidth.\nWireless: connects via access point (AP)\nWireless local area network (WLAN): Typically within building (Wi-Fi)\nWide-area cellular access networks: Provided by mobile, cellular network operator (4G/5G)\nThere are some specialized networks like enterprise networks and data center networks!\nPackets\nHosts send application message in packets.\nPackets has fixed length - L bits.\nHost transmits packet into access network at transmission rate R. (Also called link transmission rate, link capacity, and link bandwith.)\nLinks\n\nGuided media: signals propagate in solid media such as copper, fiber, and coax.\nUnguided media: signals propagate freely e.g. radio.\n\nTwo key network-core functions\nForwadring\nLocal action (In same network): move arriving packets from router's input link to appropriate router's output link.\nUses local forwarding table!\nRouting\nGlobal action (Across different network): determine source destination paths taken by packets.\nUses routing algorithms!\nPacket switching\nstore-and-forward\nStore entire packet then transmit.\nMost routers use this because of data integrity.\ncut-through\nSend packet immediately!\nFaster than store-and-forward... but network delay is already negligible, so we don't use this.\nqueueing\nPackets arrive too fast... We can queue packets!\nIf queue is full, packets are dropped/lost. (It will later be detected by the higher layer, and they will resend the packet.)\nstatistical multiplexing\nMultiplexing: multiple input, one output\nDynamically allocate resources on-demand - If only 1 input is used, allocate resources only for it.\nMost efficient method!\nCircuit switching\nUsed in traditional telephone networks.\nEach link has n circuits. If host make a &quot;call&quot;, a circuit is dedicated to source and destination.\nThey can't be shared (waste of resources if circuit is not used), but guarantees circuit-like performance.\nFDM and TDM\nFrequency Division Multiplexing (FDM): Divide frequencies into frequency bands and allocate it.\nTime Division Multiplexing (TDM): Divide time into slots and allocate it.\nWhy use packet switching?\nUsers aren't active 100% of time!\nCircuit-switching has less avaliable users.\nPacket-switching can have more users, but with high probability.\nPacket-switching is great for bursty data! (e.g. Youtube Shorts)\nBut unlike circuit-switching, packets can be lost if buffer overflows!\nPacket-switching needs protocol for reliable data transfer and congestion control.\nThere are some techniques that try to provide circuit-like behavior with packet-switching, (implemented with virtual circuits) however it's not widely used.\nISPs\nHow are we globally connected?\n\nTier-1 ISP (Internet service Providers): Covers national and international networks.\nThere are only 16 Tier-1 ISPs, and they are fully connected! but sometimes they disconnect each others\nTier-1 ISP don't charge each other, and they're connected with IXP (Internet exchange point).\nContent provider networks: Companies like google makes private networks that connects its data centers directly!\nThey pays Tier-1 ISPs but tries to avoid paying regional ISPs.\nRegional ISP: Covers national networks.\nThey pays Tier-1 ISPs to provide international networks.\n\nInterestingly, there is no qualification to become Tier-1 ISPs!\nThe concept of Tier-1 ISPs emerged naturally, and the only way to become Tier-1 ISP is to be recognized by other Tier-1 ISPs.\nPacket delay\nPacket delay happens when packet are queued and waits for transmission.\nPacket loss happens when queue of packets fills up.\nFour sources of packet delay\ndnodal=dproc+dqueue+dtrans+dpropd_{nodal} = d_{proc} + d_{queue} + d_{trans} + d_{prop}\ndnodal​=dproc​+dqueue​+dtrans​+dprop​\nprocessing delay\nNodal needs to process packet.\ne.g. check bit errors, determine output link\nqueueing delay\nTime waiting at output link for transmission.\nDepends on congestion level of router.\ntransmission delay\ndtrans=packet legnth (bits)link trasmission rate (bps)d_{trans} = \\frac{\\text{packet legnth (bits)}}{\\text{link trasmission rate (bps)}}\ndtrans​=link trasmission rate (bps)packet legnth (bits)​\nTime transmitting packet into the communication link.\npropagation delay\ndprop=length of physical link (m)propagation speed (m/s)d_{prop} = \\frac{\\text{length of physical link (m)}}{\\text{propagation speed (m/s)}}\ndprop​=propagation speed (m/s)length of physical link (m)​\nTime sending packet from the sender to the receiver.\nPropagation speed is usually about 2×108m/s2 \\times 10^8 m/s2×108m/s - nearly the speed of light!\nTraffic intensity\na: average packet arrival rate\nL: packet length (bits)\nR: link bandwidth (bit transmission rate)\nTraffic intensity=LaR=arrival rate of bitsservice rate of bits\\text{Traffic intensity} = \\frac{La}{R} = \\frac{\\text{arrival rate of bits}}{\\text{service rate of bits}}\nTraffic intensity=RLa​=service rate of bitsarrival rate of bits​\nIf traffic intensity is near 0, queueing delay is small.\nIf traffic intensity is near 1, queueing delay is large.\nIf traffic intensity is over 1, queueing delay is infinite! More work arrives than can be serviced.\nPacket loss\nWhen queue is full, no packet can arrive.\nThe most popular drop policy is tail drop - when queue is full, any arrived packets are dropped.\nLost packet may be retransmitted by previous node, by source end system, or not at all.\nThroughtput\nBandwidth is nominal capacity.\nThroughtput is actual capacity in applications. Also called effective bandwidth.\n\nInstantaneous throughput: rate at given point in time\nAverage throughput: rate over longer period of time\n\nThroughput has bottleneck link, the link with smallest bandwidth constrains end-end throughput.\nProtocol Layers\n\nApplication: supporting network applications (HTTP, IMAP, SMTP, DNS)\nTransport: process-process data transfer (TCP, UDP)\nNetwork: routing of datagrams from source to destination (IP (IP address is NOT IP!!!!), routing protocols)\nLink: data trasfer between neighboring network elements (Ethernet, 802.11 (Wi-Fi), PPP)\nPhysical: bits on the wire\n\nEach layers need header to process data.\nTransport layer adds transport-layer header to application message (called transport-layer segment).\nNetwork layer adds network-layer header to transport-layer segment (called network-layer datagram).\nLink layer adds link-layer header to network-layer segment (called link-layer frame).\nFinally, link-layer frame is sent through wire!\nSwitches unpack until link layer.\nRouters unpack until network layer.\nWhy layering?\nNetwork is extremely complex - we need modulariztaion.\nChanging one layer's implementation doesn't affect other layers!\nBut if you're really smart, you can ignore layers...\ne.g. TCP offload engine (TOE) makes network interface cards to process TCP/IP.\nOSI model\nBut aren't there 7 network layers?\nIn OSI model, two layers are added between application layer and transport layer.\n\nPresentation: allow applications to interpret meaning of data, e.g., encryption, compression, machine-specific conventions\nSession: synchronization, checkpointing, recovery of data exchange\n\nHowever, these are not in internet stack!\nThey are not always needed!\nWhen you need these services, it should be implemented in application layer. (Especially HTTPS encrypts messages!!)\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Socket Programming","url":"/posts/43/","content":"Two types of packets\n\nUDP: unreliable datagram\nTCP: reliable byte stream\n\nActually it takes the same amount of time to send UDP packet and TCP packet!!\nEven streaming services uses TCP these days.\nUDP\n\nno handshaking before sending data (no connection!)\nsender explictly attaches IP destination address and port number to each packet\nreceiver extracts sender IP address and port number from received packet\n\nProgramming UDP\nint socket(PF_INET, SOCK_DGRM, IPPROTO_UDP);int close(int socket);int sendto(int socket, char *msg, int msg_len, int flags, const struct sockaddr* dest_addr, socklen_t addrlen);int recvfrom(int socket, char *buff, int buff_len, int flags const struct sockaddr* src_addr, socklen_t* addrlen);\nFor historical reasons, intel uses little endian, and internet uses big endian... You need to tkae care endianness!\nint port;struct sockaddr_in saddr;saddr.sin_port = htons(port);\nProgramming TCP\nUDP just sends sockets.\nTCP must make connection before sending sockets!\nUsually server opens well-known port. (e.g. HTTP: 80)\n\nClient contact server\nServer creates new socket dedicated for that client\nClient and server sends messages\n\nint connect(int socket, struct sockaddr *address,int addr_len);int send(int socket, char *msg, int msg_len,int flags);int recv(int socket, char *buff, int buff_len, int flags);int listen(int socket, int backlog);\nlisten's arguments accepts queue size!\nProbably your server need 1024 queue size, if you're not accepting fast enough.\nUnfortunately, listen use signle queue, and it's not designed with a multi-core environment in mind.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"HTTP","url":"/posts/47/","content":"Application-layer protocols\n\nTransport-layer service models: Transport-layer sends data (TCP, UDP)\nClient-server paradigm (HTTP)\nPeer-to-peer paradigm (BitTorrent)\n\nHTTP\nWeb\nWeb page consists of objects, which can be HTML file, JPEG image, audio file, etc.\nWeb page consists of base HTML-file which includes several referenced objects, each addressable by a URL.\nHTTP overview\nHTTP(hypertext transfer protocol) is a web's application-layer protocol.\nIt is a client/server model:\n\nClient: Browser that requests, receives, and displays Web objects\nServer: Server sends objects in response to requests\n\nActually client is way harder than server! Browser is complicated...\nHTTP uses TCP, and it's stateless. (Server maintains no information about past client requests)\nThat's why HTTP server is fast!\nHTTP connections\n\nNon-persistent HTTP: At most one object sent over TCP connection\nPersistent HTTP: Multiple objects can be sent over single TCP connection\n\nPersistent HTTP is efficient, but it can be a burden on a server, so we use non-persistent HTTP.\nNon-persistent HTTP\n\nAt most one object is sent over TCP connection\nDownloading multiple objects requires multiple connections\n\nResponse time of non-persistent HTTP\nRound-trip time (RTT): Time for a small packet to travel from client to server and back.\nDoes not includes transmission time!!\nRTT: From packet sent to first byte received\nTransmission time: From first byte received to last byte received\nHTTP response time: 2 RTT + file transmission time\n(1st RRT initiates TCP connection, 2nd RTT requests HTTP request)\nPersistent HTTP (HTTP 1.1)\n\nServer leaves connection open after sending response\nSubsequent HTTP messages between same client/server sent over open connection\nClient sends requests as soon as it encounters a referenced object\nRequires little as one RTT per object!\n\nHTTP 2 has same message as HTTP 1.1, but it allows multiplexing - you can send part of the referenced objects!\nHTTP message\nThere are two types of HTTP messages: request, response.\nASCII is used for HTTP messages.\nHTTP request message\nFirst line is request line: (method) (URL) (version)\\r\\n e.g. GET /index.html HTTP/1.1\\r\\n\nOther lines are header line: (header field):(value)\\r\\n\nConnection: keep-alive\\r\\n means Persistent HTTP\nHeaders ends with empty line \\r\\n.\nDepending on method type, body can be followed after headers.\n\nGET: get object from server\nPOST: send user input to server\nHEAD: get metadata of object from server\nPUT: upload new file (and replace old file) to server\n\nHTTP response message\nFirst line is status line: (version) (status code) (reason)\\r\\n e.g. HTTP/1.1 200 OK\\r\\n\nOther lines are header line: (header field):(value)\\r\\n\nETag: a5b-52d015789ee9e is a hashed value of the object that is used for cache validation.\nAccept-Ranges: bytes means you can query with byte ranges, so you don't have to get the whole object.\nHeaders ends with empty line \\r\\n.\nDepending on method type, body can be followed after headers.\n\n200 OK: request succeeded\n301 Moved Permanently: requested object is moved, new location is specified in header and there is no body\n400 Bad Request: request message is not understood by server\n404 Not Found: requested object is not found on this server\n505 HTTP Version Not Supported: server doesn't support this HTTP version\n\nCookies: maintaining user/server state\nAll HTTP requests are independent of each other.\nBut sometimes we need state e.g. we want to recover from a partially-completed transaction.\nWebsites and client browser use cookies to maintain some state between transactions.\n\nHTTP response message includes cookie header line. set-cookie: 1678\nClient browser stores cookie file.\nSubsequent HTTP requests will include cookie header line. cookie: 1678\nServer can identify client with cookie and perform cookie-specific action.\n\nCookies can used for:\n\nAuthentication: who are you?\nAuthorization: what can you do?\nShopping carts\nRecommendations\nUser session state\n\nCookies and privacy\n\nFirst party cookie: cookie from website you choose to visit\nThird party cookie: cookie from website you did not choose to visit (e.g. ads, youtube, ...)\n\nIf other site's object is needed, client requests with header Referer: nytimes.com.\nThird party site can track user behavior on a given website, or even across multiple websites.\nGDPR (EU General Data Protection Regulation) says if cookies can identify an individual, explicit and informed user consent should be obtained before setting any cookies.\nWeb caches\nForward caches (proxy servers)\n\nBrowser send all HTTP requests to a local web cache server.\nIf object is in cache, it returns object to client.\nIf object is not in cache, web cache server requests object and stores it.\n\nWhen bandwidth was very expensive, forward cache was useful because you don't have to pay requests to a local server.\nHowever forward caches is not used these days because it's actually slower than direct requests, and cache ratio is about 30~40%.\nRequest/response header can include Cache-Control: max-age=&lt;seconds&gt; or Cache-Control: no-cache to tell object's allowable caching.\nReverse caches (reverse proxy)\nForward cache is closer to the client than the origin server, but reverse cache is closer to the origin server than the client.\ni.e. Forward cache is a cache for client, while rever cache is a cache for server.\nForward cache has to cache every webpage client is visiting.\nReverse cache only need to cache data from specific server!\nIt can be also used as CDNs!\n\nCDN can protect actual server from DDoS attacks.\nCDN can distribute request across multiple servers around the world.\nCDN can find closest (therefore fastest) server from client.\n\nBrowser caches\n\nClient(Browser) caches objects and store date it has cached.\nClient includes If-modified-since: &lt;date&gt; in HTTP request.\nServer responses HTTP/1.0 304 Not Modified if browser-cached copy is up-to-date.\nServer responses HTTP/1.0 200 OK and data if browser-cached copy is outdated.\nServer don't send object if browser has up-to-date cached version!\n\nMulti-object HTTP requests\nHTTP/1.1\nHTTP/1.1 introduced multiple, pipelined GETs over single TCP connection.\nHowever, server should respond in order (FCFS: first-come-first-served scheduling) to GET requests.\nHTTP/1.1 requires Host header to support virtual domain hosting.\nVirtual domain hosting refers to hosting/serving multiple domain names on a single server/machine.\n\nHead-of-line (HOL) blocking: small object may have to wait for transmission until large object is sent.\nLoss recovery (retransmitting lost TCP segments) stalls object transmission.\n\nHTTP/2\nHTTP/2 solves HOL blocking!\n\nMethods, status codes, most header fields remain unchanged from HTTP/1.1.\nTransmission order of requested objects is based on client-specified object priority.\nObjects are divided into frames (not fixed length), and frame are scheduled to mitigate HOL blocking.\nServer can push unrequested objects to client. For example, server can push css, js files while responding index.html.\nNot related to multi-object HTTP requests, but HTTP/2 introduces TLS encryption.\n\nBut HTTP/2 still has issues:\n\nHTTP/2 uses single TCP connection.\nLoss recovery problem still exists.\n\nActually to solve HOL blocking problem, browsers using HTTP/1.1 typically opened multiple parallel TCP connections.\n\n\nMost HTTP/2 connection uses TLS encryption, but it's not forced.\n\nHTTP/3\nHTTP/3 has TLS by default, uses UDP (specifically QUIC), solves loss recovery problem.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"DNS","url":"/posts/50/","content":"DNS (Domain Name System)\nInternet hosts (or routers) has IP address and name.\nDNS maps between IP address and name and vice versa.\nDNS is a distributed database implemented in hierarchy of many name servers.\nIt is actually an application-layer protocol: hosts and DNS servers communicate to resolve names. (address/name translation)\nDNS is at network's edge; It's not core function of network!\nDNS uses TCP, UDP, or even TLS!\nDNS services\n\nHostname-to-IP-address translation\nHost aliasing (Canonical, alias names)\nMail server aliasing\nLoad distribution (Replicated Web servers: many IP addresses can correspond to one name)\n\nDistributed, hierarchical databse\nDNS is organizationally, physically decentralized!\nIf client wants IP address for www.amazon.com,\n\nClient queries root DNS server to find .com DNS server\nClient queries .com DNS server to find amazon.com DNS server\nClient queries amazon.com DNS server to get IP address for www.amazon.com\n\nWhy DNS is distributed? (i.e. not centralized?)\n\nSingle point of failure: One mistake can take down the entire internet's naming system.\nCan't scale: Even single DNS server processes 2.2T DNS queries per day\nTraffic volume: Centralized DNS would be a massive bottleneck\nPerformance: Users far from centralized DNS would have higher delays\nMaintenance: No single entity should control the entire internet; Millions of different organizations are responsible for their records\n\nRoot name servers\nRoot name servers only have records for top level domain servers.\nThere are only 13 logical root name servers! (a.root-servers.net to m.root-servers.net)\nEach server is physically replicated many times to prevent attack.\n\nICANN manages top level domains.\nDNSSEC provides security.\n\nTop level domain servers\nResponsible for Top-level Domain (TLD), e.g. .com, .org, .net, .edu, .aero, .jobs, .museums, .kr, .uk, .jp\nAuthoritative servers\nOrganization's own DNS servers that provides authoritative hostname to IP mappings for organization's named hosts.\nCan be maintained by organization or service provider.\nLocal DNS name servers\nWhenever host makes DNS query, it sends query to its local DNS server.\nLocal DNS server either returns cached name-to-address translation pair (possibly out of date!), or forward request into DNS hierarchy for resolutions.\nEach ISP has its local DNS name server.\nDNS name resolution\nIterated query\nContancted server replies with name of server to contact.\n\nHost ask local DNS server.\nLocal DNS server ask root DNS server.\nRoot DNS server replies with TLD DNS server.\nLocal DNS server ask TLD DNS server.\nTLD DNS server replies with authoritative DNS server.\nLocal DNS server ask authoritative DNS server.\nAuthoritative DNS server replies with actual IP address.\n\nRecursive query\nPuts burden of name resolution on contacted name server.\n\nHost ask local DNS server.\nLocal DNS server ask root DNS server.\nRoot DNS server ask TLD DNS server.\nTLD DNS server ask authoritative DNS server.\nAuthoritative DNS server replies with actual IP address.\nTLD DNS server replies with actual IP address.\nRoot DNS server replies with actual IP address.\n\nCaching DNS information\nDNS server responses mapping with Time To Live (TTL).\nOnce a name server learns mapping, it caches mapping and immediately returns until cache expires.\nLocal name server typically cache famous sites, including TLD servers.\nIf the named host change its IP address before all TTLs have expired, the internet may not know that the host changed its IP address!\nIf DNS server don't want to be cached, it can reply with 0 TTL.\nDNS records\nRecall) DNS is distributed database storing resource records (RR).\nRR format looks like this: (name, value, type, ttl)\n\ntype=A: name is hostname, value is IP address\ntype=NS: name is domain, value is hostname of authoritative name server for this domain\ntype=CNAME: name is alias name, value is canonical name\ntype=MX: value is name of SMTP mail server associated with name\n\nDNS protocol message\nMessage header\n\nIdentification: 16 bit number for query, response uses same nubmer.\nFlags: e.g. query or reply, recursion desired, recursion available, reply is authoritative\nNumber of questions, answer RRs, authority RRs, additional RRs\n\nMessage\n\nQuestions: name, type fields for a query\nAnswers: RRs in response\nAuthority: RRs for authoritative servers\nAdditional info: additional helpful RRs that may be used\n\nGetting your info into the DNS\n\nRegister name newsite.com at DNS registrar (e.g. Network Solutions)\n\nProvide names, IP addresses of authoritative name server to registrar.\nRegistrar inserts NS, A RRs into TLD server:\n(newsite.com, dns1.newsite.com, NS)\n(dns1.newsite.com, 212.212.212.1, A)\n\n\nCreate authoritative server locally with IP address 212.212.212.1\n\ntype A record for www.newsite.com\ntype MX record for newsite.com\n\n\n\nDNS attack\n\nDDoS attacks: bombard root server or TLD server with traffic\n\nHowever, root servers are very heavily guarded!\nSo far, no one has successfully attacked the root server and taken it down.\n\n\nSpoofing attacks: Intercept DNS queries and return bogus replies. e.g. DNS cache poisoning\n\nCan be prevented by RFC from DNSSEC, but DNSSEC is not widely used...\n\n\n\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"SMTP","url":"/posts/49/","content":"E-mail\nE-mail has three major components:\n\nUser agents\nMail servers\nSMTP (Simple mail transfer protocol)\n\nUser Agents\nUser Agents compose, edit, read mail messages.\nMail servers\nMailbox contains incoming messages for user.\nMessage queue of outgoing mail messages.\nSMTP\nSMTP is a protocol between mail servers to send email messages.\nSMTP has client SMTP server and server SMTP server.\nNowdays e-mail provides web-based interface with HTTP, but it's on top of SMTP and IMAP.\nSMTP (RFC 5321)\nUses TCP to reliably transfer email messages on port 25.\n\nSMTP handshaking\n\nServer respond with 220\nClient sends HELLO\nServer respond with 250 Hello\n\n\nSMTP transfer of messages\n\nLike HTTP, the commands are in ASCII text and the responses consist of a status code and a phrase.\nSingle dot &quot;.&quot; indicates the end of a DATA section. (i.e. mail content)\n\n\nSMTP closure\n\nClient sends QUIT\nServer respond with 221\n\n\n\nHow does e-mail actually get sent?\n\nAlice uses User Agent to compose e-mail message.\nAlice's User Agent sends message to her mail server using SMTP.\nAlice's mail server place the message in message queue.\nClient side of SMTP at mail server opens TCP connection with Bob's mail server.\nSMTP client sends Alice's message to Bob's mail server.\nBob's mail server places the message in mail box.\nBob invokes User Agent to read message. (This does not use SMTP, it uses IMAP or POP3)\n\nComparison with HTTP\nIn HTTP:\n\nClient pulls data from server.\nEach object is encapsulated in its own response message.\n\nIn SMTP:\n\nClient pushes data to server.\nMultiple objects are sent in multipart message.\n\nRFC 2822 defines syntax for e-mail message, but SMTP doesn't defines e-mail message syntax.\nIt can be plain ASCII, RTF (Rich Text Format), or any other format that can be shown better.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"P2P","url":"/posts/53/","content":"Peer-to-peer (P2P) architecture\n\nArbitrary end systems directly communicate each other.\nPeers request service from other peers, peers provide service in return to other peers.\nUsed for file sharing (BitTorrent), VoIP (Skype), etc.\n\nSelf scalability - new peers bring new service capacity, and new service demands!\nFile distribution time\nHow much time does it take to distribute file (size F) from one server to N peers?\n\nusu_sus​: server upload capacity\ndid_idi​: peer i download capacity\nuiu_iui​: peer i upload capacity\n\nClient-server approach\n\nServer must sequentially send N file copies: NFusN\\frac{F}{u_s}Nus​F​\nEach client must download file copy: Fdmin\\frac{F}{d_{min}}dmin​F​\n\n∴Dc−s≥max⁡(NFus,Fdmin)\\therefore D_{c-s} \\geq \\max(N\\frac{F}{u_s}, \\frac{F}{d_{min}})\n∴Dc−s​≥max(Nus​F​,dmin​F​)\nP2P approach\n\nServer must upload at least 1 copy: Fus\\frac{F}{u_s}us​F​\nEach client must download file copy: Fdmin\\frac{F}{d_{min}}dmin​F​\nClient can upload too! Server and client upload N file copies: NFus+∑uiN\\frac{F}{u_s + \\sum u_i}Nus​+∑ui​F​\n\n∴DP2P≥max⁡(Fus,Fdmin,NFus+∑ui)\\therefore D_{P2P} \\geq \\max(\\frac{F}{u_s}, \\frac{F}{d_{min}}, N\\frac{F}{u_s + \\sum u_i})\n∴DP2P​≥max(us​F​,dmin​F​,Nus​+∑ui​F​)\nWhen P2P was common, client was usually readers, so ui&gt;diu_i &gt;d_iui​&gt;di​.\nTheoretically, P2P is faster than client-server when NNN is large!\nBitTorrent\nP2P file distribution!\n\nEach file is divided into 256KB of chunks, and peers in torrent send/receive file chunks.\nTracker server tracks peers participating in torrent!\n\nTit-for-tat in BitTorrent\nProblem: once peer has entire file, it may selfishly leave.\nSolution: use tit-fot-tat strategy!\nWhen requesting chunks, client periodically ask each peer for list of chunks that they have.\nThen client requests missing chunks from peers, rarest first.\nWhen sending chunks, client send chunks to those four peers currently sending her chunks at highest rate.\nOther peers are choked by client - they do not receive chunks.\nEvery 10 secs, client reevaluate top four peers.\nEvery 30 secs, client optimistically unchoke peer - it randomly select another peer and start sending chunks.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"CDN","url":"/posts/54/","content":"Video Streaming\nCDN was first introduced to distribute non-video content, but it is widely used in video streaming these days.\nVideo traffic is a major consumer of Internet bandwidth. (80% of residental ISP traffic in 2020!)\nVideo is sequence of images displayed at constant rate. Because sending the video as is would eat up too much traffic, we use coding!\nA single frame is chosen and called the keyframe - other frames are encoded as differences from the keyframe.\n\n\nSpatial coding: Remove redundancy within image.\n\n\nTemporal coding: Remove redundancy between image. (i.e. between frame i and frame i+1)\n\n\nCBR (constant bit rate): Video encoding rate is fixed.\n\n\nVBR (variable bit rate): Video encoding rate changes as amount of spatial, temporal coding changes.\n\n\nChallenges\nServer-to-client bandwidth will vary over time with changing network congestion levels.\nContinuous playout constraint: playout timing of client must match original timing.\nSince network delays are variable, client need a buffer to match continuous playout constraint.\nAlso, client should interact (e.g. pause, fast-forward, rewind) with video, and packet lost might happen.\nTo match continuous playout constraint, it is usually better to just ignore lost packets.\nDASH (Dynamic, Adaptive, Streaming over HTTP)\nServer:\n\nDivides video file into multiple chunks.\nEach chunk encoded at multiple different rates.\nDifferent rate encodings stored in different files.\nFiles replicated in various CDN nodes.\nManifest file: provides URLs for different chunks.\n\nClient:\n\nPeriodically estimates server-to-client bandwidth.\nConsulting manifest, requests one chunk at a time.\n\nChooses maximum coding rate sustainable given current bandwidth.\nCan choose different coding rates at different points in time (depending on available bandwidth at time), and from different servers.\n\n\n\nClient should be intelligent! client determines:\n\nwhen to request chunk (to prevent buffer starvation or buffer overflow)\nwhat encoding rate should it request (to get the highest video quality for the given bandwidth)\nwhere to request chunk (to choose server that is close to client or has high available bandwidth)\n\nContent distribution networks (CDNs)\nWe have to stream content to hundreds of thousands of simultaneous users.\nSingle large server just can't scale.\nInstead, we use multiple copies of videos at multiple geographically distributed sites!\n\nEnter deep: push CDN servers deep into many access networks (closer to user!)\nBring home: smaller number of larger clusters in POPs near access networks\n\n\nBob browses video.\nBob requests video manifest to server.\nServer responds with manifest file that contains URL of CDN servers.\nBob select CDN server with DASH.\n\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Reliable Data Transfer","url":"/posts/57/","content":"Reliable data transfer\nSender and receiver use reliable data transfer protocol to unreliable channel.\nUnreliable channel may lose, corrupt, or reorder data.\nSender and receiver do not know the state of each other. e.g. Was a message received?\nAssumes transport layer is reliable!\ni.e. data &lt;-&gt; packet is reliable\nrdt (Reliable data transfer) protocol\n\nrdt_send(): Application call it to send data.\nudt_send(): Called by rdt to transfer packet over unreliable channel to receiver\nrdt_rcv(): Called when packet arrives on receiver side of unreliable channel\ndeliver_data(): Called by rdt to deliver data to application.\n\nWe consider only unidirectional data transfer, but control info will flow in both directions.\nWe use FSMs to specify sender and receiver.\nrdt1.0: reliable transfer over a reliable channel\nWe have reliable channel, do nothing!\nSender\nvoid rdt_send(data) &#123;  packet = make_pkt(data)  udt_send(packet)&#125;\nReceiver\nvoid rdt_rcv(packet) &#123;  data = extract(packet)  deliver_data(data)&#125;\nrdt2.0: channel with bit errors\nUnderlying channel may flip bits in packet!\nWe can use checksum to detect bit errors.\nWhen error happens, we have to recover from errors!\nWe'll use stop and wait method - sender sends one packet, then waits for receiver response.\n\nACK(Acknowledgements): receiver explicitly tells sender that packet was successfully received.\nNAK(Negative acknowledgements): receiver explicitly tells sender that packet had errors.\n\nSender waits for receiver response, and retransmit packets on NAK.\nThese are called control packets, it doesn't transmits actual data.\n\nBut this has a fatal flaw!\nWhat happens if ACK/NAK is corrupted?\nWhat should we do if ACK/NAK is corrupted?\nrdt2.1: sender handling garbled ACK/NAKs\nSender doesn't know what happened at receiver.\nWe can't just retransmit: receiver doesn't know whether this is a retransmittion or a new transmittion.\nWe use sequence number!\n\nSender adds sequence number to each packet.\nReceiver checks sequence number and discard duplicate packet.\nSender retransmits current packet if ACK/NAK is corrupted.\nReceiver should send ACK when duplicate packet arrive.\n\nStill use stop and wait method - sender sends one packet, then waits for receiver response.\n\nWe can only use two sequence number (0, 1).\nWe have twice as many states because we use two sequence number.\nrdt2.2: a NAK-free protocol\nActually, NAK is redundant!\nInstead of NAK, receiver sends ACK with sequence number of the last packet received.\nDuplicate ACK at sender results in same action as NAK: we retransmit current packet.\n\nrdt3.0: channel with errors and loss\nUnderlying channel may lose packets - data or control packets.\nIn practice, bit errors are rare these days, and loss and delay are much more problematic.\nApproach: sender waits reasonable amount of time for ACK!\n\nSender waits reasonable amount of time for ACK.\nSender retransmits if no ACK is received in this time.\n\nPacket can be just delayed, and retransmission will be duplicate.\nHowever, we're already handling duplicate packet!\n\nOnly sender maintains timeout. i.e. receiver don't have to retransmit ACKs.\nReceiver is same from rdt2.2!\nPerformance of rdt3.0 (stop-and wait)\nU: sender's utilization, i.e. fraction of time sender busy seding\nLet's assume link has 1Gbps bandwidth, propagation delay is 15ms, and packet size is 8000 bits.\n\nTime to transmit packet into channel: 8000bits / 1Gbps = 8μs\nRound-trip time: 2 propagation delay = 30ms\n\nUtilization = 8μs / (30ms + 8μs) = 0.00027\nSender is almost always idle!\nPipelining\nSend multiple packets at once!\nRange of sequence numbers must be increased.\nSender and receiver need buffering.\nIn theory, sender can send until it receive response.\ni.e. Sender can send bandwidth * RTT bits of packets!\nGo-Back-N\n\n\nSender have a window up to N consecutive transmitted but unACKed packets.\n\n\nOn receiving ACK(n), move window forward to begin at n + 1.\n\n\nTimeout should be checked only for oldest packet sent.\n\n\nIf timeout(n) happenes, retransmit every packet from n.\n\n\nReceiver always record highest in-order sequence number of correctly-received packet so far called rcv_base.\n\n\nOn receipt of out-of-order packet, receiver can discard or buffer it.\n\nBuffering is better, but sometimes you have to implement in hardware (e.g. RDMA), so it is possible for receiver to just discard packet.\n\n\n\nReceiver always ACK with rcv_base when it receives any packet.\n\n\nSelective repeat\n\n\nSender uses a window up to N packets.\n\n\nSender maintains timer for each unACKed packet.\n\n\nSender timeout/retransmits indivually for unACKed packets.\n\n\nOn receiving ACK(n), mark packet n as received.\n\n\nIf n is the smallest unACKed sequence number, advance window to next unACKed sequence number.\n\n\nReceiver individually ACK all correctly received packets\n\n\nOn receipt of out-of-order packet, receiver buffer it.\n\n\nOn receipt of in-order packet, receiver deliver buffered, in-order packets to application, and remove packets from buffer.\n\n\nHead-of-line blocking happens here!\nIf the smallest unACKed packet is delayed, the whole transmission is delayed.\nSequence number space\nWe can't increase number infinitely!\nGo-Back-N use at least N + 1 sequence numbers.\nSelective repeat use at least 2N sequence numbers.\nIn reality, packets can sent and received out of order.\nSpecification says that MSL (Maximum Segment Lifetime) should be 120s ~ 3 min.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"TCP","url":"/posts/60/","content":"TCP\n\nPoint-to-point: One sender and one receiver\nReliable, in-order byte stream: No need to message boundaries\nFull duplex data: Bi-directional data flow in same connection. i.e. client and server can send the data at the same time.\nMSS (Maximum Segment Size)\nCumulative ACKs: only ACK consecutive packets\nConnection-oriented: Handshaking needed\nFlow control with receiver window size (to not overwhelm receiver)\nCongestion control to avoid overflowing network\n\nTCP segment header\n\nsource, destination port number\nsequence number: first byte's sequence number (we assign sequence number per each byte)\nacknowledgement number: sequence number of next expected byte (ACK n, other host's sequence number)\nlength: only 4 bits!!\nreceiver window size: max 65536, can be increased in options\noptions: e.g. increase length of receiver window, congestion control, tcp options, ...\nchecksum\n\nTCP timeout\nTimeout should be longer than RTT (round trip time), but RTT varies!\nToo short timeout will have unnecessary retransmissions.\nToo long timeout will have slow reaction to segment loss.\nSince RTT varies, timeout should vary, so TCP estimates RTT!\nSampleRTT is the measured time from segment transmission until ACK receipt (ignoring retransmissions).\nWe should average several recent SampleRTT, so we use EstimatedRTT=(1−α)⋅EstimatedRTT+α⋅SampleRTT\\text{EstimatedRTT} = (1-\\alpha) \\cdot \\text{EstimatedRTT} + \\alpha \\cdot \\text{SampleRTT}EstimatedRTT=(1−α)⋅EstimatedRTT+α⋅SampleRTT.\nThis is called Exponential Weighted Moving Average (EWMA) because influence of past sample decreases exponentially fast.\nTypical value of α is 0.125.\nSince timeout should be longer than RTT, we use safety margin.\nDevRTT is EWMA of SampleRTT deviation from EstimatedRTT.\nDevRTT=(1−β)⋅DevRTT+β⋅∣SampleRTT−EstimatedRTT∣\\text{DevRTT} = (1-\\beta) \\cdot \\text{DevRTT} + \\beta \\cdot |\\text{SampleRTT} - \\text{EstimatedRTT}|\nDevRTT=(1−β)⋅DevRTT+β⋅∣SampleRTT−EstimatedRTT∣\nTimeoutInterval=EstimatedRTT+4⋅DevRTT\\text{TimeoutInterval} = \\text{EstimatedRTT} + 4 \\cdot \\text{DevRTT}\nTimeoutInterval=EstimatedRTT+4⋅DevRTT\nTypical value of β is 0.25.\nThis is not an optimal timeout interval, these values are used to make the computer easier to calculate.\nTCP Sender\n\nCreate segments with sequence number, then start timer with expiration interval.\nIf timeout, retransmit segment and restart timer. (Unlike Go-back-N, send only unACKed segment)\nIf ACK received and acknowledges previously unACKed segments, update ACKed segments, then send unACKed segments and restart timer if unACKed segments exists.\nIf ACK received and acknowledges previously ACKed segments thrice (i.e. three duplicated ACKs / four same ACKs), fast retransmit - immediately retransmit segment.\n\nTCP Receiver\n\nIf in-order segment arrives and all segments are already ACKed, send delayed ACK, wait up to 500ms for next segment.\nIf in-order segment arrives and one other segment has ACK pending (i.e. receiver was delaying ACK), immediately send cumulative ACK (thus ACKing both in-order segments).\nIf out-of-order segment with higher-than-expect sequence number arrives (i.e. gap detected), immediately send duplicate ACK.\nIf segment that partially or completely fills the gap, immediately send ACK.\n\nTCP flow control\nIf data is transferred too fast, the socket buffer will overflow.\nAny packet will be dropped when buffer overflows!\nFlow control: Receiver controls sender, so sender won't overflow receiver's buffer by transmitting too much, too fast.\nTCP segmgent header have &quot;receive window&quot; value, the number of bytes receiver willing to accept.\nTCP receiver advertises free buffer space in rwnd field in TCP header.\nRcvBuffer size is set via socket options. Default value is 4KB, but typically 64KB is used. (64KB is maximum because header value is 16bits.)\nSender limits amount of unACKed (in-flight) data to rwnd value.\nThis guarantees that receive buffer will not overflow!\nIn practice, the buffer size can be changed dynamically!\nAlso, 64KB is too small for Gbps internet...\nWe added window scale factor (0 ~ 14) for TCP option.\nThis shifts rwnd value, so theoretically, the buffer size can be up to 1GB (64KB * 2^14).\nTCP connection management\nDuring handshake process, we agree to establish connection, and we agree on connection parameters.\ne.g. sequence numbers, RcvBuffer size, MSS (Maximum Segment Size), ...\n2-way handshake doesn't work\n2-way handshake is not enough!\nDelay is variable, messages can be lost, reordered, retransmitted, etc.\ne.g. client retrasmitted handshake message (req_conn), but this message is super delayed and server accepts it after client termination! (half open connection)\n3-way handshake\nSeq is Sequence number, SYNbit is control packet.\n\nClient send TCP SYN msg (with SYNbit=1, Seq=x)\nServer responds TCP SYNACK msg (with SYNbit=1, Seq=y, ACKbit=1, ACKnum=x+1)\nClient establish connection, and responds with ACK msg (with ACKbit=1, ACKnum=y+1)\nServer receive ACK msg, then establish connection.\n\nSYNbit is ON only in the first two handshake packets.\nACKbit is OFF only in the first handshake packet. (recall: we removed NAK!)\nNormally handshake doesn't have a payload, but there are some RFCs (fast open) that allows sending data with handshake.\nClosing a TCP connection\n\nClient and server each close their side of connection by sending FIN msg (with FINbit = 1)\nReceiver should respond FIN with ACK.\nReceiver can respond FIN with FIN and ACK if they don't have data to send.\n\nThis is not handshake, simultaneous FIN exchanges can be handled!\n\nPassive close: connection is closed from other side. Send FIN when ready to close, and close connection after receiving ACK.\nActive close: connection is closed from your side. After both FIN is ACKed, we wait for 2MSL (Maximum Segment Lifetime, normally 2MSL = 120s!) before actually closing connection.\nThis can handle retransmitted FIN, dropped ACK, and doesn't have to worry that FIN might close next connection.\n\nTCP State Transition Diagram\n\nRST is sent when you want to close connection abruptly.\nTheorically, two client can try to connect each other, and simultaneous SYN can be sent.\nThey'll transition to SYN_SENT state, then send SYNACK (transition to SYNC_TCVD), then connection will established when receiving ACK (transition to ESTABLISHED)\nTransition from LISTEN to SYN_SENT is in RFC, but it's not used in practice.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Multiplexing & UDP","url":"/posts/55/","content":"Transport layer\nTransport layer provide logical communication between application processes running on different hosts.\nSender breaks application messages into segments, then passes to network layer.\nReceiver reassembles segments into messages, then passes to application layer.\nTransport layer and network layer\nTransport layer is communication between processes. (TCP, UDP)\nNetwork layer is communication between hosts. (IP, Router)\nMultiplexing &amp; Demultiplexing\nSender sends messages from any application to network layer.\nThis is called multiplexing: Sender handle data from multiple sockets, then send it to single network.\nReceiver must pass application message to matching application.\nThis is called demultiplexing: Receiver chooses right socket (a.k.a. port number) and pass message to application.\nTransport header\nMultiplexing &amp; Demultiplexing is implemented using transport header.\nHost receives IP datagrams:\nEach datagram has source IP address and destination IP address.\nEach datagram carries one trasport-layer segment (i.e. application data with transport header).\nEach segment has source port number and destination port number.\nHost uses IP addresses &amp; port number to direct segment to appropriate socket!\nConnectionless demultiplexing (UDP)\nUDP only needs destination IP address and destination port number.\nReceiver uses destination IP address and destination port number to direct UDP segment to appropriate socket.\nIP/UDP datagrams with same destination port number but different source IP address and/or source port numbers will be directed to same socket.\nConnection-oriented demultiplexing (TCP)\nTCP socket is identified by 4-tuple: source IP address, source port number, destination IP address, destination port number.\nReceiver uses all four values to direct TCP segment to appropriate socket.\nEach TCP socket is associated with a different connecting client.\nDestination AND source should be same!\nEven if packets have same destination port number, it will be demultiplexed to different sockets if their source IP address is different.\nUser Datagram Protocol (UDP)\n\nNo connection state\nSmall header size (8 bytes) - almost bare packet!\nNo connection (handshaking), no congestion control\n\nBut congestion control is necessary, user (application layer) have to do it!\nUDP has checksum, but its optional...\n\n\nUDP segments may be lost or delivered out-of-order even in best effort service.\n\nUDP are used in:\n\nStreaming multimedia apps (discarding packet is better than retransmitting)\nDNS (but DNS can use TCP, UDP, or both)\nHTTP/3 (HTTP-over-QUIC) (but reliable transfer is needed - application layer has congestion control)\n\nActually Google wanted to make new protocol, but because of backward compatibility, it used UDP + congestion control\n\n\n\nUDP segment header\n\nsource port number\ndestination port number\nlength (in bytes, including header)\nchecksum (optional)\n\nUDP checksum\nUDP checksum detect errors like flipped bits. It's not error correction code!\nSender treat UDP segment (including UDP header and IP addresses) as sequence of 16-bit integers and sum it.\nSender set checksum field value as 1's complement of sum. (i.e., if you add checksum to sum, it will be 0)\nReceiver compute checksum and check if computed checksum equals checksum field value.\nTheorically, checksum can be correct even if error happens in content (or even in checksum).\nBut these days bit flip rarely happens, and checksum is useful!\nAlso, it is simple so it can be implemented in hardware!\nUnfortunately, UDP checksum is optional...\nIf checksum field is 0, it means checksum is not used.\nIf an actual checksum is 0, we shoule set checksum field to all 1s.\nIs UDP faster?\nActually, TCP can be faster!\nUDP only sends limited number of packets at a time.\nIf you're sending huge data, you need to split it into packets and send it.\nBecause of context switching, sending lots of packet in UDP is actually slower than TCP!\nIn TCP, kernel split data into packets so we have less context switching.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Congestion Control","url":"/posts/67/","content":"Congestion\nToo many sources are sending too much data too fast for network to handle!\nManifestations: long delays (queueing in router buffers), packet loss (buffer overflow at routers)\nDifferent from flow control! Flow control is needed when one sender is too fast for one receiver.\nCongestion control is needed when too many senders are sending too fast.\nCongestion scenario 1: Two hosts\n\nLets assume two hosts are using one link, sending at same input rate.\nIf input rate λin\\lambda_{in}λin​ approaches R/2? (half of link capacity)\nOutput rate λout\\lambda_{out}λout​ is capped at R/2, and delay goes to infinite.\nIf we assume finite buffers, packet loss will happen.\nEven if application-layer input rate is λin\\lambda_{in}λin​, transport-layer input rate is higher λin′≥λin\\lambda_{in}&#x27; \\geq \\lambda_{in}λin′​≥λin​ because it includes retransmissions.\nBecause of packet loss and unneeded duplicates (packet was sent but host didn't know), packets are retransmitted more often at high congestion.\nEven if λin\\lambda_{in}λin​ approaches to R/2, λout\\lambda_{out}λout​ can't reach R/2, thus decreasing maximum achievable throughput.\nCongestion scenario 2: Four hosts\n\nLest assume four senders are sending to each other.\nIf red input rate λin,λin′\\lambda_{in}, \\lambda_{in}&#x27;λin​,λin′​ increases, the top link buffer will fill and packet loss will happen.\nHowever, since blue shares the same link, all arriving blue packets will be lost, making blue's throughput to 0.\nCongestion on one connection can starve other connections!\nAlso, when packet is dropped, any upstream transmission capacity and buffering used for that packet has been wasted.\ne.g. When blue packet is lost because the top link buffer was full, all the resources used on the left link to send the blue packet are wasted.\nCongestion on one link costs every link along the connection!\nCongestion control\nEnd-end congestion control\nCongestion control without no explicit feedback from network.\nHost infers congestion from observed loss, delay.\nTCP already does this!\nTCP AIMD\nApproach: senders can increase sending rate until packet loss occurs, then decrease sending rate on loss event.\n\nAdditive Increase (AI): Increase sending rate by 1 MSS (maximum segment size) every RTT until loss detected.\nMultiplicative Decrease (MD): Cut sending rate at each loss event.\n\nTCP Tahoe: Cut to 1 MSS when loss detected by timeout. Timeout have to wait too long time for congestion control.\nTCP Reno: Cut in half on loss detected by triple duplicate ACK.\nThis is an improved version of TCP Tahoe!\n\n\n\n\nAIMD probes for bandwidth in a sawtooth pattern.\nImplementing AIMD\ncwnd is TCP window size, indicating how many bytes can be in flight at any one time.\ni.e. (Last byte sent - Last byte ACKed) should be less than or equal to cwnd.\nThe send rate of TCP is proportional to cwnd.\nSend rate=cwndRTTbytes/sec\\text{Send rate} = \\frac{\\text{cwnd}}{\\text{RTT}} \\text{bytes/sec}\nSend rate=RTTcwnd​bytes/sec\n\nIn practice, slow start and fast recovery is used.\nWhen connection begins, cwnd is set to 1 MSS, then cwnd is doubled every RTT.\nIf cwnd reaches threshold ssthresh, switch to congestion avoidance. (Additive Increase)\nWhen timeout is detected, cwnd is set to 1 MSS, and ssthresh is set to half of current cwnd. (TCP Tahoe)\nLike slow start, cwnd is doubled until it reaches threshold.\nFast recovery is used when triple duplicate ACK is detected. (TCP Reno)\nSet cwnd and ssthresh to half of current cwnd, then increase cwnd by 3 MSS.\nWhy? Triple duplicate ACK means that at least three packets have arrived. To maintain number of in flight packets, we increase cwnd.\nAlso, cwnd is increased by 1 MSS for each duplicate ACK received.\nAfter new ACK is received, switch to congestion avoidance.\n\nTo double cwnd in the slow start state, we increase cwnd by 1 MSS for each new ACK.\nFor every RTT, cwndMSS\\frac{\\text{cwnd}}{\\text{MSS}}MSScwnd​ packets are ACKed, so cwnd will be increased by cwnd. (i.e. doubled)\nBut we need to increase cwnd linearly in the congestion avoidance state. To do this, we increase cwnd by (MSS * MSS / cwnd) for each new ACK.\nFor every RTT, cwndMSS\\frac{\\text{cwnd}}{\\text{MSS}}MSScwnd​ packets are ACKed, so cwnd will be increased by 1 MSS.\nSince the internet is fast enough, we use inital cwnd as 10 MSS instead of 1 MSS.\nAlso, we use scale factor to scale the initial sshthresh, so we can use (64K &lt;&lt; scale factor) as the initial sshthresh.\nTCP CUBIC\nInsight: Linear is too slow, how can we approach maximum sending rate faster?\nSolution: Increase sending rate as a function of the cube of the distance between current time and K.\nK is tuneable point in time when TCP window size will reach W_{max}.\n\n\nLarger increases when further away from K\nSmaller increases when near K\n\nIf good K is selected, TCP CUBIC will ramp up to the WmaxW_{max}Wmax​ faster, but then approach WmaxW_{max}Wmax​ slower than TCP AIMD.\nMost popular web servers use TCP CUBIC instead of TCP AIMD.\nDelay-based TCP congestion control\nTCP AIMD, CUBIC increase TCP's sending rate until packet loss occurs at the bottleneck link.\nInsight: Focus on congested bottleneck link!\nIf we can keep the bottleneck link full, we can achieve maximum sending rate.\nWe guess congestion of bottleneck link by measuring RTT.\nWe assume uncongested throughput with congestion window cwnd is cwndRTTmin\\frac{\\text{cwnd}}{\\text{RTT}_{\\text{min}}}RTTmin​cwnd​.\nIf measured throughput is very close to uncongested throughput, increase cwnd linearly since path is not congested.\nIf measured throughput is far below uncongested throughput, decrease cwnd linearly since path is congested.\nDelay-based congestion control is less aggressive than loss-based congestion control.\nGoogle's backbone network (BBR)\nBBR uses model-based congestion control to estimate the bottleneck link's bandwidth by probing.\nEarlier version (BBRv1) was much more aggressive than TCP CUBIC, so YouTube was able to use higher streaming bitrates and starve other connections.\nNetwork-assisted congestion\nTCP ECN, ATM (Asynchronous Transfer Mode), DECbit protocols used this.\nNetwork-assisted congestion should alert congestion before buffer is full.\nResponse packet should arrive to sender to control congestion!\nTCP ECN (Explicit congestion notification)\n\nTwo ECN bits in IP header is marked by network router to indicate congestion.\nDestination checks congestion indication, and set ECE bit (in TCP header) on ACK segment to notify sender of congestion.\n\nTCP header and IP header are both used!\nIt's rarely used...\nTCP fairness\nGoal: If K TCP sessions share same bottleneck link of bandwidth R, each should have average rate of R/K.\n\nAIMD is fair!\nAI gives slope of 1, and MD decreases throughput proportionally, thus moving towards equal bandwidth share.\nUnfair network apps\nMultimedia apps often use UDP, and send audio/video at constant rate, tolerating packet loss.\nThis bypass TCP congestion control!\nWeb browsers usually open multiple parallel connections between two hosts.\nHTTP/2 uses one connection, but it's not forced.\nEvolving transport-layer functionality\nIn 1983/1/1, ARPANET was able to change every internet protocol from NCP to the TCP/IP.\nNowdays, internet is too big, so it's nearly impossible to change protocol from TCP, UDP. (ossification)\nHTTP/3 moved to UDP because tweaking TCP was not enough.\nThere were many different flavors of TCP developed for specific scenarios.\n\nLong, fat pipes (Large data transfers): Many packets are in flight, loss can shutdown pipeline\nWireless networks: Loss due to noisy wireless links, mobility\nLong-delay links: Extremely long RTTs\nData center networks: Latency sensitive\nBackground traffic flows: Have low priority, but still background TCP flows are needed.\n\nQUIC: Quick UDP Internet Connections\nQUIC is an application-layer protocol on top of UDP, but it actually acts as a transport-layer protocol.\nQUIC increases performance of HTTP, and have more functionality. (e.g. migrating connection)\nUltimately, it tries to replace HTTP (over TLS) over TCP.\n\nError and congestion control: Parallel to well-known TCP congestion controls.\nConnection establishment: Reliability, congestion control, authentication, encryption, state are established in one RTT. (TCP + TLS need 2 serial handshakes, UDP merge these into 1 handshake!)\nParallelism: Multiple application-level streams are multiplexed, no HOL blocking!\n\nClients doesn't know if server uses QUIC in advance.\nSolution: trial and error! Modern browser tries QUIC handshake over UDP. If QUIC failes, HTTP/2 or HTTP/1.1 over TCP is used.\nHowever, since Chrome is made from google, it knows that Google uses QUIC.\nTCP Throughput\nLets assume loss occurs when window size is WWW.\nSince window size is halved, average throughput is 3W/4 per RTT.\nCan we get better representation?\nPeriodic Model: Congestion Window Behavior\nLets say W(i)W(i)W(i) is the number of segments sent during ith RTT, and loss occurs when WLW_LWL​ segments are sent.\nAt the end of the ith RTT,\n\nW(t)=W(t−1)+1W(t) = W(t - 1) + 1W(t)=W(t−1)+1 if there was no packet loss.\nW(t)=WL2W(t) = \\frac{W_L}{2}W(t)=2WL​​ if there was a packet loss.\nThe period of window shrinkage is T=WL2⋅RTTT = \\frac{W_L}{2} \\cdot RTTT=2WL​​⋅RTT.\n\nIf ppp is a loss probability, number of segments sent every TTT is N=1p,∵N=p⋅1+(1−p)⋅(N+1)N = \\frac{1}{p}, \\because N = p \\cdot 1 + (1-p) \\cdot (N + 1)N=p1​,∵N=p⋅1+(1−p)⋅(N+1).\nWe can also compute number of segments sent using W(t)W(t)W(t): N=∑i=0WL2(WL2+i)=3WL4(WL2+1)≈3WL28N = \\sum_{i=0}^{\\frac{W_L}{2}} \\left( \\frac{W_L}{2} + i \\right) = \\frac{3W_L}{4}\\left( \\frac{W_L}{2} + 1 \\right) \\approx \\frac{3W_L^2}{8}N=∑i=02WL​​​(2WL​​+i)=43WL​​(2WL​​+1)≈83WL2​​\n∴WL=83p\\therefore W_L = \\sqrt{\\frac{8}{3p}}\n∴WL​=3p8​​\nTherfore, average throughput is the total amount of bytes sent per period T is:\nSˉ=N⋅MSSWL2⋅RTT=MSSRTT32p\\bar{S} = \\frac{N \\cdot MSS}{\\frac{W_L}{2} \\cdot RTT} = \\frac{MSS}{RTT} \\sqrt{\\frac{3}{2p}}\nSˉ=2WL​​⋅RTTN⋅MSS​=RTTMSS​2p3​​\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Network Layer","url":"/posts/69/","content":"Network layer\nNetwork layer transport segment from sending to receiving host.\nSender encapsulates segments into datagrams, passes to link layer.\nReceiver delivers segments to transport layer protocol.\nNetwork layer protocols is in every internet device, including hosts and routers.\nRouters examine header fields in all IP datagrams passing through it.\nThen it moves datagrams from input ports to output ports to transfer datagrams along end-end path.\nTwo key network-layer functions\nForwarding: Move packets from a router's input link to appropriate router output link.\nRouting: Determine route taken by packets from source to destination.\nForwarding determine route in single router.\nRouting determine route from source to destination.\nThere are two types of routing algorithms:\n\nTraditional routing algorithms: Implemented in routers.\nSoftware-defined networking (SDN): Implemented in remote servers, widely used in datacenters.\n\nData plane, control plane\nWe can view this as a data plane and control plane.\nData plane is local, per-router function that determines how datagram arriving on router input port is forwarded to router output port.\nControl plane is network-wide logic that determines how datagram is routed among routers from source host to destination host.\nThere are two types of control plane:\n\nPer-router control plane: Individual routing algorithm components in each and every router interact in the control plane.\nSoftware-defined networking (SDN) control plane: Remote controller computes, installs forwarding tables in routers.\n\nSDN is a background process, it only updates forwarding tables only when the network topology changes.\nNetwork service model\nNetwork architectures uses service model.\ne.g. ATM used constant bit rate model, that guarantees constant rate bandwidth.\nModern Internet uses best effort service model.\nIt tries it best, but there are no guarantees on bandwidth, loss, order, timing, etc.\nBest effort service model was very successful!\n\nBecause of its simplicity, internet is widely deployed and adopted.\nSufficient provisioning of bandwidth allows performance of real-time applications such as voice, video services.\nReplicated, application-layer distributed services (e.g. datacenters, CDNs) connecting close to clients' networks allow services to be provided from multiple locations.\n\nRouters\nRouter acrhitecture\nRouter has input ports, output ports, and high-speed switching fabric that connects input and output ports. (data plane)\nRouting processor controls high-speed switching fabric. (control plane)\nInput port and output port can be same in real routers.\nInput port functions\n\nPhysical layer: Check line termination\nLink layer: Check protocol (e.g. Ethernet)\nDecentralized switching: queueing &amp; lookup output port &amp; forwarding\n\nNote that output port is exactly opposite!\nBuffer -&gt; link layer (add protocol) -&gt; add line termination\nDecentralized switching\nUsing header field values parsed from link layer, lookup output port using forwarding table in input port memeory.\nInput port queueing: If datagrams arrive faster than forwarding rate into switch fabric, we queue datagrams.\nIt's also called match (find) + action (forwarding).\nGoal: complete input port processing at line speed, i.e. input port's bandwidth.\nNote that datagram can't arrive faster than line speed!\nThat's the definition of bandwidth...\nDestination-based forwarding\n\nDestination-based forwarding: forward based only on destination IP address, traditional way\nGeneralized forwarding: forward based only on any set of header field values.\n\n\nDestination-based forwarding use longest prefix matching!\nUse longest address prefix from forwarding table entry that matches destination address.\nThere's a dedicated hardware called ternary content addressable memories. (TCAMs)\nTCAM can retrieve address in one clock cycle, regardless of table size!\nSwitching fabric\nSwitching fabric transfer packet from input link to appropriate output link.\nSwitching rate is a rate at which packets can be transferred from inputs to outputs. It's often measured as a multiple of input/output line rate.\nIf N input/output ports's desired rate is R, switching rate is NR.\nSwitching via memory\nFirst generation routers used CPU to control switching directly.\nPacket is copied to system's memory, but speed is limited by memory bandwidth.\nWe need 2 system bus crossings per datagram.\nSwitching via bus\nDatagram from input port memory to output port memory via a shared bus.\nSwitching speed is limited by bus bandwidth, and every port receives message because every port is connected to bus.\ne.g. Cisco 5600 use 32Gbps bus, which is sufficient speed for access routers.\nSwitching via interconnction network\nSwitching fabric can have interconnection network inside.\ne.g. Crossbar network connects input port and output port like NxN array.\ne.g. Multistage switch use smaller sized switches to implement large sized switch.\nCisco CRS router use 8 switching &quot;planes&quot; in parallel, so we can speed up and scale up switching.\nInput port queueing\nIf switch fabric is slower than input port combined, input queueing occurs.\nHead-of-the-Line (HOL) blocking: queued datagram at front of queue prevents others in queue from moving forward\n\nOutput port contention: Only one datagram can be transferred to output port, other datagrams are blocked\nOne packet time later: Packet inside input queue experience HOL blocking\n\nOutput port queueing\nIf switch fabric is faster than link transmission rate, output port queueing occurs.\nDrop policy/Scheduling discipline: choose which datagram should be drop/sent\nDatagram can be lost at output port!\nRFC 3439: Average buffering should be equal to (typical) RTT times link capacity C.\nRecent recommendation says with N flows, buffering should be equal to RTT⋅CN\\frac{\\text{RTT} \\cdot C}{\\sqrt{N}}N​RTT⋅C​.\nToo much buffering can increase delays!\n\nLong RTT has sluggish TCP response, poor performance for realtime apps\nDelay-based congestion control: TCP congestion control aims to keep bottlenectk link full\n\nBuffer Management\nDrop: Which packet should be added to buffer, or dropped when buffer is full?\n\nTail drop: Drop any arriving packets when full\nPriority: Drop/remove based on priority\n\nc.f. RED (Random early detection) randomly drop packets when buffer is partially full.\nIf marking is possible, instead of dropping packets, two ECN bits in IP header is marked to notify congestion.\nPacket scheduling\nPacket scheduling decide which packet to send next on link.\n\nFirst come, first served\nPriority\nRound robin\nWeighted fair queueing\n\nFCFS (First come, first served)\nPackets are trasmitted in order of arrival to output port. Also known as FIFO. (First in, first out)\nFCFS is widely used in real life. e.g. standing in line\nPriority\nWe have high priority queue and low priority queue.\nArriving traffic is classified and queued by class.\nWe send packet from high priority queue before sending packet from low priority queue.\nPacket should be sent completely to send other packets.\nArriving high priority packet isn't sent until low priority packet is sent completely.\nRound robin\nWe have multiple queue, and header field is used for classification\nServer cyclically scans class queues, sending one complete packet from each class (if available) in turn.\nWFQ (Weighted fair queueing)\nGeneralized round robin!\nEach class i has weight wiw_iwi​, and gets weighted amount of service wi∑iwi\\frac{w_i}{\\sum_i w_i}∑i​wi​wi​​ in each cycle.\nActually unfair queueing\nNetwork Neutrality\n\nTechnical: How an ISP should share/allocation its resources?\nSocial, economic: Encouraging innovation, competition, and protecting free speech\n\ne.g. US regulation says ISP shouldn't prioritize paid users.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"IP","url":"/posts/73/","content":"Network layer\nPath-selection algorithms is implemented in\n\nrouting protocols: OSPF (inside 1 ISP), BGP (over multiple ISPs)\nSDN controller: Used in DC (data center)\n\nNetwork layer uses IP protocol, but ICMP protocol is also used.\nICMP is a network layer protocol that reports error between routers.\nIP Datagram\n\nEach header is 32 bits (4 bytes).\nTheoretical maximum length of datagram is 64K bytes, but typically IP datagram's maximum length is 1500 bytes or less.\nIP header size is 20 bytes in IPv4. (40 bytes in IPv6)\nTheorically IP header can include more options, but it is not used.\nPayload is typically a TCP or UDP segment.\nFirst header\n\nFirst four bits is IP protocol version numer (4, 6)\nThe IPv5 protocol used to exist as an experiment, but it is no longer in use.\nHeader length (If header length is 5, actual header length is 5 * 4bytes = 20 bytes)\nType of service (e.g. ECN)\nTotal datagram length as bytes\n\nSecond header\nSecond header is used for fragmentation.\n\n16-bit packet identifier\nFlags (Is fragmented?)\nFragment offset (divided by 8)\n\nIP fragmentation\nIP packet size can't exceed MTU (Maximum Transmission Unit). MTU is usually 1500 bytes.\nIf IP packet is larger than MTU, router will split packet into multiple MTU size packets. (fragmentation)\nIf offset is 0, this packet is the first fragment.\nIf flag is OFF, this packet is the last fragment.\ne.g. Let's assume we need to send 4000 byte datagram, which has 20 bytes IP header.\nInitial header looks like this: length=4000, ID=x, fragflag=0, offset=0.\nThis datagram is split into 3 packets:\n\nlength=1500, ID=x, fragflag=1, offset=0\nlength=1500, ID=x, fragflag=1, offset=185 (1480/8)\nlength=1040, ID=x, fragflag=0, offset=370\n\nIP reassembly\nWe can reassembly IP packet with 16-bit identifier.\nHowever, if packet is lost serious problem happens.\nSince this is done in network layer, (at the router) we don't have retransmission and timeout.\nInstead, we prove path MTU (smallest MTU in the path) and don't use fragmentation at all. (i.e. limit IP packet size to less than or equal to path MTU)\nThere is a path MTU discovery protocol, but it's not widely used (and can be abused).\nInstead, we just think everybody is using 1500 byte MTU.\nOther headers\n\nTTL (Time to live): Remaining max hops (decremented at each router)\nUpper layer protocol (TCP/UDP)\nHeader checksum (only header, doesn't check payload)\n32-bit source IP address\n32-bit destionation IP address\n\nIP addressing\nIP address is a 32-bit identifier associated with each host or router interface.\nIt is usually represented as a dotted-decimal IP address notation. e.g. 223.1.1.1\nInterface is a connection between host/router and physical link.\nRouter typically have multiple interfaces, and host typically has one or two interfaces. Typically host use wired Ethernet and wireless 802.11 (Wi-Fi).\nSubnet\nIf we detach each interface from its host or router, we'll have islands of isolated networks, called subnet.\nSubnet is a device interfaces that can physically reach each other without passing through an intervening router.\nCIDR (Classless InterDomain Routing)\nSubnet is represented as a CIDR format a.b.c.d/x. e.g. 223.1.3.0/24\nx is the number of bits in subnet part of IP address. (i.e. high-order x bits are same)\n\nSubnet part: Devices in same subnet have common high order bits.\nHost part: Remaining low order bits are different for each devices.\n\nInitially, IP address was classful, e.g. Class A,B,C,D,E IP addresses.\n\nClass A: Starts with 0 (0~127), subnet part is 8 bits.\nClass B: Starts with 10 (128~191), subnet part is 16 bits.\nClass C: Starts with 110 (192~223), subnet part is 24 bits.\nClass D: Starts with 1110 (224~239), subnet is not allowed.\nClass E: Experimental, subnet is not allowed.\n\nBut today we use classless IP address, and we can choose arbitrary length of subnet part.\nWhy? Because IPv4 address space is not enough!\nWe want a better utilization of IP addresses.\nDHCP (Dynamic Host Configuration Protocol)\nIf host joins the network, it dynamically get IP address from server.\nWidely used in wireless networks (Wi-Fi), but wired networks can also use DHCP.\nDHCP can return more than just allocated IP address, e.g. name and IP address of DNS server, subnet mask, address of first-hop router for client.\nProblem: What if DHCP server is malicious and gives already assigned IP address?\nModern DHCP server have authentication and its own defense mechanism (because host can be malicious too).\nDHCP messages\n\nHost broadcasts DHCP discover message. (optional)\nDHCP server responds with DHCP offer message. (optional)\nHost requests IP address with DHCP request message.\nDHCP server sends address with DHCP ack message.\n\nHost use 0.0.0.0 as a source IP address until it receives DHCP ack message. (i.e. until host gets IP address)\nHost/DHCP server use 255.255.255.255 as a destination IP address because host doesn't have a IP address.\nIf destination is 255.255.255.255, this message will be broadcast to all machines in the network.\nSince we cannot distinguish hosts with IP address, we use transaction ID.\nNetwork's IP address\nNetwork gets allocated portion of its provider ISP's address space.\ne.g. ISP has 200.23.16.0/20 IP space.\nThis ISP can allocate its address space to 8 organization, such as:\n200.23.16.0/23, 200.23.18.0/23, ..., 200.23.30.0/23\nISP receives all IP packets destined for 200.23.16.0/20.\nThen ISP can send IP packets to organizations depending on IP address, e.g. 200.23.16.0/23 goes to organization 1.\nRecall) This is same as longest prefix match in routers!\nISP's IP address\nICANN (Internet Corporation for Assigned Names and Numbers) allocates IP addresses through 5 regional registries (RRs).\nICANN also manages DNS root zone, delegates individual TLD, etc.\nICANN allocated last chunk of IPv4 addresses to RRs in 2011.\nIPv6 has 128-bit address space, which is enough for assigning different IP addresses to each device.\nHowever, IPv6 is not widely used because IPv4 was enough and IPv6 didn't make internet faster...\nIn fact, router can have smaller forwarding table if there are fewer networks.\nNAT (Network address translation)\nAll devices in local network share just one IPv4 address for the internet!\n(Sharing multiple IP address is possible, but typically only 1 IP address is shared.)\nAdvantages:\n\nOnly one IP address is needed from provider ISP\nCan change address of host in local network without notifying outside world\nCan change ISP without changing addresses of devices in local network\nSecurity: Devices inside local network is not directly addressable &amp; visible by outside world\n\nNAT router\n\nFor each outgoing datagram, NAT router replaces source (local IP address + port number) to (NAT IP address + new port number).\nNAT router store translation pairs in NAT translation table.\nFor each incoming datagram, NAT router replaces destination (NAT IP address + port number) to (local IP address + port number) using NAT translation table.\n\nCriticism of NAT\n\nRouter should only process up to network layer, but NAT process up to transport layer (replaces port number)\nAddress shortage is supposed to be solved by IPv6\nViolates end-to-end argument (network-layer device should only transport packets)\nClient can't connect to server behind NAT (other method such as port forwarding is required)\n\nBut NAT is extensively used in home networks, institutional networks, and 4G/5G cellular networks.\nIPv6\nMotivation: 32-bit IPv4 address space is completely allocated\nOther motivations: More options - uses 40 byte fixed length header (c.f. IPv4 uses 20 byte header)\ne.g. enable different network-layer tratment of flows, speed processing/forwarding\nHow does network operate with mixed IPv4 and IPv6 routers?\nWe use tunneling: IPv6 datagram is carried as payload in IPv4 datagram among IPv4 routers.\nRouters determine IPv4 subpass, and use tunneling inside IPv4 pass. (We use IPv6 datagram directly outside IPv4 pass.)\nIPv6 datagram format\n\nPriority: Identify priority among datagrams in flow\nFlow label: Identify datagrams in same flow (but concept of flow is not well defined...)\nPayload length\nHop limit: Same as TTL\nSource/destination address (128-bit)\n\nUnlike IPv4, IPv6 doesn't have checksum, fragmentation/reassembly, options.\nIPv6 options are available as upper-layer, next-header protocol at router.\nIPv6 adoption\nGoogle claims that about 48% of clients use IPv6. However 52% of clients still uses IPv4...\nKorea tends to use IPv4, and Japan tends to use IPv6. China have to use IPv6.\nIPv6 came out 25 years ago, why is it taking so long?\nThere are hundreds of thousands of ISPs... They don't want to move to IPv6 because it doesn't make them money.\nSDN (Software-Defined Networking)\nReview: each router contains a forwarding table that forwards packets based on destination IP address.\nSDN is software-defined, so we can use other fields and other operations.\nAlso, we can use algorithms other than shortest path, e.g. use backup links, prioritize some packets.\nGeneralized forwarding\nWe can abstract generalized forwarding into &quot;Match + Action&quot; abstraction.\n\nMatch: Any pattern values in any packet header fields are matched. (e.g. MAC address, port number, etc.)\nActions: In addition to forwarding, you can also drop, copy, modify, and log packets. (Traditional router should only transport packets.)\n\nGeneralized forwarding examples\n\nRouter matches longest destination IP prefix, then forward it out a link.\nFirewall matches IP addresses and TCP/UDP port numbers, then permit or deny it.\nNAT matches IP address and port, then rewrite IP address and port.\n\nMiddleboxes\nMiddlebox is an any intermediary box performing functions apart from normal, standard functions of an IP router on the data path between a source host and destination host.\ne.g. NAT, Firewalls, Web cache\nInitially middleboxes started with blackbox hardware solutions, but it moved toward whitebox hardware implementing open API.\nIt can be implemented with programmable local actions via match + action, and it is moving towards innovation/differentiation in software.\nIP hourglass\nCriticism of IP?\nThere are many protocols in physical, link, transport, and application layers.\ne.g. HTTP/SMTP/QUIC/DASH, TCP/UDP, Ethernet/Wi-Fi/Bluetooth, copper/radio/fiber\nBut there is only one network layer protocol: IP.\nWhy? IP is very simple and reliability is not needed.\nIP must be implemented by every internet-connected devices.\nMiddleboxes are trying to solve this issue?\nThere is only one network layer protocol, but there are many middleboxes operating inside network layer.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Routing","url":"/posts/89/","content":"Network-layer functions\n\nForwarding: Move packets from router's input to appropriate router output in data plane.\nRouting: Determine route taken by packets from source to destination in control plane.\n\nPer-router control is a traditional way to implement control plane.\nLogically centralized control (SDN) is now used.\nRouting algorithm\nGoal: Determine good paths (e.g. least cost, fastest, least congested) from sending hosts to receiving host, through network of routers.\nWe can abstract network into graphs!\nCost of direct link can be defined as 1, inverse of bandwidth, or rate of congestion.\n\n\nGlobal: All routers have complete topology, link cost info. Also called link state algorithms.\n\n\nDecentralized: Routers initially only know link costs to attached neighbors, use iterative process of computation by exchanging info with neighbors. Also called distant vector algorithms.\n\n\nStatic: Routes change slowly over time\n\n\nDynamic: Routes can change quickly, by periodic updates or in response to link cost changes.\n\n\nDijkstra's link-state routing algorihm\n\n\nCentralized: All routers have complete topology\n\n\nIterative: After k iterations, routers know the least cost path to k destinations\n\n\ncx,yc_{x,y}cx,y​ is a direct link cost from node x to y, cx,y=∞c_{x,y} = \\inftycx,y​=∞ if x, y is not direct neighbors\n\n\nD(v)D(v)D(v) is a current estimate of cost of least-cost-path from source to v.\n\n\np(v)p(v)p(v) is a predecessor node along path from source to v.\n\n\nN′N&#x27;N′ is a set of nodes whose least-cost-path is known.\n\n\n\nN′=uN&#x27; = {u}N′=u\nFor all nodes v,\n\nIf v is adjacent to u, D(v)=cu,v,p(v)=uD(v) = c_{u,v}, p(v) = uD(v)=cu,v​,p(v)=u\nElse D(v)=∞,p(v)=NULLD(v) = \\infty, p(v) = \\text{NULL}D(v)=∞,p(v)=NULL\n\n\nLoop until N′=VN&#x27; = VN′=V\n\nFind w not in N′N&#x27;N′ such that D(w)D(w)D(w) is a minimum\nAdd w to N′N&#x27;N′\nFor all nodes v adjacent to w and not in N',\n\nIf D(v)&gt;D(w)+cw,vD(v) &gt; D(w) + c_{w,v}D(v)&gt;D(w)+cw,v​, update D(v)D(v)D(v) and p(v)=wp(v) = wp(v)=w\n\n\n\n\n\nDiscussion of Dijkstra's link-state algorithm\n\nTies can exist, it's up to implementation.\nWith priority queue, algorithm complexity is O((N+M)log⁡N)O((N + M) \\log N)O((N+M)logN).\nBut each router must broadcast its link state information to have complete topology!\nEfficient broadcast algorithms can broadcast in O(N)O(N)O(N).\nSince there are N routers, overall message complexity is O(N2)O(N^2)O(N2). (NOT broadcast time complexity)\n\nRoute oscillation of Dijkstra's algorithm\n\nUnlike in graph theory, real network's link costs depend on traffic volume.\nIf the cost changes because of traffic, route oscillation is possible!\nOne way to reduce oscillation is to broadcast at random time to prevent all routers from broadcasting at the same time.\nBellman-Ford's distance vector algorithm\nLet cx,yc_{x,y}cx,y​ is a direct link cost from node x to y, and Dx(y)D_x(y)Dx​(y) is the cost of least-cost path from x to y.\nThen, Bellman-Ford (BF) equation states that Dx(y)=min⁡v(cx,v+Dv(y))D_x(y) = \\min_v (c_{x,v} + D_v(y))Dx​(y)=minv​(cx,v​+Dv​(y)).\n\nEach router wait for change in local link cost or message from neighbor.\nWhen router receives new DV estimate or new local link cost, it recomputes DV estimates using BF equation Dx(y)=min⁡v(cx,v+Dv(y))D_x(y) = \\min_v (c_{x,v} + D_v(y))Dx​(y)=minv​(cx,v​+Dv​(y)).\nIf DV estimate to any destination has changed, notify other neighbors.\nAfter enough iterations, the DV estimate converges to the actual least cost.\n\nDiscussion of Bellman-Ford's distance vector algorithm\n\nUnlike LS(link-state) algorithm, we don't know actual path to the destination.\nUnlike LS algorithm, we don't have a fixed time complexity because convergence time varies.\nIterative, asynchronous: Each local iteration is caused by local link cost change or DV update message from neighbor.\nDistributed, self-stopping: Each node notifies neighbors only if necessary. (i.e. only when its DV changes)\nLess robustness: If router malfunctions, LS algorihm only advertises incorrect link cost, and each router computes only its own table.\nHowever, DV algorithm advertises incorrect path cost, and each router's table is used by others, so error propagate through the network.\n\nCount-to-infinity problem of Bellman-Ford's algorithm\nGood news travels fast - reduced link cost require only few iterations.\nBad news travels slow - increased link cost require many iterations.\nCount-to-infinity problem: For one iteration, the path cost can increase by up to the direct link cost. If link cost has increased much more than the smallest direct link cost, it will take much more iteration to converge to the actual least cost.\nEven worse, if a link is disconnected resulting in a new link cost of infinity, these routers will count up to infinity!\nSolution: If DV estimate is updated by z's DV estimate, do not broadcast DV estimate update to z.\nScalable routing algorithm\nWe can have billions of destinations in practice, routing table exchange would swamp links! In fact, we can't even store all destinations in routing tables.\nAutonomous systems (AS)\nWe aggregate routers into regions known as autonomous systems. (a.k.a. domains)\n\nIntra-AS (a.k.a. intra-domain): Routing among within same AS\nAll routers in AS must run same intra-domain protocol.\nWe have a gateway router at edge of its own AS, which has links to routers in otehr AS'es.\nInter-AS (a.k.a. inter-domain): Routing among AS'es\n\nGateway routers perform inter-domain routing as well as intra-domain routing.\n\n\n\nForwarding table is configured by intra-AS and inter-AS routing algorithms.\nIntra-AS routing determine entries for destinations within AS.\nInter-AS routing &amp; intra-AS routing determine entries for external destinations.\nWhy differentiate intra-AS and inter-AS?\nIntra-AS have single admin, so policy is less of an issue, and we can focus on performance.\nInter-AS have multiple admins that wants to control over how its traffic is routed, so policy dominates over performance.\nIntra-AS routing protocols\n\nRIP (Routing Information Protocol)\n\nClassic DV algorithm, with DVs exchanged every 30 seconds.\nNo longer widely used.\n\n\nEIGRP (Enhanced Interior Gateway Routing Protocol)\n\nBetter DV based algorithm!\nFormerly Cisco-proprietary, became open in 2013.\n\n\nOSPF (Open Shortest Path First)\n\nClassic LS algorithm\nISO standard IS-IS (Intermediate System to Intermediate System) protocol is same as OSPF. (c.f. OSPF is RFC standard)\n\n\n\nOSPF routing\n\nEach router floods OSPF link-state advertisements to all other outers directly over IP (rather than using TCP/UDP)\nEach router has full topology, so we use Dijkstra's algorithm to compute forwarding table.\n\n\nMultiple link costs metrics possible. e.g. bandwidth, delay\nTo ensure security, all OSPF messages are authenticated to prevent malicious intrusion.\n\nHierarchical OSPF\nTwo-level hierarchy is used to prevent route oscillation from spreading throughout the entire network.\nHierarchy also helps us to reduce number of routers to compute distances!\n\nBackbone is connected to areas and other ASes.\n\nBoundary router connects to other Ases.\nBackbone router runs OSPF limited to backbone.\n\n\nArea is only connected to backbone.\n\nLocal routers flood LS only in its area, and compute routing within its area.\nArea border routers summarize distances to destinations in own area, and advertise in backbone.\n\n\nLink-state advertisements is flooded only in area or backbone.\nEach node has detailed area topology, but only knows direction to reach other destinations.\n\nInter-AS routing protocols\nBGP (Border Gateway Protocol) is the de facto inter-domain routing protocol.\nSubnets can advertise its existence, and the destinations it can reach to rest of the internet.\nBorder Gateway Protocol (BGP)\nBGP shares reachability, not cost!\n\neBGP: Obtain subnet reachability information from neighboring ASes\niBGP: Propagate reachability information to all AS-internal routers.\n\nBGP path advertisement\nA BGP advertised route consists of a prefix and its attributes.\nPrefix contains destination being advertised.\nAttribute AS-PATH contains list of ASes through which prefix advertisement has passeed.\nAttribute NEXT-HOP contains specific internal-AS router to next-hop AS.\neg. AS X advertises path AS X, a to AS Y gateway using eBGP. (i.e. AS X promises to AS Y that it will forward datagrams towards a.)\nAS Y propagates path AS X, a to all AS Y routers using iBGP.\nTo prevent loop, AS doesn't advertise route if path contains itself.\nIf every BGP router do the same, loop won't be advertised.\nBGP messages\nBGP session: Two BGP routers exchange BGP messages over semi-permanent TCP connection.\nGateway routers are directly connected, so we can use TCP connection.\n\nOPEN: Opens TCP connection to remote BGP peer and authenticates sending BGP peer\nUPDATE: Advertises new path (or withdraws old path)\nKEEPALIVE: Keeps connection alive in absence of UPDATES. Also used to acknowledge OPEN request.\nNOTIFICATION: Reports errors in previous message. Also used to close connection.\n\nPolicy-based routing\nBGP is policy-based routing!\nAS receiving route advertisement uses import policy to accept/decline path.\nIf AS received multiple paths to destination, it can choose any specific path.\nAS can also determine whether to advertise received path to other neighboring ASes.\nc.f. Why wouldn't AS help each other?\nBecuase AS didn't pay other AS...\nBy following path advertisement, you can see the money flow between ASes.\ne.g. If customer is dual-homed (i.e. attached to two networks), customer won't want provider network to route via them.\nTherefore, customer network don't advertise route to provider network.\nHot potato routing\nRouters chooses local gateway that has least intra-domain cost.\nWhy? AS is greedy... Don't care about inter-domain cost!\nSoftware-Defined Networking (SDN)\nInternet network layer was historically implemented via distributed, per-router control approach.\nMonolithic router contains switching hardware, runs proprietary implementation of internet standrad protocols in proprietary router OS.\nDifferent middleboxes are used for different network layer functions.\nBut internet network layer is being rethought as a network control plane.\n\nPer-router control plane: Individual routing algorithm components are in each and every routers.\nSDN control plane: Remote controller computes, installs forwarding tables in routers.\n\nSDN analogy made manifactures to use open interfaces instead of specialized applications, OS, and hardware!\nBenefits of SDN\n\nEasier network management: Avoid router misconfigurations, greater flexibility of traffic flows.\nTable-based forwarding allows programming routers.\n\nCentralized programming is easier because we can compute tables centrally and distribute it.\nDistributed programming is harder because we should compute tables as a result of distributed algorithm implemented in each router.\n\n\nCan perform complex traffic engineering.\n\nIf we want to use different route, we had to redefine link weights so traffic routing algorithm computes to different routes.\nIf we want to split traffic to multiple routes, we had to use new routing algorithm.\nIf we want to route differently based on source, we can't do this with destination-based forwarding.\n\n\n\nElements of SDN\n\nData-plane switches\n\nFast, simple, commodity switches implementing generalized data-plane forwarding in hardware\nFlow (forwarding) table is computed and installed under controller supervison\nAPI for table-based switch control (e.g. OpenFlow)\nProtocol for communicating with controller\n\n\nSDN controller (network OS)\n\nMaintain network state information e.g. state of network links, switches, services\nInteracts with network control applications above via northbound API\nInteracts with network switches below via southbound API\nImplemented as distributed system for performance, scalability, fault-tolerance, and robustness\n\n\nNetwork-control applications\n\nImplement control functions using lower-level services, API provided by SDN controller\nCan be provided by third party, e.g. other than routing vendor, or DSN controller\n\n\n\nOpenFlow protocol\n\nOperates between controller and switch\nUse TCP to exchange messages (encryption is optional)\n\nController-to-switch\nAsynchronous (switch-to-controller)\nSymmetric\n\n\nDistinct from OpenFlow API\n\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Link Layer","url":"/posts/91/","content":"Link layer\n\nNodes: Hosts and routers\nLinks: Communication channels that connect adjacent nodes along communication path\nFrame: Layer-2 packet that encapsulates datagram\n\nLink layer has responsibility of transferring datagram from one node to physically adjacent node over a link.\nLink layer is implemented in each-and-every host, typically in network interface card (NIC) or on a chip.\nContext\n\nDatagram is tranferred by different link protocols over different links. (e.g. Wi-Fi, Ethernet)\nEach link protocol provides different services. (e.g. reliable/unreliable)\n\nServices\n\nFraming: Ecncapsulate datagram into frame, adding header and trailer\nLink access: Medium can be shared, channel access control needed. (e.g. MAC address)\nReliable delivery between adjacent nodes\n\nBut if link has low bit-error rate, we don't use it and let the higher layer handle reliable delivery. (e.g. TCP)\nWireless link has high bit-error rate, so we need reliable delivery in link layer.\n\n\nFlow control: Pacing between adjacent sending and receiving nodes\nError detection: Receiver detects errors caused by signal attenuation or noise, then it signals retransmission or drops frame.\nError correction: Receiver identifies and corrects bit errors without retransmission.\nHalf-duplex and full-duplex: Can both ends of link transmit at the same time?\n\nError detection and correction\n\nD: Data bits that are protected by error checking, may include header fields\nEDC: Error detection (and correction) bits that detects errors.\n\nParity checking\n\nSingle bit parity: Can detect single bit errors\nTwo-dimensional bit parity: Can detect all of 1, 2, 3 bit errors and most of 4 bit errors, and correct all of 1 bit errors.\n\nEven parity: Set parity bit so there is an even number of 1's\nRecall) Internet checksum\nTreat contents of UDP segment as a sequence of 16-bit integers.\nChecksum value is the addition of the segment content.\nCyclic Redundancy Check (CRC)\n\nD: data bits\nG: bit pattern (called generator), of r+1 bits\nR: r bits of CRC\n\nWe want to choose R such that &lt;D, R&gt; is divisble by G.\nSince &lt;D, R&gt; is divisible by G, D⋅2r⊕R=nGD \\cdot 2^r \\oplus R = nGD⋅2r⊕R=nG. Equivalently, D⋅2r=nG⊕RD \\cdot 2^r = nG \\oplus RD⋅2r=nG⊕R.\nTherefore, R should be the remainder of D⋅2r÷GD \\cdot 2^r \\div GD⋅2r÷G. (Real implementation doesn't use division.)\nBy choosing good G, we can detect all burst errors less than r+1 bits, and any odd bits errors.\nWe can also detect burst errors greater than r+1 bits with probability of 1−2−r1 - 2^{-r}1−2−r.\ne.g. 32bit CRC uses G=100000100110000010001110110110111\nMultiple Access Protocols\n\nInterference: two or more simultaneous transmissions by nodes.\nCollision - A node received two or more signals at the same time.\n\nMultiple access protocol is a distributed algorithm that determines how nodes share channel, or when node can transmit.\nCommunication about channel sharing must use the channel itself!\nGoal: When M nodes want to transmit and channel has send rate up to R bps, each nodes should send at rate R/M.\nMultiple access links\n\nPoint-to-point: point-to-point link between ethernet switch\nBroadcast: Wire or medium is shared. e.g. Wi-Fi, 4G, satellite\n\nChannel partitioning\nDivide channel into smaller pieces, and allocate piece to node for exclusive use.\nTDMA (Time division multiple access)\nEach station gets fixed length (= packet transmission time) slot in each round.\nUnused slots go idle.\nFDMA (Frequency division multiple access)\nChannel spectrum is divided into frequency bands.\nEach station is assigned fixed frequency band.\nUnused bands go idle.\nRandom access\nChannel is not divided, and collisions are allowed.\nNodes should recover from collisions.\nSlotted ALOHA\nAssumptions:\n\nAll frames have same size\nTime is divided into equal size slots\nNodes start to transmit only slot beginning\nNodes are synchronized\nAll nodes can detect collision (2 or more nodes transmit in same slot)\n\nEach node transmit until collision happens.\nWhen collision happens, node retrnasmits frame with probability p until success.\nIf N nodes have many frames to send, and each transmit in slot with probability p,\nProbability of one node successfuly send in one slot is p(1−p)N−1p(1-p)^{N-1}p(1−p)N−1.\nProbability of any node successfuly send in one slot is Np(1−p)N−1Np(1-p)^{N-1}Np(1−p)N−1.\nWhen p=1Np = \\frac{1}{N}p=N1​, probability is maximized. (1−1N)N−1\\left( 1-\\frac{1}{N} \\right)^{N-1}(1−N1​)N−1\nWhen N goes to infinity, we get max efficiency 1e≈37%\\frac{1}{e} \\approx 37\\%e1​≈37%.\nPros:\n\nIf only one node is active, it can transmit at full rate of channel\nHighly decentralized: only slots in nodes need to be in sync\n\nCons:\n\nCollision + Idle slots\nNodes should be able to detect collision in less than time to transmit packet\nClock should be synchronized.\n\nPure ALOHA\nUnslotted ALOHA! Without synchronization, we just transmit at any time.\nFrame sent at t0t_0t0​ will collide with other frames sent in [t0−1,t0+1][t_0 - 1, t_0 + 1][t0​−1,t0​+1].\nSince time slot is doubled, probability of one node successfuly send in one slot is p(1−p)2(N−1)p(1-p)^{2(N-1)}p(1−p)2(N−1).\nWhen p=12N−1p = \\frac{1}{2N-1}p=2N−11​, probability of any node successfuly send in one slot is maximized. N2N−1(1−12N−1)2(N−1)\\frac{N}{2N-1}\\left( 1-\\frac{1}{2N-1} \\right)^{2(N-1)}2N−1N​(1−2N−11​)2(N−1)\nMax efficiency is halved: 12e≈18%\\frac{1}{2e} \\approx 18\\%2e1​≈18%!\nCSMA/CD\nSimple CSMA (Carrier sense multiple access): listen before transmit.\nIf channel is idle, transmit the entire frame.\nIf channel is busy, defer the transmission.\nCSMA/CD: CSMA with collision detection!\nBecause of propagation delay, two nodes may not hear each other's just started transmission, leading to collision.\nWithout CD, entire packet transmission time is wasted.\nWith CD, transmission is aborted on collision detection, so the amount of time wasted in collisions is reduced.\nEthernet CSMA/CD algorithm\n\nNIC receives datagram from network layer and creates frame.\nIf channel is idle, start frame transmission.\nIf channel is busy, wait until channel is idle, then transmit.\nIf NIC detects another transmission while sending (collision), abort and send jam signal.\nAfter aborting, NIC enters binary (exponential) backoff.\n\nAfter mmm th collsion, NIC chooses K at random from 0 ~ 2m−12^m -12m−1. (0 is possible!!!)\nNIC waits K⋅512K \\cdot 512K⋅512 bit times, then return to step 2.\n\n\n\nLet tpropt_{prop}tprop​ is the maximum propagation delay between two nodes in LAN,\nand ttranst_{trans}ttrans​ is the time to transmit max-size frame.\nThen the efficiency of CSMA/CD is:\ne=ttransttrans+5tprop=11+5tpropttranse = \\frac{t_{trans}}{t_{trans} + 5t_{prop}} = \\frac{1}{1 + 5\\frac{t_{prop}}{t_{trans}}}\ne=ttrans​+5tprop​ttrans​​=1+5ttrans​tprop​​1​\n(Collision detection 2tprop2t_{prop}2tprop​ + Jam signal tpropt_{prop}tprop​ + Collision detection again 2tprop2t_{prop}2tprop​)\nIf tprop→0t_{prop} \\to 0tprop​→0 or ttrans→∞t_{trans} \\to \\inftyttrans​→∞, e→1e \\to 1e→1.\nBetter performance than ALOHA with simple, cheap, decentralized algorithm!\nCollision should be detected before whole frame is sent, so Etherenet defines least frame size. (eg. 64 byte in 10Mbps Ethernet)\nTaking turns\nNodes takes turns, but nodes with more to send can take longer turns.\nPolling\nController node invites other nodes (clients) to transmit in turn.\nTypically used with dumb devices. e.g. Bluetooth.\nCons: Polling overhead, Latency (should wait for their turn), Single point of failure (controller)\nToken passing\nWe use a special message named token.\nControl token is passed from one node to next sequentially.\nThe nodes forms a ring structure.\nCons: Token overhead, Latency (should wait until token arrives), Single point of failure (token)\ne.g. Machine with token can fail, token message can be lost\nPros: Collision is always avoided.\nCable access network\n\nCable access network uses FDM, TDM, and random access!\n\nMutliple downstream: Single CMTS transmits into FDM channels. No collision!\nMultiple upstream channels: Multiple clients transmits into TDM channels.\nMultiple access: Clients should send special message to CMTS before sending data (using random access). This message can have collision!\nClients assume that collision happened if they fail to receive response. They use exponential backoff until it receives response.\n\nRandom access is used in access request because message is small.\nTDM is used after time slot is reserved for fast transfer.\nDOCSIS (Data over cable service interface specification)\nDOCSIS use FDM over downstream, and TDM over upstream.\nDownstream MAP assigns some upstream slots.\nUpstream use allocated TDM slots, or use minislots (with random access) for slot requests.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"LAN","url":"/posts/92/","content":"MAC address\nIP address is used for layer 3, MAC address is used for layer 2!\nMAC (or LAN or physical or Ethernet) address is used locally to get frame from one interface to another physically-connected interface.\n48-bit MAC address is burned in NIC ROM, but  it is software settable nowdays.\ne.g. 1A-2F-BB-76-09-AD\nQ. How do I get a unique MAC address?\nA. There are few network card manufacturers, and each network card manufacturers have unique prefix of MAC address.\nMAC address is portable; We can move interface from one LAN to another.\nc.f. IP address is not portable! It depends on IP subnet to which node is attached.\nARP (Address resolution protocol)\nDetermine interface's MAC address using its IP address.\nEach IP node (host, router) on LAN has ARP table.\nARP table contains IP/MAC address mappings for some LAN nodes.\nARP table also contains TTL (Time To Live), time after which address mapping will be forgotten. (Typically 20 minutes)\nARP in same network\n\nA broadcasts ARP query, containing B's IP address. Source MAC, source IP, target IP are set correctly, and destination MAC address is set to all ones. (FF-FF-FF-FF-FF-FF)\nWe need to broadcast because we use Layer 2 to transfer IP packet, which requires a MAC address.\nAll nodes on LAN receive ARP query.\nB replies to A with ARP response, setting source MAC, source IP, target MAC, target IP correctly.\nA receives B's reply, then store source MAC (i.e. B's MAC address) into its ARP table.\n\nARP in other network\nAssume that we are sending a datagram from A to B via router R.\nA knows B's IP address, IP address of first hop router R (via DHCP), and R's MAC address (via ARP).\nA can't get B's MAC address via ARP because they are not in the same LAN (i.e. same network).\n\nA send link-layer frame to R with A's MAC address as a source and R's MAC address as a destination.\n(Link-layer of) R pass IP datagram (from A to B) up to network-layer.\n(Network-layer of) R determines outgoing interface, then pass IP datagram back to link-layer.\n(Link-layer of) R creates link-layer frame to B with R's MAC address as a source and B's MAC address as a destination.\nB receives link-layer frame, extracts IP datagram, then pass payload up to application.\n\nEthernet\nEthernet is the first and the most dominant wired LAN technology.\n\n\nSimple and cheap\n\n\nKept up with speed race (from 10Mbps to 400Gbps)\n\n\nSingle chip can have multiple speeds\n\n\nConnectionless: No handshaking between sending and receiving NICs(Network Interface Card)\n\n\nUnreliable: Receiving NIC doesn't send ACKs or NAKs to sending NIC. (Only higher layer can detect dropped data frame e.g. TCP)\n\n\nUse unslotted CSMA/CD with binary backoff as a Ethernet's MAC protocol.\n\n\nPhysical topology of Ethernet\n\nBus\n\nCoaxial cable (bus) connects all nodes.\nPopular through mid 90s.\nAll nodes can collide with each other.\n\n\nSwitch\n\nActive link-layer switch is in center, connected with all nodes.\nWidely used today.\nEach incoming link runs a separate Ethernet protocol, so nodes do not collide with each other.\n\n\n\nEthernet frame structure\nPreamble\nUsed to synchronize receiver, sender clock rates.\nIt consists of 7 bytes of 10101010 followed by one byte of 10101011.\nAddresses\n6 bytes of destination MAC address, followed by 6 bytes of source MAC address.\nIf adapter receives frame with matching destination address, or with broadcast address, it passes data in frame to network layer protocol.\nOtherwise, adapter discards frame.\nType\nDetermines higher layer protocol, mostly IP.\nHowever, any network-layer protocol can be used in theory. e.g. Novell IPX, AppleTalk\nThe receiver can use type bytes in a demultiplexer.\nPayload\nNetwork-layer datagram, mostly IP datagram.\nCRC\nCyclic redundancy check at receiver.\nIf error is detected, frame is dropped.\n802.3 Ethernet standards\nThere are many different Ethernet standards!\n\nStandardize the link and physical layers\nCommon MAC protocol and frame format\nDifferent speeds: 2Mbps ~ 40Gbps\nDifferent physical layer media: Fiber, cable\n\nEthernet switch\nSwitch examine incoming frame's MAC address, selectively forward frame to one-or-more outgoing links when frame is to be forwarded on segment, uses CSMA/CD to access segment.\nTheoretically, switch is a link-layer device, i.e. it only looks at Ethernet frames and doesn't look at IP datagrams.\nIn fact, other network-layer protocols can be used too! (Because that's why we have layered networks...)\nHowever, in real life, we use a Layer 3 switch which is a combination of a switch and a router.\nMultiple simultaneous transmissions\nEach host have a dedicated, direct connection to switch.\nSwitches can buffer packets, and each link have its own Ethernet protocol and collision domain.\nFull duplex connection is possible without collisions!\nBut connection sharing either ends can't happen simultaneously.\nSwitch forwarding table\nSwitch has a switch table that records MAC address of host and interface to reach it.\nSwitch forwarding table can be self-learned!\nWhen frame is received to switch, switch records MAC address of the sender and its interface.\nIf destination is not yet found, switch will flood the frame. (i.e. It will forward on all interfaces except arriving interface.)\nInterconnecting switches\nSwitches can be interconnected!\nSelf learning works exactly the same!\nHowever, loop can happen if switch is connected in loop.\nSwitches have algorithm that makes spanning tree from its graph.\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Link Virtualization","url":"/posts/93/","content":"VLAN (Virtual Local Area Network)\nMotivation\nIf user change point of attachment, All layer-2 broadcast traffic must cross entire LAN. (e.g. ARP, DHCP, unknown MAC)\nAlso, user might wants to remain logically attached to certain switch even when it moves.\nPort-based VLANs\nSwitch ports are grouped by switch management software.\nSingle physical switch can operate as multiple virtual switches!\nPorts can be dynamically assigned among VLANs, and forwarding between VLANs is done via routing. (Switchs and routers are combined!)\nTrunk port: VLANs can be defined over multiple physical switches! Switches use trunk port to carries frames between VLANs.\n802.1Q protocol adds 12bit VLAN ID field for frame headers.\nLink Virtualization\nMultiprotocol label switching (MPLS)\nFor high-speed IP forwarding, we use fixed length label instead of longest prefix matching.\n\nFaster lookup than longest prefix matching\nIdeas from virtual circuit (VC) approach\nBut IP datagram still keeps IP address!\n\nMPLS header is included between ethernet header and IP datagram.\nMPLS capable routers\na.k.a. Label switched router\nForward packets to outgoing intreface based only on label value. (without IP address!!)\nMPLS forwarding table is distinct from IP forwarding tables.\nMPLS forwarding decisions can differ from those of IP!\nIt can use source address to route flows to same destination differently. (traffic engineering)\nIt can also precompute backup paths and re-route flows quickly if link fails.\nMPLS signaling\nUnlike OSPF or IS-IS link state flooding protocols, MPLS also carry info such as: link bandwidth, amount of reserved link bandwidth, etc.\nDatacenter networks\nOver 100,000 hosts are connected! Often closely coupled, in close proximity.\n\nMultiple applications, each serving massive numbers of clients\nReliability is hard\nManaging/balancing load, avoiding processing, networking, and data bottlenecks\n\nDatacenter network elements\n\n\nBorder router: Connections to outside of datacenters\nTier-1 switch: Connecting to (about 16) tier-2 switches\nTier-2 switch: Connecting to (about 16) TOR switches\nTOR (Top of rack) switch: One per each rack! 40 ~ 100 Gbps Ethernet\nServer racks: 20 ~ 40 server blades (hosts)\n\nThis structure makes rich interconnection among switches and racks!\nMultiple routing paths between same racks exists, so throughput and reliability is increased.\nApplication-layer routing\nLoad balancer receives external client requests, then directs workload within data center.\nResults are returned to external client, while hiding data center internals from client.\nDatacenter protocols\n\nLink layer: RoCE (Remote DMA over converged Ethernet)\nTransport layer: ECN (Explicit congestion notification) is used for congestion control (currently experimenting hop-by-hop congestion control)\nRouting, management: SDN is widely used within/among organizations' datacenters.\nRelated services and datas are placed as close as possible (e.g. in same rack) to minimize tier-2, tier-1 communication.\n\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Network Security","url":"/posts/85/","content":"Network security\n\nConfidentiality: only sender, intended receiver should understand message contents\n\nSender encrypts message\nReceiver decrypts message\n\n\nAuthentication: sender, receiver want to confirm identity of each other\nMessage integrity: sender, receiver want to ensure message not altered (in transit, or afterwards) without detection\nAccess and availability: services must be accessible and available to users\n\nAttacks on internet security\nBob and Alice want to communicate securely each other.\nTrudy (intruder) may interrupt!\n\nEavesdrop: intercept messages\nActively insert messages into connection\nImpersonation: can fake (spoof) source address in packet (or any field in packet)\nHijacking: take over ongoing connection by removing sender or receiver, then inserting himself in place\nDenial of Service: prevent service from being used by others (e.g. by overloading resources)\n\nCryptography\nAlice wants to send a plaintext message mmm.\nShe sends a ciphertext KA(m)K_A(m)KA​(m) encrypted with her encryption key KAK_AKA​.\nBob decrypt it with Bob's decryption key KBK_BKB​, and get the original message mmm.\nBreaking an encryption scheme\n\nCipher-text only attack: Trudy has ciphertext she can analyze.\n\nBrute force: Search through all keys\nStatistical analysis\n\n\nKnown-plaintext attack: Trudy has plaintext and corresponding ciphertext. (e.g. ciphertext for a, b, c, ...)\nChosen-plaintext attack: Trudy can get ciphertext for chosen plaintext.\n\nSymmetric key cryptography\nBob and Alice share same (symmetric) key KSK_SKS​.\nBob and Alice should agree on key value.\nSubstitution cipher\nSubstitute one letter for another.\nEncryption key is mapping from set of 26 letters (alphabets) to set of 26 letters.\nWe can also use n subtitution ciphers (e.g. M1,M2,M3,M4,M5M_1, M_2, M_3, M_4, M_5M1​,M2​,M3​,M4​,M5​) and cycling pattern. (e.g. M1,M3,M4,M5,M2M_1, M_3, M_4, M_5, M_2M1​,M3​,M4​,M5​,M2​).\nFor each new letter, use subsequent substitution pattern in cyclic pattern.\ne.g. When encrypting dog, encrypt d with M1M_1M1​, encyrpt o with M3M_3M3​, then encrypt g with M4M_4M4​.\nEncryption key is n substitution ciphers and cyclic pattern.\nAttack on Substitution cipher\nIf input is same, output is always same! Statistical analysis can be used to get the mapping.\nDES, AES\nDES (Data Encyrption Standard) is US encryption standard, (NIST 1993) but it can be decrypted in less than a day with bruteforce.\nHowever, there are no known good analytic attack.\n3DES was used briefly, which encrypt 3 times using DES, with 3 different keys.\nLater AES replaced DES in November 2001.\nBrute force decryption takes 149 trillion years for AES.\nThis is widely used, and even CPU and datacenter network cards have hardware implementation for AES.\nPublic key cryptography\nAlice and Bob have public key and private key.\nPublic encryption key is known to everyone, but private decryption key is known only to them.\n\nAlice encrypt plaintext message mmm with Bob's public key KB+K_B^+KB+​.\nBob decrypt ciphertext KB+(m)K_B^+(m)KB+​(m) with Bob's private key KB−K_B^-KB−​.\n\nCiphertext can't decrypted with public key!\nWe can just open public key to anyone.\nRequirements of public key cryptography\n\nPrivate key can decrypt message encrypted with public key.\nPrivate key can't be computed from public key.\n\nObviously, public key and private keys are paired!\nWe cannot change only one of the keys.\nRSA (Rivest, Shamir, Adelson) algorithm\nEvery message can be represented as bit pattern, and bit pattern can be uniquely represented by an integer number.\nTherefore, we can encrypt any message if we can encrypt any number.\nRSA is actually slower than AES, so normally we use RSA to only establish secure connection.\nFirst, Bob and Alice use RSA to exchange a symmetric session key.\nThen, they use symmetric key cryptography to actually send the data.\nCreating public/private key pair\n\nChoose two large prime numbers p, q. (Normally 1024 bits)\nn=pq,z=(p−1)(q−1)n = pq, z = (p - 1)(q - 1)n=pq,z=(p−1)(q−1).\nChoose e&lt;ne &lt; ne&lt;n that has no common factors with zzz. (i.e. e and z are relatively prime)\nChoose ddd such that ed−1ed-1ed−1 is divisible by zzz. (i.e. ed mod z=1ed \\bmod z = 1edmodz=1)\nPublic key is KB+=(n,e)K_B^+ = (n, e)KB+​=(n,e), and private key is KB−=(n,d)K_B^- = (n, d)KB−​=(n,d).\n\nEncryption and decryption of RSA\n\nTo encrypt message m&lt;nm &lt; nm&lt;n, compute c=me mod nc = m^e \\bmod nc=memodn.\nTo decrypt ciphertext ccc, compute m=cd mod nm = c^d \\bmod nm=cdmodn.\n\nEuler's Theorem states that if gcd⁡(x,n)=1\\gcd(x, n) = 1gcd(x,n)=1, then xz≡1(modn)x^z \\equiv 1 \\pmod nxz≡1(modn).\nRecall) n=pq,z=(p−1)(q−1)n = pq, z = (p-1)(q-1)n=pq,z=(p−1)(q−1).\nTherefore, ∀x,y,xy mod n=xy mod z mod n\\forall x,y, x^y \\bmod n = x^{y \\bmod z} \\bmod n∀x,y,xymodn=xymodzmodn if gcd⁡(x,n)=1\\gcd(x, n) = 1gcd(x,n)=1.\n∴cd mod n=(me mod n)d mod n=med mod n=med mod z mod n=m1 mod n=m\\begin{align*}\n\\therefore c^d \\bmod n &amp;= \\left( m^e \\bmod n \\right)^d \\bmod n \\\\\n&amp;= m^{ed} \\bmod n \\\\\n&amp;= m^{ed \\bmod z} \\bmod n \\\\\n&amp;= m^1 \\bmod n \\\\\n&amp;= m\n\\end{align*}∴cdmodn​=(memodn)dmodn=medmodn=medmodzmodn=m1modn=m​\nIn fact, public key and private key can be applied in any order!\n(me mod n)d mod n=med mod n=(md mod n)e mod n\\left( m^e \\bmod n \\right)^d \\bmod n = m^{ed} \\bmod n = \\left( m^d \\bmod n \\right)^e \\bmod n\n(memodn)dmodn=medmodn=(mdmodn)emodn\nAttacking RSA?\nTo get a private key from public key, essentially you need to find factors of nnn.\nHowevery, factoring a big number is very hard!\nIf quantum computers are commercialized, prime factorization will become easier and RSA will be vulnerable.\nThere are post-quantum cryptography which is safe even when quantum computers are used.\nAuthentication\nGoal: Bob wants Alice to prove her identity to him\nap1.0\nAlice says &quot;I am Alice&quot;, but Trudy can do the same..\nap2.0\nAlice says &quot;I am Alice&quot; with Alice's IP address, but Trudy can do the same... (Packet spoofing)\nap3.0\nAlice says &quot;I am Alice&quot; with Alice's password, but Trudy can record and repeat Alice's packet. (Playback attack)\n(Alice can encrypt her password with Bob's public key, but playback attack still works)\nap4.0\nAlice says &quot;I am Alice&quot;, then Bob send nonce to Alice.\nNonce is a random number used only once-in-a-lifetime.\nAlice must return nonce, encrypted with shared secret key.\nThis works, but how do we get shared secret key in the first place?\nap5.0\nAlice says &quot;I am Alice&quot;, then Bob send nonce to Alice.\nAlice return nonce encrypted with Alice's private key.\nBob gets Alice's public key, then verify nonce.\nAfterwards, Bob send message to Alice using Alice's public key!\nAlso called challenge-response protocol!\nRecall) RSA can swap private/public keys! Encrypted message with private key can decrypted with public key.\nMan in the middle (MITM) attack\n\nProblem: We don't verify whether public key actually belongs to Alice!\nTrudy poses as Alice to Bob, and as Bob to Alice.\nBob gets Trudy's public key instead of Alice's public key!\nTrudy can read every message (or even modify it).\nMessage Integrity\nDigital Signatures\nGoal: Verifable and nonforgeable. Recipient (Alice) can prove to someone that sender (Bob), and no one else, must have signed document.\n\nBob sends message m with signature KB−(m)K_B^-(m)KB−​(m). (message encrypted with Bob's private key)\nAlice verifies signature with Bob's public key.\nIf KB+(KB−(m))=mK_B^+(K_B^-(m)) = mKB+​(KB−​(m))=m, Alice can verify that:\n\nBob signed m\nNo one else signed m\nBob signed m and not m'\n\n\n\nMessage digests\nIt is computationally expensive to encrypt/decrypt long messages.\nGoal: We need fixed-length, easy-to-compute digital fingerprint!\nInstead of the entire message, we apply hash function to message, then encrypte it.\nHashed message is called message digest.\n\nBob sends message m with signature KB−(H(m))K_B^-(H(m))KB−​(H(m)).\nAlice receives m, and apply hash function to get H(m)H(m)H(m).\nAlice verifies signature with Bob's public key, then check if KB+(KB−(H(m)))=H(m)K_B^+(K_B^-(H(m))) = H(m)KB+​(KB−​(H(m)))=H(m).\n\nHash function algorithms\nMD5 was widely used, but deprecated due to vulnerability. (Collisions can made in less than 1 second)\nSHA-1 is used, but it was not secure enough, and it should be phased out by 2030.\nCurrently SHA-256 is used.\nWe can get more secure hash function by increasing hash length. (e.g. SHA-512, SHA-3)\nBut secure hash functions are slower, so SHA-256 is favored.\nPublic key certification authorities (CA)\nProblem: Signature doesn't help MITM atack, we still doesn't know whether the public key belongs to Alice!\nWe need a way to verify which public key belongs to an entity.\nCA binds public key to particular entity.\nWhen entity E registers its public key with CA, CA creates certificate binding identity E to its public key.\nEssentially it is a E's public key encrypted with CA's private key. KCA−(KE+)K_{CA}^-(K_E^+)KCA−​(KE+​).\nReceiver can verify certificate with CA's public key to get E's public key. KCA+(KCA−(KE+))=KE+K_{CA}^+(K_{CA}^-(K_E^+)) = K_E^+KCA+​(KCA−​(KE+​))=KE+​.\nProblem: How do we even believe CA?? + How do we get CA's public key in the first place?\nSolution: Browsers hardcode known CA's public keys...\nSecure message\nHow can Alice send message m to Bob, with confidentiality, message integrity, and authentication?\n\nAlice generates message digest with Alice's private key. KA−(H(m))K_A^-(H(m))KA−​(H(m)).\nCombine message and message digest to one message. (Message integrity, Authentication) M=m+KA−(H(m))M = m + K_A^-(H(m))M=m+KA−​(H(m))\nAlice generates random symmetric private key KSK_SKS​.\nAlice encrypts message with symmetric key. KS(M)K_S(M)KS​(M)\nBob needs to know the symmetric key too, Alice encrypts symmetric key with Bob's public key. KB+(KS)K_B^+(K_S)KB+​(KS​)\nAlice sends Bob KS(M)K_S(M)KS​(M) and KB+(KS)K_B^+(K_S)KB+​(KS​). (Confidentiality)\n\nAlice need three keys: Alice's private key, Bob's public key, new symmetric key.\n\nBob decrypt KB+(KS)K_B^+(K_S)KB+​(KS​) with his private key and recover KSK_SKS​.\nBob decrypt KS(M)K_S(M)KS​(M) with KSK_SKS​ and recover M=m+KA−(H(m))M = m + K_A^-(H(m))M=m+KA−​(H(m)).\nBob apply hash function to message. H(m)H(m)H(m)\nBob decrypt message digest using Alice's public key. KA+(KA−(H(m)))K_A^+(K_A^-(H(m)))KA+​(KA−​(H(m)))\nBob verify whether two are same.\n\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"TLS","url":"/posts/94/","content":"Transport-layer security (TLS)\nWidely deployed security protocol above the transport layer!\nIt is supported by almost all browsers, and web servers.\nTLS provides:\n\nConfidentiality via symmetric encryption\nIntegrity via cryptographic hashing\nAuthentication via public key cryptography\n\nTLS protocol\n\nHandshake: Alice, Bob use their certificates, private keys to authenticate each other, exchange or create shared secret.\nKey derivation: Alice, Bob use shared secret to derive set of keys.\nData transfer: Stream data transfer! Send data as a series of records.\nConnection closure: Special messages is used to securely close connection.\n\nInitial handshake\n\n(Bob establishes TCP connection with Alice.)\nBob sends t-tls hello to Alice.\nAlice sends public key certificate to prove that Alice is really Alice.\nBob sends Alice a master secret key MS encrypted with Alice's public key, which will be used to generate all other keys for TLS session.\n\nTCP + TLS handshake requires 3 RTT before client can start receiving data!\nTLS keys\nIt is considered bad to use same key for more than one cryptographic function.\nTLS use different keys for MAC (message authentication code) and encryption.\n\nKcK_cKc​: encryption key for data sent from client to server\nMcM_cMc​: MAC key for data sent from client to server\nKsK_sKs​: encryption key for data sent from server t oclient\nMsM_sMs​: MAC key for data sent from server to client\n\nKeys are derived from key derivation function (KDF).\nIt takes master secret (and some addition random data) to create new keys.\nTLS encryption\nRecall) TCP provides byte stream abstraction.\nProblem: Message integrity can happen only after all data received!\nSolution: break stream in series of records!\nEach client-to-server record carries data's length and a MAC, created using McM_cMc​.\nFinally, the whole record is encrypted with KcK_cKc​.\nProblem: re-ordering and replay attacks are possible.\nSolution: Use TLS sequence numbers and nonce!\nWhen duplicate data arrives, ignore it.\nProblem: Truncation attack is possible. (Attacker forges TCP connection close segement (FIN) to close connection in the middle.)\nSolution: Record type! Record now have a type field, where type 0 indicates data and type 1 indicates close. If TLS closure record isn't arrived, TCP close packet is not legitimate.\nTLS 1.3\n\nUnlike TLS 1.2, which had 37 cipher suites to choose from, TLS 1.3 has only five cipher suites to choose from.\n\n4 of them are based on AES. TLS_AES_256_GCM_SHA384 is widely used, even in network cards!\n\n\nIt requires Diffie-Hellman (DH) for key exchange, instead of RSA. (TLS request occurs too frequently, but RSA is too slow.)\nIt combines encryption and authentication algorithm for data rather than serial encryption, then authentication. (Called authenticated encryption)\n\nNo MAC needed!\n\n\n\nTLS 1.3 handshake\nTLS 1.3 handshake need 1 RTT.\n\nClient hello: guesses key agremment protocol, parameters, and indicates cipher suites it supports.\nServer hello: Chooses key agreement protocol, parameters, cipher suite, and sends server-signed certificate.\nClient checks server certificate, generates key, then makes application request.\n\nHowever, 0 RTT handshake is possible if we're resuming earlier connection!\n\nClient sends encrypted application data together in client hello message.\nApplication data is encrypted with resumption master secret, which was used in the previous connection.\nServer verifies resumption master secret is valid, then sends (encrypted) response together in server hello message.\n\n0 RTT handshake is vulnerable to reply attacks!\nBut maybe it is OK for client requests that doesn't modifies server state. (e.g. HTTP GET requests)\nIPsec\nIPsec provides datagram-level encryption, authentication, integrity for both user traffic and control traffic. (e.g. BGP, DNS messages)\n\nTransport mode: Only datagram payload is encrypted and authenticated\nTunnel mode: Entire datagram is encrypted and authenticated.\nEncrypted datagram is encapsulated in new datagram with new IP header, tunneled to destination.\n\nIPsec protocols\n\nAuthentication Header (AH) protocol: Provides source authentication and data integrity, but not confidentiality. (i.e. plain text)\nEncapsulation Security Protocol (ESP): Provides source authentication, data integrity, and confidentiality.\nObviously more widely used than AH.\n\nSecurity associations (SAs)\nBefore sending data, security association (SA) is established from sending to receiving entity.\nEnding, receiving entities maintain state information about SA. (Just like TCP, TCP endpoints also maintain state info!)\nIP is connectionless, but IPsec is connection-oriented!\nSA stores:\n\n32-bit identifier Security Parameter Index (SPI)\nOrigin SA interface\nDestination SA interface\nType of encryption used &amp; encryption key\nType of integrity check used &amp; authentication key\n\nIPsec datagram\n\nWe assume that we are using tunnel mode ESP.\nIPsec datagram encapsulates original IP datagram.\n\nESP header has SPI and sequence number.\n\nSequence number prevents replay attacks; If datagram with same sequence number is arrived, replay attack happened.\n\n\nESP trailer has padding for block ciphers.\nMAC in ESP auth field is created with shared secret key.\n\nESP tunnel mode actions\nSender router:\n\nAppend ESP trailer to original datagram.\nEncrypt the datagram using algorithm &amp; key specified by SA\nAppend ESP header to front of the encrypted datagram.\nCreate authentication MAC using algorithm &amp; key specified by SA\nAppend MAC to the payload\nCreate new IP header and its field, then address new IP datagram to tunnel endpoint.\n\n\nFor new SA, sender initializes with sequence number 0.\nEach time datagram is sent, sender increments sequence number.\nOnly packets in window are tracked for duplicates.\nIf duplicate is found, replay attack happened!\n\nSecurity of tunnel mode\nTrudy cannot do anything!\n\nShe can't see original contents of datagram. (including headers!)\nShe can't flip bits without detection. (integrity!)\nShe can't replay a datagram. (sequence number!)\nShe can't masquerade as router using router's IP address. (actually possible, but Trudy cann't guess correct SA state/key at all)\n\nIPsec security databases\nSecurity Policy Database (SPD)\nFor given datagram, sender needs to know if it should use IPsec, and which SA to use.\nSPD instructs what to do!\nSecurity Association Database (SAD)\nEndpoints holds state of SA in SAD.\nWhen sending/receiving IPsec datagram, router accesses SAD to determine how to process it.\nReceiving router can examine SPI in IPsec datagram, then index SAD with SPI.\nSAD instructs how to do it!\nInternet Key Exchange (IKE)\nWe cannot manually establish IPsec SAs for every connections!\ne.g. VPN has hundreds of endpoints.\nWe use IPsec IKE for establishing SA! Handshake will establish SA automatically.\nIKE authenticate with either PSK or PKI.\n\nPSK (Pre-shared secret): Both sides start with shared secret. Run IKE to authenticate each other and to generate IPsec SAs, including encryption and authentication keys.\nPKI (Public/private keys and certificates): Both sides start with public/private key pair and certificates. Run IKE to authenticate each other, then obtain IPsec SAs.\n\nIKE has two phases!\n\nPhase 1: Establish bi-directional IKE SA (different from IPsec SA).\n\nAggressive mode: Use fewer messages.\nMain mode: Provides identity protection and is more flexible.\n\n\nPhase 2: Use ISAKMP to securely negotiate IPsec pair of SAs.\n\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Wireless Networks","url":"/posts/95/","content":"Wireless and mobile networks\n\nMore wireless (mobile) phone subscribers than fixed (wired) phone subscribers!\nMore mobile-broadband-connected devices than fixed-broadband-connected devices.\n\nTwo important challenges:\n\nWireless: Communication over wirelss link.\nMobility: Handling the mobile user who changes point of attachment to network.\n\nElements of a wireless network\n\nWireless hosts: May be stationary (non-mobile) or mobile! e.g. Laptop, smartphone, IoT ...\nBase station: Typically connected to wired network, responsible for sending packets between wired network and wireless hosts in its area.\nWireless link: Typically used to connect mobiles to base station.\nAlso used as a backbone link e.g. On the island, under the sea, ...\nWireless link standard started with 1 character (e.g. 802.11b, 802.11g), but is currently using 2 characters... (e.g. 802.11be)\n\nThere are two modes of a wireless network.\n\nInfrastructure mode: Base station connects mobiles into wired network.\n\nHandoff: Mobiles change base station which provides connection into wired network.\n\n\nAd hoc mode: No base stations! Nodes transmit to other nodes within link coverage. e.g. sensor network\n\nCharacteristics of wireless link\n\nDecreased signal strength: Radio signal attenuates as it propagates through matter.\nInterference from other sources: Wireless network frequencies are shared by many devices. Note that you should pay if you want to use certain frequencies!\nMultipath propagation: Radio signal reflects off objects ground, arriving at destination at slightly different times.\nConnection transitivity: Based on terrains and distance, transitivity doesn't holds.\ni.e. Even though A, B are connected, and B, C are connected, A, C might not be conected.\n\nSNR (signal-to-noise ratio)\n\nLarger SNR means it is easier to extract signal from noise.\nIf physical layer is given, increasing power can increase SNR and decrease BER. (bit error rate)\nIf SNR is given, we should choose physical layer that meets BER requirement.\nWi-Fi (IEEE 802.11)\nWireless host communicates with base station (AP, Access point).\nBasic Service Set (BSS) (a.k.a. cell) in infrastructure mode contains:\n\nWireless hosts\nAccess point (AP)\nAd hoc mode (hosts only)\n\nChannels\nSpectrum is divided into channels at different frequencies.\nAP admin chooses frequency for AP.\nArriving host must associate with an AP.\n\nHost scans channels, listening for beacon frames containing AP's name (SSID) and MAC address.\nHost selects AP to associate with.\nHost perform authentication.\nHost typically run DHCP to get IP address in AP's subnet.\n\nInterference is possible! Channel can be same as that chosen by neighboring AP.\nIn fact, other devices (e.g. microwave) might use Wi-Fi frequencies!\nModern Wi-Fi AP choose different channel when interference happens.\nPassive/Active scanning\n\nPassive Scanning\n\nBeacon frames are sent from APs.\nAssociation request frame is sent from host to selected AP.\nAssociation response frame is sent from selected AP to host.\n\n\nActive Scanning\n\nHost broadcast request frame.\nAP probe request frame from host.\nHost probe response frames sent from APs.\nAssociation request frame is sent from host to selected AP.\nAssociation response frame is sent from selected AP to host.\n\n\n\nMultiple access\nWi-Fi have no collision detection!\n\nDue to fading, transmitting signal is high, but received signal is weak.\nNot all collisions can be sensed. e.g. fading, hidden terminal (A can see B, C can see B, but A can't see C)\n\nCSMA/CA\nWe use CSMA/CA to avoid collisions. (CA stands for Collision Avoidance!)\n\nIf sender sense channel idle for DIFS (Distributed Interframe Space), transmit entire frame.\nIf sender sence channel busy,\n\nChoose random backoff time. (with exponential backoff)\nTimer counts down while channel is idle.\nTimer is frozen while channel is busy.\nTransmit entire frame when timer expires.\nIf no ACK was received, double backoff and repeat this process.\n\n\n\nReceiver should return ACK after SIFS (Short Interframe Space) passed.\nSIFS is shorter than DIFS, so other senders can check whether channel is busy.\nRTS-CTS exchange\nIdea: sender reserves channel use for data frames using small reservation packets.\n\nSender transmits small RTS (request-to-send) packet to AP using CSMA.\nAP broadcaasts CTS (clear-to-send) in response to RTS.\nSender transmits data frame.\nOther stations defer transmissions until transmission is finished.\nAP broadcast ACK in response to data frame.\n\nRTS can collide, but we don't lose too much time because RTS is small.\nExponential backoff is used for RTS.\nAddressing\nWi-Fi frame has four addresses.\n\nAddress 1: MAC address of wireless host or AP that receives this frame\nAddress 2: MAC address of wireless host or AP that transmits this frame\nAddress 3: MAC address of router interface to which AP is attached\nAddress 4: Only used in ad hoc mode\n\nAdvanced features\nMobility\nHost can change AP while remaining in same IP subnet.\nIP address can remain same!\nSwitch will self-learn which AP is associated with H1.\nRate adaptation\nAP dynamically change transmission rate as mobile moves.\nSNR (Signal-to-noise ratio) will vary when mobile moves.\nBER (Bit error rate) increases as SNR decreases. (i.e. node moves away from AP)\nWhen BER becomes too high, AP switch to lower transmission rate to reduce BER.\nPower management\nNode can send &quot;I am going to sleep until next beacon frame&quot; message to AP.\nAP will not transmit frames to this node. Frames destined to this node will be buffered.\nBeacon frame contains list of mobiles with AP-to-mobile frames waiting to be sent.\nNode can stay awake only when frame destined to itself exists.\nBluetooth\nFamous personal area network! Only supports less than 10m.\n\nMaster/Client device: Master polls clients, and grant requests for client transmissions.\nClients gets dedicated slots using TDM. (1 slot = 625 μs)\nParked mode: Clients can go to sleep and later wakeup.\nFHSS (Frequency-hopping spread spectrum): Sender uses 79 frequency channels in known, with pseudo-random order. (FDM)\nOther devices not in piconet will only interfere in some slots.\nBootstrapping: Nodes detects master's hopping pattern and plug-and-play into piconet.\n\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Cellular networks","url":"/posts/96/","content":"4G/5G cellular networks\nThe solution for wide-area mobile internet!\nWe have more mobile-broadband-connected devices than fixed-broadband-connected devices.\nWidespread deployment/use!\n4G is available 97% of the time in Korea!\nComparison with wired internet\nSimilarities:\n\nEdge/core distinction, but both belong to same carrier\nGlobal cellular network: a network of networks\nWidespread use of protocols (e.g. HTTP, DNS, TCP, UCP, IP, data/control planes, SDN, ...)\nActually interconnected to wired internet (After the AP, everything is wired!)\n\nDifferences\n\nDifferent wireless link layer\nMobility as a first class service\nUser identity via SIM card\nBusiness model: Users subscribe to a cellular provider\n\nRoaming is needed on visited nets\n\n\n\nElements of 4G LTE architecture\n\nAll of the wired network uses IP packet!\n\nMobile device\n\nDevices (e.g. smartphones) with 4G LTE radio.\n64-bit IMSI (INternational Mobile Subscriber Identity) stored on SIM (Subscriber Identity Module) card.\nSimilar to MAC address!\n\n\nBase station\n\nManages wireless radio resources, mobile devices in its coverage area (called cell).\nCoordinates device authentication with other elements.\nSimilar to Wi-Fi AP, but:\n\nActive role in user mobility\nCoordinates with nearby base stations to optimize radio use (e.g. handover)\n\n\n\n\nServing Gateway (S-GW)\n\nGateway between mobile cellular network and P-GW\n\n\nPDN Gateway (P-GW)\n\nGateway between mobile cellular network and internet\nLooks like any other internet gateway router\nProvides NAT services\n\n\nHome Subscriber Server (HSS)\n\nStores info about mobile devices for which the HSS's network is their home network\nWorks with MME in device authentication\n\n\nMobility Management Entity (MME)\n\nDevice authentication coordinated with HSS\nManage mobile device (e.g. device handover, tracking/paging device location)\nPath (tunneling) setup from mobile to P-GW\n\n\nOther routers\n\nUsed for tunneling\n\n\n\nCellular network have 3 authentications!\nBase station (eNode-B), HSS, and MME!\nData/Control plane separation\n\nData plane: Use IP tunneling between AP, S-GW, and P_GW.\nControl plane: Protocols between MME, HSS, S-GW, P-GW for mobility management, security, and authentication.\nLTE Protocols\n\nPacket Data Convergence: Header compression, encryption\nRadio Link Control (RLC): Fragmentation, reassembly, reliable data transfer\nMedium Access: Requesting, using radio transmission slots\nUse OFDM (orthogonal frequency division multiplexing) which combines FDM and TDM.\n\nAssociating with a BS\n\nBS broadcasts primary synch signal every 5ms on all frequencies.\nMobile find a primary sync signal, then locates 2nd synch signal on this frequency.\nMobile gets info broadcast by BS, containing channel bandwidth, configurations, and BS's cellular carrier info.\nMobile selects which BS to associate with, then establishes connection.\n\nSleep modes\nLTE mobile may sleep to conserve battery.\n\nLight sleep: After hundreds of milliseconds of inactivity\nMobile wake up periodically (for hundreds of milliseconds) to check for downstream transmissions.\nDeep sleep: After 5~10 secs of inactivity\nMobile may change cells while deep sleeping, but it should re-establish association.\n\nRoaming\nSIM card has ISMI, so any network can detect mobile's home network. (HSS, Home Subscriber Server)\nCarrier networks are interconnected with each other, using IP.\nIf mobile visits other network, visited mobile carrier network sends data to home mobile carrier network('s P-GW).\n5G\n10x increase in peak bitrate, 10x decrease in latency, 100x increase in traffic capacity over 4G!\n5G NR (new radio) uses two frequency bands. (FR1: 450 MHz ~ 6 GHz, FR2: 24 GHz ~ 52 GHz)\nNot backward-compatible with 4G...\n5G uses millimeter wave frequencies! It has much higher data rates, but over short distances.\nOnly available for pico-cells, (10~100m diameter) so massive, dense deployment of new base stations is required.\nMobility management\nWe want devices to move among APs (in one provider network or across multiple provider networks) while maintaining ongoing connections.\nIn fact, network routers can do this!\nRouters advertise well-known name, address, or number of visiting mobile node via routing table exchange.\nRouting tables can indicate where each mobile is located via longest prefix match.\nHowever, we have billions of mobiles! Routers can't store every devices information in routing table.\nNormally, end systems handles mobility management. Functionality at the edge!\n\nIndirect routing: Communication from correspondent to mobile goes through home network, then forwarded to remote mobile.\nDirect routing: Correspondent gets foreign address of mobile, then sends data directly to mobile.\n\nMobility of ISP/Wi-Fi\nWe don't have a home network for ISP and Wi-Fi.\nIf we attach to different ISP or Wi-Fi, we need different credentials and different connections. (No mobility...)\nArchitectures for 4G-like mobility exists, but it is not used.\nRegistration\nHome network needs to know where you are!\nMobile associates with visited mobility manager.\nVisited mobility manager registers mobile's location with home HSS.\nIndirect routing\n\nCorrespondent uses home address as datagram destination address.\nHome gateway forwards to remote (visited) gateway.\nVisited gateway router forwards to mobile.\nMobile replies to visited network.\nVisited gateway forwards reply to correspondent (via home network or directly).\n\nPros: Mobile is transparent to correspondent! Ongoing connections can be maintained automatically.\nCons: Triangle routing: Inefficient when correspondent and mobile are in same network.\nDirect routing\n\nCorrespondent contacts home HSS, then gets mobile's visited network.\nCorrespondent uses foreign address as datagram destination address.\nVisited gateway forwards to mobile, then forwards reply to correspondent directly.\n\nPros: Efficient (No triangle routing)\nCons: Non-transparent to correspondent: Correspondent should remember foreign address, and handle when mobile changes its visited network.\nMobility in 4G networks\n\nBase station association: Mobile provides IMSI to identify itself.\nControl-plane configuration: Visited network's MME and home network's HSS establish control-plane state.\nData-plane configuration: Visited network's MME configures forwarding tunnels for mobile. Visited network and home network establish tunnels from home P-GW to mobile.\nMobile handover: Mobile device changes its point of attachment to vsited network\n\nConfiguring LTE control-plane elements\n\nMobile communicates with visited MME via BS control-plane channel.\nMME uses mobile's IMSI info to contact mobile's home HSS.\nVisited BS and mobile select parameters for BS-mobile data-plane radio channel.\n\nConfiguring LTE data-plane tunnels\n\nVisited S-GW to visited BS tunnel: Change endpoint IP address of tunnel\nVisited S-GW to home P-GW tunnel: Implementation of indirect routing\n\nGPRS tunneling protocol (Tunneling via GTP): Mobile's datagram to streaming server is encapsulated using GTP inside UDP inside datagram.\nHandover between BSs in same cellular network\n\nSource BS detects mobile might leave its coverage area.\nSource (current) BS selects target BS, then sends handover request message to target BS.\nTarget BS pre-allocates radio time slots, and responds with HR ACK with info for mobile.\nSource BS informs mobile of new BS. Mobile can now send via new BS!\nHandover looks complete to mobile, but S-GW still forwards to source BS...\nSource BS stops sending datagrams to mobile. Instead, it forwards to new BS.\nTarget BS informs MME that it is new BS for mobile.\nMME instructs S-GW to change tunnel endpoint to be new BS.\nTarget BS ACKs back to source BS to inform handover complete.\nNow source BS can release resources!\nMobile's datagrams now flow through new tunnel from target BS to S-GW.\n\nMobile IP\nHistoric architecture that was standardized 20 years ago.\nLong before ubiquitous smartphones and 4G support!\nNo wide deployment/use because Wi-Fi, 2G, 3G was enough at the time?\nHome agent, foreign agent, tunneling existed.\nSimilar to HSS, P-GW, MME, S-GW...\nImpact on higher layer protocols\nLogically, impact is minimal! Best effort service model still remains unchanged.\nTCP and UDP can run over wireless and mobile networks.\nBut performance impact is high!\n\nPacket loss/delay due to bit-errors and handover loss.\nTCP interprets loss as congestion (not error of wireless network), so it will decrease congestion window unnecessarily.\nReal time traffic is delayed.\nBandwidth is a scarce resource for wireless link, unlike wired link.\n\n","categories":["SNU","4-1","컴퓨터네트워크"]},{"title":"Introduction","url":"/posts/31/","content":"왜 왔냐요\n과목명이 멋져요 과목명을 바꿔야겠다\nReal introduction\nHow can we run AI (in GPU) efficiently?\n\nWhat is AI accelerator chip?\nHow can we design AI accelerator chip?\nHow can we run neural networks on the AI accelerator chip (or GPU)?\nHow can we run CNN/LLM on the AI accelerator chip (or GPU)?\n(If possible) why HBM is more important?\n\nLecture goal\nUnderstanding hardware/software system design issues/methods with a real system design example\nHardware design\nMatrix-matrix (MM) multiplication accelerator\nPreviously we used Verilog, but these days we have python-based hardware description language, Amaranth!\nSoftware design\nNeural network code running on CPU\nIt should communicate with hardware MM accelerator.\nOptimizing software/hardware design\n\nTiling a.k.a. blocking\nReduced precision (e.g. 8-bit computation)\nZero skipping in matrix-matrix multiplication\n\nOptimization includes runtime optimization and energy optimization.\nQ. How do we optimize power with Amaranth(simulation)?\nA. Even Verilog uses power estimation. We estimate power usage for each step in design process.\n","categories":["SNU","4-1","하드웨어시스템설계"]},{"title":"Image Recognition Neural Network","url":"/posts/51/","content":"Multi-Layer Perceptron (MLP)\nBiological Analogy\nThis is actually similar to Retinal Ganglion Cell (RGC, 망막 신경절 세포)\nRGC is connected to a group of photoreceptors which detects the existence of light.\nThe pattern and motion information is then transferred to Primary Visual Cortex (V1).\nV1 neuron receives information from multiple RGCs and output single response, just like weighted sum + activation function.\nV1 is known to have 6 layers which is similar to filters in CNN.\nV1 neurons in the same column (ocular-dominance column) detects line segment.\nAdjacent columns detects slightly different line orientations.\nA simple cells detects a line segment of a specific direction.\nA complex cell receives information from multiple simple cells, and detects a line segment.\nThis is similar to layers - it can extract high level feature from low level feature!\nUnfortunately, human brain is too slow(?)\n\nA synaptic connection takes about 20ms.\nFull recognition takes about 120ms (5~7 synaptic connection).\nRecognising to acting takes about 100ms (4~5 synaptic connection).\nThe total time is 180~260ms.\n\nFor a specific task, neural networks can outperform humans!\nXOR Problem\nXOR can't be seperated with hyperplane.\nHowever, we can seperate XOR if we introduce additional dimension!\nDeeper network can have a higher order function, so it can give better representation.\nTraining MLP\nForward pass computes MLP, back propagation trains MLP.\nBack propagation compute derivative of output and change weights according to learning rate.\nIf derivate of activation function is less than 1, deep network's weight won't change. (diminishing effect of backpropagation)\nReLU is used because its derivate is 1, so diminishing effect doesn't happen.\nConvolution\n\nInput: H×H×CH \\times H \\times CH×H×C\nFilters: MMM of R×R×CR \\times R \\times CR×R×C\nOutput: E×E×ME \\times E \\times ME×E×M\n\nOutput channels are also called as feature maps.\n\nNumber of parameters: R×R×CR \\times R \\times CR×R×C\nNumber of multiplications: R×R×C×E×E×M=(parameters)×(output dimension)R \\times R \\times C \\times E \\times E \\times M = \\text{(parameters)} \\times \\text{(output dimension)}R×R×C×E×E×M=(parameters)×(output dimension)\n\nBecause of mathematical definition, we use different index order for input and kernel.\ny[n]=∑i=−kkx[n−k]w[k]y[n] = \\sum_{i=-k}^k x[n-k]w[k]\ny[n]=i=−k∑k​x[n−k]w[k]\n","categories":["SNU","4-1","하드웨어시스템설계"]},{"title":"Verilog Design","url":"/posts/39/","content":"Google TPU\nGoogle TPU Version 1.0 does 64,000 multiplication per cycle!\nIt can multiply 256x256 matrix!\nTwo important things in parallel computing are making small unit and providing new data every cycle.\nHow does Google TPU multiply 256x256 matrix?\nSystolic array\n\nLet's assume we'll multiply 3x3 matrix.\nSytolic aray has 3x3 Multiply-Accumulate (MAC) units.\nMAC gets two input, multiply it, and add it to register (accumulate).\nReset signal sets accumulator to 0.\nThe input will be received for 5 cycles from two directions!\nEach cycle, MAC does MAC, and pass inputs to corresponding direction.\nAfter 7 cycles, each MAC holds result!\nTPUv4i\nTPUv4i uses 32x32 systolic array with 4x4 dot product to multiply 128x128 matrix.\nInstead of sending numbers one at a time, we send 4 numbers at a time.\n4 MAC need 4 multiplication, 4 addition, and 4 registers.\n4x4 dot product only need 4 multiplication, 3 addition, and 1 register!\nHDL-Based Design flow\n\nHardware Specification using C or Matlab\nBehavioral/RTL Modeling (HDL)\nBehavioral/RTL Modeling (HDL)\nSynthesis\nGate-level Simulation\nPlace &amp; Route (connecting wires)\nVerify timing constraints (e.g. wire delays)\nBit-stream Generation\nFPGA test\nASIC Design (Test passed, now make real hardware!)\n\nVerliog\nValues\nUses sized number e.g. 4'b11, 5'd3, etc.\nThe size can be omitted for 32-bit numbers.\nx value means conflicting drivers, i.e. connected to both supply voltage and ground.\nz value means floating state, i.e. not connected to both supply voltage and ground.\nGenerally we use x for an unknown value.\nData Types\nWire\nRepresents connection. They don't hold values!\nReg\nRepresents data storage.\nBehaves like memory - they hold values until assigned.\nReg doesn't represents a physical register!\ninteger represents 32 bit signed reg.\nTime\nTime is unsigned value that is at least 64 bits.\nUsed for storing and manipulating simulation time.\nVector\nSize of wire or register.\nThey can hold multiple bits!\nArray\nCollection of single entities.\nwire [7:0] x[3:0]; means 4 of 8 bit vectors.\nOperators\nArithmetic, Bitwise, etc.\nSome operators may not be synthesizable! (e.g. % (modulus), ** (exponent), &lt;&lt;&lt;, &gt;&gt;&gt; (arithmetic shift), ===, !== (case equality))\nReduction operators\nCarry out a bit-wise operation on vector and yield a 1-bit result.\ne.g. &amp;A, ^A, ~|A, etc.\nEquality operators\n==, != yields x if any operand is x or z.\n===, !== yields 1 if two operands exactly match. (including x or z!)\nDelay control\nRegular delay control\nDefers the execution of the entire statement.\nUsually used for test bench. e.g. #10 y = 1;\nIntra-Assignment delay control\nDefers the assignment to the left variable.\nIn y = #25 ~x;, ~x is calculated at #0, and assignment happens at #25.\nBlocks\ninitial block\nAll initial block run at #0 once.\nalways block\nAlways block starts at #0, and executes continuously.\nWe can trigger always block with signals.\nalways @(posedge clock or negedge reset_n)    if (!reset_n) q &lt;= 1`b0;    else          q &lt;= d;\nAssignments\nBlocking assignments (=) are executed in the program order.\nNon-Blocking assignment (&lt;=) are executed concurrently.\nParameterized Modules\nSimilar to generics!\nmodule hazard_static (x, y, z, f);    parameter delay1 = 2, delay2 = 5;    and #delay2 a1 (b, x, y);    not #delay1 n1 (a, x);    and #delay2 a2 (c, a, z);    or #delay2 o2 (f, b, c);endmodule defparam example1.delay1 = 4, example1.delay2 = 8;hazard_static example1 (x, y, z, f);hazard_static #(4, 8) example2 (x, y, z, f);hazard_static #(.delay2(4), .delay1(6)) example3 (x, y, z, f);  \nloops\nwhile statement\nwhile (i &lt;= 7) begin    if (data[i] == 0) out = out + 1;    i = i + 1; end\nfor statement\nfor (i = 0; i &lt;= 7; i = i + 1)    if (data[i] == 0) out = out + 1;\nrepeat statement\nCompiler repeats statement for fixed number of times.\nrepeat(128) begin    $display(&quot;Count = %d&quot;, count);    count = count + 1;end\ngenerate block\nCompiler unrolls generate block to statements.\ngenvar i;generate for (i = 0; i &lt; SIZE; i = i + 1) begin: bit    assign bin[i] = ^gray[SIZE-1:i];end endgenerate\nModule Modeling Styles\nStructural style\nnor g1 (b, x, y);not g2 (a, x);and g3 (c, a, z);nor g4 (f, b, c);\nDataflow style\nassign cgt = (a &gt; b);assign clt = (a &lt; b);assign ceq = (a == b);\nBehavioral or algorithmic style\nalways @(*) begin    case (select)        2&#x27;b11: y = in3;        2&#x27;b10: y = in2;        2&#x27;b01: y = in1;        2&#x27;b00: y = in0;        default: y = N&#123;1&#x27;b0&#125;;    endcaseendalways @(*) begin    // x bits are ignored in casex    casex (in)        4&#x27;b1xxx: y = 3;        4&#x27;b01xx: y = 2;        4&#x27;b001x: y = 1;        4&#x27;b0001: y = 0;        default: y = 2&#x27;bx;    endcaseend\nSynthesizing conditional statements\nComplete conditional statement will usually synthesized into a 2-to-1 multiplexer.\nalways @(enable or data)     if (enable) y = data;    else y = x;\nLatch inference\nIncomplete conditional statement or assigning itself will be synthesized into latch.\nTo avoid this, initialize variable before conditional statement or only use complete conditional statement.\nalways @(select or data)     case (select)        2&#x27;b00: y = data[select];        2&#x27;b01: y = data[select];        2&#x27;b10: y = data[select];        // no default statement, latch inference    endcasealways @(enable or data)     if (enable) y = data;    // no else statement, latch inferencealways @(enable or data)     if (enable) y = data;    else y = y; // latch inferencealways @(posedge clk)     if (enable) y &lt;= data;    else y &lt;= y; // NOT latch inference, this redundant expression will be removed\nFilp-flop inference\nSimilarly, assignment to a register will be synthesized into flip-flop.\nalways @(posedge clk)     y &lt;= x // flip-flop inference\nFilp-flops\nFilp-flops sample input only once in a clock period, while latches sample input when the clock is high.\n// D-type flip-flopmodule DFF (clk, d, q);input clk, d;output reg q;    always @(posedge clk) q &lt;= d;endmodule// asynchronous reset D-type flip-flopmodule DFF_async_reset (clk, reset_n, d, q);input clk, reset_n, d;output reg q;always @(posedge clk or negedge reset_n)    if (!reset_n) q &lt;= 0;    else          q &lt;= d;endmodule// synchronous reset D-type flip-flopmodule DFF_sync_reset (clk, reset_n, d, q);input clk, reset_n, d;output reg q;always @(posedge clk)    if (!reset_n) q &lt;= 0;    else          q &lt;= d;endmodule\nMemory elements\nIt's just n-bit D-Type flip-flops!\nmodule register (clk, load, reset_n, din, qout);parameter N = 4;input clk, load, reset_n;  input [N-1:0] din;output reg [N-1:0] qout;always @(posedge clk or negedge reset_n)   if (!reset_n) qout &lt;= &#123;N&#123;1&#x27;b0&#125;&#125;; // async reset   else if (load) qout &lt;= din; // only load when signaled, so we don&#x27;t have to provide the data every cycle    // verilog inferences else quout &lt;= qout; endmodule\nNormally, we use register file with one-write and two-read ports.\nMost operation has 2 inputs and 1 output. (e.g. A + B = C)\nparameter M = 4;   // number of address bitsparameter N = 16;  // number of words, N = 2^Mparameter W = 8;   // number of bits in a wordinput          clk, wr_enable;input  [W-1:0] din;output [W-1:0] douta, doutb;input  [M-1:0] rd_addra, rd_addrb, wr_addr;reg    [W-1:0] reg_file [N-1:0];assign douta = reg_file[rd_addra];assign doutb = reg_file[rd_addrb];always @(posedge clk)    if (wr_enable) reg_file[wr_addr] &lt;= din;\nLarge register file is often implemented with SRAM (Synchronous Random Access Memory) circuit.\nparameter N = 16;  // number of words, N = 2^Mparameter A = 4;   // number of address bitsparameter W = 8;   // number of bits in a wordinput      [A-1:0] addr;input      [W-1:0] din;input              cs, wr, clk;output reg [W-1:0] dout;reg        [W-1:0] ram[N-1:0];always @(posedge clk)    if (cs) begin // chip select signals        if (wr) ram[addr] &lt;= din;        else dout &lt;= ram[addr];    end\nShift registers\nNormally used in multiplication and divison.\nWe need load signal to load data in 1 cycle!\nmodule universal_shift_register (clk, reset_n, s1, s0, lsi, rsi, din, qout);parameter N = 4;  // define the default sizeinput              clk, reset_n, s1, s0, lsi, rsi;input      [N-1:0] din;output reg [N-1:0] qout;always @(posedge clk or negedge reset_n)   if (!reset_n) qout &lt;= &#123;N&#123;1&#x27;b0&#125;&#125;;   else case (&#123;s1,s0&#125;)        2&#x27;b00: ; // qout &lt;= qout;           // No change        2&#x27;b01: qout &lt;= &#123;rsi, qout[N-1:1]&#125;;  // Shift right        2&#x27;b10: qout &lt;= &#123;qout[N-2:0], lsi&#125;;  // Shift left        2&#x27;b11: qout &lt;= din;                 // Parallel load   endcaseendmodule\nCounters\nCounters are implemented by using previous output as a clock!\n// an N-bit ripple counter using generate blocksparameter N = 4; // define the size of counterinput              clk, enable, reset_n;output reg [N-1:0] qout;genvar             i;generate for (i = 0; i &lt; N; i = i + 1) begin: ripple_counter    if (i == 0) // specify LSB        always @(negedge clk or negedge reset_n)            if (!reset_n)    qout[0] &lt;= 1&#x27;b0;            else if (enable) qout[0] &lt;= ~qout[0];    else // specify the rest bits        always @(negedge qout[i-1] or negedge reset_n)             if (!reset_n)    qout[i] &lt;= 1&#x27;b0;            else if (enable) qout[i] &lt;= ~qout[i];end endgenerate\nOf course, we can just let verilog to make it.\nmodule updn_bincounter (clk, reset, eup, edn, qout, cout, bout);parameter          N = 4;input              clk, reset, eup, edn;output reg [N-1:0] qout;output             cout, bout;always @(posedge clk)   if (reset)    qout &lt;= &#123;N&#123;1&#x27;b0&#125;&#125;;  // synchronous reset   else if (eup) qout &lt;= qout + 1;   // up counter   else if (edn) qout &lt;= qout - 1;   // down counterassign #1 cout = (&amp;qout) &amp; eup;      // generate carry outassign #1 bout = (~|qout) &amp; edn;     // generate borrow out\nFinite-State Machines (FSMs)\nM=(I,O,S,d,l)M = (I, O, S, d, l)\nM=(I,O,S,d,l)\n\nI,O,SI, O, SI,O,S are finite, nonempty sets of inputs, outputs, and states.\nd:I×S→Sd: I \\times S \\rightarrow Sd:I×S→S is a state transition function.\nlll is a output function.\n\nMealy machine: l:I×S→Ol: I \\times S \\rightarrow Ol:I×S→O\nMoore machine: l:S→Ol: S \\rightarrow Ol:S→O\n\n\n\nFSM Modeling Style\n\n\nInitialize and update the state register\n always @(posedge clk or negedge reset_n)    if (!reset_n) present_state &lt;= A;    else          present_state &lt;= next_state;\n\n\nDetermine next state\n always @(present_state or x)     case (present_state)        A: if (x) next_state &lt;= B; else next_state &lt;= C;\n\n\nDetermine output and internal registers\n always @(present_state) // Moore machinealways @(present_state or x) // Mealey machine    case (present_state)        A: out = 0;        B: out = 1;\n\n\nLogic Optimization\nOptimization has two goals - combinational circuit optimization and state minimization.\nCombinational circuit optimization reduces logic, and state minimization reduces flip-flops.\nCombinational circuit optimization: Two-level simplification\n\nImplicant: elements of ON-set or DC-set that can be combined to form a subcube\nPrime implicant (PI): implicant that can't be combined with another to form a larger subcube\nEssential prime implicant (EPI): PI that participates in all possible covers of the ON-set, i.e. PI that covers an element of ON-set uniquely.\n\nGoal: Cover the ON-set with as few prime implicants as possible!\nQuine-McCluskey Method (Q-M Method) is an exact algorithm that reduces a minterm expression to a minimal form.\n\nFind all PIs.\n\nAssume don't cares as ON-set.\nFind two product terms that has exactly one-bit difference and combine them.\nMark product terms that were combined to reduce.\nRepeat until no more combinations are possible.\nRemaining unmarked product terms are PIs.\n\n\nSelect a minimal set of PIs using a prime implicant chart (PI chart).\n\nCreate a PI chart.\n\nList all PIs in the rows.\nList all ON-set minterms in the columns. (Ignore don't cares)\nMark minterms that are coverd by each PI.\n\n\nSelect all EPIs (PI that covers a minterm uniquely).\nSelect a minimum number of additional non-EPIs that covers remaining ON-set.\n\n\n\nSequential circuit optimization: State minimization\n\nEquivalent States: States with same output and same transition behavior.\nFor all input combinations, states transition to same or equivalent states.\n\nGoal: identify and combine equivalent states!\nRow Matching Method is a polynomial time procedure that minimize states.\n\nPlace all states.\nInitially partition set based on output behavior. (Ignore transition behavior.)\nPartition each set based on transition behavior. (Ignore output behavior.)\nIf states transition to same set, it has same transition behavior.\nRepeat partitioning with transition until no further partitioning is required.\n\n","categories":["SNU","4-1","하드웨어시스템설계"]},{"title":"Large Language Model","url":"/posts/70/","content":"Word Embedding\nEmbedding is a mapping between anything and a vector.\nEach word is represented as a vector.\nSimilarity between words can be measured in the vector space called embedding space.\nWord2Vec\nGiven a center word (with one-hot coding input), Word2Vec predicts context words (words before and after center word).\nBecause we are using one-hot vector, a column of weight becomes the hidden layer - called word vector.\nAny2Vec?\nUsing same idea, anything can be represented as a vector.\ne.g. product vector, hotel vector, youtube vector (in recommender system)\nRecurrent Neural Networks (RNN)\n\nEach input word change hidden state.\nThe input sequence and output sequence can be any length. (one to one, one to many, many to one, many to many)\n\nNeural Machine Translation (NMT)\nInput sequence are fully feeded without any output. (Encoder RNN)\nSTART token indicates start of translation. Decoder RNN outputs each word and use it as the next input.\nAlignment problem\nInput sequence and output sequence don't always match one-by-one.\nAlignment can be many-to-one.\nBottleneck problem\nRNN only has 1 encoding for the source sentence.\nCan it capture all the infromation from thousands of words before?\nTransformer\nAttention\nInstead of single hidden state, every hidden states of encoder RNN are used!\nIn decoder RNN, each hidden state determines attention scores.\nWith attention scores and encoder RNN's hidden state, attention vector is obtained and concatenated with decoder hidden state.\nTerms of transformer\n\nQuery: Decoder's intermediate output (hidden state)\nKey: Used to calculate attention scores\nValue: Encoder's hidden state\n\nAttention(Q,K,V)=softmax⁡(QKTdk)V\\text{Attention}(Q,K,V) = \\operatorname{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\nAttention(Q,K,V)=softmax(dk​​QKT​)V\nBERT (Bi-directional Encoder Representation from Transformers)\nInput can affect hidden state from the past!\nUse whole sentence to produce output.\nIf the BERT model is sufficiently pre-trained, the larger BERT models give better accuracy without overfitting.\nUsually transformer's input is token embedding vector + position embedding vector.\nPre-trained BERT model can used in different tasks:\n\nSentence pair classification (2 input sentences, 1 output class)\nSingle sentence classification (1 input sentence, 1 output class)\nQuestion answering (2 input sentences (question, paragraph), 1 output sentence)\nSingle sentence tagging (1 input sentence, 1 output sentence)\n\nMulti-Head Attention\nFor each head, we use different dimensions.\nIt's similar to the ensemble model - use different attention patterns to learn multiple relationships.\nMasked language model (MLM)\nBERT is pre-trained with BooksCorpus and Wikipedia.\nRandom tokens were masked and BERT had to predict a masked word in a given sentence.\n\nThis can make BERT learn context naturally!\nWe don't have to label datas or create input-output pairs, dataset itself is a training data!\n\nChatGPT (GPT-3)\n\nPre-train GPT.\nMasked language model and 570GB of datasets were used.\nSupervised learning with good examples.\nExamples of input and output are prepared by human labelers or existing sets of text.\nReward model training\nHuman labeler gives a rating to the output of GPT-3.\nOpenAI trained reward model which predicts the rating on the given input.\nReinforcement Learning\n\nChatGPT generates output.\nReward model evaluates the output.\nFine-tune the GPT-3 based on the reward.\n\n\n\nChatGPT is still RNN based - it only generate one word at a time!\nTraining Transformer\n\nModel size of Multi-Head Attention Transformer\nInput matrix: L x H (L words of vector embedding size H)\nHead size, dimension: h, d (i.e. H = h x d)\nFor each head, Q, K, V is calculated from weight matrix H x d.\nEach head (L x d) is concated to L x H matrix, then output is calculated from weight matrix H x H. (i.e. weight size is H2H^2H2)\nTherefore, total model size of MH attention is h⋅3⋅Hd+H2=4H2h \\cdot 3 \\cdot Hd + H^2= 4H^2h⋅3⋅Hd+H2=4H2.\nFinally, feed-forward network (FFN) is used at output, from H to 4H and back to H.\nEach step's weight is H⋅4H=4H2H \\cdot 4H = 4H^2H⋅4H=4H2, so total weight size is 8H28H^28H2.\nTherefore, model size per layer is 12H212H^212H2, and parameter ratio for MH attention and FFN is 1:2.\nComputation cost of Multi-Head Attention Transformer\nAssume matrix multiplication of N x K and K x M is NKM.\nFor each head, Q, K, V is caclculated for each word: multiplication of L x H and H x d (LHdLHdLHd)\nFor each head, QKTQK^TQKT is calculated: multiplication of L x d and d x L (L2dL^2dL2d)\nFor each head, QKT⋅VQK^T \\cdot VQKT⋅V is calculated (assume softmax is calculated): multiplication of L x L and L x d (L2dL^2dL2d)\nEach head is concated to L x H matrix, then output is calculated: multiplication of L x H and H x H (LH2LH^2LH2)\nTherefore, total multiplications of MH attention is h(3LHd+L2d+L2d)+LH2=4LH2+2L2Hh(3LHd + L^2d + L^2d) + LH^2 = 4LH^2 + 2L^2Hh(3LHd+L2d+L2d)+LH2=4LH2+2L2H.\nFinally, feed-forward network (FFN) is used at output, from H to 4H and back to H.\nMultiplication of L x H and H x 4H (4LH24LH^24LH2), and L x 4H and 4H x H (4LH24LH^24LH2)\nTherefore, total multiplications of FFN is 8LH28LH^28LH2.\nTherefore, multiplications per layer is 2L2H+12LH22L^2H + 12LH^22L2H+12LH2.\nIf we assume L=HL = HL=H, total multiplications per layer is 14L314L^314L3, and parameter ratio for MH attention and FFN is 6:8.\nTrillion Parameter Models\nTransformers now have more than trillion parameters.\ne.g. BERT has N=298 transformer layers, and H=17480 hidden dimension, resulting in total N⋅12H2≈1.08×1012N \\cdot 12H^2 \\approx 1.08 \\times 10^{12}N⋅12H2≈1.08×1012 parameters.\nIf we assume sequence length (number of tokens in a sentence) is L=2048, and batch size is B=2048, per-layer activation size in training is H⋅L⋅B=7.3×1010H \\cdot L \\cdot B = 7.3 \\times 10^{10}H⋅L⋅B=7.3×1010.\nObviously, GPU memory is not enough to hold 1T parameters.\nOptimizer like adam requires two more parameter for each weight, so we need more than 10 times memory.\nWe need to use thousands GPUs to train parameters!\nParallelism\n\n\nData Parallelism (DP): Each GPU has a entire model, and data is distirbuted among GPUs. Weight should be synchronized!\nModel Parallelism (MP): Each layer is divided to multiple GPUs. Activation should be transfered among GPUs!\nPipeline Parallelism (PP): Weight, activation is too big for GPU to handle. We divide each stage to multiple GPUs.\n\n\nDP, MP, PP can be used together to train model across multiple GPUs.\ne.g. Model with 2TB parameters, 256 layers can be trained with 4096 GPUs using 8-way MP, 64-way PP, and 8-way DP.\nAll Reduce vs. All Gather\n\nAll Reduce (in DP): Accumulate weight updates across different mini-batches to obtain global weight updates. Computation is used.\nAll gather (in MP): Copying a set of weights or activations to other nodes. Only collect without computation.\n\nZeRO-DP (Zero Redundancy Optimizer)\nModel size is too large, we divide parameters into N GPUs!\nInstead of holding every parameter in each GPUs, each GPU hold portion of parameters.\nWhen we need to compute, use all gather to get all parameters, compute, then hold portion of parameters.\nUnlike DP, we don't have to hold every parameters!\nUnlike MP, we use all gather on parameters instead of activation!\nZeRO-Offload\nWe need large memory because optimizer like adam need states.\nSolution: Store optimizer states in CPU memory for entire training!\nCPU runs Adam code to update weight.\nWith ZeRO-Offload, we can even train large model in single GPU, and model can be scaled very easily.\nHowever, we now have a bottleneck in CPU-GPU bandwidth (ultimately bottleneck in CPU memory).\nBig companies use DRAM per GPU, so we can achieve higher bandwidth and larger memory.\nTest-time Scaling\nChatGPT o1 use self refinement with chain of thought: every input, reasoning, output from previous turn is used as next turn's input.\nLog of compute cost is proportional to accuracy.\nAlphaGo and Poker AI was able to achieve better performance with searching (i.e. thinking) for 30 seconds before answering.\nIn natural language, chain-of-thought prompting is used. Instead of just giving input and output, giving input, output with reasoning can get better output.\nLarge models on difficult problems can benefit the most.\nThe more time it takes to search, the better the answer will be.\nDeepSeek-R1 Training\n\nPhase 1: Cold Start (Same supervised learning)\nPhase 2: Reasoning (Learn how to search, by giving longer time to answer)\nPhase 3: Rejection Sampling SFT (Self training with good results)\nPhase 4: Diverse RL (Check helpfulness and harmfulness)\n\nEven when DeepSeek-R1 was not instructed to use chain of thought, it was able to learn chain of thought on its own!\nMore steps and longer output resulted in better answer.\n","categories":["SNU","4-1","하드웨어시스템설계"]},{"title":"NPU (Hardware Accelerator for Neural Networks)","url":"/posts/71/","content":"Pipelined tile based multiplications\n\nWe need to multiply large matrices with small systolic array.\nSystolic array use tile pipelining! It keeps get next tile's input to remove idle cycles.\nConvolution on GPU\nWe use BLAS (Basic Linear Algebra Subprograms) library for matrix multiplication.\ncuBLAS and cuDNN both duplicate input image into large array, so we can conmpute convolution with matrix multiplication.\ncuBLAS actually created large matrix, while cuDNN stores original image and reformat when it goes to GPU cache.\ncuDNN better utilizes main memory at a small cost of software's on-the-fly data reformatting!\nHowever, in hardware accelerator, hardware should reformat image.\nIn TPU architecture, systolic data setup will convert 2D window into 1D vector to perform matrix multiplication.\nWeight stationary in Systolic Array\nGoogle found out memory bandwidth determines performance. Why?\nReason 1: M*V multiplication of MLP and RNN is bandwidth-intensive operation because weights are used only once.\nc.f. M*V multiplication is highly used in recommendation system.\nSolution 1: Large batch changes matrix-vector multiplication to matrix-matrix multiplication which offers higher data reuse.\nIn case of interactive service, (e.g. recommendation) batch formation will delay user service therby limit batch size.\nSolution 2: Weight stationary in Systolic Array\nInstead of pipelining weights, we load weights at once in systolic array.\nDuring MM, we need high bandwidth only for one of the two matrices from the internal buffer.\nThus, in case of low memory bandwidth, weight stationary can offer much better performance than output stationary!\nInternal buffer -&gt; high bandwidth, provide each tile\nDRAM -&gt; low bandwidth, we store each tile into systolic array and reuse it.\nProblem 1: latency exists! We should hide latency of reading weights with computation.\nWe should compare latency of reading the weights of next tile from slow DRAM vs. latency of systolic array multiplication on current tile.\nProblem 2: We need large partial sum buffer and accumulate partial sums.\nHiding latency of reading weights\nDue to low bandwidth of DRAM, systolic array have to wait until DRAM read the next tile.\nWe can use higher DRAM bandwidth, better DRAM bandwidth utilization (e.g. int8), or ... just increase systolic array computation time?!?!?!\nComputing larger matrix can reuse DRAM's tile more, so we can hide latency.\nBy having larger batch size, we make input matrix taller, thus reusing DRAM's tile more!\nIn case of interactive service, we can process multiple user's input as a batch.\n\nMemory-Compute Balance: At ideal case, computation time and memory access time should be same!\nIf memory access is longer, we have memory bottleneck.\nIf computation takes longer, we have compute bottleneck.\nThis is why batch size is important!\ne.g. Unlike TPUv1, TPUv2 used High Bandwidth Memory (HBM) and 16-bit bfloat.\nSince memory access time is faster, we should make computation time faster to maintain the memory-compute balance.\nTPUv2 also use bfloat16 instead of float32.\nUnlike float16, bfloat16 have same exponent as float32.\nTheorically, represenatable value range are same with float32! (From 10−3810^{-38}10−38 to 3⋅10383 \\cdot 10^{38}3⋅1038)\nImprovements in TPUs\nTPUv2\n\nTPUv2 have interconnect router that can directly connect multiple TUP chips.\nTPUv2 uses two TPU core, PCIe queues (to connect with CPU, GPUs can't run without CPU's command!), and bfloat16.\nTPU core has vector unit. Vector unit have 32K x 32bit vector memory, can perform elementwise operation with 2 ALUs, and send and retrieve vectors from matrix multipy unit.\nTPUv2 has 128x128 systolic array!\nSmaller matrix have better utilization, larger matrix have better data reuse.\nBy considering 256x256, 128x128, 64x64, they chose 128x128 as the best option.\nTPUv3\nTPUv3 is similar to TPUv2.\nNow we use 2 systolic arrays per TPU Core, HBM with better bandwidth and capacity, interconnect router with more nodes, etc.\n\nEach TPU Board has 4 TPU chips.\nTPUs are interconnected in 2D Torus.\nEach TPU Board is connected to eac Host (CPU) wiht PCI-e.\n\nGoogle run benchmarks with 6 production applications they're using, 2 of each MLP, CNN, RNN.\nRNN, CNN had linear scaling, but MLP (recommendation system) didn't had linear scaling.\nTPUv4i\n\nTPUv4i have single core (Tensor core) with 4 systolic arrays connected to vector memory.\nCMEM is a cache between HBM and VMEM.\nIt can cache previously computed matrix!\nEach systolic array compute 32x32 matrix multiplication with 4x4 dot product.\n4 inputs and 4 weights are computed at once in 4 input-adder tree.\nBy removing 3 accumulators, We could reduce 40% in area and 25% in power with respect to 128x128 systolic array.\nTPUv4i also used bfloat16 and int8.\nProblem: poor segmentation degrades the result of the image quality enhancement.\nVision for driving assistance and autonomy\nTesla Hydranet has 8 cameras, 16 timesteps, and 32 batch size.\nWe need to process 4096 HD images in a single forward pass!\nTeslar FSD Chip\nFirst chip to actually accelerate convolution!\nThey used systolic array with 8bit input.\nTo provide and store large input datas, Teslar have large SRAMs with high bandwidth.\nExaPOD: Teslar Supercomputer\nSelf-driving cars need lots of training data. Tesla used 1.5PB for final dataset!\n\nSingle D1 Training chip has 5x5 chips that can be connected in four directions to arrange in tiles.\nIt is connected to CPU with PCIe, and every training data is stored in CPU's memory.\nThere is no large memory in D1 chip! Therefore we want to avoid data repetation.\n\nParameters for two layers are distributed across two tiles, splitting across channels.\nInputs are shared across four tiles, splitting across batches. (1/4 of batch)\nFirst layer's parameter are replicated to create full parameters in two tiles.\nThese parameters are replicated into other two tiles.\nFirst layer is run\nSecond layer's parameter are replicated into other two tiles. (1 copy per 2 tiles for MP)\nRemove replicated first parameters and input\nSplit each first layer's output (per 1/4 of batch) into two tiles, spliting across channels\nSecond layer is run (output two partial sums per 1/4 of batch)\nAdd each two partial sums (per 1/4 of batch)\nRemove replicated second parameters\nNow we've done two layers of CNNs, and parameters are still in chips! (can be reused)\n\nDojo compiler\nMost engineers in Teslar is software engineer!\nBecause their design was unique, they had to make their own compiler for pytorch.\nResult was better than NVIDIA's solution!\nOther NPUs\nAMD MI300X Series\n\nUse 8 HBM stacks for memory\nStacks of silicon dies for compute\nTend to have smaller server size, and consumes less power\n\nSambaNova's Chiplet based Accelerator\n\nBest chiplet-based inference accelerator\nLarge silicon die in the middle, surrounded by DRAM and HBM\n\nCerebras Wafer Scale Engine (WSE)\n\nProposes wafer as a large silicon chip\nc.f. Usually you cut wafer to get desired amount of chips\nAbout 1 million cores are inside one wafer!\nZero skipping - faster computation for sparse weights\n\nZero skipping\nSignificant portion of input values in a CNN is zero!\nPruning and ReLU makes zero values.\nSwarmX\nMotivation: matrix-matrix multiplication can be viewed as a sum of scalar-vector multiplication.\nIf scalar is zero, we skip!\nNVIDIA Tensor Core\nTensor Core performs 64 multiplications per cycle. (4 instances of 4x4 outer product multiplier)\nThis can multiply 4x4 matrix in 1 cycle.\nEach outer product multiplier gets row/column of matrix.\nIn 1 cycle, outer product performs outer product, (similar to systolic array) and adder tree adds outer product's elements.\nZero Skipping in Nvidia A100\nFor each 4 parameter, we prune 2 parameters.\nWe can shrink parameters into half, but we need memory for non-zero indices.\nSparse Tensor Core will select input activation at non-zero indices with mux.\nWe need 2 bits for indices - 12.5% for 16bit data, 25% overhead for 8bit data\nWhen training, We prune half of parameters and update weight only for parameters that aren't pruned.\nSparse Tensor Cores will multiply 16x32 and 32x8 matrix in 2 cycles.\nSamsung NPU v1\nExynos has zero weight skipping!\nFor each kernel element, we produce intermediate output by multiplying kernel element and corresponding input feature map.\nIf kernel is 3x3, we need 9 cycles.  '\nHowever, if kernel has zero values, we can skip to save cycles.\nThis is the first commercial neural network accelerator!\nSamsung NPU v2\nZero-activation skipping!\n\nFeature-map-aware zero skipping: Move feature map and weight to fill in zero features, so we can perform less dot products.\nFeature-map lossless compressor: If feature map has many zero features, we can compress into smaller feature map.\n\nPrecision in Nvidia A100 and H100\nWe use more smaller types - float8, int8, etc.\nIf the data size is halved, we only need to fetch half the weights (or we can compute twice the weight in the same time), so performance is doubled!\nNVIDIA has found that smaller types don't affect accuracy as much for different networks.\nFuture of accelerator\nReasoning model needs more token generations.\nBut power limits data center; We should maximize number of token generated per power!\nPower efficiency is becoming more and more important.\nProbably the only hope for startup companies to beat big tech?\nModern GPU can access other GPU memories. (NVLINK)\nGPU can now access 155TB of memory at high bandwidth!\nConfigurations may change in 1T parameter model training.\nNetwork speed is also becoming important.\n","categories":["SNU","4-1","하드웨어시스템설계"]},{"title":"Reading Data from Memory","url":"/posts/82/","content":"Memory Address Space\n\nUser process uses virtual address space, while hardware such as memory uses physical address space.\nDevice has its own MMU (memory management unit) for virtual to physical address translation.\nVirtual address space\nBecause of memory fragmentation, large contiguous memory resource in physical address is hard to find.\nPage table is used to keep virtual to physical address mapping information.\nIt stores virtual page number, access rights, and its corresponding physical page number.\nThe offset (usually 12 bits, page size is 4KB) remains the same for both the virtual and physical addresses.\nTo reduce size of page table, we often use hierarchical page table.\ne.g. 10-bit L1 index, 10-bit L2 index, 12-bit offset\nTranslation Look-aside Buffers (TLB)\nAddress translation is very expensive!\nHierarchical page table makes it worse, 2-level page table need 3 memory accesses for each physical memory access.\nc.f. This happens whenever we access hardware components!!\nWe cache translations in TLB. If TLB hits, only single memory access is needed!\nUsually we have seperate TLB for instruction and data.\nBut if TLB misses, we still need multiple memory accesses.\nWe use MMU cache to store intermediate page table's cache.\nIOMMU\nEach hardware devices (including CPU) has its own virtual address space and page table.\nMore than 15 years ago, each hardware device used a contiguous region of physical address directly.\nHowever, this wastes main memory resources if not all hardware devices are being used.\nMoreover, devices such as smartphones have so many hardware devices that we run out of main memory.\nIOMMU provides demand paging, which enables better utilization of memory resource and offers more memory resource for user applications.\nTo reduce cost of page table and TLB miss, we share page table with CPU, and we use L2 TLB (shared by devices and CPU) as well as L1 TLB (local to device).\nInterconnect\nAXI (Advanced Extensable Interface) Protocol\n\nWrite address channel (AW, input)\nWrite data channel (W, input)\nWrite response channel (B, output)\nRead address channel (AR, input)\nRead data channel (R, output)\n\nEach channel have valid &amp; ready signals. (handshake signals)\nAXI can send 128 bit at once, but only use 32 bit as a data.\nOther bits are used for signals, e.g. handshake signals, burst length/size/type, protection type, write strobes, ...\nBurst transfer\nWrite address channel have signals that determines burst transfer.\nBurst transfer can send data over multiple cycles, while sending address information for only 1 cycle.\n\nAWADDR[31:0] Write addr ID\nAWLEN[3:0] Burst length\nAWSIZE[2:0] Burst size (Actually data is called beat, so this should be called beat size?)\nAWBURST[1:0] Burst type\n\nWe also have read version of burst transfer! (e.g. ARADDR, ARLEN, ...)\n\nWide transfer: Burst data's size is same as word size.\n\nNarrow transfer: Burst data's size is smaller than word size.\nIn narrow transfer, we can determine where data exists (e.g. whether it starts from MSB) by looking at address.\nOf course, sending multiple data at once can save clock cycle.\nHowever, due to hardware limitations, data might not be available at once.\nType of burst transfer\n\nINCR: Normal behavior, incrementing-address burst.\nFIXED: Fixed-address burst, for FIFOs.\nWRAP: Incrementing-address burst, but wraps to a lower address at the wrap boundary\n\nMultiple Outstanding Requests\n\nBecause we merged multiple data requests into single burst transfer request, address channel have free cycles.\nIf we can use this free cycles and send another burst transfer request during burst transfer, we can reduce average memory latency.\nIf we can access multiple memory banks at once, and send multiple data at once, we can achieve parallelism!\ne.g. Non-blocking caches!\nIf cache can hit under multiple miss (cache can send hit data when fetching miss data), or miss under miss (cache can fetch multiple miss data in parallel), we can reduce effective miss penalty!\nHandshake signal\nStop-and wait method\nTraditional stop-and-wait method use request and acknowledge.\n\nSender sends request + data at clock 0.\nReceiver receives, and set acknowledge at clock 1.\nNow sender can send next request + data at clock 2.\n\nProblem: Sender can't send data at each cycle!\nSingle Credit-based Flow Control\nInstead of acknowledge signal, receiver uses ready signal (credit).\nReceiver set ready signal if receiver have free space.\nSender can send next request + data if ready signal was set at the previous clock.\nWe can send data at each cycle! 100% utilization can be achieved.\nData is transfered when valid (request signal) and ready signal are both set at the rising edge of the clock.\nImplementation of Interconnect\n\nInterconnect can be implemented with decoders and arbiters.\nDecoder send data to the desired output, and arbiter receive data from the desired input.\nIn practice, not all decoders and arbiters are connected.\nArbitration scheme\nWhat should we do when arbiter receive multiple inputs at once?\n\nFixed priority: Easy, but starvation occurs at worst case\nRound robin: Cycle priority! Looks fine, but depending on the request pattern, this is actually not fair\nLRG (least recently granted): (We use hybrid method, so each group's priority is fixed.) If a master was granted access, that master will have the lowest priority in the group at the next cycle.\n\nDRAM\n\nData is stored in 2D array of memory cell.\nRow address decoder selects target row, then read the entire word.\nColumn decoder selects bit line, then read/write specific bits.\nSingle DRAM consists multiple data bank, and each bank consists multiple 2D array of memory cell.\nTo achieve parallelism, memory requests should be served by multiple banks, or by the same row.\nDRAM cell\n\nEach cell can be viewed as half full capacitor, switch, and capacitor that stores data (fully charged if data is 1, empty if data is 0).\n\nCharge Sharing: Both capacitor shares charge.\nSensing &amp; Amplification: Sense amplifier amplifies difference to make both capacitor full/empty.\nPrecharging: Switch is turn off, and capacitor is restored to the half full state.\n\nWe can read before amplification is fully finished!\nIn fact, after row access, column access happens when we can read data from DRAM.\nDRAM Timing\n\ntRCt_{RC}tRC​: Minimum time from the start of one row access to the start of the next row access. tRC=tRAS+tRPt_{RC} = t_{RAS} + t_{RP}tRC​=tRAS​+tRP​\ntRCDt_{RCD}tRCD​: Minimum time from row activation to column activation. (i.e. minimum time when data is available to read)\ntCLt_{CL}tCL​: Column activation latency.\ntRASt_{RAS}tRAS​: Activation latency.\ntRPt_{RP}tRP​: Precharge latency.\n\nUsually DRAM timing is written as CL - tRPt_{RP}tRP​ - tRCDt_{RCD}tRCD​.\ne.g. 3-3-3 means 3 cycles for each of activation, read/write, and precharge.\nDRAM pin\nInput/output pin is called DQ pin.\nIt acts like a global bus, and multiple banks are connected to DQ.\nHowever, this means we cannot read multiple banks at once.\nCurrently DRAM reads data from memory banks in serial manner.\nMake it parallel and get the Turing Award!\nDDR, DDR2, DDR3 means the clock cycle of the bus (DQ).\nDDR2 has 400MHz clock, DDR3 has 800MHz clock.\nThis is easily achieved by reading more data at once, then send it using higher frequency clock.\nHowever, reducing internal latency is very hard because internal latency is proportional to the memory area.\nReading a single tile from DRAM\nRecall) We were doing matrix multiplication!\nHow do we read 4x4 tile matrix?\n\nSending four read requests sequentially will give long latency of tile read.\nInstead of storing matrix in one bank, if we can store four rows in different banks, (bank interleaving) we can achieve higher memory utilization!\nAddress mapping\nHow can we remap the original address for bank interleaving?\nConventionally, physical address is mapped as &lt;bank, row, column&gt;.\nHowever, if we remap as &lt;row, bank, column&gt;, we can naturally get bank interleaving!\nThis is called RBC mapping.\nSpatial Locality\nThe best memory pattern is to store tile matrix in a single row of the bank.\nWe only need a single activation stage!\nThis is usually done by modifing source code, compiler will do the rest!\nHot and highly correlated arrays are allocated to the near memory addresses, i.e. on the same DRAM row.\n","categories":["SNU","4-1","하드웨어시스템설계"]},{"title":"Efficient LLMs","url":"/posts/88/","content":"Reducing model size\nPruning\nIn fact, pruning also comes from the biological system!\nNumber of synapses increases before 2 years old, but decreases after 2 years old.\nProbably to reduce resource (a.k.a. energy) usage?\nIn NN, we prune small output neurons and small weight connections.\nThen we train the remaining weights, and we repeat this process.\n\nModel size is reduced\nFaster download\nLess multiplication, a.k.a. less energy consumption!\nWeights tend to diverge\n\nc.f. Reading from main memory consumes 6400 times more energy than adding an integer. Float multiplication consumes 37 times more energy than adding an integer.\nQuantization\nCompressed Sparse Row (CSR)\n\nRow pointer stores how many nonzero elements exists before that row.\nColumn Indices stores column indices of each values.\nValues stores values.\n\nSimilarly, Compressed Sparse Column (CSC) can be defined.\nWeight clustering\nWe limit the number of effective weights by clustering weights.\nFor each weight, we only store index of the cluster (only 2 bit if there are 4 clusters!) and centroids, so we don't have to waste 32 bit for each weight.\nUnfortunately, we lost information about individual weight.\nDuring training, all the gradients are grouped by cluster and summed together, then only centroids are updated.\nCompression\nInstead of using whole index range, using difference between indices can reduce range of integer, thus reducing number of bits to be used.\nHuffman Encoding is used to further reduce integer values.\nUsing Pruning + Quantization + Compression can compress about 40 times smaller, but has the same error rate as before!\nPer-Layer Sensitivity\nLayers near the input is the most sensitive layer. (i.e. accuracy loss after pruning is higher.)\nLayers near the outpt is the least sensitive layer.\nThat is why we optimize/compress aggressively only to the layers near the output!\nMore pruning\nStructured pruning\nSimple pruning hardly reduces the runtime of GPU.\nSparse matrix multiplication library is only effective when more than 90% of values are pruned.\nMost neural networks prune about 50~70% of weights.\nHow can we obtain performance gain from pruning?\nWe should use structured pruning - pruning considering structure!\nc.f. Actually this is zero-skipping problem!\nOuter product multiplier\nRecall) Outer product multiplier reads each row/column from matrices, then it accumulates outer products.\nTo skip reading from matrix, we need zero column and a way to represent zero columns.\nHow do we make a zero column? Simple: Just prune with columns!\nInstead of individual weights, we prune columns whose sum of absolute weights is small.\nAlso, we can just reduce size of input matrix instead of using zero columns and additional metadata to represent zero columns.\nWe can use the same hardware as if we didn't use pruning!\nWe can speed up to 8 times faster, but we have a higher accuracy drop (~1%).\nAlso, in CNN, only the center of the filters remains after pruning.\n2:4 pruning\nNvidia prunes 2 weights for every 4 weights.\nThen we use non-zero indices to select only the values corresponding to the non-zero weights.\nThis can be done in hardware using mux.\n\nPermutation can be applied before pruning to avoid excessive pruning.\nGood permutation increases total magnitude. i.e. more information is keeped after pruning.\n2:4 pruning with permutation hardly loses accuracy!\nLottery ticket pruning\nPruning shows that NNs can be reduced in size.\nInstead of reducing trained NN, can we train a sparse NN from scratch?\nThe Lottery Ticket Hypothesis: A randomly-initialized, dense neural network contains a subnetwork that is initialized such that, when trained in isolation, it can match the test accuracy of the original network after training for at most the same number of iterations.\ni.e. There may exist a pruned model (subnetwork) that can match the test accuracy of the original network.\nIterative Magnitude Pruning\nTo demonstrate this hypothesis, they trained a dense NN, pruned it, initialized it again, and repeated this process.\nAt first, researchers reset the weights to the same initial value every time.\nHowever, this only worked for small-scale tasks and failed for deep networks. (Scaling limitation)\nInstead, researchers reset the weights to the values after a small number of k training iterations. (k = 6)\nMore quantization\nLow precision data type\nLow precision data type can reduce memory cost (less memory access, smaller matrix size) and computation cost (less energy consumption, less gate count).\nGPUs already supports int4 and float8!\nfloat8 is especially used in mobile devices and servers. (e.g. ChatGPT!)\n32 bit floating point\nFloating point is represented as sign, exponent, and mantissa (or significand).\n32 bit floating point uses 1 sign bit, 8 exponent bits, and 23 mantissa bits.\nS, E, M represent (−1)S⋅(1.M)⋅2E−bias(-1)^S \\cdot (1.M) \\cdot 2^{E-bias}(−1)S⋅(1.M)⋅2E−bias, where bias is 127 in 32 bit floating point.\nIf E is 0, we use (−1)S⋅(0.M)⋅21+E−bias(-1)^S \\cdot (0.M) \\cdot 2^{1+E-bias}(−1)S⋅(0.M)⋅21+E−bias, which is called subnormal.\nIf E is 255 (all 1), we represent infinities or NaNs.\nActually bias is the center point of the unsigned value of exponent bits! We chose this way to represent both the smallest and largest values.\n16 bit floating point\nfloat16 uses 1 sign bit, 5 exponent bits, and 10 mantissa bits.\nbfloat16 uses 1 sign bit, 8 exponent bits, and 7 mantissa bits.\nSince bfloat16 has the same number of exponent bits as 32 bit floating point, it can represent the same range as 32 bit floating point.\nAlso, converting to bfloat16 is really easy! We can just truncate mantissa bits.\n8 bit floating point\nThere are multiple representation for 8 bit floating point.\nE4M3 uses 1 sign bit, 4 exponent bits, and 3 mantissa bits.\nE5M2 uses 1 sign bit, 5 exponent bits, and 2 mantissa bits.\nE4M3\n\nbias of exponent is 7.\nMax normal is S.1111.110(2)=448S.1111.110_{(2)} = 448S.1111.110(2)​=448.\nMin normal is S.0001.000(2)=2−6S.0001.000_{(2)} = 2^{-6}S.0001.000(2)​=2−6.\nMax subnormal is S.0000.111(2)=0.875⋅2−6S.0000.111_{(2)} = 0.875 \\cdot 2^{-6}S.0000.111(2)​=0.875⋅2−6.\nMin subnormal is S.0000.001(2)=2−9S.0000.001_{(2)} = 2^{-9}S.0000.001(2)​=2−9.\nS.1111.111(2)S.1111.111_{(2)}S.1111.111(2)​ represents NaN.\nWe can't represent infinities.\n\nBecause of the nature of floating point representation, larger value have larger rounding error, and the bin size (difference between representable values) is halved if we lower the exponent.\nE5M2\n\nbias of exponent is 15.\nMax normal is S.11110.11(2)=57344S.11110.11_{(2)} = 57344S.11110.11(2)​=57344.\nMin normal is S.00001.00(2)=2−14S.00001.00_{(2)} = 2^{-14}S.00001.00(2)​=2−14.\nMax subnormal is S.00000.11(2)=0.75⋅2−15S.00000.11_{(2)} = 0.75 \\cdot 2^{-15}S.00000.11(2)​=0.75⋅2−15.\nMin subnormal is S.00000.01(2)=2−16S.00000.01_{(2)} = 2^{-16}S.00000.01(2)​=2−16.\nS.11111.01(2),S.11111.10(2),S.11111.11(2)S.11111.01_{(2)}, S.11111.10_{(2)}, S.11111.11_{(2)}S.11111.01(2)​,S.11111.10(2)​,S.11111.11(2)​ represents NaN.\nS.11111.00(2)S.11111.00_{(2)}S.11111.00(2)​ represents infinities.\n\nTraining FP8 models\nWe just train FP8 (E5M2) model with the same hyperparameters of FP16 training.\nFP8 is widely used due to easy training and no quality loss.\nHowever, FP16 is still used for some tasks that require higher precision such as semantic segmentation.\nQuantization-Aware Training (QAT)\n\nTrain a network in full precision. (FP) Note that full precision typically means bfloat16!\nQuantize FP weights/activations and execute the model.\nBack-propagate the error (using quantized weights/activations and FP gradient) and update the FP weights.\n\nWe can train with quantized input/weight/model!!!!\nWe use FP weights for back-propagate to not ignore the small weight changes.\nWhy QAT is important? Quantization is necessary in LLMs.\nBut simple quantization can't adjust weights to quantized model.\nQAT can train quantized model, so it can have a better result.\nFake Quantization\nQuantization operation is simulated in PyTorch; quantized model still maintains the data types of weights and activations as full precision.\nReal quantization will be done when deployed on devices.\nLet's say S is scale (real range/int range) and Z is int value for real value 0. (usually middle of the int range)\nThen original real value r is quantized to an integer q=round⁡(rS)+Zq = \\operatorname{round}(\\frac{r}{S}) + Zq=round(Sr​)+Z.\nBut in training, a faked quantized real value r′=S(q−Z)r&#x27; = S(q-Z)r′=S(q−Z) is used instead of the original real value.\nQuantized error makes rrr and r′r&#x27;r′ have a rounding error.\nStraight-Through Estimator (STE)\n\nProblem: Derivative of round is either 0 or infinity.\nSolution: When back-propagating, STE approximate round function to identity function. i.e. STE uses 1 as the gradient.\nAdvanced quantization\nMixed Precision Network based on Precision Highway\n\nModern NN has skip connection.\nSkip connection use high precision activation, while original flow use low precision matrix computation.\nWhy? Quantization operation generates quantization error.\nThe error is propagated and accumulated across layers, so DNNs had significant quantization error.\nSkip connection makes a route that doesn't have quantization error, so DNNs can be trained with quantization.\nClipping (truncating)\nInstead of using full range of floating point number, we clip values into certain range.\nTruncating large magnitude values (with low frequency) enables us to reduce quantization error.\nPACT (Parameterized Clipping Activation Function)\nTrain clipping threshold with parameter α\\alphaα.\nWhy? Each layer have differnet activation range.\nActivation function (bounded RELU) yyy is given as\ny=0.5(∣x∣−∣x−α∣+α)={0,x∈(−∞,0)x,x∈[0,α)α,x∈[α,+∞)y = 0.5 (|x| - |x-\\alpha| + \\alpha) = \\begin{cases}\n0, &amp; x \\in (-\\infty, 0) \\\\\nx, &amp; x \\in [0, \\alpha) \\\\\n\\alpha, &amp; x \\in [\\alpha, +\\infty)\n\\end{cases}y=0.5(∣x∣−∣x−α∣+α)=⎩⎨⎧​0,x,α,​x∈(−∞,0)x∈[0,α)x∈[α,+∞)​\nAssuming we're using kkk bit integer, quantized activation yqy_qyq​ is given as\nyq=round⁡(y⋅2k−1α)⋅α2k−1y_q = \\operatorname{round}\\left(y \\cdot \\frac{2^k - 1}{\\alpha} \\right) \\cdot \\frac{\\alpha}{2^k - 1}\nyq​=round(y⋅α2k−1​)⋅2k−1α​\nLet L=LCE+λ2∑∣α∣2L = L_{CE} + \\frac{\\lambda}{2} \\sum |\\alpha|^2L=LCE​+2λ​∑∣α∣2, (cross entropy loss with L2 regularization) then we can learn α\\alphaα with back propagation!\n∂L∂α=∂L∂yq∂yq∂α+λ∣α∣,∂yq∂α=∂yq∂y∂y∂α={0,x∈(−∞,α)1,x∈[α,+∞)\\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial y_q}\\frac{\\partial y_q}{\\partial \\alpha} + \\lambda|\\alpha|,\n\\frac{\\partial y_q}{\\partial \\alpha} = \\frac{\\partial y_q}{\\partial y}\\frac{\\partial y}{\\partial \\alpha} = \\begin{cases}\n0, &amp; x \\in (-\\infty, \\alpha) \\\\\n1, &amp; x \\in [\\alpha, +\\infty)\n\\end{cases}∂α∂L​=∂yq​∂L​∂α∂yq​​+λ∣α∣,∂α∂yq​​=∂y∂yq​​∂α∂y​={0,1,​x∈(−∞,α)x∈[α,+∞)​\nRecall) ∂yq∂y=1\\frac{\\partial y_q}{\\partial y} = 1∂y∂yq​​=1 because of STE\nLSQ (Learned Step size Quantization)\nWhy don't we use range as a parameter?\nWe'll use step size sss as a parameter.\nLet vvv is a original value, vˉ\\bar{v}vˉ is a quantized integer value, v^\\hat{v}v^ is a quantized real value, and QN,QPQ_N, Q_PQN​,QP​ is range of quantized integer.\nvˉ=⌊clip⁡(vs,−QN,QP)⌉={round⁡(vs)if −QN&lt;vs&lt;QP−QNif vs≤−QNQPif vs≥QPv^=vˉ⋅s\\begin{align*}\n\\bar{v} &amp;= \\left\\lfloor \\operatorname{clip}\\left( \\frac{v}{s}, -Q_N, Q_P \\right) \\right\\rceil =\n\\begin{cases}\n\\operatorname{round}\\left(\\frac{v}{s}\\right) &amp; \\text{if } -Q_N &lt; \\frac{v}{s} &lt; Q_P\\\\\n-Q_N &amp; \\text{if } \\frac{v}{s} \\leq -Q_N\\\\\nQ_P &amp; \\text{if } \\frac{v}{s} \\geq Q_P\n\\end{cases} \\\\\n\\hat{v} &amp;= \\bar{v} \\cdot s\n\\end{align*}vˉv^​=⌊clip(sv​,−QN​,QP​)⌉=⎩⎨⎧​round(sv​)−QN​QP​​if −QN​&lt;sv​&lt;QP​if sv​≤−QN​if sv​≥QP​​=vˉ⋅s​\n∂L∂s=∂L∂v^∂v^∂s∂v^∂s=∂round⁡(vs)∂s⋅s+round⁡(vs)⋅∂s∂s=∂round⁡(vs)∂vs∂vs∂s⋅s+round⁡(vs)=1⋅−vs2⋅s+round⁡(vs)=−vs+⌊vs⌉if −QN&lt;vs&lt;QP∴∂v^∂s={−vs+⌊vs⌉if −QN&lt;vs&lt;QP−QNif vs≤−QNQPif vs≥QP\\begin{align*}\n\\frac{\\partial L}{\\partial s} &amp;= \\frac{\\partial L}{\\partial \\hat{v}}\\frac{\\partial \\hat{v}}{\\partial s} \\\\\n\\frac{\\partial \\hat{v}}{\\partial s} &amp;= \\frac{\\partial \\operatorname{round}\\left(\\frac{v}{s}\\right)}{\\partial s} \\cdot s + \\operatorname{round}\\left(\\frac{v}{s}\\right) \\cdot \\frac{\\partial s}{\\partial s} \\\\\n&amp;= \\frac{\\partial \\operatorname{round}\\left(\\frac{v}{s}\\right)}{\\partial \\frac{v}{s}}\\frac{\\partial \\frac{v}{s}}{\\partial s} \\cdot s + \\operatorname{round}\\left(\\frac{v}{s}\\right) \\\\\n&amp;= 1 \\cdot -\\frac{v}{s^2} \\cdot s + \\operatorname{round}\\left(\\frac{v}{s}\\right) \\\\\n&amp;= -\\frac{v}{s} + \\left\\lfloor \\frac{v}{s} \\right\\rceil \\text{if } -Q_N &lt; \\frac{v}{s} &lt; Q_P \\\\\n\\therefore \\frac{\\partial \\hat{v}}{\\partial s} &amp;= \\begin{cases}\n-\\frac{v}{s} + \\left\\lfloor \\frac{v}{s} \\right\\rceil &amp; \\text{if } -Q_N &lt; \\frac{v}{s} &lt; Q_P\\\\\n-Q_N &amp; \\text{if } \\frac{v}{s} \\leq -Q_N\\\\\nQ_P &amp; \\text{if } \\frac{v}{s} \\geq Q_P\n\\end{cases}\n\\end{align*}∂s∂L​∂s∂v^​∴∂s∂v^​​=∂v^∂L​∂s∂v^​=∂s∂round(sv​)​⋅s+round(sv​)⋅∂s∂s​=∂sv​∂round(sv​)​∂s∂sv​​⋅s+round(sv​)=1⋅−s2v​⋅s+round(sv​)=−sv​+⌊sv​⌉if −QN​&lt;sv​&lt;QP​=⎩⎨⎧​−sv​+⌊sv​⌉−QN​QP​​if −QN​&lt;sv​&lt;QP​if sv​≤−QN​if sv​≥QP​​​\nLSQ has a wider window of error backpropagation than PACT. (−vs+⌊vs⌉-\\frac{v}{s} + \\left\\lfloor \\frac{v}{s} \\right\\rceil−sv​+⌊sv​⌉ vs. 0 or 1)\nThus, LSQ may offer better accuracy than PACT.\nGradient Instability\n\nObviously, optimal weight may not be accurately represented with low precision.\nQuantization moves optimal weight to higher/lower quantized weight.\nThis cause gradients flip in subsequent training iterations(oscillations), making training unstable.\nMoreover, weight will converge towards the rounding boundary (i.e. quantized weight) instead of optimal weight because learning rate decreases over time.\nDiffQ\nLet Q(w,B)=round⁡(w(2B−1))2B−1Q(w, B) = \\frac{\\operatorname{round}\\left( w\\left( 2^B-1 \\right) \\right)}{2^B - 1}Q(w,B)=2B−1round(w(2B−1))​, Δ=Max−Min2B−1\\Delta = \\frac{\\text{Max} - \\text{Min}}{2^B - 1}Δ=2B−1Max−Min​.\nThen quantization QQQ incurs rounding error [−Δ2,Δ2]\\left[ -\\frac{\\Delta}{2}, \\frac{\\Delta}{2} \\right][−2Δ​,2Δ​].\nInstead of actual rounding operation, we can mimic quantization by adding quantization error using uniform distribution!\nQ~(x,B)=x+Δ2U[−1,1]\\tilde{Q}(x, B) = x + \\frac{\\Delta}{2} \\mathcal{U}[-1,1]\nQ~​(x,B)=x+2Δ​U[−1,1]\nFor training loss, we use cross entropy loss L(⋅)L(\\cdot)L(⋅) and model size M(b)M(b)M(b) for regulation.\nTypically, each layer use its own number of bits assigned to all the weights of the layer.\nM(b)=∑i=1# of layersBw(i)⋅Nw(i)L=min⁡w,bL(fQ~(w,b)))+λM(b)\\begin{align*}\nM(b) &amp;= \\sum_{i=1}^{\\text{\\# of layers}} B_w(i) \\cdot N_w(i) \\\\\nL &amp;= \\min_{w, b} L\\left( f_{\\tilde{Q}(w, b))} \\right) + \\lambda M(b)\n\\end{align*}M(b)L​=i=1∑# of layers​Bw​(i)⋅Nw​(i)=w,bmin​L(fQ~​(w,b))​)+λM(b)​\nSmaller λ\\lambdaλ gives higher accuracy and larger model.\nLarger λ\\lambdaλ gives smaller model and lower accuracy.\nTraining becomes stable, and accuracy gets better with reduced model size!\n\nOnly floating point values are used, no oscillations due to quantization!\nNumber of bits B is trainable, so we can learn number of bits for each weight or activation!\nNo STE, use exact derivatives and back propagation!\n\nNIPQ (Noise Injection Pseudo Quantization)\nDiffQ + PACT! Train number of bits and the clipping threshold.\nWe can use this to train 4-8 mixed precision.\n\nSelect layer-wise bit from {4, 5, 6, 7, 8}.\nPush each bits to the possible candidates {4, 8} using pseudo-quantization noise.\nFix all bits to {4, 8} and train with real quantization.\n\nQuantization Layer\nGoal: Y=WX+bY=WX+bY=WX+b should be done with quantized integer.\nRecall) r=S(q−Z)r = S(q - Z)r=S(q−Z)\nWe use ZW=0,Zb=0,Sb=SWSX,qbias=qb−ZXqWZ_W=0, Z_b=0, S_b = S_WS_X, q_{bias} = q_b - Z_Xq_WZW​=0,Zb​=0,Sb​=SW​SX​,qbias​=qb​−ZX​qW​.\nSY(qY−ZY)=SW(qW−ZW)SX(qX−ZX)+Sb(qb−Zb)SY(qY−ZY)=SWSXqW(qX−ZX)+SbqbqY−ZY=SWSXSYqW(qX−ZX)+SbSYqb=SWSXSY(qWqX−ZXqW+qb)=SWSXSY(qWqX+qbias)∴qY=SWSXSY(qWqX+qbias)+ZY\\begin{align*}\nS_Y\\left( q_Y - Z_Y \\right) &amp;= S_W\\left( q_W - Z_W \\right)S_X\\left( q_X - Z_X \\right) + S_b\\left( q_b - Z_b \\right) \\\\\nS_Y\\left( q_Y - Z_Y \\right) &amp;= S_WS_Xq_W\\left( q_X - Z_X \\right) + S_bq_b \\\\\nq_Y - Z_Y &amp;= \\frac{S_WS_X}{S_Y} q_W\\left( q_X - Z_X \\right) + \\frac{S_b}{S_Y} q_b \\\\\n&amp;= \\frac{S_WS_X}{S_Y}\\left( q_Wq_X - Z_Xq_W + q_b \\right) \\\\\n&amp;= \\frac{S_WS_X}{S_Y}\\left( q_Wq_X + q_{bias} \\right) \\\\\n\\therefore q_Y &amp;= \\frac{S_WS_X}{S_Y}\\left( q_Wq_X + q_{bias} \\right) + Z_Y\n\\end{align*}SY​(qY​−ZY​)SY​(qY​−ZY​)qY​−ZY​∴qY​​=SW​(qW​−ZW​)SX​(qX​−ZX​)+Sb​(qb​−Zb​)=SW​SX​qW​(qX​−ZX​)+Sb​qb​=SY​SW​SX​​qW​(qX​−ZX​)+SY​Sb​​qb​=SY​SW​SX​​(qW​qX​−ZX​qW​+qb​)=SY​SW​SX​​(qW​qX​+qbias​)=SY​SW​SX​​(qW​qX​+qbias​)+ZY​​\nNote that both qbq_bqb​ and qbiasq_{bias}qbias​ are 32 bits.\nWhen performing qWqX+qbiasq_Wq_X + q_{bias}qW​qX​+qbias​, we scale to 32-bit int, then rescale to N-bit int when multiplying SWSXSY\\frac{S_WS_X}{S_Y}SY​SW​SX​​.\nSimiliary, we can define quantized convolution layer, where qbias=qb−Conv⁡(qW,ZX)q_{bias} = q_b - \\operatorname{Conv}(q_W, Z_X)qbias​=qb​−Conv(qW​,ZX​)\nY=Conv⁡(W,X)+bqY=SWSXSY(Conv⁡(qW,qX)+qbias)+ZY\\begin{align*}\nY &amp;= \\operatorname{Conv}(W, X) + b \\\\\nq_Y &amp;= \\frac{S_WS_X}{S_Y}\\left( \\operatorname{Conv}(q_W, q_X) + q_{bias} \\right) + Z_Y\n\\end{align*}YqY​​=Conv(W,X)+b=SY​SW​SX​​(Conv(qW​,qX​)+qbias​)+ZY​​\nVSQ(Vector-Scale Quantization)\nFor each 64-element input vector, we use additional quantization scale.\nWhy? One scale per matrix yields higher quantization noise.\nBy using two scale factors, (one per matrix, one per vector) we can reduce quantization noise.\nMX (Microscaling) Format\nInstead of using different scale per data, MX share scale for a block of data.\nEach scale have only 8 bits of exponent, without mantissa bit.\nBinary Network\nThe lowest precision network!\n\nBinary-Weight network: Only weights are binary, activations are 32-bit floating point values. It does not hurt accuracy while reducing weight size significantly.\nXNOR-Net: Weights and activations are binary. Operation is easier, (XNOR instead of matrix multiplication) but accuracy is reduced.\n\nWe interpret binary weight {0, 1} as {-1, +1}.\nBinary Weights\nGoal: If BBB is a binary weight, use weight as a W=αBW = \\alpha BW=αB.\nIf III is an input, actual computaation is done with IW=(I⊕B)αIW = (I \\oplus B)\\alphaIW=(I⊕B)α.\n⊕\\oplus⊕ is convolution, but we can compute without multiplication because of binary weight.\nWe have to find α∗,B∗\\alpha^*, B^*α∗,B∗ that minimizes J(B,α)=∥W−αB∥2J(B, \\alpha) = \\lVert W - \\alpha B \\rVert^2J(B,α)=∥W−αB∥2.\nSince W is fixed, WTW=cW^TW = cWTW=c is constant.\nAlso, since B∈{−1,+1}B \\in \\{-1, +1\\}B∈{−1,+1}, BTB=nB^TB = nBTB=n is constant.\n∴J(B,α)=α2BTB−2αWTB+WTW=nα2−2WTBα+c∴B∗=arg max⁡BWTB=sign⁡(W)∴α∗=WTB∗n=WTsign⁡(W)n=∑∣Wi∣n=1n∥W∥1\\begin{align*}\n\\therefore J(B, \\alpha) &amp;= \\alpha^2 B^TB - 2\\alpha W^TB + W^TW \\\\\n&amp;= n\\alpha^2 - 2W^TB \\alpha + c \\\\\n\\therefore B^* &amp;= \\argmax_B{W^TB} = \\operatorname{sign}(W) \\\\\n\\therefore \\alpha^* &amp;= \\frac{W^TB^*}{n} = \\frac{W^T\\operatorname{sign}(W)}{n} = \\frac{\\sum \\left| W _i \\right|}{n} = \\frac{1}{n} \\lVert W \\rVert_1\n\\end{align*}∴J(B,α)∴B∗∴α∗​=α2BTB−2αWTB+WTW=nα2−2WTBα+c=Bargmax​WTB=sign(W)=nWTB∗​=nWTsign(W)​=n∑∣Wi​∣​=n1​∥W∥1​​\nTiling\nMatrix-Vector multiplication accelerator\nEach PE can multiply + accumulate. (MAC)\nWith two PE, (this can be increased) We can multiply 2x2 matrix and 2x1 vector in two cycles.\nWith 2n cycles, We can multiply 2x2n matrix and 2nx1 vector.\nIn actual hardware, We multiply 2nx2 matrix and 2x1 vector to make partial sum 2nx1. (Weight Stationary Mode)\nThis is better because we can reuse vector elements, but we need to hold partial sums.\nSoftware/Hardware Partitioning\nMV multiplication is done in NPU, but tiling (looping over matrix) is done in CPU.\nWe also need IO between CPU and NPU.\nMemory Hierarchy\nModern GPU have memory hierarchy: Register - L1 cache/Shared memory - L2 cache\nTensor Core can use two-level tiling to exploit the fastest memory.\nEach input tile pair is first fetched from main memory to shared memory.\nEach thread fetch vectors from shared memory to registers and calculates outer product.\nRecall) Tensor core calculates 4x4 matrix in 1 cycle with adder tree.\nWith two tensor cores, we can calculate AB + C in 1 cycle.\nBalancing compute capability and memory bandwidth\nRecall) Compute time and memory reading time should be same to reduce idle time!\nWe can use hardware solution (higher DRAM bandwidth, better DRAM utilization with low precision, etc.), but we should use software solution first.\n\nMultiply larger matrix\nProcess multiple user inputs (i.e. batch) to make larger input matrix, while using same weight matrix.\n\nLLM Serving\n\nMemory capacity: Determines model size in serving\nMemory bandwidth: Determines latency\n\ne.g. Harry Potter series have about 1 million words.\nLlama3 8B model on an A100 GPU takes 1 hour to read Harry Potter series.\nIs this servable??\nBatching in LLM services\nWe can reuse model parameters with batches to increase throughput.\nWhy? Memory is the bottleneck!\n\nOrca(Continuous batching): fill free slots in batch with new incoming sequences\n\nKV Cache\nBatching can reuse parameters, but key &amp; value are used only once.\nWe need KV cache to store key and values, which becomes very large if batch size increases!\nMulti-head attention need key and values for each head!\nInstead, we use grouped-query or multi-query attention to reuse key and values between heads, thus reducing KV cache size.\nvLLM (PagedAttention)\nStore key and value vectors like pages in OS memory.\nMemory is allocated on demand instead of fixed size array.\nThis reduces the memory resource required for KV cache, which allow us to adopt larger batch!\nFlash attention\nBefore preparing output, we need to calculate every attention score of input. This need O(N2)O(N^2)O(N2) memory!\nFlash attention use tiling idea!\nWe compute QK by traditional(?) tiling, then we compute softmax with online method, without storing every score in main memory!\nThis reduce read/write to memory.\nSpeculative Decoding\nBatch reuse parameters by generating multiple sequences at once.\nIn single sequence generation, we didn't reuse parameters, therefore we used matrix multiplication.\nSpeculative Decoding reuse parameters in single sequence generation just like batch.\n\nSmall LLM generates multiple tokens. (Obviously running small LLM multiple times is faster than running large LLM once)\nLarge LLM process and checks generated multiple tokens.\nThis is done in parallel, so we can reuse parameters over each tokens.\nWe accept tokens from small LLM until it matches large LLM, then we discard leftover tokens.\n\nSpeculative decoding don't need training, and it is 3x faster.\nIt is also proven that this yields same probability distribution with original LLM!\nSelf Speculative Decoding\nSometimes, we don't want to use two models.\nInstead, we make large LLM that generates multiple next tokens. (e.g. next 4 tokens)\nAfter the first run, we can use large LLM is batch mode just like speculative decoding.\nLayerSkip\nLayerSkip use early exit self-speculative decoding!\nOften, output tokens appears in earlier layers.\nWe speculate each layer's output and early exit if output token is found.\nEfficient LLM models\nMicrosoft Ph-3\nTraining small model with selected good data improves model quality.\n\nFiltered code-language dataset (which is a subset of stackoverflow) trained with labels from GPT-4\nTextbook dataset generated by GPT-3.5\nExercises of textbook dataset generated by GPT-3.5\n\nMicrosoft only used these three training data!\nLongLoRA\n\nReasoning needs longer input sequence and more token generations because of chain of thought.\nLongLoRA consider local attention (only recent ones) and shift it to consider every input token.\nLocal attention significantly reduces KV cache, therefore we can use longer sequence.\nIn practice, we use global attention (use every input sequence) in some heads, and local attention (only recent ones) in some heads.\ne.g. Gemma 3 repeats 1 global attention layer and 5 local attention layer.\nMInference\n\nMicrosoft categorized attention patterns into three categories. (A shape head, Vertical-slash head, Block-sparse head)\nWe train model and categorize attention patterns.\nThen, we use only part of attention considering category.\nLLM without transformers\nRNN vs. Transformer\nRNN have statically fixed weights after training.\nHowever, RNN only use the final state of the network.\nTransformer have dynamically determined weights by inputs, so it is good at input-dependent tasks. (e.g. scanning)\nHowever, transformer need to remember every previous states of the network.\nRetentive Network\nRNN of local attention network?\nWe don't need global states, only last state is needed!\nMamba\nRNN with input dependent weights.\nNext hidden state is calculated by input/output dependent parameters.\nJamba\nMix of transformer and Mamba.\nWhy? RNN still use only one hidden state.\nBy mixing transformers, we can see multiple hidden states!\nPTQ (Post-training quantization)\nQAT requires large computation cost and memory.\n\nComputation cost: forward/backward passes + weight updates\nMemory cost: Large amount of activation and large number of parameters\n\nPTQ quantize weight parameters and activations after training.\nPTQ is a practical choice for LLM due to the compute and memory cost of QAT.\nHowever, in case of CNN, PTQ is adopted only when we cannot setup QAT. e.g. training datasets are unavailable.\nMotivation\nWhat if we run large language models on iPhone?\nAll parameters should be read to the main memory before running LLM.\nTherefore, parameter reading time (=size/bandwidth) determines the latency of the entire model.\nAlso, since other apps shares memory capacity and bandwidth, performance of the other apps will be hurt.\nWe need to reduce memory cost!\nQAT is not enough, PTQ is needed!\nQuantization on data with outliers\nAs the model size gets larger than 6 billion parameters, outlier features emerge.\nFew channels exhibit extremely large activation values while others have activation values with small magnitude.\nJust applying quantization will quantize all small magnitude values to 0, leaving only the outliers.\nLLM.int8()\nSeperate computation of normal and outlier data!\nNormal data is quantized. (int8)\nOutlier data is not quantized. (float16)\nSmoothQuant\n\nActivation has way higher magnitude than weights.\nSolution: Migrate magnitudes of activations into the weights!\nFirst, we scale down the outlier channels, then we scale up the corresponding weights.\nWe can have same output without having outliers!\nOPTQ(GPTQ)\nAfter doing quantization we change the remaining weights to mitigate the negative effect of quantization error.\nwqw_qwq​ is a weight parameter in the row, and δF\\delta_FδF​ is a weight update on the remaining weights due to the quantization of wqw_qwq​.\nFor each row, weight is quantized independently, then remaining weights and inverse hessian is updated.\nwq=arg min⁡wq(quant⁡(wq)−wq)2[HF−1]qqδF=−wq−quant⁡(wq)[HF−1]qq⋅(HF−1):,qH−q−1=(H−1−1[H−1]qqH:,q−1Hq,:−1)−q\\begin{align*}\nw_q &amp;= \\argmin_{w_q} \\frac{\\left( \\operatorname{quant}(w_q) - w_q \\right)^2}{[H_F^{-1}]_{qq}} \\\\\n\\delta_F &amp;= -\\frac{w_q - \\operatorname{quant}(w_q)}{[H_F^{-1}]_{qq}} \\cdot (H_F^{-1})_{:,q} \\\\\nH_{-q}^{-1} &amp;= \\left( H^{-1} - \\frac{1}{[H^{-1}]_{qq}} H_{:,q}^{-1} H_{q,:}^{-1} \\right)_{-q}\n\\end{align*}wq​δF​H−q−1​​=wq​argmin​[HF−1​]qq​(quant(wq​)−wq​)2​=−[HF−1​]qq​wq​−quant(wq​)​⋅(HF−1​):,q​=(H−1−[H−1]qq​1​H:,q−1​Hq,:−1​)−q​​\nQuaRot\nUltimately, we want to quantize matrix multiplication, not just weights and activation.\nAssume that quantization is linear operation, we can introduce rotation matrix to reduce outliers.\nThis paper was published in 2024, but this became the standard method to remove outliers!\nY=q(W)q(X)=q(W)RTRq(X)=q(W)RTq(RX)Y = q(W)q(X) = q(W)R^TRq(X) = q(W)R^Tq(RX)\nY=q(W)q(X)=q(W)RTRq(X)=q(W)RTq(RX)\nWe can use hadamard matrix as a rotation matrix. Hadamard spreads outlier uniformly. e.g. (1,0,…,0)→(1N,1N,…,1N)\\left( 1, 0, \\ldots, 0 \\right) \\rightarrow \\left( \\frac{1}{\\sqrt{N}}, \\frac{1}{\\sqrt{N}}, \\ldots, \\frac{1}{\\sqrt{N}} \\right)(1,0,…,0)→(N​1​,N​1​,…,N​1​)\nHadamard matrix can be obatined recursively!\nIt is efficient in both software and hardware.\nH2=[111−1]H2N=H2⊗HN=[HNHNHN−HN]\\begin{align*}\nH_2 &amp;= \\begin{bmatrix}\n1 &amp; 1 \\\\\n1 &amp; -1\n\\end{bmatrix} \\\\\nH_{2N} &amp;= H_2 \\otimes H_N = \\begin{bmatrix}\nH_N &amp; H_N \\\\\nH_N &amp; -H_N\n\\end{bmatrix}\n\\end{align*}H2​H2N​​=[11​1−1​]=H2​⊗HN​=[HN​HN​​HN​−HN​​]​\nPre-multiply rotation\n\nWe can pre-multiply rotation matrix Q, H to weights!\nInstead of using input/output directly, we use rotated input and get rotated output.\nBut we can quantize weight matrix without considering outliers!\n\nHowever, in multi-head attention, to store KV cache in quantized format, we have to rotate it during runtime. (hadamard before quantize, and after dequantize)\nOnline rotation makes runtime overhead!\nQServe\nW4A8KV4: Weight, KV cache is 4 bits, activation is 8 bits.\nGoal: No online rotation!\nApply rotation only to up layer in FFN, and QKV projection layer in attention.\nNo rotation to out layer in FFN and KV cache!\nHowever, we still have to handle outliers in KV cache.\nQServe use SmoothQuant to remove outliers.\nWe scale up query vectors while scaling down KV vectors. (Especially, value vector tends to have more outliers.)\nSpinQuant\nWe want to train rotation matrix too!\nRotation matrix should hold RTR=IR^TR = IRTR=I.\nHowever, this constraint is too strict due to the floating point error.\nWe use Cayley SGD to solve this problem!\nΔR(Y)\\Delta R(Y)ΔR(Y) is Cayley transform, which transforms skew-symmetric Y into rotation matrix.\nLet G is the gradient of quantization error.\nThen we can update rotation matrix like this.\nG=∇RLQ(R∣W,X)G^=GRT−12RRTGRTY=G^−G^TR′=ΔR(Y)R=(I−α2Y)−1(I+α2Y)R\\begin{align*}\nG &amp;= \\nabla_R \\mathcal{L}_Q(R|W, X) \\\\\n\\hat{G} &amp;= GR^T - \\frac{1}{2}RR^TGR^T \\\\\nY &amp;= \\hat{G} - \\hat{G}^T \\\\\nR&#x27; &amp;= \\Delta R(Y)R = \\left( I - \\frac{\\alpha}{2}Y \\right)^{-1} \\left( I + \\frac{\\alpha}{2}Y \\right)R\n\\end{align*}GG^YR′​=∇R​LQ​(R∣W,X)=GRT−21​RRTGRT=G^−G^T=ΔR(Y)R=(I−2α​Y)−1(I+2α​Y)R​\nSpinQuant also use four rotation matrix!\n\nR1R_1R1​ = Global rotation (Applied to everywhere)\nR2R_2R2​ = Value rotation\nR3R_3R3​ = KV rotation\nR4R_4R4​ = Down rotation in FFN\n\nR1, R2, R3, and R4 are important in that order, so Cayley SGD is used for R1.\nDiffusion LLM\nTraditional LLM produced only one word at a time.\nDiffusion LLM generates multiple tokens at a time, and it can correct previous output tokens.\nDiffusion LLM need less iterations to generate output, so it can have a lower cost than traditional decoder LLM.\n","categories":["SNU","4-1","하드웨어시스템설계"]},{"title":"비선형방정식의 근","url":"/posts/102/","content":"수치적인 근\n방정식의 근은 오차범위 내에서만 파악 가능\nex) 10x=310x = 310x=3의 정확한 근을 표현할 수 없음\n따라서 방정식의 근을 찾는다는건 사실 ∣f(x)∣&lt;ε|f(x)| &lt; \\varepsilon∣f(x)∣&lt;ε이 되는 값의 범위을 찾는다고 생각해야 함.\n실수를 다룰 때는 정확하게 a==b인지 검사할 수 없다!!\n이분법\nf(x)f(x)f(x)가 연속함수고 f(a)f(b)&lt;0f(a)f(b) &lt; 0f(a)f(b)&lt;0이라면 f(x)=0f(x)=0f(x)=0인 x∈(a,b)x \\in (a,b)x∈(a,b)가 존재한다.\n매번 c=a+b2c = \\frac{a+b}{2}c=2a+b​를 계산해서 f(a)f(c)f(a)f(c)f(a)f(c)나 f(c)f(b)f(c)f(b)f(c)f(b) 중 음수인 쪽으로 구간을 계속 절반으로 줄이면 된다.\nMachine epsilon 때문에 log⁡2∣a−b∣ε\\log_2\\frac{|a-b|}{\\varepsilon}log2​ε∣a−b∣​번 이내로 반드시 해가 구해진다는 장점이 있음!\n특히 함수가 비해석적이여도 연속적이면 무조건 적용가능!!\n하지만 복소함수로 간다면 해를 찾을 수 없음..\n뉴턴법\n함수 f의 정확한 근이 x∗x^*x∗이고 그 근처의 점 xxx가 있을 때, 테일러 전개에 의해\n0=f(x∗)=f(x)+(x∗−x)f′(x)+⋯0 = f(x^*) = f(x) + (x^* - x)f&#x27;(x) + \\cdots\n0=f(x∗)=f(x)+(x∗−x)f′(x)+⋯\n첫 항만 보면 x∗≈x−f(x)f′(x)x^* \\approx x - \\frac{f(x)}{f&#x27;(x)}x∗≈x−f′(x)f(x)​라고 볼 수 있다!\n물론 절단오차가 있지만, 이 방법을 계속해서 적용시키면 해가 구해진다!\n대부분 함수는 뉴턴법으로 풀리고, 특히 복소함수도 풀린다.\n단점) f′(x)f&#x27;(x)f′(x)가 0에 가까워 값이 급격하게 변하거나 f(x)f′(x)\\frac{f(x)}{f&#x27;(x)}f′(x)f(x)​ 부호가 계속 변해 진동만 할 경우 (함수가 대칭이면 자주 발생함) 해를 찾는데 오래 걸리거나 못 구할 수도 있다...\n할선법\nf′(x)f&#x27;(x)f′(x)를 구하기 어려운 경우 아래 근사를 사용할 수 있다.\nf′(x)≈fn−fn−1xn−xn−1∴xn+1=xn−f(xn)f′(xn)=xn−fnxn−xn−1fn−fn−1\\begin{align*}\nf&#x27;(x) &amp;\\approx \\frac{f_n - f_{n-1}}{x_n - x_{n-1}} \\\\\n\\therefore x_{n+1} &amp;= x_n - \\frac{f(x_n)}{f&#x27;(x_n)} \\\\\n&amp;= x_n - f_n \\frac{x_n - x_{n-1}}{f_n - f_{n-1}}\n\\end{align*}f′(x)∴xn+1​​≈xn​−xn−1​fn​−fn−1​​=xn​−f′(xn​)f(xn​)​=xn​−fn​fn​−fn−1​xn​−xn−1​​​\n뉴턴법과 달리 처음에 두 점이 필요하며, 반드시 ∣f0∣&gt;∣f1∣|f_0| &gt; |f_1|∣f0​∣&gt;∣f1​∣이여야 한다.\n다중근의 경우\nP(x)=(x−α)nQ(x)P(x) = (x - \\alpha)^n Q(x)\nP(x)=(x−α)nQ(x)\n다중근을 갖는 다항식의 경우 조그만 변화로도 오차가 급속도로 커지게 된다.\n이 경우에는 미분한 값으로 나눠주면 다중근의 영향이 소거된다.\nP(x)=(x−α)nQ(x)P′(x)=(x−α)n−1(nQ(x)+(x−α)Q′(x))f(x)=P(x)P′(x)=(x−α)Q(x)nQ(x)+(x−α)Q′(x)\\begin{align*}\nP(x) &amp;= (x - \\alpha)^n Q(x) \\\\\nP&#x27;(x) &amp;= (x - \\alpha)^{n-1} (nQ(x) + (x - \\alpha) Q&#x27;(x)) \\\\\nf(x) &amp;= \\frac{P(x)}{P&#x27;(x)} = (x - \\alpha) \\frac{Q(x)}{nQ(x) + (x - \\alpha) Q&#x27;(x)}\n\\end{align*}P(x)P′(x)f(x)​=(x−α)nQ(x)=(x−α)n−1(nQ(x)+(x−α)Q′(x))=P′(x)P(x)​=(x−α)nQ(x)+(x−α)Q′(x)Q(x)​​\nf(x)f(x)f(x)는 x=αx = \\alphax=α에서 단일근을 가지므로, 다중근을 가질 것으로 예상되는 경우 f(x)=0f(x) = 0f(x)=0의 해를 구하면 된다.\n사실 QR 반복법으로 해를 구한 다음 다중근의 존재여부를 판단하는 방식이 상위호환이다\n이 경우 뉴턴법을 적용시키기엔 f′(x)f&#x27;(x)f′(x)가 너무 더러우므로 할선법을 사용한다.\n연속대입법\ng(x)=x,∣g′(x)∣&lt;1g(x) = x, |g&#x27;(x)| &lt; 1\ng(x)=x,∣g′(x)∣&lt;1\n위 형태로 주어진 방정식의 경우, xn+1=g(xn)x_{n+1} = g(x_n)xn+1​=g(xn​)을 반복하여 근을 구할 수 있다.\n사실 뉴턴법과 할선법은 연속대입법의 일종으로 볼 수 있다!\nxn+1=xn−f(xn)f′(xn)g(x)=x−f(x)f′(x)\\begin{align*}\nx_{n+1} &amp;= x_n - \\frac{f(x_n)}{f&#x27;(x_n)} \\\\\ng(x) &amp;= x - \\frac{f(x)}{f&#x27;(x)}\n\\end{align*}xn+1​g(x)​=xn​−f′(xn​)f(xn​)​=x−f′(x)f(x)​​\nxn+1=xn−fnxn−xn−1fn−fn−1g(x)=x−f(x)x−x∗f(x)−f(x∗)\\begin{align*}\nx_{n+1} &amp;= x_n - f_n \\frac{x_n - x_{n-1}}{f_n - f_{n-1}} \\\\\ng(x) &amp;= x - f(x) \\frac{x - x^*}{f(x) - f(x^*)}\n\\end{align*}xn+1​g(x)​=xn​−fn​fn​−fn−1​xn​−xn−1​​=x−f(x)f(x)−f(x∗)x−x∗​​\n오차 및 수렴특성\n이론상 반복하면 수렴하지만... 얼마나 반복해야할까?\n실제 근을 α\\alphaα, 오차를 en=α−xne_n = \\alpha - x_nen​=α−xn​이라고 하자.\n연속대입법의 경우\nα=g(α)\\alpha = g(\\alpha)α=g(α)이고 xk+1=g(xk)x_{k+1} = g(x_k)xk+1​=g(xk​)이므로, 평균값 정리로부터\n∃ξk∈(α,xk) s.t. ek+1=α−xk+1=g(α)−g(xk)=(α−xk)g′(ξk)∴∣ek+1ek∣=∣g′(ξk)∣\\begin{align*}\n\\exists \\xi_k \\in (\\alpha, x_k) \\  s.t. \\  e_{k+1} &amp;= \\alpha - x_{k+1} \\\\\n&amp;= g(\\alpha) - g(x_k) \\\\\n&amp;= (\\alpha - x_k)g&#x27;(\\xi_k) \\\\\n\\therefore \\left| \\frac{e_{k+1}}{e_k} \\right| &amp;= |g&#x27;(\\xi_k)|\n\\end{align*}∃ξk​∈(α,xk​) s.t. ek+1​∴​ek​ek+1​​​​=α−xk+1​=g(α)−g(xk​)=(α−xk​)g′(ξk​)=∣g′(ξk​)∣​\n따라서 항상 ∣g′(x)∣≤r&lt;1|g&#x27;(x)| \\leq r &lt; 1∣g′(x)∣≤r&lt;1이라면 ∣ek+1∣≤rk∣e1∣|e_{k+1}| \\leq r^k |e_1|∣ek+1​∣≤rk∣e1​∣이므로 오차가 0으로 수렴한다.\n이때 오차의 비율이 선형적으로 감소하므로, 연속대입법은 선형적 수렴정도를 가진다.\n뉴턴법의 경우\nf(α)=0f(\\alpha) = 0f(α)=0이고 xk+1=xk−f(xk)f′(xk)x_{k+1} = x_k - \\frac{f(x_k)}{f&#x27;(x_k)}xk+1​=xk​−f′(xk​)f(xk​)​이므로\nek+1=α−xk+1=α−xk+f(xk)f′(xk)=ek+f(xk)−f(α)f′(xk)\\begin{align*}\ne_{k+1} &amp;= \\alpha - x_{k+1} \\\\\n&amp;= \\alpha - x_k + \\frac{f(x_k)}{f&#x27;(x_k)} \\\\\n&amp;= e_k + \\frac{f(x_k) - f(\\alpha)}{f&#x27;(x_k)}\n\\end{align*}ek+1​​=α−xk+1​=α−xk​+f′(xk​)f(xk​)​=ek​+f′(xk​)f(xk​)−f(α)​​\n이 때 테일러 전개에 의해\n∃ξk∈(α,xk) s.t. f(α)=f(xk)+(α−xk)f′(xk)+12(α−xk)2f′′(ξk)∴f(xk)−f(α)=−ekf′(xk)−12ek2f′′(ξk)∴ek+1=ek+−ekf′(xk)−12ek2f′′(ξk)f′(xk)=−12ek2f′′(ξk)f′(xk)\\begin{align*}\n\\exists \\xi_k \\in (\\alpha, x_k) \\  s.t. \\  f(\\alpha) &amp;= f(x_k) + (\\alpha - x_k)f&#x27;(x_k) + \\frac{1}{2}(\\alpha - x_k)^2f&#x27;&#x27;(\\xi_k) \\\\\n\\therefore f(x_k) - f(\\alpha) &amp;= -e_kf&#x27;(x_k) - \\frac{1}{2}e_k^2f&#x27;&#x27;(\\xi_k) \\\\\n\\therefore e_{k+1} &amp;= e_k + \\frac{-e_kf&#x27;(x_k) - \\frac{1}{2}e_k^2f&#x27;&#x27;(\\xi_k)}{f&#x27;(x_k)} \\\\\n&amp;= -\\frac{1}{2}e_k^2\\frac{f&#x27;&#x27;(\\xi_k)}{f&#x27;(x_k)}\n\\end{align*}∃ξk​∈(α,xk​) s.t. f(α)∴f(xk​)−f(α)∴ek+1​​=f(xk​)+(α−xk​)f′(xk​)+21​(α−xk​)2f′′(ξk​)=−ek​f′(xk​)−21​ek2​f′′(ξk​)=ek​+f′(xk​)−ek​f′(xk​)−21​ek2​f′′(ξk​)​=−21​ek2​f′(xk​)f′′(ξk​)​​\n이때 오차의 비율이 제곱에 비례하여 감소하므로, 뉴턴법은 2차적 수렴정도를 가진다.\n한편 테일러 전개에서 ek2e_k^2ek2​ 항을 무시하면 ek+1e_{k+1}ek+1​이 제곱에 비례한다면서 이게 뭔 개소리지 무시할 수 있는거 맞나 ek=−f(xk)f′(xk)e_k = -\\frac{f(x_k)}{f&#x27;(x_k)}ek​=−f′(xk​)f(xk​)​이므로,\n∣ek+1ek∣=∣−12ekf′′(ξk)f′(xk)∣=12∣f(xk)f′′(ξk)(f′(xk))2∣\\left| \\frac{e_{k+1}}{e_k} \\right| = \\left| -\\frac{1}{2}e_k\\frac{f&#x27;&#x27;(\\xi_k)}{f&#x27;(x_k)} \\right| = \\frac{1}{2} \\left| \\frac{f(x_k)f&#x27;&#x27;(\\xi_k)}{(f&#x27;(x_k))^2} \\right|\n​ek​ek+1​​​=​−21​ek​f′(xk​)f′′(ξk​)​​=21​​(f′(xk​))2f(xk​)f′′(ξk​)​​\n이다. 따라서 12∣f(xk)f′′(ξk)(f′(xk))2∣&lt;1\\frac{1}{2} \\left| \\frac{f(x_k)f&#x27;&#x27;(\\xi_k)}{(f&#x27;(x_k))^2} \\right| &lt; 121​​(f′(xk​))2f(xk​)f′′(ξk​)​​&lt;1이여야 뉴턴법이 수렴하게 된다.\n일반적으로 ∣f′(xk)∣|f&#x27;(x_k)|∣f′(xk​)∣가 큰 값을 가지거나 변곡점이 없을 때 뉴턴법이 잘 수렴한다.\n또한, ek=−f(xk)f′(xk)e_k = -\\frac{f(x_k)}{f&#x27;(x_k)}ek​=−f′(xk​)f(xk​)​, xk+1=xk−f(xk)f′(xk)x_{k+1} = x_k - \\frac{f(x_k)}{f&#x27;(x_k)}xk+1​=xk​−f′(xk​)f(xk​)​에서 ek≈xk+1−xke_k \\approx x_{k+1} - x_kek​≈xk+1​−xk​다. 오차를 계산 도중에 예측할 수 있다!\n가위치법\n안 쓰니까 재미로 보기\n이분법은 함수값을 사용하지 않는다 -&gt; 함수가 직선에 가깝다면 직선을 이용하는게 더 효과적이지 않을까?\n단순 cn=an+bn2c_n = \\frac{a_n + b_n}{2}cn​=2an​+bn​​ 대신 (an,f(an)),(bn,f(bn))(a_n, f(a_n)), (b_n,f(b_n))(an​,f(an​)),(bn​,f(bn​))을 지나는 직선이 x축과 만나는 점 cn=an−f(an)an−bnf(an)−f(bn)c_n = a_n - f(a_n) \\frac{a_n - b_n}{f(a_n) - f(b_n)}cn​=an​−f(an​)f(an​)−f(bn​)an​−bn​​을 새로운 경계점으로 사용하는 방법을 가위치법이라고 한다.\n문제점) 함수가 직선에 가깝지 않을수가 있음...\n함수값의 변화가 완만한 경우 한쪽의 경계점이 계속 고정되는 단점이 있다 (정체점 문제)\n수정 가위치기법\n이전과 똑같은 경계점은 (an,f(an))(a_n, f(a_n))(an​,f(an​)) 대신 (an,f(an)2)(a_n, \\frac{f(a_n)}{2})(an​,2f(an​)​)를 사용한다!\n의외로 이렇게 하면 뉴턴법 급으로 빠르게 수렴한다고 한다...\n뮬러법\n안 쓰니까 재미로 보기 2\n진짜 몰라도 됨 시험도 안 냄\n왜 두 개만 쓰지? 점 세 개를 가지고 포물선을 만들자!\n\nx1&lt;x2&lt;x3x_1 &lt; x_2 &lt; x_3x1​&lt;x2​&lt;x3​ 세 점을 잡는다. (놀랍게도 함수값 조건은 없다! 대신 함수값 부호가 전부 같으면 [x1,x3][x_1, x_3][x1​,x3​] 밖에 근이 있을 수도 있으므로 수렴하는데 오래 걸릴 수 있다...)\n세 점 (x1,f1),(x2,f2),(x3,f3)(x_1, f_1), (x_2, f_2), (x_3, f_3)(x1​,f1​),(x2​,f2​),(x3​,f3​)을 지나는 이차방정식을 구하고, x2x_2x2​와 제일 가까운 근 x0x_0x0​을 구한다.\nx0,x1,x2,x3x_0, x_1, x_2, x_3x0​,x1​,x2​,x3​ 중 x0x_0x0​과 제일 멀리 떨어져있는 점 하나를 버린다.\n남은 세 점을 다시 x1&lt;x2&lt;x3x_1 &lt; x_2 &lt; x_3x1​&lt;x2​&lt;x3​이 되도록 재배치 한 다음 위 과정을 반복한다.\n\n당연히 이차방정식을 구하고 근을 구하는 과정이 제일 거지같은데, 세 점 (x1,f1),(x2,f2),(x3,f3)(x_1, f_1), (x_2, f_2), (x_3, f_3)(x1​,f1​),(x2​,f2​),(x3​,f3​)을 지나는 이차방정식은\np(x)=f1+f2−f1x2−x1(x−x1)+f3−f2x3−x2−f2−f1x2−x1x3−x1(x−x1)(x−x2)p(x) = f_1 + \\frac{f_2 - f_1}{x_2 - x_1} (x - x_1) + \\frac{\\frac{f_3 - f_2}{x_3 - x_2} - \\frac{f_2 - f_1}{x_2 - x_1}}{x_3 - x_1}(x - x_1)(x - x_2)\np(x)=f1​+x2​−x1​f2​−f1​​(x−x1​)+x3​−x1​x3​−x2​f3​−f2​​−x2​−x1​f2​−f1​​​(x−x1​)(x−x2​)\n이고, m1=f2−f1x2−x1,m2=f3−f2x3−x2,a=m2−m1x3−x1,b=m1+a(x2−x1),c=f2m_1 = \\frac{f_2 - f_1}{x_2 - x_1}, m_2 = \\frac{f_3 - f_2}{x_3 - x_2}, a = \\frac{m_2 - m_1}{x_3 - x_1}, b = m_1 + a(x_2 - x_1), c = f_2m1​=x2​−x1​f2​−f1​​,m2​=x3​−x2​f3​−f2​​,a=x3​−x1​m2​−m1​​,b=m1​+a(x2​−x1​),c=f2​로 정의하면\np(x)=f2+m1(x−x2)+a(x−x1)(x−x2)=f2+m1(x−x2)+a((x−x2)+(x2−x1))(x−x2)=a(x−x2)2+(m1+a(x2−x1))(x−x2)+f2=a(x−x2)2+b(x−x2)+c\\begin{align*}\np(x) &amp;= f_2 + m_1(x - x_2) + a(x - x_1)(x - x_2) \\\\\n&amp;= f_2 + m_1(x - x_2) + a((x - x_2) + (x_2 - x_1))(x - x_2) \\\\\n&amp;= a(x - x_2)^2 + (m_1 + a(x_2 - x_1))(x - x_2) + f_2 \\\\\n&amp;= a(x - x_2)^2 + b(x - x_2) + c\n\\end{align*}p(x)​=f2​+m1​(x−x2​)+a(x−x1​)(x−x2​)=f2​+m1​(x−x2​)+a((x−x2​)+(x2​−x1​))(x−x2​)=a(x−x2​)2+(m1​+a(x2​−x1​))(x−x2​)+f2​=a(x−x2​)2+b(x−x2​)+c​\n가 되므로 위 이차방정식의 근은 x−x2=−b±b2−4ac2ax - x_2 = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}x−x2​=2a−b±b2−4ac​​다.\n구간의 크기를 줄여 수렴시키기 위해서 x2x_2x2​에 제일 가까운 근을 선택해야 하므로 분자가 제일 작은 근을 골라야 하는데, 너무 가까운 두 수를 빼면 오차가 커지는 문제점이 있다.\n따라서 근의 공식을 변형시켜 루트 부분이 분모로 가게 만들고 분모가 제일 큰 경우를 선택한다.\n∴x0={x2−2cb+b2−4acb&gt;0x2−2cb−b2−4acb≤0\\therefore x_0 = \\begin{cases}\nx_2 - \\frac{2c}{b + \\sqrt{b^2 - 4ac}} &amp; b &gt; 0 \\\\\nx_2 - \\frac{2c}{b - \\sqrt{b^2 - 4ac}} &amp; b \\leq 0\n\\end{cases}∴x0​={x2​−b+b2−4ac​2c​x2​−b−b2−4ac​2c​​b&gt;0b≤0​\n베어스토우법\n이것도 QR 반복법에 대체돼서 안 씀\nn차 다항식이 사실 어떤 이차다항식으로 나눠진다고 가정\npn(x)=anxn+an−1xn−1+⋯+a1x+a0=(x2+rx+s)(bnxn−2+bn−1xn−3+⋯+b1x+b0)+b1x+b0\\begin{align*}\np_n(x) =&amp; a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_1x + a_0 \\\\\n=&amp; (x^2 + rx + s)(b_nx^{n-2} + b_{n-1}x^{n-3} + \\cdots + b_1x + b_0) \\\\\n&amp;+ b_1x + b_0\n\\end{align*}pn​(x)==​an​xn+an−1​xn−1+⋯+a1​x+a0​(x2+rx+s)(bn​xn−2+bn−1​xn−3+⋯+b1​x+b0​)+b1​x+b0​​\n만약 b1x+b0=0b_1x + b_0 = 0b1​x+b0​=0이라면, x2+rx+s=0x^2+rx+s = 0x2+rx+s=0에서 두 개의 근을 구할 수 있음\nr,sr, sr,s를 b1x+b0=0b_1x + b_0 = 0b1​x+b0​=0이 되는 참값, r∗,s∗r^*, s^*r∗,s∗를 추정값, 참값과 추정값의 오차를 Δr=r−r∗,Δs=s−s∗\\Delta r = r - r^*, \\Delta s = s - s^*Δr=r−r∗,Δs=s−s∗라고 하자.\nb0,b1b_0, b_1b0​,b1​을 r,sr, sr,s에 대한 함수로 보고 테일러 전개를 하면\n0=b0(r,s)≈b0(r∗,s∗)+Δr∂b0∂r+Δs∂b0∂s0=b1(r,s)≈b1(r∗,s∗)+Δr∂b1∂r+Δs∂b1∂s∴ Δr∂b0∂r+Δs∂b0∂s=−b0(r∗,s∗)Δr∂b1∂r+Δs∂b1∂s=−b1(r∗,s∗)∴ [ΔrΔs]=[∂b0∂r∂b0∂s∂b1∂r∂b1∂s]−1[−b0(r∗,s∗)−b1(r∗,s∗)]\\begin{align*}\n0 = b_0(r,s) &amp;\\approx  b_0(r^*, s^*) + \\Delta r \\frac{\\partial b_0}{\\partial r} + \\Delta s \\frac{\\partial b_0}{\\partial s} \\\\\n0 = b_1(r,s) &amp;\\approx  b_1(r^*, s^*) + \\Delta r \\frac{\\partial b_1}{\\partial r} + \\Delta s \\frac{\\partial b_1}{\\partial s} \\\\\n\\therefore\\ &amp; \\Delta r \\frac{\\partial b_0}{\\partial r} + \\Delta s \\frac{\\partial b_0}{\\partial s} = -b_0(r^*, s^*) \\\\\n&amp;\\Delta r \\frac{\\partial b_1}{\\partial r} + \\Delta s \\frac{\\partial b_1}{\\partial s} = -b_1(r^*, s^*) \\\\\n\\therefore\\ &amp; \\begin{bmatrix}\n\\Delta r \\\\\n\\Delta s\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{\\partial b_0}{\\partial r} &amp; \\frac{\\partial b_0}{\\partial s} \\\\\n\\frac{\\partial b_1}{\\partial r} &amp; \\frac{\\partial b_1}{\\partial s}\n\\end{bmatrix}^{-1} \\begin{bmatrix}\n-b_0(r^*, s^*) \\\\\n-b_1(r^*, s^*)\n\\end{bmatrix}\n\\end{align*}0=b0​(r,s)0=b1​(r,s)∴ ∴ ​≈b0​(r∗,s∗)+Δr∂r∂b0​​+Δs∂s∂b0​​≈b1​(r∗,s∗)+Δr∂r∂b1​​+Δs∂s∂b1​​Δr∂r∂b0​​+Δs∂s∂b0​​=−b0​(r∗,s∗)Δr∂r∂b1​​+Δs∂s∂b1​​=−b1​(r∗,s∗)[ΔrΔs​]=[∂r∂b0​​∂r∂b1​​​∂s∂b0​​∂s∂b1​​​]−1[−b0​(r∗,s∗)−b1​(r∗,s∗)​]​\n(bi)r≔∂bi∂r(b_i)_r \\coloneqq \\frac{\\partial b_i}{\\partial r}(bi​)r​:=∂r∂bi​​, (bi)s≔∂bi∂s(b_i)_s \\coloneqq \\frac{\\partial b_i}{\\partial s}(bi​)s​:=∂s∂bi​​라고 정의하면 (b0)r,(b0)s,(b1)r,(b1)s(b_0)_r, (b_0)_s, (b_1)_r, (b_1)_s(b0​)r​,(b0​)s​,(b1​)r​,(b1​)s​만 구하면 Δr,Δs\\Delta r, \\Delta sΔr,Δs를 구할 수 있다!!!\n왜 아무도 안 쓰는지 알겠다\n미분값을 구하는것도 정말 아름다운데, 위의 방정식을 전개한 뒤 계수비교하면\nbn=anbn−1=an−1−rbnbn−2=an−2−rbn−1−sbn⋮b1=a1−rb2−sb3b0=a0−sb2\\begin{align*}\nb_n &amp;= a_n \\\\\nb_{n-1} &amp;= a_{n-1} - rb_n \\\\\nb_{n-2} &amp;= a_{n-2} - rb_{n-1} - sb_n \\\\\n&amp;\\vdots \\\\\nb_1 &amp;= a_1 - rb_2 - sb_3 \\\\\nb_0 &amp;= a_0 - sb_2\n\\end{align*}bn​bn−1​bn−2​b1​b0​​=an​=an−1​−rbn​=an−2​−rbn−1​−sbn​⋮=a1​−rb2​−sb3​=a0​−sb2​​\n이므로, 간단한 편미분을 통해\n(bn)r=0(bn)s=0(bn−1)r=−bn(bn−1)s=0(bn−2)r=−bn−1−r(bn−1)r−s(bn)r(bn−2)s=−bn−s(bn)s−r(bn−1)s⋮⋮(b1)r=−b2−r(b2)r−s(b3)r(b1)s=−b3−s(b3)s−r(b2)s(b0)r=−s(b2)r(b0)s=−b2−s(b2)s\\begin{aligned}\n(b_n)_r &amp;= 0 &amp; (b_n)_s &amp;= 0 \\\\\n(b_{n-1})_r &amp;= -b_n &amp; (b_{n-1})_s &amp;= 0 \\\\\n(b_{n-2})_r &amp;= -b_{n-1} - r(b_{n-1})_r - s(b_n)_r &amp; (b_{n-2})_s &amp;= -b_n -s(b_n)_s - r(b_{n-1})_s \\\\\n&amp;\\vdots &amp; &amp;\\vdots\\\\\n(b_1)_r &amp;= -b_2 - r(b_2)_r - s(b_3)_r &amp; (b_1)_s &amp;= -b_3 - s(b_3)_s - r(b_2)_s \\\\\n(b_0)_r &amp;= -s(b_2)_r &amp; (b_0)_s &amp;= -b_2 - s(b_2)_s\n\\end{aligned}\n(bn​)r​(bn−1​)r​(bn−2​)r​(b1​)r​(b0​)r​​=0=−bn​=−bn−1​−r(bn−1​)r​−s(bn​)r​⋮=−b2​−r(b2​)r​−s(b3​)r​=−s(b2​)r​​(bn​)s​(bn−1​)s​(bn−2​)s​(b1​)s​(b0​)s​​=0=0=−bn​−s(bn​)s​−r(bn−1​)s​⋮=−b3​−s(b3​)s​−r(b2​)s​=−b2​−s(b2​)s​​\n임을 알 수 있다.\n따라서 ai,bi,(bi)r,(bi)sa_i, b_i, (b_i)_r, (b_i)_sai​,bi​,(bi​)r​,(bi​)s​ 배열 4개를 준비한 다음 간단한 DP를 통해 Δr,Δs\\Delta r, \\Delta sΔr,Δs를 구할 수 있다...\nΔr,Δs\\Delta r, \\Delta sΔr,Δs가 충분히 작아질 때까지 반복하면 된다...\n근데 이따구로 구해도 다중근의 경우 성능이 떨어지는 단점이 있다...\n","categories":["SNU","4-2","공학수학 3"]},{"title":"수의 체계","url":"/posts/101/","content":"정수\n다 알다시피 2n2n2n비트 숫자는 −2n-2^n−2n 부터 2n−12^n - 12n−1 까지 표현가능\n실수\nx=±1.m×2cx = \\pm 1.m \\times 2^c\nx=±1.m×2c\n단정도 (single)에선 sign 1비트, exponent 8비트, mantissa 23비트\n배정도는 (double)에선 sign 1비트, exponent 11비트, mantissa에 52비트\n단정도에서 c=e−127c = e - 127c=e−127\n특이하게 exponent가 all 0면 x=±0.m×2cx = \\pm 0.m \\times 2^cx=±0.m×2c로 계산하고, (c는 e=1일 때의 값)\nexponent가 all 1이면 m이 0이면 무한대, 아닐 경우 NaN\n머신입실론 (Machine epsilon)\n발생할 수 있는 상대오차의 최대치.\nmantissa 비트가 1만 변했을 때의 오차다.\n즉, 단정도에서 ε=2−23\\varepsilon = 2^{-23}ε=2−23, 배정도에서 ε=2−52\\varepsilon = 2^{-52}ε=2−52\n중요) 부동소수점 방식은 상대오차가 일정함!! 절대오차는 달라짐 (작은 실수는 절대오차가 작고, 큰 실수는 절대오차가 큼)\n오차\n마무리오차 (round-off error)\n컴퓨터에 수를 반올림해서 저장하느라 발생하는 오차.\n특히 덧셈 등 두 수의 연산을 수행할 때 발생하는 마무리오차는 확산 마무리오차라고 하며, 대부분의 경우 ε\\varepsilonε이랑 같다.\n하지만 크기가 비슷한 두 수를 빼거나 크기가 너무 다른 두 수를 더할 때는 ε\\varepsilonε보다 커진다.\nc.f. 유효수상실\n서로 비슷한 두 수를 빼면 유효숫자가 줄어듦\ne.g. a가 1.23456789고 b가 1.23456788이라면 a, b는 유효숫자가 9자리지만 a−ba-ba−b는 유효숫자가 1자리임\n해결방안 1) 뺄 때는 식을 변형시켜서 덧셈을 사용\ne.g. 1+x−1=x1+x+1\\sqrt{1+x} - 1 = \\frac{x}{\\sqrt{1+x}+1}1+x​−1=1+x​+1x​\n해결방안 2) 더할 때는 grouping(N개씩 묶어서 더함) 또는 작은 수 부터 더함\ne.g. ∑i=1N1i4\\sum_{i=1}^N \\frac{1}{i^4}∑i=1N​i41​는 i=1i=1i=1 부터 더하는 것보다 i=Ni=Ni=N 부터 더하는게 훨씬 정확함\n절단오차\n문제의 해석에서 발생하는 오차.\n대표적으로 테일러전개는 당연히 무한개의 항을 전부 더할 수 없는데, 만약 아래 식처럼 3번째 항에서 끊었다면 뒤따르는 나머지 항들이 오차가 됨.\nf(x+h)≈f(x)+hf′(x)+h22!f′′(x)f(x+h) \\approx f(x) + hf&#x27;(x) + \\frac{h^2}{2!}f&#x27;&#x27;(x)\nf(x+h)≈f(x)+hf′(x)+2!h2​f′′(x)\n일반적으로 절단오차가 작아지면 마무리오차가 커지기 때문에, 오차가 제일 줄어드는 적당한 선에서 계산해야함.\n","categories":["SNU","4-2","공학수학 3"]},{"title":"수치선형대수","url":"/posts/108/","content":"행렬의 기초 개념\n행렬에서 제일 중요한 두 문제는 연립방정식 Ax=bAx = bAx=b와 고유값문제 Ax=λxAx = \\lambda xAx=λx다. 우선 Ax=bAx = bAx=b를 푸는 방법을 알아보고 Ax=λxAx = \\lambda xAx=λx는 다음 장에서 다룬다.\n왜 그러는지 모르겠지만 여기서는 AAA를 행렬, AiA_iAi​를 i번째 행벡터, AjA^jAj를 j번째 열벡터, AijA_i^jAij​를 aija_{ij}aij​원소라고 하자.\n아니 행렬 거듭제곱은 어쩌려고 이딴 notation을\neAe^AeA는 행렬 테일러전개로 정의되는데\n그럼 Ax=bAx = bAx=b는 AiA^iAi들의 선형결합으로 bbb를 나타낼 수 있냐는 문제가 된다.\n그럼 반대로 x=A−1bx = A^{-1}bx=A−1b이므로 역행렬은 행벡터의 선형결합으로 x를 나타내는 방식이다?\nA−1=Aadjdet⁡AA^{-1} = \\frac{A^{adj}}{\\det A}A−1=detAAadj​ adjugate 행렬도 행벡터의 선형결합을 의미하는거\n그럼 행벡터 x 열벡터가 더 이쁘니까 AA−1AA^{-1}AA−1 말고 A−1AA^{-1}AA−1A로 써야한다??\nCramer's rule: xi=det⁡Aidet⁡Ax_i = \\frac{\\det A_i}{\\det A}xi​=detAdetAi​​는 b에서 AiA^iAi로 내린 수직선이 만나는 점을 의미하는거다????\n너무 추상적인 개념이니까 그냥 아래 리스트나 보도록 하자\n\n소행렬 AijA_{ij}Aij​는 i행, j열을 소거한 행렬\nn×nn \\times nn×n은 정사각행렬 (square matrix)\na11,…,anna_{11}, \\ldots, a_{nn}a11​,…,ann​은 주대각 (principal diagonal)\n주대각 아래쪽이 0이면 위삼각행렬 (upper triangular matrix, U), 주대각 위쪽이 0이면 아래삼각행렬 (lower triangular matrix, L)\n주대각이 아닌 모든 원소가 0이면 대각행렬 (diagonal matrix, D)\n주대각이 모두 같은 대각행렬은 스칼라행렬 (scalar matrix, S)\n그게 1이면 단위행렬 (unit matrix, I)\n행렬의 기본행연산: 두 행 교환, 상수곱, 한 행의 상수배 더함\n행 열 바꾸면 전치 (transpose)\n주대각 합은 트레이스 (trace)\nxTyx^TyxTy는 두 벡터의 내적 (x⋅yx \\cdot yx⋅y), xyTxy^TxyT는 두 벡터의 외적\n\n행렬식\n\ndet⁡A2×2=ad−bc\\det A_{2 \\times 2} = ad - bcdetA2×2​=ad−bc\n두 행 바꾸면 det 부호 바뀜\n한 행에 다른 행의 상수배를 더해도 det는 같음\n한 행에 상수배를 하면 행렬식도 상수배가 됨\ndet⁡(sA)=sndet⁡A\\det(sA) = s^n \\det Adet(sA)=sndetA\ndet⁡AB=det⁡Adet⁡B\\det AB = \\det A \\det BdetAB=detAdetB\ndet⁡Ak=(det⁡A)k\\det A^k = (\\det A)^kdetAk=(detA)k (k=−1k=-1k=−1, 즉 역행렬일 때도 마찬가지)\ndet⁡AT=det⁡A\\det A^T = \\det AdetAT=detA\ndet⁡A‾=det⁡A‾\\det \\overline{A} = \\overline {\\det A}detA=detA (complex conjugate)\ndet⁡L=det⁡U=a11a22⋯ann\\det L = \\det U = a_{11} a_{22} \\cdots a_{nn}detL=detU=a11​a22​⋯ann​\ndet⁡Aadj=(det⁡A)n−1\\det A^{adj} = (\\det A)^{n-1}detAadj=(detA)n−1 (∵Aadj=det⁡A⋅A−1\\because A^{adj} = \\det A \\cdot A^{-1}∵Aadj=detA⋅A−1)\n\nEinstein notation: AixiA^ix_iAixi​처럼 iii가 중복될 경우 ∑iAixi\\sum_i A^ix_i∑i​Aixi​를 의미하는 것 (즉 귀찮으면 시그마 생략함)\nCrammer's rule: Ax=bAx = bAx=b의 양변에 AadjA^{adj}Aadj를 곱하면 I⋅det⁡Ax=AadjbI \\cdot \\det A x = A^{adj}bI⋅detAx=Aadjb\n역행렬\n\n(A−1)−1=A(A^{-1})^{-1} = A(A−1)−1=A\n(AB)−1=B−1A−1(AB)^{-1} = B^{-1}A^{-1}(AB)−1=B−1A−1\n(Ak)−1=(A−1)k(A^k)^{-1} = (A^{-1})^k(Ak)−1=(A−1)k\n(sA)−1=s−1A−1(sA)^{-1} = s^{-1}A^{-1}(sA)−1=s−1A−1\nA‾−1=A−1‾\\overline{A}^{-1} = \\overline{A^{-1}}A−1=A−1 (complex conjugate)\ndet⁡A−1=(det⁡A)−1\\det A^{-1} = (\\det A)^{-1}detA−1=(detA)−1\n\nc.f. A=ATA = A^TA=AT면 symmetric\nA−1=ATA^{-1} = A^TA−1=AT면 orthogonal\nA=A−1=ATA = A^{-1} = A^TA=A−1=AT면 householder..????\nH=I−2vvTH = I - 2vv^TH=I−2vvT인 행렬인데 수치선대에서 자주 쓴다고 함\n가우스 소거, 가우스-조단 소거\n기본행연산으로 삼각행렬 만들던가 III 만들던가\n만들면 연립방정식 풀기 가능\n근데 피봇팅 중요!!\n제일 큰 숫자를 1행으로 땡겨오고 가우스 소거를 해야 오차가 줄어든다\n반복법\nAx=bAx = bAx=b에서 ∑j=1naijxj=bj\\sum_{j=1}^n a_{ij}x_j = b_j∑j=1n​aij​xj​=bj​\ni번째 방정식만 보면 xi=1aii(bi−∑j=1,j≠inaijxj)x_i = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1, j \\neq i}^n a_{ij}x_j \\right)xi​=aii​1​(bi​−∑j=1,j=in​aij​xj​) 이므로 이걸 반복시키면 수렴함\n수렴하려면 행렬이 대각지배성 ∣aii∣≥∑j=1,j≠in∣aij∣|a_{ii}| \\geq \\sum_{j=1, j \\neq i}^n |a_{ij}|∣aii​∣≥∑j=1,j=in​∣aij​∣을 가져야함\nScarborough 수렴조건이라고도 함\n자코비 방식 (쓰레기):\nxik+1=1aii(bi−∑j=1,j≠inaijxjk)x_i^{k+1} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1, j \\neq i}^n a_{ij}x_j^k \\right)xik+1​=aii​1​(bi​−∑j=1,j=in​aij​xjk​)\n가우스 자이델 방식 (조금 나음):\nxik+1=1aii(bi−∑j=1i−1aijxjk+1−∑j=i+1naijxjk)x_i^{k+1} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{k+1} - \\sum_{j=i+1}^{n} a_{ij}x_j^k \\right)xik+1​=aii​1​(bi​−∑j=1i−1​aij​xjk+1​−∑j=i+1n​aij​xjk​)\n이걸 좀 더 생각해서 x∗x^*x∗를 컴퓨터처럼 현재 기억장소에 저장된 값이라고 생각하면 (?)\nxik+1=1aii(bi−∑j=1i−1aijxjk+1−∑j=i+1naijxjk)=xik+1aii(bi−∑j=1i−1aijxjk+1−∑j=inaijxjk)=xik+1aii(bi−∑j=1naijxj∗)\\begin{align*}\nx_i^{k+1} &amp;= \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{k+1} - \\sum_{j=i+1}^{n} a_{ij}x_j^k \\right) \\\\\n&amp;= x_i^k + \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{k+1} - \\sum_{j=i}^{n} a_{ij}x_j^k \\right) \\\\\n&amp;= x_i^k + \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{n} a_{ij}x_j^* \\right)\n\\end{align*}xik+1​​=aii​1​(bi​−j=1∑i−1​aij​xjk+1​−j=i+1∑n​aij​xjk​)=xik​+aii​1​(bi​−j=1∑i−1​aij​xjk+1​−j=i∑n​aij​xjk​)=xik​+aii​1​(bi​−j=1∑n​aij​xj∗​)​\n즉 컴퓨터에서 더 쉽게 할 수 있다! 벡터 저장하는 변수 하나만 사용하면 계속 반복법 사용 가능\n이완법\n위에껄 대략 xik+1=xik+Δxkx_i^{k+1} = x_i^k + \\Delta x^kxik+1​=xik​+Δxk이라고 쓸 수 있음\n이완법은 수렴속도를 조절하기 위해서 이완계수를 도입하여 xik+1=xik+ωΔxkx_i^{k+1} = x_i^k + \\omega \\Delta x^kxik+1​=xik​+ωΔxk로 돌리는거임\n0&lt;ω&lt;10 &lt; \\omega &lt; 10&lt;ω&lt;1이면 하향이완,\nω&gt;1\\omega &gt; 1ω&gt;1이면 상향이완\n근데 ω≥2\\omega \\geq 2ω≥2면 발산하므로 상향이완은 보통 1.3~1.5\n물론 뭘 쓸지는 시행착오로 정해야 함\n수치적 난점\n행렬 수치해석 특) 개판임\n\nA의 원소가 조금만 변해도 해가 크게 변함\n대각지배성 아니면 ㅈ됨\ndetA⋅detA−1det A \\cdot det A^{-1}detA⋅detA−1이 1이 안 나옴\nAA−1AA^{-1}AA−1이 III가 안 나옴\n(A−1)−1(A^{-1})^{-1}(A−1)−1이 AAA가 안 나옴\nA−1(A−1)−1A^{-1}(A^{-1})^{-1}A−1(A−1)−1는 III가 정말정말 안 나옴\n\n다행히(?) 수학자들이 어떤 행렬이 불량조건을 갖는지 (즉 개판인지) 알아냄\n대표적인 불량조건 행렬: 힐버트 행렬 (aij=1i+j−1a_{ij} = \\frac{1}{i + j - 1}aij​=i+j−11​)\n역행렬이 엄청 커짐!\n불량조건의 정량화\n행렬 정규(matrix norm) 아니 norm을 대체 왜 정규로 번역함\n하여튼 행렬 norm을 정의하자\n∥A∥∞=max⁡1≤i≤n∑j=1n∣aij∣\\|A\\|_{\\infty} = \\max_{1 \\leq i \\leq n} \\sum_{j=1}^{n} |a_{ij}|\n∥A∥∞​=1≤i≤nmax​j=1∑n​∣aij​∣\n으로 정의하면 행렬의 조건수 ∥A∥∞∥A−1∥∞\\|A\\|_{\\infty}\\|A^{-1}\\|_{\\infty}∥A∥∞​∥A−1∥∞​가 클수록 행렬이 불량하다\n일반적으로 조건수가 10s10^s10s면 유효숫자도 sss개가 상실된다\n3대각행렬과 TDMA\n대각행렬은 대각에 하나\n3대각행렬은 대각 위 아래로 하나가 더 있음\n3장에서 반드시 알아야 할 것??\n[a1c10⋯0b2a2c2⋯00b3a3⋯0⋮⋮⋮⋱⋮000⋯an][x1x2x3⋮xn]=[d1d2d3⋮dn]\\begin{bmatrix}\na_1 &amp; c_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\\nb_2 &amp; a_2 &amp; c_2 &amp; \\cdots &amp; 0 \\\\\n0 &amp; b_3 &amp; a_3 &amp; \\cdots &amp; 0 \\\\\n\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n0 &amp; 0 &amp; 0&amp; \\cdots &amp; a_n\n\\end{bmatrix}\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} = \\begin{bmatrix}\nd_1 \\\\\nd_2 \\\\\nd_3 \\\\\n\\vdots \\\\\nd_n\n\\end{bmatrix}​a1​b2​0⋮0​c1​a2​b3​⋮0​0c2​a3​⋮0​⋯⋯⋯⋱⋯​000⋮an​​​​x1​x2​x3​⋮xn​​​=​d1​d2​d3​⋮dn​​​\naixi+bixi−1+cixi+1=dia_ix_i + b_ix_{i-1} + c_ix_{i+1} = d_iai​xi​+bi​xi−1​+ci​xi+1​=di​를 풀어야 한다!\nxi=Pixi+1+Qix_i = P_ix_{i+1} + Q_ixi​=Pi​xi+1​+Qi​같은 점화식이 있다고 가정하자.\n\nP1=−c1a1,Q1=d1a1P_1 = -\\frac{c_1}{a_1}, Q_1 = \\frac{d_1}{a_1}P1​=−a1​c1​​,Q1​=a1​d1​​ (∵a1x1+c1x2=d1\\because a_1x_1 + c_1x_2 = d_1∵a1​x1​+c1​x2​=d1​)\nPi=−ciai+biPi−1,Qi=di−biQi−1ai+biPi−1P_i = -\\frac{c_i}{a_i + b_iP_{i-1}}, Q_i = \\frac{d_i - b_iQ_{i-1}}{a_i + b_iP_{i-1}}Pi​=−ai​+bi​Pi−1​ci​​,Qi​=ai​+bi​Pi−1​di​−bi​Qi−1​​ (aixi+bixi−1+cixi+1=dia_ix_i + b_ix_{i-1} + c_ix_{i+1} = d_iai​xi​+bi​xi−1​+ci​xi+1​=di​에다가 xi−1=Pi−1xi+Qi−1x_{i-1} = P_{i-1}x_i + Q_{i-1}xi−1​=Pi−1​xi​+Qi−1​ 대입)\nxn=Qnx_n = Q_nxn​=Qn​ (∵cn=0\\because c_n = 0∵cn​=0)\nxi=Pixi+1+Qix_i = P_ix_{i+1} + Q_ixi​=Pi​xi+1​+Qi​로 나머지 x값들 다 구함\n\nLU분해\n이것도 반드시 알아야함\nA=LUA = LUA=LU인 L, U 찾기\n여담) 이게 되면 Ax=bAx=bAx=b는 LUx=bLUx=bLUx=b니까 개쉬워짐\n[a11a12⋯a1na21a22⋯a2n⋮⋮⋱⋮an1an2⋯ann]=[l110⋯0l21l22⋯0⋮⋮⋱⋮ln1ln2⋯lnn][u11u12⋯u1n0u22⋯u2n⋮⋮⋱⋮00⋯unn]\\begin{bmatrix}\na_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\\na_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\na_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn}\n\\end{bmatrix} = \\begin{bmatrix}\nl_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\\nl_{21} &amp; l_{22} &amp; \\cdots &amp; 0 \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\nl_{n1} &amp; l_{n2} &amp; \\cdots &amp; l_{nn}\n\\end{bmatrix}\\begin{bmatrix}\nu_{11} &amp; u_{12} &amp; \\cdots &amp; u_{1n} \\\\\n0 &amp; u_{22} &amp; \\cdots &amp; u_{2n} \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n0 &amp; 0 &amp; \\cdots &amp; u_{nn}\n\\end{bmatrix}​a11​a21​⋮an1​​a12​a22​⋮an2​​⋯⋯⋱⋯​a1n​a2n​⋮ann​​​=​l11​l21​⋮ln1​​0l22​⋮ln2​​⋯⋯⋱⋯​00⋮lnn​​​​u11​0⋮0​u12​u22​⋮0​⋯⋯⋱⋯​u1n​u2n​⋮unn​​​\n사실 우변의 미지수가 더 많음 (좌변은 n2n^2n2개, 우변은 n2+nn^2 + nn2+n개)\n그래서 nnn개의 항은 값을 정해줘야 함\n\n크라우트(Crout): uii=1u_{ii} = 1uii​=1\n두리틀(Dolittle): lii=1l_{ii} = 1lii​=1\n촐레스키(Choleski): lii=uiil_{ii} = u_{ii}lii​=uii​\n\n크라우트 방식으로 구하면 방정식을 전부 만든 다음,\nL의 첫 열, U의 첫 행, L의 두번째 열, U의 두번째 행... 이 순서대로 구하면 됨\n기본행연산을 활용한 LU분해\n두리틀 방식 lii=1l_{ii} = 1lii​=1과 결과가 같음\n\n행연산을 통해 A를 U로 만들고, 같은 행연산을 I에도 함\n이때 반드시 가우스 소거의 순서대로 해야함; 첫 번째 열을 맨 위 빼고 전부 0으로, 두 번째 열을 위 두개 빼고 전부 0으로, ...\nI에 행연산 적용시킨 행렬의 주대각을 제외한 나머지 원소의 부호를 뒤집으면 L이 됨\n\n사실 마지막 과정은 기본행연산의 역행렬을 구한 것임\n가우스 소거의 순서대로 하면 부호만 뒤집어도 역행렬이 됨\n크라우트가 보고싶으면 먼저 이렇게 두리틀을 구한 다음 대각성분을 빼서 UUU를 DUDUDU로 바꾸고 LDLDLD를 LLL으로 바꾸면 됨\n촐레스키 분해\n사실 A=UTUA = U^TUA=UTU 분해를 촐레스키 분해라고 부름\n만약 A가 양의 정부호라면 (Positive definite, i.e. ∀x≠0,xTAx&gt;0\\forall x \\neq 0, x^TAx &gt; 0∀x=0,xTAx&gt;0) A=UTUA = U^TUA=UTU로 유일하게 분해됨 번역 레전드네\n유일하게 분해되므로 그냥 LU분해 방식을 쓰고 대각만 맞추면 된다\n또는 크라우트 방법을 약간 조정해서 한번에 촐레스키를 구해도 됨\n양의 정부호를 잘 몰라서 시험에는 안 나오지만 양의 정부호가 뭔지 수학자들 앞에서 아는척 할 수 있음\nQR 분해\nQ는 직교행렬 (orthogonal matrix, i.e. Q−1=QTQ^{-1} = Q^TQ−1=QT)\nR은 사실 U인데 QU는 발음이 이상해서 QR라고 함\n사실 Q를 구하는건 정규직교기저를 구하는 것과 같음\n그람 슈미트를 사용해서 AAA의 열벡터들의 정규직교기저를 구한다!\nprojv(a)=a⋅vv⋅vv=a⋅vvv⋅vproj_v(a) = \\frac{a \\cdot v}{v \\cdot v} v = a \\cdot \\frac{vv}{v \\cdot v}projv​(a)=v⋅va⋅v​v=a⋅v⋅vvv​를 정의하자 vv가 대체 무슨 notation임 몰라요 그렇게 쓰래\nvvvvvv는 vvTvv^TvvT 행렬을 나타내는 notation이라고 한다...\n만약 단위벡터 u라면 proju(a)=(a⋅u)u=a⋅uuproj_u(a) = (a \\cdot u)u = a \\cdot uuproju​(a)=(a⋅u)u=a⋅uu\n\nu1=a1∣a1∣u_1 = \\frac{a_1}{|a_1|}u1​=∣a1​∣a1​​\nv2=a2−a2⋅u1u1=a2(I−u1u1)v_2 = a_2 - a_2 \\cdot u_1 u_1 = a_2 (I - u_1u_1)v2​=a2​−a2​⋅u1​u1​=a2​(I−u1​u1​)\nu2=v2∣v2∣u_2 = \\frac{v_2}{|v_2|}u2​=∣v2​∣v2​​\nv3=a3−a3⋅u1u1−a3⋅u2u2=a3(I−u1u1−u2u2)v_3 = a_3 - a_3 \\cdot u_1 u_1 - a_3 \\cdot u_2 u_2 = a_3 (I - u_1u_1 - u_2u_2)v3​=a3​−a3​⋅u1​u1​−a3​⋅u2​u2​=a3​(I−u1​u1​−u2​u2​)\n위 과정 계속 반복\n\nR은 QTAQ^TAQTA를 계산해서 구하면 됨\n곱하는 행렬을 PiP_iPi​라고 하면 P1=IP_1 = IP1​=I, Pi+1=Pi−uiuiP_{i+1} = P_i - u_iu_iPi+1​=Pi​−ui​ui​처럼 계산할 수 있다.\n만약 QR분해가 된다면 Ax=bAx = bAx=b는 Rx=QTbRx = Q^TbRx=QTb로 바꿔서 풀 수 있다!\nLU분해보다 더 쉽다\n","categories":["SNU","4-2","공학수학 3"]},{"title":"행렬 고유값문제","url":"/posts/109/","content":"기초 개념\n\n\n고유값 λ\\lambdaλ, 고유벡터 xxx: Ax=λxAx = \\lambda xAx=λx를 만족하는 값.\n\n\n스펙트럼: 역행렬이 있는 행렬 A는 n개의 고유값과 n개의 고유벡터를 가지는데, 이 때 모든 고유값의 집합 {λ1,λ2,⋯ ,λn}\\{ \\lambda_1, \\lambda_2, \\cdots, \\lambda_n \\}{λ1​,λ2​,⋯,λn​}을 스펙트럼이라고 함.\n\n\n최소고유값, 최대고유값: ∣λ1∣≤∣λ2∣≤⋯≤∣λn∣|\\lambda_1| \\leq |\\lambda_2| \\leq \\cdots \\leq |\\lambda_n|∣λ1​∣≤∣λ2​∣≤⋯≤∣λn​∣으로 정렬했을 때 λ1\\lambda_1λ1​을 최소고유값, λn\\lambda_nλn​을 최대고유값이라고 함.\n\n\n스펙트럼 반지름: 최대고유값을 스펙트럼 반지름이라고도 함.\n\n\n대수적다중도(algebraic multiplicity): k개의 고유값이 서로 같은 값을 가질 때 k를 대수적다중도라고 함.\n\n\n기하적다중도(geometric multiplicity): 고유값이 k개의 고유벡터를 가질 때 k를 기하적다중도라고 함.\n\n\n특성행렬식: det⁡(A−λI)=0\\det(A - \\lambda I) = 0det(A−λI)=0\n\n\n특성다항식: det⁡(A−λI)\\det(A - \\lambda I)det(A−λI)를 λ\\lambdaλ에 관한 n차다항식 D(λ)D(\\lambda)D(λ)로 본 경우\n\n\ndet⁡(A−λI)=(a11−λ)(a22−λ)⋯(ann−λ)+(aii−λ)꼴의 항이 최대 n-2개 있는 항들=(λ1−λ)(λ2−λ)⋯(λn−λ)=0\\begin{align*}\n\\det(A - \\lambda I) &amp;= (a_{11} - \\lambda)(a_{22} - \\lambda) \\cdots (a_{nn} - \\lambda) + \\\\\n&amp; (a_{ii} - \\lambda) \\text{꼴의 항이 최대 n-2개 있는 항들} \\\\\n&amp;= (\\lambda_1 - \\lambda)(\\lambda_2 - \\lambda) \\cdots (\\lambda_n - \\lambda) \\\\\n&amp;= 0\n\\end{align*}det(A−λI)​=(a11​−λ)(a22​−λ)⋯(ann​−λ)+(aii​−λ)꼴의 항이 최대 n-2개 있는 항들=(λ1​−λ)(λ2​−λ)⋯(λn​−λ)=0​\n따라서 λ1λ2⋯λn=det⁡(A−0⋅I)=det⁡A\\lambda_1 \\lambda_2 \\cdots \\lambda_n = \\det(A - 0 \\cdot I) = \\det Aλ1​λ2​⋯λn​=det(A−0⋅I)=detA,\nλ1+λ2+⋯+λn=λn−1의 계수=a11+a22+⋯+ann=trace⁡A\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n = \\lambda^{n-1} \\text{의 계수} = a_{11} + a_{22} + \\cdots + a_{nn} = \\operatorname{trace} Aλ1​+λ2​+⋯+λn​=λn−1의 계수=a11​+a22​+⋯+ann​=traceA\n대각화\nA∗=B−1ABA^* = B^{-1}ABA∗=B−1AB를 A의 상사행렬(similar matrix)이라고 하고, 이러한 변환을 상사변환(similarity transformation)이라고 함.\n이때 고유벡터를 열벡터로 갖는 행렬 X, 고유값을 주대각 원소로 갖는 대각행렬 D를 생각하자.\nAX=XDAX = XDAX=XD이므로, D=X−1AXD = X^{-1}AXD=X−1AX가 되고, 이를 행렬의 대각화라고 한다.\n고유값의 성질\n\nAkA^kAk의 고유값은 λik\\lambda_i^kλik​\nαA\\alpha AαA의 고유값은 αλi\\alpha \\lambda_iαλi​\nA−αIA - \\alpha IA−αI의 고유값은 λi−α\\lambda_i - \\alphaλi​−α\n상사행렬 A∗=B−1ABA^* = B^{-1}ABA∗=B−1AB의 고유값은 λi\\lambda_iλi​ (즉 고유값은 똑같음)\nA가 삼각행렬이면 고유값은 주대각원소 aiia_{ii}aii​\n이는 특성행렬식을 전개했을 때 (a11−λ)(a22−λ)⋯(ann−λ)(a_{11} - \\lambda)(a_{22} - \\lambda) \\cdots (a_{nn} - \\lambda)(a11​−λ)(a22​−λ)⋯(ann​−λ)가 돼서 자명하다.\n\n행렬식 탐색법\n케일리-해밀튼 정리: 특성다항식 D(λ)=0D(\\lambda) = 0D(λ)=0을 행렬다항식으로 봤을 때, 원래 행렬 A는 이 다항식의 근이 됨. 즉 D(A)=0D(A) = 0D(A)=0을 만족함.\n즉 아래 두 식이 모두 성립한다.\nD(λ)=−λn+c1λn−1+⋯+cn−1λ+1=0D(A)=−An+c1An−1+⋯+cn−1A+1=0\\begin{align*}\nD(\\lambda) &amp;= -\\lambda^n + c_1 \\lambda^{n-1} + \\cdots + c_{n-1} \\lambda + 1 = 0 \\\\\nD(A) &amp;= -A^n + c_1 A^{n-1} + \\cdots + c_{n-1} A + 1 = 0 \\\\\n\\end{align*}D(λ)D(A)​=−λn+c1​λn−1+⋯+cn−1​λ+1=0=−An+c1​An−1+⋯+cn−1​A+1=0​\nFadeev-Leverrier의 방법: 아래 점화식으로 특성방정식 계수를 모두 구할 수 있음\nB1=Ac1=trace⁡B1B2=A(B1−c1I)c2=12trace⁡B2⋮⋮Bn=A(Bn−1−cn−1I)cn=1ntrace⁡Bn\\begin{aligned}\nB_1 &amp;= A &amp; c_1 &amp;= \\operatorname{trace} B_1\\\\\nB_2 &amp;= A(B_1 - c_1 I) &amp; c_2 &amp;= \\frac{1}{2} \\operatorname{trace} B_2 \\\\\n&amp;\\vdots &amp; &amp;\\vdots \\\\\nB_n &amp;= A(B_{n-1} - c_{n-1} I) &amp; c_n &amp;= \\frac{1}{n} \\operatorname{trace} B_n\n\\end{aligned}\nB1​B2​Bn​​=A=A(B1​−c1​I)⋮=A(Bn−1​−cn−1​I)​c1​c2​cn​​=traceB1​=21​traceB2​⋮=n1​traceBn​​\n거쉬고린(Gershgorin) 정리: eigenvalue의 범위를 특정할 수 있음\n∣z−aii∣≤∑j=1,j≠in∣aij∣|z - a_{ii}| \\leq \\sum_{j=1, j \\neq i}^{n} |a_{ij}|\n∣z−aii​∣≤j=1,j=i∑n​∣aij​∣\n각 주대각원소마다 거쉬고린 원(Gershgorin's circle)이 n개 나오는데, 모든 eigenvalue는 이 n개의 원의 합집합에 속해있다.\n단점: 이렇게 열심히 해봤자 겨우 특성방정식만 구할 수 있음.. 특성방정식의 근은 따로 구해야 함\n거듭제곱법\n\n고유벡터의 초기 추측값 x(0)x^{(0)}x(0)을 정함. (보통 1만 있는 벡터로 정함)\ny(k+1)=Ax(k)y^{(k+1)} = Ax^{(k)}y(k+1)=Ax(k)를 구하고 y(k+1)y^{(k+1)}y(k+1)에서 제일 절대값이 큰 원소를 λ(k+1)\\lambda^{(k+1)}λ(k+1)이라고 한다.\nx(k+1)=1λ(k+1)y(k+1)x^{(k+1)} = \\frac{1}{\\lambda^{(k+1)}}y^{(k+1)}x(k+1)=λ(k+1)1​y(k+1)으로 다시 절대값을 최대 1로 만든 다음 위 과정을 반복한다.\nλ,x\\lambda, xλ,x 중 한 쪽이 허용오차 범위에 들어오면 종료한다.\n\n위 방법으로 최대고유값 λn\\lambda_nλn​과 고유벡터 xnx_nxn​을 쉽게 구할 수 있지만, λn−1\\lambda_{n-1}λn−1​과 비슷하거나 중근인 경우 (즉, 둘이 같은 경우) 수렴하지 않는다.\n원리) 임의의 벡터를 고유벡터들의 선형결합으로 쓸 수 있으므로, x(0)=c1​v1​+c2​v2​+⋯+cn​vnx^{(0)} = c_1​v_1​ + c_2​v_2​ + \\cdots + c_n​v_nx(0)=c1​​v1​​+c2​​v2​​+⋯+cn​​vn​으로 쓸 수 있다.\n∴Akx(0)=Ak(c1​v1​+c2​v2​+⋯+cn​vn)=c1λ1kv1+c2λ2kv2+⋯+cnλnkvn≈c1λ1kv1\\begin{align*}\n\\therefore A^kx^{(0)} &amp;= A^k (c_1​v_1​ + c_2​v_2​ + \\cdots + c_n​v_n) \\\\\n&amp;= c_1 \\lambda_1^k v_1 + c_2 \\lambda_2^k v_2 + \\cdots + c_n \\lambda_n^k v_n \\\\\n&amp;\\approx c_1 \\lambda_1^k v_1\n\\end{align*}∴Akx(0)​=Ak(c1​​v1​​+c2​​v2​​+⋯+cn​​vn​)=c1​λ1k​v1​+c2​λ2k​v2​+⋯+cn​λnk​vn​≈c1​λ1k​v1​​\n역거듭제곱법\nAx=λx  ⟺  A−1x=1λxAx = \\lambda x \\iff A^{-1}x = \\frac{1}{\\lambda}xAx=λx⟺A−1x=λ1​x\n따라서 A−1A^{-1}A−1에서 거듭제곱법을 쓰면 최소고유값을 (정확히는 최소고유값의 역수를) 구할 수 있다.\n하지만 역행렬을 구하는 것은 비효율적이므로 대신 Ay(k+1)=x(k)Ay^{(k+1)} = x^{(k)}Ay(k+1)=x(k)를 LU분해나 가우스 소거법을 이용하여 구한다.\n고유값 이동법\nAx=λx  ⟺  (A−sI)x=(λ−s)xAx = \\lambda x \\iff (A - sI)x = (\\lambda - s)xAx=λx⟺(A−sI)x=(λ−s)x\n따라서 A−sIA - sIA−sI의 고유값은 λ−s\\lambda -sλ−s이므로, A−sIA - sIA−sI로 고유값을 이동시켜 수렴속도를 가속시키거나 새로운 고유값을 구할 수 있다.\n예를 들어 거듭제곱법으로 최대고유값 λn\\lambda_nλn​을 구했다면, A−λnIA - \\lambda_n IA−λn​I에서 거듭제곱법을 하면 다른 고유값을 구할 수 있다.\n고유벡터의 결정\n고유값 λ\\lambdaλ를 구해도 고유벡터를 구하려면 (A−λI)x=0(A - \\lambda I) x = 0(A−λI)x=0을 풀어야 한다.\n하지만 이 방정식은 무한히 많은 해를 가지므로, 미지상수가 필요하다.\ne.g. 고유벡터가 x=t(0,1,1)+s(1,0,−1)x = t(0, 1, 1) + s(1, 0, -1)x=t(0,1,1)+s(1,0,−1)같은 식일 수도 있다.\nB=A−λIB = A - \\lambda IB=A−λI라고 하고, Bx=0Bx = 0Bx=0의 해를 구해보자.\n\n가우스 조단 소거 후, B의 k번째 열이 0벡터라면 eke_kek​는 고유벡터이다. 만약 B의 n번째 열이 0벡터라면, 여기서 그만둔다.\n만약 bnn=0b_{nn} = 0bnn​=0이라면 xn=0,−1x_n = 0, -1xn​=0,−1 두 가지 경우 모두 조사한다.\n\n\nxn=0x_n = 0xn​=0인 경우, 마지막 행/열을 제거한 (n-1) x (n-1) 행렬에 대해서 다시 반복한다.\nxn=−1x_n = -1xn​=−1인 경우, 마지막 열을 소스벡터로 두고 (n-1) x (n-1) 행렬에 대해서 연립방정식을 다시 푼다. (즉, Bn−1xn−1=bnB_{n-1} x_{n-1} = b_{n}Bn−1​xn−1​=bn​)\n\n\n만약 bnn=1b_{nn} = 1bnn​=1이라면 xn=0x_n = 0xn​=0으로 두고 조사한다.\n최종 고유벡터가 영벡터라면 고유값이 틀렸다고 판단한다.\n\n하우스홀더 변환\nA∗=PAP,P=I−2uuT∣u∣2A^* = PAP, P = I - 2\\frac{uu^T}{|u|^2}\nA∗=PAP,P=I−2∣u∣2uuT​\n여기서 P를 하우스홀더 행렬이라고 하고, P=PT=P−1P = P^T = P^{-1}P=PT=P−1을 만족한다.\n하우스홀더 변환을 n-2번 하면 헤센버그 행렬(주대각 바로 아래의 대각원소를 제외하고 대각선 아래의 원소가 전부 0인 행렬)이 된다. 특히 A가 대칭이면 (A = A^T), 3대각행렬이 된다.\nc.f. 상사행렬 꼴이므로 이 방법으로 기존 행렬과 고유값이 같은 헤센버그 행렬을 구할 수 있다.\n하우스홀더 변환이 성립하는 u를 찾기 위해 α=2∣u∣2\\alpha = \\frac{2}{|u|^2}α=∣u∣22​, β=uTAu\\beta = u^T A uβ=uTAu라고 하면,\nA∗=(I−αuuT)A(I−αuuT)=(I−αuuT)(A−αAuuT)=A−α(uuTA+AuuT)+α2uuTAuuT=A−α(uuTA+AuuT)+α2βuuTaij∗=aij−α∑s=1n(uiusasj+aisusuj)+α2βuiuj\\begin{align*}\nA^* &amp;= (I - \\alpha uu^T)A(I - \\alpha uu^T) \\\\\n    &amp;= (I - \\alpha uu^T)(A - \\alpha Auu^T) \\\\\n    &amp;= A - \\alpha(uu^T A + Auu^T) + \\alpha^2 uu^T Auu^T \\\\\n    &amp;= A - \\alpha(uu^T A + Auu^T) + \\alpha^2 \\beta uu^T \\\\\na_{ij}^* &amp;= a_{ij} - \\alpha \\sum_{s=1}^n (u_iu_sa_{sj} + a_{is}u_su_j) + \\alpha^2 \\beta u_iu_j\n\\end{align*}A∗aij∗​​=(I−αuuT)A(I−αuuT)=(I−αuuT)(A−αAuuT)=A−α(uuTA+AuuT)+α2uuTAuuT=A−α(uuTA+AuuT)+α2βuuT=aij​−αs=1∑n​(ui​us​asj​+ais​us​uj​)+α2βui​uj​​\nA가 4x4 행렬인 경우를 가정하면, uT=[0u2u3u4]u^T = \\begin{bmatrix}0 &amp; u_2 &amp; u_3 &amp; u_4\\end{bmatrix}uT=[0​u2​​u3​​u4​​]로 잡고, A∗A^*A∗의 첫 열 아래쪽을 0으로 만들어야 한다. (즉, a31∗=a41∗=0a_{31}^* = a_{41}^* = 0a31∗​=a41∗​=0)\nuuTuu^TuuT의 첫 행과 첫 열은 전부 0이므로, A∗A^*A∗의 첫 열과 관계가 없다.\n따라서 A−α(uuTA)A - \\alpha(uu^TA)A−α(uuTA)의 첫 열이 A∗A^*A∗의 첫 열이 된다.\na31∗=0=a31−αu3(u2a21+u3a31+u4a41)a41∗=0=a41−αu4(u2a21+u3a31+u4a41)∴a31u3=a41u4=α(u2a21+u3a31+u4a41)\\begin{align*}\na^*_{31} &amp;= 0 = a_{31} - \\alpha u_3 (u_2 a_{21} + u_3 a_{31} + u_4 a_{41}) \\\\\na^*_{41} &amp;= 0 = a_{41} - \\alpha u_4 (u_2 a_{21} + u_3 a_{31} + u_4 a_{41}) \\\\\n\\therefore \\frac{a_{31}}{u_3} &amp;= \\frac{a_{41}}{u_4} = \\alpha (u_2 a_{21} + u_3 a_{31} + u_4 a_{41})\n\\end{align*}a31∗​a41∗​∴u3​a31​​​=0=a31​−αu3​(u2​a21​+u3​a31​+u4​a41​)=0=a41​−αu4​(u2​a21​+u3​a31​+u4​a41​)=u4​a41​​=α(u2​a21​+u3​a31​+u4​a41​)​\n따라서 u3=a31,u4=a41u_3 = a_{31}, u_4 = a_{41}u3​=a31​,u4​=a41​로 놓으면 정의에 의해 1α=∣u∣22=u22+a312+a4122\\frac{1}{\\alpha} = \\frac{|u|^2}{2} = \\frac{u_2^2 + a_{31}^2 + a_{41}^2}{2}α1​=2∣u∣2​=2u22​+a312​+a412​​이므로 위 방정식은 (u2−a21)2=a212+a312+a412(u_2 - a_{21})^2 = a_{21}^2 + a_{31}^2 + a_{41}^2(u2​−a21​)2=a212​+a312​+a412​가 된다.\n∴u2=γ+a21,γ=sign⁡(a21)a212+a312+a412\\therefore u_2 = \\gamma + a_{21}, \\gamma = \\operatorname{sign}(a_{21})\\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2}\n∴u2​=γ+a21​,γ=sign(a21​)a212​+a312​+a412​​\n마무리오차를 줄이기 위해 절대값이 큰 쪽을 택한다.\n일반적으로 n x n 행렬 A의 k번째 반복 과정에서 하우스홀더 변환을 위한 u벡터는\nγ=sign⁡(ak+1,k)∑j=k+1naj,k2,ui={01≤i≤kak+1,k+γi=k+1ai,ki&gt;k+1\\gamma = \\operatorname{sign}(a_{k+1, k})\\sqrt{\\sum_{j=k+1}^n a_{j,k}^2},\nu_i = \\begin{cases}\n0 &amp; 1 \\leq i \\leq k \\\\\na_{k+1, k} + \\gamma &amp; i = k+1 \\\\\na_{i, k} &amp; i &gt; k + 1\n\\end{cases}γ=sign(ak+1,k​)j=k+1∑n​aj,k2​​,ui​=⎩⎨⎧​0ak+1,k​+γai,k​​1≤i≤ki=k+1i&gt;k+1​\n이며, 하우스홀더 행렬은 아래와 같다.\nα=2∣u∣2=1γ2+γak+1,k,P=I−αuuT\\alpha = \\frac{2}{|u|^2} = \\frac{1}{\\gamma^2 + \\gamma a_{k+1, k}}, P = I - \\alpha uu^T\nα=∣u∣22​=γ2+γak+1,k​1​,P=I−αuuT\nQR 반복법\n상블록 삼각행렬(Upper-block-triangular matrix)는 행렬 A를 여러 개의 블록으로 나누었을 때 (일반적으로 1x1이랑 2x2로만 나눔), 블록 단위로 삼각행렬인 행렬이다.\nA=[A11A12A13⋯A1k0A22A23⋯A2k00A33⋯A3k⋮⋮⋮⋱⋮000⋯Akk]A =\n\\begin{bmatrix}\nA_{11} &amp; A_{12} &amp; A_{13} &amp; \\cdots &amp; A_{1k} \\\\\n0 &amp; A_{22} &amp; A_{23} &amp; \\cdots &amp; A_{2k} \\\\\n0 &amp; 0 &amp; A_{33} &amp; \\cdots &amp; A_{3k} \\\\\n\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n0 &amp; 0 &amp; 0 &amp; \\cdots &amp; A_{kk}\n\\end{bmatrix}A=​A11​00⋮0​A12​A22​0⋮0​A13​A23​A33​⋮0​⋯⋯⋯⋱⋯​A1k​A2k​A3k​⋮Akk​​​\n상블록 삼각행렬의 경우 det를 간단하게 각 대각블록의 det의 곱으로 나타낼 수 있다.\ndet⁡(A)=det⁡(A11)det⁡(A22)⋯det⁡(Akk)\\det(A) = \\det(A_{11})\\det(A_{22}) \\cdots \\det(A_{kk})\ndet(A)=det(A11​)det(A22​)⋯det(Akk​)\n따라서 상블록 삼각행렬의 특성다항식도 1~2차 다항식들의 곱으로 나타낼 수 있으므로, 고유값을 곧바로 구할 수 있다.\n이미 하우스홀더 변환으로 기존 행렬 A와 고유값이 같은 헤센버그 행렬 A0A_0A0​을 구하는 방법을 배웠다.\n따라서 헤센버그 행렬을 상블록 삼각행렬로 변환시키는 방법만 구하면 된다.\nAk=QkRkA_k = Q_kR_kAk​=Qk​Rk​로 QR분해 한 뒤, Ak+1=Qk−1AkQkA_{k+1} = Q_k^{-1}A_kQ_kAk+1​=Qk−1​Ak​Qk​인 상사변환을 정의하면\nAk+1=Qk−1QkRkQk=RkQkA_{k+1} = Q_k^{-1}Q_kR_kQ_k = R_kQ_kAk+1​=Qk−1​Qk​Rk​Qk​=Rk​Qk​고, 상사변환이니 고유값이 같고, AkA_kAk​가 헤센버그 행렬이면 Ak+1A_{k+1}Ak+1​도 헤센버그 행렬이 된다.\n이때 이 과정, 즉 QR분해 후 행렬을 바꿔서 RQ로 곱하는 과정을 반복하면 Ak+1A_{k+1}Ak+1​이 점점 상블록 삼각행렬로 수렴하게 된다.\n고유값 이동\nAk−skI=QkRk,Ak+1=RkQk+skIA_k - s_kI = Q_kR_k, A_{k+1} = R_kQ_k + s_kIAk​−sk​I=Qk​Rk​,Ak+1​=Rk​Qk​+sk​I 처럼 고유값 이동을 했다가 취소하는 방법으로 수렴을 더 가속시킬 수 있다.\n고유값이 sks_ksk​에 가까울수록 수렴이 가속된다.\nRk=QkT(Ak−skI)R_k = Q_k^T(A_k - s_kI)Rk​=QkT​(Ak​−sk​I)를 대입하면 Ak+1=Qk−1AkQkA_{k+1} = Q_k^{-1}A_kQ_kAk+1​=Qk−1​Ak​Qk​, 즉 고유값이 보존된다는 것을 보일 수 있다.\n이중 QR 분해\n일반적으로 고유값은 복소수이므로, 실수를 원소로 가지는 행렬에 고유값 이동을 그대로 적용하기엔 어려움이 따른다.\n하지만 켤레복소수도 합과 곱이 실수라는 점을 사용하면 이러한 문제를 해결할 수 있다.\nAk−skI=QkRkAk+1−sk+1I=Qk+1Rk+1Ak+1=Qk−1AkQk∴Ak−sk+1I=Qk(Ak+1−sk+1I)Qk−1=QkQk+1Rk+1Qk−1∴(Ak−sk+1I)(Ak−skI)=QkQk+1Rk+1Rk\\begin{align*}\nA_k - s_kI &amp;= Q_kR_k \\\\\nA_{k+1} - s_{k+1}I &amp;= Q_{k+1} R_{k+1} \\\\\nA_{k+1} &amp;= Q_k^{-1}A_kQ_k \\\\\n\\therefore A_k - s_{k+1}I &amp;= Q_k(A_{k+1} - s_{k+1}I)Q_k^{-1} \\\\\n&amp;= Q_kQ_{k+1} R_{k+1}Q_k^{-1} \\\\\n\\therefore (A_k - s_{k+1}I)(A_k - s_kI) &amp;= Q_kQ_{k+1} R_{k+1}R_k\n\\end{align*}Ak​−sk​IAk+1​−sk+1​IAk+1​∴Ak​−sk+1​I∴(Ak​−sk+1​I)(Ak​−sk​I)​=Qk​Rk​=Qk+1​Rk+1​=Qk−1​Ak​Qk​=Qk​(Ak+1​−sk+1​I)Qk−1​=Qk​Qk+1​Rk+1​Qk−1​=Qk​Qk+1​Rk+1​Rk​​\n따라서 (Ak−sk+1I)(Ak−skI)=QR(A_k - s_{k+1}I)(A_k - s_kI) = QR(Ak​−sk+1​I)(Ak​−sk​I)=QR로 QR분해 한 뒤, Ak+2=QTAkQA_{k+2} = Q^TA_kQAk+2​=QTAk​Q로 QR반복법을 수행할 수 있다.\nsk,sk+1s_k, s_{k+1}sk​,sk+1​의 값으로는 고유값의 합은 trace, 곱은 det인걸 이용하여, AkA_kAk​의 오른쪽 아래 2x2 행렬을 BkB_kBk​라고 하면\n(Ak−sk+1I)(Ak−skI)=Ak2−trace⁡Bk⋅Ak+det⁡Bk⋅I(A_k - s_{k+1}I)(A_k - s_kI) = A_k^2 - \\operatorname{trace} B_k \\cdot A_k + \\det B_k \\cdot I(Ak​−sk+1​I)(Ak​−sk​I)=Ak2​−traceBk​⋅Ak​+detBk​⋅I로 정할 수 있다.\n프로베니우스 행렬\n프로베니우스 행렬(Frobenius matrix)로 알려진 다음의 행렬\nA=[−an−1−an−2⋯−a1−a010⋯0001⋯00⋮⋮⋱⋮⋮00⋯10]\\mathbf{A} = \\begin{bmatrix}\n-a_{n-1} &amp; -a_{n-2} &amp; \\cdots &amp; -a_1 &amp; -a_0 \\\\\n1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\\n0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0\n\\end{bmatrix}A=​−an−1​10⋮0​−an−2​01⋮0​⋯⋯⋯⋱⋯​−a1​00⋮1​−a0​00⋮0​​\n의 특성방정식은 λn+an−1λn−1+⋯+a1λ+a0=0\\lambda^n + a_{n-1} \\lambda^{n-1} + \\cdots + a_1\\lambda + a_0 = 0λn+an−1​λn−1+⋯+a1​λ+a0​=0임이 알려져있다.\n따라서 이 행렬의 고유값을 구하는 문제와 임의의 n차 다항식의 근을 구하는 문제는 동치다.\n프로베니우스 행렬은 이미 헤센버그 행렬이므로, QR 반복법을 이용하여 특성방정식의 근을 구할 수 있다.\n이 방법이 n차 다항식의 근을 구하는 방법 중에서 제일 우수한 것으로 알려져 있다!\n호텔링 수축법\n(시험범위 아님!!)\n점점 오래 걸리고 오차도 많이 쌓이지만 처음 몇 개 구할때는 괜찮음\n대칭행렬 A=ATA = A^TA=AT의 경우 서로 다른 고유벡터는 수직(orthogonal)하다.\n이를 이용해 대칭행렬의 고유값을 쉽게 구할 수 있다.\n왠진 모르겠지만 이번엔 ∣λ1∣≥∣λ2∣≥⋯≥∣λn∣|\\lambda_1| \\geq |\\lambda_2| \\geq \\cdots \\geq |\\lambda_n|∣λ1​∣≥∣λ2​∣≥⋯≥∣λn​∣으로 정렬해보자.\n거듭제곱법으로 λ1,x1\\lambda_1, x_1λ1​,x1​을 구할 수 있다. (고유벡터는 정규화되었다고 가정)\n이때 A2=A−λ1x1x1TA_2 = A - \\lambda_1x_1x_1^TA2​=A−λ1​x1​x1T​라고 하면,\nA2x1=Ax1−λ1x1x1Tx1=Ax1−λ1x1=0A2xj=Axj−λ1x1x1Txj=Axj=λjxj(j≠1)\\begin{align*}\nA_2x_1 &amp;= Ax_1 - \\lambda_1x_1x_1^Tx_1 = Ax_1 - \\lambda_1x_1 = 0 \\\\\nA_2x_j &amp;= Ax_j - \\lambda_1x_1x_1^Tx_j = Ax_j = \\lambda_j x_j (j \\neq 1)\n\\end{align*}A2​x1​A2​xj​​=Ax1​−λ1​x1​x1T​x1​=Ax1​−λ1​x1​=0=Axj​−λ1​x1​x1T​xj​=Axj​=λj​xj​(j=1)​\n따라서 A2A_2A2​의 고유값은 0,λ2,…,λn0, \\lambda_2, \\ldots, \\lambda_n0,λ2​,…,λn​이고, 고유벡터는 x1,x2,…,xnx_1, x_2, \\ldots, x_nx1​,x2​,…,xn​이다.\n그런데 λ1\\lambda_1λ1​은 최대고유값이였으므로, A2A_2A2​의 고유값은 ∣λ2∣≥⋯≥∣λn∣≥0|\\lambda_2| \\geq \\cdots \\geq |\\lambda_n| \\geq 0∣λ2​∣≥⋯≥∣λn​∣≥0이 된다.\n따라서 A2A_2A2​에 거듭제곱법을 쓰면 λ2,x2\\lambda_2, x_2λ2​,x2​를 구할 수 있고, 이를 반복해서 모든 고유값을 구할 수 있다.\n하지만 이전 단계에서 구한 고유값과 고유벡터를 계속 사용하기 때문에 오차가 누적된다는 단점이 있어 대형 계산에는 적합하지 않다.\n자코비 방법\n대칭행렬을 회전변환을 사용한 상사변환만을 사용하여 대각화시키는 방법이다.\nA=[a11a12a21a22],Q=[cos⁡θ−sin⁡θsin⁡θcos⁡θ],A∗=Q−1AQ=[a11∗a12∗a21∗a22∗]A = \\begin{bmatrix}\na_{11} &amp; a_{12} \\\\\na_{21} &amp; a_{22}\n\\end{bmatrix},\nQ = \\begin{bmatrix}\n\\cos\\theta &amp; -\\sin\\theta \\\\\n\\sin\\theta &amp; \\cos\\theta\n\\end{bmatrix},\nA^* = Q^{-1}AQ = \\begin{bmatrix}\na_{11}^* &amp; a_{12}^* \\\\\na_{21}^* &amp; a_{22}^*\n\\end{bmatrix}A=[a11​a21​​a12​a22​​],Q=[cosθsinθ​−sinθcosθ​],A∗=Q−1AQ=[a11∗​a21∗​​a12∗​a22∗​​]\n라고 할 때, a12∗=a21∗=a12cos⁡2θ+(a22−a11)sin⁡2θ2=0a_{12}^* = a_{21}^* = a_{12}\\cos 2\\theta + (a_{22} - a_{11}) \\frac{\\sin 2\\theta}{2} = 0a12∗​=a21∗​=a12​cos2θ+(a22​−a11​)2sin2θ​=0일 조건은\n{tan⁡2θ=2a12a11−a22a11≠a22θ=π4a11=a22\\begin{cases}\n\\tan 2\\theta = \\frac{2a_{12}}{a_{11} - a_{22}} &amp; a_{11} \\neq a_{22} \\\\\n\\theta = \\frac{\\pi}{4} &amp; a_{11} = a_{22}\n\\end{cases}{tan2θ=a11​−a22​2a12​​θ=4π​​a11​=a22​a11​=a22​​\n이다. 이때 상사행렬 A∗A^*A∗는 원래의 행렬 AAA와 동일한 고유값을 가지므로, λ1=a11∗,λ2=a22∗\\lambda_1 = a_{11}^*, \\lambda_2 = a_{22}^*λ1​=a11∗​,λ2​=a22∗​이다.\nn x n행렬로 확장하면 계속 2x2 행렬을 찾아서 대각화 시키는 것을 반복해 최종적으로 전체 행렬을 대각화시킨다.\n\nX0=I,A0=AX_0 = I, A_0 = AX0​=I,A0​=A로 둔다.\nr, s를 ∣ars∣=max⁡i≠j∣aij∣|a_{rs}| = \\max_{i \\neq j} |a_{ij}|∣ars​∣=maxi=j​∣aij​∣로 잡는다. ∣ars∣&lt;ϵ|a_{rs}| &lt; \\epsilon∣ars​∣&lt;ϵ이면 종료한다.\nQkQ_kQk​를 I에서 r, s번째 행, 열만 [cos⁡θ−sin⁡θsin⁡θcos⁡θ]\\begin{bmatrix}\n\\cos\\theta &amp; -\\sin\\theta \\\\\n\\sin\\theta &amp; \\cos\\theta\n\\end{bmatrix}[cosθsinθ​−sinθcosθ​]로 바꾼 행렬로 정의한다.\nθ\\thetaθ를 {tan⁡2θ=2arsarr−assarr≠assθ=π4arr=ass\\begin{cases}\n\\tan 2\\theta = \\frac{2a_{rs}}{a_{rr} - a_{ss}} &amp; a_{rr} \\neq a_{ss} \\\\\n\\theta = \\frac{\\pi}{4} &amp; a_{rr} = a_{ss}\n\\end{cases}{tan2θ=arr​−ass​2ars​​θ=4π​​arr​=ass​arr​=ass​​로 정한다.\nAk+1=QkTAkQk,Xk+1=XkQkA_{k+1} = Q_k^T A_k Q_k, X_{k+1} = X_k Q_kAk+1​=QkT​Ak​Qk​,Xk+1​=Xk​Qk​로 놓고 반복한다.\n실제로 행렬곱을 계산할 필요는 없고 아래 값들만 바꾸면 된다.\n\nars∗=asr∗=0arr∗=arrcos⁡2θ+asssin⁡2θ+2arscos⁡θsin⁡θass∗=arrsin⁡2θ+asscos⁡2θ−2arscos⁡θsin⁡θair∗=ari∗=aircos⁡θ+aissin⁡θ(i≠r,s)ais∗=asi∗=aiscos⁡θ−airsin⁡θ(i≠r,s)xir∗=xircos⁡θ+xissin⁡θxis∗=xiscos⁡θ−xirsin⁡θ\\begin{align*}\na_{rs}^* &amp;= a_{sr}^* = 0 \\\\\na_{rr}^* &amp;= a_{rr}\\cos^2\\theta + a_{ss}\\sin^2\\theta + 2a_{rs}\\cos\\theta\\sin\\theta \\\\\na_{ss}^* &amp;= a_{rr}\\sin^2\\theta + a_{ss}\\cos^2\\theta - 2a_{rs}\\cos\\theta\\sin\\theta \\\\\na_{ir}^* &amp;= a_{ri}^* = a_{ir}\\cos\\theta + a_{is}\\sin\\theta (i \\neq r, s) \\\\\na_{is}^* &amp;= a_{si}^* = a_{is}\\cos\\theta - a_{ir}\\sin\\theta (i \\neq r, s) \\\\\nx_{ir}^* &amp;= x_{ir}\\cos\\theta + x_{is}\\sin\\theta \\\\\nx_{is}^* &amp;= x_{is}\\cos\\theta - x_{ir}\\sin\\theta\n\\end{align*}ars∗​arr∗​ass∗​air∗​ais∗​xir∗​xis∗​​=asr∗​=0=arr​cos2θ+ass​sin2θ+2ars​cosθsinθ=arr​sin2θ+ass​cos2θ−2ars​cosθsinθ=ari∗​=air​cosθ+ais​sinθ(i=r,s)=asi∗​=ais​cosθ−air​sinθ(i=r,s)=xir​cosθ+xis​sinθ=xis​cosθ−xir​sinθ​\n끝나면 고유값과 고유벡터가 한번에 구해지지만, 행렬의 크기가 커지면 연산이 과도하게 필요하다.\n대칭행렬의 고유값, 고유벡터\n대칭행렬의 고유값은 모두 실수이고, 하우스홀더 변환시 3대각행렬이 된다.\nPAP=A∗=[a1b10⋯00b1a2b2⋯000b2a3⋯00⋮⋮⋮⋱⋮⋮000⋯an−1bn−1000⋯bn−1an]PAP = A^* =\n\\begin{bmatrix}\na_1 &amp; b_1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\\nb_1 &amp; a_2 &amp; b_2 &amp; \\cdots &amp; 0 &amp; 0 \\\\\n0 &amp; b_2 &amp; a_3 &amp; \\cdots &amp; 0 &amp; 0 \\\\\n\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\\n0 &amp; 0 &amp; 0 &amp; \\cdots &amp; a_{n-1} &amp; b_{n-1} \\\\\n0 &amp; 0 &amp; 0 &amp; \\cdots &amp; b_{n-1} &amp; a_n\n\\end{bmatrix}\nPAP=A∗=​a1​b1​0⋮00​b1​a2​b2​⋮00​0b2​a3​⋮00​⋯⋯⋯⋱⋯⋯​000⋮an−1​bn−1​​000⋮bn−1​an​​​\n위 특성방정식은 pk(λ)=(ak−λ)pk−1(λ)−bk−12pk−2(λ)p_k(\\lambda) = (a_k - \\lambda)p_{k-1}(\\lambda) - b_{k-1}^2 p_{k-2}(\\lambda)pk​(λ)=(ak​−λ)pk−1​(λ)−bk−12​pk−2​(λ)라는 수열로 정의되고, 최종적으로 고유값은 D(λ)=pn(λ)=0D(\\lambda) = p_n(\\lambda) = 0D(λ)=pn​(λ)=0으로 구한다. (pk(λ)p_k(\\lambda)pk​(λ)는 왼쪽 위 k x k 행렬의 특성방정식, p0(λ)=1p_0(\\lambda) = 1p0​(λ)=1)\n위 수열의 근을\npk−1(μ)=0,μ1&lt;⋯&lt;μi&lt;⋯&lt;μk−1pk(λ)=0,λ1&lt;⋯&lt;λi&lt;⋯&lt;λk\\begin{align*}\np_{k-1}(\\mu) &amp;= 0, \\mu_1 &lt; \\dots &lt; \\mu_i &lt; \\dots &lt; \\mu_{k-1} \\\\\np_k(\\lambda) &amp;= 0, \\lambda_1 &lt; \\dots &lt; \\lambda_i &lt; \\dots &lt; \\lambda_k\n\\end{align*}pk−1​(μ)pk​(λ)​=0,μ1​&lt;⋯&lt;μi​&lt;⋯&lt;μk−1​=0,λ1​&lt;⋯&lt;λi​&lt;⋯&lt;λk​​\n이라고 하고, μ0=−∞,μk=∞\\mu_0 = -\\infty, \\mu_k = \\inftyμ0​=−∞,μk​=∞라고 하면 항상 μi−1&lt;λi&lt;μi\\mu_{i-1} &lt; \\lambda_i &lt; \\mu_iμi−1​&lt;λi​&lt;μi​가 성립한다.\n따라서 pk(λ)=0p_k(\\lambda) = 0pk​(λ)=0의 근을 이분법을 연속적으로 적용하여 구할 수 있다.\n참고로 거쉬고린 정리를 사용해서 이분법의 범위를 줄일 수 있다.\n여담으로 위 방식대로 구하면 행렬식도 det⁡A=pn(0)\\det A = p_n(0)detA=pn​(0)으로 구할 수 있다.\n고유벡터를 구할 때는 먼저 A∗A^*A∗의 고유벡터를 구한 뒤 AAA의 고유벡터를 구한다.\nA∗−λIA^* - \\lambda IA∗−λI를 전개하면 고유벡터는 아래 식으로 구할 수 있다.\nx1=1,xi=(−1)i−1pi−1(λ)b2b3⋯bix_1 = 1, x_i = \\frac{(-1)^{i-1} p_{i-1}(\\lambda)}{b_2b_3 \\cdots b_i}\nx1​=1,xi​=b2​b3​⋯bi​(−1)i−1pi−1​(λ)​\n이 벡터 xxx는 A∗=PAPA^* = PAPA∗=PAP의 고유벡터이므로, AAA의 고유벡터는 PxPxPx로 구하면 된다.\n","categories":["SNU","4-2","공학수학 3"]},{"title":"보간과 곡선근사","url":"/posts/110/","content":"기초 개념\n\n보간: 모든 점을 다 지나가야 함\n곡선근사: 모든 점을 제일 효과적으로 근사해야 함\n\n좀 더 수학적으로 쓰면 주어진 자료 (xi,fi)(x_i, f_i)(xi​,fi​)가 있을 때,\n미리 선택한 기저함수 ϕi\\phi_iϕi​들의 선형결합으로(∑wiϕi\\sum w_i\\phi_i∑wi​ϕi​) 주어진 자료를 표현하는 것\n\n보간: ∑wiϕi\\sum w_i\\phi_i∑wi​ϕi​가 (xi,fi)(x_i, f_i)(xi​,fi​)를 다 지남\n곡선근사 ∑wiϕi\\sum w_i\\phi_i∑wi​ϕi​가 (xi,fi)(x_i, f_i)(xi​,fi​)랑 제일 가까움\n\n여기서 ϕi\\phi_iϕi​를 1,x,x2,…1, x, x^2, \\ldots1,x,x2,…를 선택하면 다항식 보간/근사가 됨\n참고) ax2+bx+cax^2 + bx + cax2+bx+c 같은 식으로 다항식을 쓰면 xnx^nxn 오차가 너무 커져서 안 됨!!\n수치해석에서는 (ax+b)x+c(ax + b)x + c(ax+b)x+c 같은 식으로 다항식을 씀\n라그랑주 보간\n실제로 쓸일 없음 왜냐? 너무 오래 걸림\n간단하게 두 점 (x0,f0),(x1,f1)(x_0, f_0), (x_1, f_1)(x0​,f0​),(x1​,f1​)으로 시작하면 아래와 같이 라그랑주 정규다항식을 정의할 수 있음.\nL0(x)=x−x1x0−x1,L1(x)=x−x0x1−x0L_0(x) = \\frac{x - x_1}{x_0 - x_1}, L_1(x) = \\frac{x - x_0}{x_1 - x_0}\nL0​(x)=x0​−x1​x−x1​​,L1​(x)=x1​−x0​x−x0​​\n위 다항식은 Li(xj)L_i(x_j)Li​(xj​)가 i=ji = ji=j면 1, i≠ji \\neq ji=j면 0이기 때문에 아래와 같이 간단하게 보간할 수 있음.\nf(x)=f0L0(x)+f1L1(x)f(x) = f_0L_0(x) + f_1L_1(x)\nf(x)=f0​L0​(x)+f1​L1​(x)\n일반적으로 점 n+1개 (x0,f0),…,(xn,fn)(x_0, f_0), \\ldots, (x_n, f_n)(x0​,f0​),…,(xn​,fn​)이 주어질 때 라그랑주 정규다항식은\nLi(x)=(x−x0)⋯(x−xi−1)(x−xi+1)⋯(x−xn)(xi−x0)⋯(xi−xi−1)(xi−xi+1)⋯(xi−xn)L_i(x) = \\frac{(x - x_0)\\cdots(x - x_{i-1})(x - x_{i+1})\\cdots(x - x_n)}{(x_i - x_0)\\cdots(x_i - x_{i-1})(x_i - x_{i+1})\\cdots(x_i - x_n)}\nLi​(x)=(xi​−x0​)⋯(xi​−xi−1​)(xi​−xi+1​)⋯(xi​−xn​)(x−x0​)⋯(x−xi−1​)(x−xi+1​)⋯(x−xn​)​\n으로 주어지고, 라그랑주 보간은\nPn(x)=f0L0(x)+⋯+fnLn(x)P_n(x) = f_0L_0(x) + \\cdots + f_nL_n(x)\nPn​(x)=f0​L0​(x)+⋯+fn​Ln​(x)\n로 주어짐.\n또한 라그랑주 보간의 오차도 알려져 있는데, 어떤 x0≤ξ≤xnx_0 \\leq \\xi \\leq x_nx0​≤ξ≤xn​에 대해 오차 e(x)=f(x)−Pn(x)e(x) = f(x) - P_n(x)e(x)=f(x)−Pn​(x)는\ne(x)=(x−x0)⋯(x−xn)(n+1)!f(n+1)(ξ)e(x) = \\frac{(x - x_0)\\cdots(x - x_n)}{(n+1)!}f^{(n+1)}(\\xi)\ne(x)=(n+1)!(x−x0​)⋯(x−xn​)​f(n+1)(ξ)\n이다. 따라서 f(x)f(x)f(x)가 n차 미만 다항식인 경우 (n+1)계도함수가 0이라 오차가 0이 된다. (즉, 모든 점을 정확하게 지난다.)\n뉴턴 전진/후진 보간\n전진보간법\nΔ1f1=f2−f1x2−x1\\Delta^1 f_1 = \\frac{f_2 - f_1}{x_2 - x_1}Δ1f1​=x2​−x1​f2​−f1​​을 전진분할차분, 또는 간단히 전진차분이라고 한다.\n마찬가지로 2계미분의 전진차분은 Δ2f1=Δ1f2−Δ1f1x3−x1\\Delta^2 f_1 = \\frac{\\Delta^1 f_2 - \\Delta^1 f_1}{x_3 - x_1}Δ2f1​=x3​−x1​Δ1f2​−Δ1f1​​으로 정의된다.\n이걸 주어진 n+1개의 점 (x0,f0),…,(xn,fn)(x_0, f_0), \\ldots, (x_n, f_n)(x0​,f0​),…,(xn​,fn​)으로부터 1계미분, 2계미분, ..., n계미분을 구한 것을 전진분할차분표, 또는 전진차분표라고 한다.\n이제 보건다항식을 Pn(x)=a0+a1(x−x0)+⋯+an(x−x0)⋯(x−xn−1)P_n(x) = a_0 + a_1(x - x_0) + \\cdots + a_n(x - x_0) \\cdots (x - x_{n-1})Pn​(x)=a0​+a1​(x−x0​)+⋯+an​(x−x0​)⋯(x−xn−1​)이라고 하자.\nx=x0x = x_0x=x0​을 대입하면 a0=f0=Δ0f0a_0 = f_0 = \\Delta^0 f_0a0​=f0​=Δ0f0​\nx=x1x = x_1x=x1​을 대입하면 f1=f0+a1(x−x0),∴a1=Δ1f0f_1 = f_0 + a_1(x - x_0), \\therefore a_1 = \\Delta^1 f_0f1​=f0​+a1​(x−x0​),∴a1​=Δ1f0​\n마찬가지로 x=xix = x_ix=xi​를 대입하면 ai=Δif0a_i = \\Delta^i f_0ai​=Δif0​이 된다.\n이를 뉴턴의 전진보간법이라고 하는데, 임의의 xi,…,xjx_i, \\ldots, x_jxi​,…,xj​ 구간에 대해서 보간하거나 새로운 점 (xn+1,fn+1)(x_{n+1}, f_{n+1})(xn+1​,fn+1​)을 추가할 때 기존 전진차분표를 재활용할 수 있어 유용하다.\n차항규칙\n앞에서 오차는\ne(x)=(x−x0)⋯(x−xn)(n+1)!f(n+1)(ξ)e(x) = \\frac{(x - x_0)\\cdots(x - x_n)}{(n+1)!}f^{(n+1)}(\\xi)\ne(x)=(n+1)!(x−x0​)⋯(x−xn​)​f(n+1)(ξ)\n라고 했었는데, 사실 여기서 근사적으로 Δn+1f0≈f(n+1)(ξ)(n+1)!\\Delta^{n+1} f_0 \\approx \\frac{f^{(n+1)}(\\xi)}{(n+1)!}Δn+1f0​≈(n+1)!f(n+1)(ξ)​으로 볼 수 있다.\n즉, 차분표를 한 번 더 계산하여\n∣e(x)∣≈∣(x−x0)⋯(x−xn)Δn+1f0∣|e(x)| \\approx |(x - x_0)\\cdots(x - x_n)\\Delta^{n+1} f_0|\n∣e(x)∣≈∣(x−x0​)⋯(x−xn​)Δn+1f0​∣\n으로 근사할 수 있고, 다음 항을 사용해서 차항규칙이라고 한다.\n후진보간법\n∇1f2=f2−f1x2−x1\\nabla^1 f_2 = \\frac{f_2 - f_1}{x_2 - x_1}\n∇1f2​=x2​−x1​f2​−f1​​\n하지만 ∇nfi=Δnfi−n\\nabla^n f_i = \\Delta^n f_{i-n}∇nfi​=Δnfi−n​$이므로 사실 차이가 없다.\nPn(x)=Δ0f0+Δ1f0(x−x0)+⋯+Δnf0(x−x0)⋯(x−xn−1)=∇0fn+∇1fn(x−xn)+⋯+∇nfn(x−xn)⋯(x−x1)∣e(x)∣≈∣(x−x0)⋯(x−xn)∇n+1fn∣\\begin{align*}\nP_n(x) &amp;= \\Delta^0 f_0 + \\Delta^1 f_0(x - x_0) + \\cdots + \\Delta^n f_0(x - x_0) \\cdots (x - x_{n-1}) \\\\\n&amp;= \\nabla^0 f_n + \\nabla^1 f_n(x - x_n) + \\cdots + \\nabla^n f_n(x - x_n) \\cdots (x - x_1) \\\\\n|e(x)| &amp;\\approx |(x - x_0)\\cdots(x - x_n)\\nabla^{n+1} f_n|\n\\end{align*}Pn​(x)∣e(x)∣​=Δ0f0​+Δ1f0​(x−x0​)+⋯+Δnf0​(x−x0​)⋯(x−xn−1​)=∇0fn​+∇1fn​(x−xn​)+⋯+∇nfn​(x−xn​)⋯(x−x1​)≈∣(x−x0​)⋯(x−xn​)∇n+1fn​∣​\n여담) 미분과 차분\n미적분에서 xnx^nxn은 차분에서 x(x+1)⋯(x+n−1)x(x+1) \\cdots (x+n-1)x(x+1)⋯(x+n−1)과 같음\n∫1n!xndx=1(n+1)!xn+1,∑1n!x(x+1)⋯(x+n−1)=1(n+1)!x(x+1)⋯(x+n)\\int \\frac{1}{n!}x^ndx = \\frac{1}{(n+1)!}x^{n+1}, \\sum \\frac{1}{n!}x(x+1) \\cdots (x+n-1) = \\frac{1}{(n+1)!}x(x+1) \\cdots (x+n)\n∫n!1​xndx=(n+1)!1​xn+1,∑n!1​x(x+1)⋯(x+n−1)=(n+1)!1​x(x+1)⋯(x+n)\n반대로 1nxn\\frac{1}{n}x^nn1​xn의 미분은 xn−1x^{n-1}xn−1, 1nx(x+1)⋯(x+n−1)\\frac{1}{n}x(x+1) \\cdots (x+n-1)n1​x(x+1)⋯(x+n−1)의 차분은 x(x+1)⋯(x+n−2)x(x+1) \\cdots (x+n-2)x(x+1)⋯(x+n−2)\n실제로 뉴턴다항식의 점 개수를 무한대로 보내면 테일러급수가 된다!\n등간격 뉴턴보간\nxi+1=xi+hx_{i+1} = x_i + hxi+1​=xi​+h인 경우 전진차분을 훨씬 간단한 형태로 나타낼 수 있음.\nΔ1f1=f2−f1h,Δ2f1=f1−2f2+f32!h2\\Delta^1 f_1 = \\frac{f_2 - f_1}{h}, \\Delta^2 f_1 = \\frac{f_1 - 2f_2 + f_3}{2! h^2}\nΔ1f1​=hf2​−f1​​,Δ2f1​=2!h2f1​−2f2​+f3​​\n일반적으로 fif_ifi​들의 계수는 파스칼의 삼각형과 같고, 부호는 교대로 바뀐다.\nΔ5fi=fi+5−5fi+4+10fi+3−10fi+2+5fi+1−fi5!h5\\Delta^5 f_i = \\frac{f_{i+5} - 5f_{i+4} + 10f_{i+3} - 10f_{i+2} + 5f_{i+1} - f_i}{5!h^5}\nΔ5fi​=5!h5fi+5​−5fi+4​+10fi+3​−10fi+2​+5fi+1​−fi​​\nΔkfi\\Delta^k f_iΔkfi​에서 fif_ifi​들로 연산한 부분을 △kfi\\triangle^k f_i△kfi​라고 하자. (즉, Δkfi=△kfik!hk)\\Delta^k f_i = \\frac{\\triangle^k f_i}{k! h^k})Δkfi​=k!hk△kfi​​)\n한편 x=x0+shx = x_0 + shx=x0​+sh라고 놓으면 s=x−x0hs = \\frac{x - x_0}{h}s=hx−x0​​이므로\n(x−x0)=hs(x−x0)(x−x1)=h2s(s−1)⋮(x−x0)(x−x1)⋯(x−xk−1)=hks(s−1)⋯(s−k+1)\\begin{align*}\n(x - x_0) &amp;= hs \\\\\n(x - x_0)(x - x_1) &amp;= h^2 s(s-1) \\\\\n&amp;\\vdots \\\\\n(x - x_0)(x - x_1) \\cdots (x-x_{k-1}) &amp;= h^k s(s-1) \\cdots (s - k + 1)\n\\end{align*}(x−x0​)(x−x0​)(x−x1​)(x−x0​)(x−x1​)⋯(x−xk−1​)​=hs=h2s(s−1)⋮=hks(s−1)⋯(s−k+1)​\n등의 관계가 성립된다.\n따라서 아래 보건다항식을 이렇게 단순화시킬 수 있다.\nPn(x)=Δ0f0+Δ1f0(x−x0)+⋯+Δnf0(x−x0)⋯(x−xn−1)=∑k=0n△kf0s(s−1)⋯(s−k+1)k!=∑k=0n△kf0(sk)\\begin{align*}\nP_n(x) &amp;= \\Delta^0 f_0 + \\Delta^1 f_0(x - x_0) + \\cdots + \\Delta^n f_0(x - x_0) \\cdots (x - x_{n-1}) \\\\\n&amp;= \\sum_{k=0}^n \\triangle^kf_0 \\frac{s(s-1) \\cdots (s-k+1)}{k!} \\\\\n&amp;= \\sum_{k=0}^n \\triangle^kf_0 \\binom{s}{k}\n\\end{align*}Pn​(x)​=Δ0f0​+Δ1f0​(x−x0​)+⋯+Δnf0​(x−x0​)⋯(x−xn−1​)=k=0∑n​△kf0​k!s(s−1)⋯(s−k+1)​=k=0∑n​△kf0​(ks​)​\n후진차분의 경우 ∇kfi=▽kfik!hk)\\nabla^k f_i = \\frac{\\triangledown^k f_i}{k! h^k})∇kfi​=k!hk▽kfi​​)로 정의하면\nPn(x)=∑k=0n▽kfn(s+k−1k)P_n(x) = \\sum_{k=0}^n \\triangledown^k f_n \\binom{s+k-1}{k}\nPn​(x)=k=0∑n​▽kfn​(ks+k−1​)\n최소제곱 회귀\nn개의 데이터 (x1,f1),⋯ ,(xn,fn)(x_1, f_1), \\cdots, (x_n, f_n)(x1​,f1​),⋯,(xn​,fn​)이 주어져있을 때, r개의 기저함수 u1(x),⋯ ,ur(x)u_1(x), \\cdots, u_r(x)u1​(x),⋯,ur​(x)의 선형결합 u(x)=∑aiuiu(x) = \\sum a_iu_iu(x)=∑ai​ui​로 주어진 데이터의 최적근사를 구한다.\n최소제곱회귀는 주어진 데이터와의 절대오차 ei=∣fi−u(xi)∣e_i = |f_i - u(x_i)|ei​=∣fi​−u(xi​)∣의 제곱의 합을 최소로 한다.\nS=∑i=1nei2S = \\sum_{i=1}^n e_i^2S=∑i=1n​ei2​이라고 하면 이를 최소로 하는 계수는 ∂S∂aj=0\\frac{\\partial S}{\\partial a_j} = 0∂aj​∂S​=0이여야 하므로\n∂S∂aj=−2∑i=1nuj(xi)(fi−a1u1(xi)−a2u2(xi)−⋯−arur(xi))=0\\frac{\\partial S}{\\partial a_j} = -2 \\sum_{i=1}^n u_j(x_i)(f_i - a_1u_1(x_i) - a_2u_2(x_i) - \\cdots - a_ru_r(x_i)) = 0\n∂aj​∂S​=−2i=1∑n​uj​(xi​)(fi​−a1​u1​(xi​)−a2​u2​(xi​)−⋯−ar​ur​(xi​))=0\n∴∑i=1nuj(xi)(a1u1(xi)+a2u2(xi)+⋯+arur(xi))=∑i=1nuj(xi)fi\\therefore \\sum_{i=1}^n u_j(x_i)(a_1u_1(x_i) + a_2u_2(x_i) + \\cdots + a_ru_r(x_i)) = \\sum_{i=1}^n u_j(x_i)f_i\n∴i=1∑n​uj​(xi​)(a1​u1​(xi​)+a2​u2​(xi​)+⋯+ar​ur​(xi​))=i=1∑n​uj​(xi​)fi​\n를 만족해야 한다.\n이때 상관계수를 아래와 같이 정의한다.\nfave=1n∑i=1nfi,S0=∑i=1n(fi−fave)2,r2=S0−SS0f_{ave} = \\frac{1}{n}\\sum_{i=1}^n f_i, S_0 = \\sum_{i=1}^n(f_i - f_{ave})^2, r^2 = \\frac{S_0 - S}{S_0}\nfave​=n1​i=1∑n​fi​,S0​=i=1∑n​(fi​−fave​)2,r2=S0​S0​−S​\n만약 오차가 없어 S=0S = 0S=0이라면 r=1r = 1r=1이 된다.\n행렬표현\n∑i=1nuj(xi)(a1u1(xi)+a2u2(xi)+⋯+arur(xi))=∑i=1nuj(xi)fi\\sum_{i=1}^n u_j(x_i)(a_1u_1(x_i) + a_2u_2(x_i) + \\cdots + a_ru_r(x_i)) = \\sum_{i=1}^n u_j(x_i)f_i\ni=1∑n​uj​(xi​)(a1​u1​(xi​)+a2​u2​(xi​)+⋯+ar​ur​(xi​))=i=1∑n​uj​(xi​)fi​\n를 행렬로 나타낼 수 있다.\nU=[u1(x1)u1(x2)⋯u1(xn)u2(x1)u2(x2)⋯u2(xn)⋮⋮⋱⋮ur(x1)ur(x2)⋯ur(xn)],a=[a1a2⋮ar],f=[f1f2⋮fn]U = \\begin{bmatrix}\nu_1(x_1) &amp; u_1(x_2) &amp; \\cdots &amp; u_1(x_n)\\\\\nu_2(x_1) &amp; u_2(x_2) &amp; \\cdots &amp; u_2(x_n)\\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\\nu_r(x_1) &amp; u_r(x_2) &amp; \\cdots &amp; u_r(x_n)\n\\end{bmatrix},\na = \\begin{bmatrix}\na_1 \\\\ a_2 \\\\ \\vdots \\\\ a_r\n\\end{bmatrix},\nf = \\begin{bmatrix}\nf_1 \\\\ f_2 \\\\ \\vdots \\\\ f_n\n\\end{bmatrix}\nU=​u1​(x1​)u2​(x1​)⋮ur​(x1​)​u1​(x2​)u2​(x2​)⋮ur​(x2​)​⋯⋯⋱⋯​u1​(xn​)u2​(xn​)⋮ur​(xn​)​​,a=​a1​a2​⋮ar​​​,f=​f1​f2​⋮fn​​​\n으로 정의하면 위 식은 간단하게 UUTa=UfUU^Ta = UfUUTa=Uf가 된다.\n다항식 회귀\nui(x)=xiu_i(x) = x^iui​(x)=xi로 놓으면 (i는 0부터 r까지 있다고 가정) 위의 행렬은 아래처럼 나타낼 수 있다.\nU=[11⋯1x1x2⋯xn⋮⋮⋱⋮x1rx2r⋯xnr]U = \\begin{bmatrix}\n1 &amp; 1 &amp; \\cdots &amp; 1\\\\\nx_1 &amp; x_2 &amp; \\cdots &amp; x_n\\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\\nx_1^r &amp; x_2^r &amp; \\cdots &amp; x_n^r\n\\end{bmatrix}U=​1x1​⋮x1r​​1x2​⋮x2r​​⋯⋯⋱⋯​1xn​⋮xnr​​​\n비선형 회귀\n비선형 회귀는 다항식 회귀로 식을 변형시키는 경우가 많다.\ny=aebx→ln⁡y=ln⁡a+bxy=axb→ln⁡y=ln⁡a+bln⁡xy=ax+b→y=ab−1b(xy)\\begin{align*}\ny = ae^{bx} &amp;\\rightarrow \\ln y = \\ln a + bx \\\\\ny = ax^b &amp;\\rightarrow \\ln y = \\ln a + b \\ln x \\\\\ny = \\frac{a}{x+b} &amp;\\rightarrow y = \\frac{a}{b} - \\frac{1}{b}(xy)\n\\end{align*}y=aebxy=axby=x+ba​​→lny=lna+bx→lny=lna+blnx→y=ba​−b1​(xy)​\n스플라인 보간\n매우 중요!! 제일 현실적인 보간!! 시험에 나옴!!\nn개의 데이터를 단순히 n차방정식으로 보간해버리면 차수가 너무 높아 오차가 심해지는 문제가 있다.\n이 경우 각 소구간 xi−1≤x≤xix_{i-1} \\leq x \\leq x_ixi−1​≤x≤xi​를 보간한 뒤, 보간한 각 함수가 잘 이어지는 spline이 되도록 만든다.\n각 점마다 fi,fi′′f_i, f_i&#x27;&#x27;fi​,fi′′​가 주어졌다고 하고, 양쪽 스플라인의 기울기가 (즉, 1차미분이) 연속인 조건을 추가하자. (사실 spline의 양 끝점에서 f,f′,f′′f, f&#x27;, f&#x27;&#x27;f,f′,f′′가 같아야 한다는 말임)\n3차함수로 n개의 spline을 만들 경우,\n\n미지수는 4n개다.\n각 3차함수의 양 끝점 fif_ifi​가 같아야 하므로 방정식 2n개가 나온다.\n이웃한 3차함수의 미분값 fi′f_i&#x27;fi′​가 같아야 하므로 방정식 (n-1)개가 나온다.\n이웃한 3차함수의 2계도함수 fi′′f_i&#x27;&#x27;fi′′​가 같아야 하므로 방정식 (n-1)개가 나온다.\n\n아니면 싹다 fi′′f_i&#x27;&#x27;fi′′​로 만들어버렸을 때 fi′′f_i&#x27;&#x27;fi′′​의 관계식은 (n-1)개인데 미지수는 f0′′f_0&#x27;&#x27;f0′′​부터 fn′′f_n&#x27;&#x27;fn′′​까지 (n+1)개 있다고 봐도 된다.\n따라서 2개의 조건을 추가로 정의해줘야 하고, f0′′f_0&#x27;&#x27;f0′′​과 fn′′f_n&#x27;&#x27;fn′′​을 보통 정의해준다.\n\n자연 3차 스플라인: f0′′=fn′′=0f_0&#x27;&#x27; = f_n&#x27;&#x27; = 0f0′′​=fn′′​=0\nf0′′=f1′′,fn−1′′=fn′′f_0&#x27;&#x27; = f_1&#x27;&#x27;, f_{n-1}&#x27;&#x27; = f_n&#x27;&#x27;f0′′​=f1′′​,fn−1′′​=fn′′​\n선형보외: f1′′−f0′′x1−x0=f2′′−f1′′x2−x1,fn′′−fn−1′′xn−xn−1=fn−1′′−fn−2′′xn−1−xn−2\\frac{f_1&#x27;&#x27; - f_0&#x27;&#x27;}{x_1 - x_0} = \\frac{f_2&#x27;&#x27; - f_1&#x27;&#x27;}{x_2 - x_1}, \\frac{f_n&#x27;&#x27; - f_{n-1}&#x27;&#x27;}{x_n - x_{n-1}} = \\frac{f_{n-1}&#x27;&#x27; - f_{n-2}&#x27;&#x27;}{x_{n-1} - x_{n-2}}x1​−x0​f1′′​−f0′′​​=x2​−x1​f2′′​−f1′′​​,xn​−xn−1​fn′′​−fn−1′′​​=xn−1​−xn−2​fn−1′′​−fn−2′′​​\n\n위의 모든 방법은 어떻게든 aifi′′+bifi−1′′+cifi+1′′=dia_if_i&#x27;&#x27; + b_if_{i-1}&#x27;&#x27; + c_if_{i+1}&#x27;&#x27; = d_iai​fi′′​+bi​fi−1′′​+ci​fi+1′′​=di​ 꼴의 방정식을 만든 다음 TDMA(3대각행렬 푸는거)를 사용하여 구한다.\n여담으로 각 구간의 spline은 fi,fi+1,fi′′,fi+1′′f_i, f_{i+1}, f_i&#x27;&#x27;, f_{i+1}&#x27;&#x27;fi​,fi+1​,fi′′​,fi+1′′​만 주어지면 변수 4개가 주어졌기 때문에 3차방정식을 확정지을 수 있다.\n이 내용이 싹다 계산되어있으니 나중에 궁금하면 교재를 찾아보자.....\n체비셰프 보간\n시험에 안 나옴\n지금까지 배운 보간은 (xi,fi)(x_i, f_i)(xi​,fi​)가 미리 주어진 경우로, 수동적(passive) 보간이라고 함.\n반면에 구간이 주어져있을때 xix_ixi​를 능동적으로 선택하는 경우를 능동적(active) 보간이라고 함.\n구간 −1≤x≤1-1 \\leq x \\leq 1−1≤x≤1에서 체비셰프 다항식은 두가지 방법으로 표현된다. (둘 다 같은 식이다)\nTn(x)=cos⁡(n−cos⁡−1x)T0(x)=1,T1(x)=x,Ti(x)=2xTi−1(x)−Ti−2(x)\\begin{gather}\nT_n(x) = \\cos(n  - \\cos^{-1} x) \\\\\nT_0(x) = 1, T_1(x) = x, T_i(x) = 2xT_{i-1}(x) - T_{i-2}(x)\n\\end{gather}Tn​(x)=cos(n−cos−1x)T0​(x)=1,T1​(x)=x,Ti​(x)=2xTi−1​(x)−Ti−2​(x)​​\n항상 ∣Tn(x)∣≤1|T_n(x)| \\leq 1∣Tn​(x)∣≤1이며, 체비셰프 다항식의 근은 ncos⁡−1x=(n−0.5−k)πn\\cos^{-1}x  = (n - 0.5 - k)\\pincos−1x=(n−0.5−k)π에서\nxk=cos⁡(n−12−k)πn,0≤k&lt;nx_k = \\cos \\left( n - \\frac{1}{2} - k \\right)\\frac{\\pi}{n}, 0 \\leq k &lt; n\nxk​=cos(n−21​−k)nπ​,0≤k&lt;n\n대체 이게 뭐임??\n우선 일원다항식을 정의해야 하는데, 그냥 xnx^nxn 계수가 1인 n차다항식을 일원다항식이라고 한다.\n이때 놀랍게도 −1≤x≤1-1 \\leq x \\leq 1−1≤x≤1에서 가장 작은 최대값을 가지는 n차 일원다항식은 Tn(x)2n−1\\frac{T_n(x)}{2^{n-1}}2n−1Tn​(x)​임이 알려져있다. (최대값은 12n−1\\frac{1}{2^{n-1}}2n−11​)\n우리의 오차식 e(x)=(x−x0)⋯(x−xn)(n+1)!f(n+1)(ξ)e(x) = \\frac{(x - x_0)\\cdots(x - x_n)}{(n+1)!}f^{(n+1)}(\\xi)e(x)=(n+1)!(x−x0​)⋯(x−xn​)​f(n+1)(ξ)를 되살펴보면 ∣(x−x0)⋯(x−xn)∣|(x - x_0)\\cdots(x - x_n)|∣(x−x0​)⋯(x−xn​)∣를 줄이면 오차를 줄일 수 있다는 사실을 알 수 있다.\n근데 이게 뭐다? (n+1)차 일원다항식이다!\n따라서 (x−x0)⋯(x−xn)(x - x_0) \\cdots (x - x_n)(x−x0​)⋯(x−xn​)이 체비셰프 다항식이면 오차를 줄일 수 있고, 반대로 x0,⋯ ,xnx_0, \\cdots, x_nx0​,⋯,xn​을 체비셰프 다항식 Tn+1(x)T_{n+1}(x)Tn+1​(x)의 근으로 선택하면 오차를 줄일 수 있다.\n만약 구간이 a≤x≤ba \\leq x \\leq ba≤x≤b라면, x=a+b−a2(z+1)x = a + \\frac{b-a}{2}(z+1)x=a+2b−a​(z+1)으로 간단하게 −1≤z≤1-1 \\leq z \\leq 1−1≤z≤1로 구간을 바꿀 수 있다.\nzk=cos⁡(n−12−k)πn,xk=a+b−a2(zk+1)z_k = \\cos \\left( n - \\frac{1}{2} - k \\right)\\frac{\\pi}{n}, x_k = a + \\frac{b-a}{2}(z_k + 1)\nzk​=cos(n−21​−k)nπ​,xk​=a+2b−a​(zk​+1)\n참고로 이 모든 과정은 점을 구하는데 사용한거고, 이제 구한 점으로 차분법이든 뭐든 아무 수동적 보간을 하면 된다...\n에르미트 보간\n역시 시험에 안 나옴\n왜 함수값만 보간하냐? 기울기도 보간하면 안 됨? 하는게 에르미트 보간이다.\n이제 데이터가 (xi,fi,fi′)(x_i, f_i, f_i&#x27;)(xi​,fi​,fi′​)로 주어진다.\n참고) 스플라인 보간은 fi′′f_i&#x27;&#x27;fi′′​를 사용, 얘는 fi′f_i&#x27;fi′​를 사용\n우선 너무 어지러우므로 등간격을 가정하고 구간 xi−1≤x≤xix_{i-1} \\leq x \\leq x_ixi−1​≤x≤xi​를 s=x−xi−1xi−xi−1=x−xi−1hs = \\frac{x - x_{i-1}}{x_i - x_{i-1}} = \\frac{x - x_{i-1}}{h}s=xi​−xi−1​x−xi−1​​=hx−xi−1​​을 사용해 구간을 0≤s≤10 \\leq s \\leq 10≤s≤1으로 바꾸자.\n이때 f(s)=fi−1+(fi−fi−1)s+As(s−1)+Bs2(s−1)f(s) = f_{i-1} + (f_i - f_{i-1})s + As(s-1) + Bs^2(s-1)f(s)=fi−1​+(fi​−fi−1​)s+As(s−1)+Bs2(s−1)은 f(0)=fi−1,f(1)=fif(0) = f_{i-1}, f(1) = f_if(0)=fi−1​,f(1)=fi​를 만족한다.\n미분하고 방정식을 열심히 풀고 대입하면 3차 방정식이 나온다....\n","categories":["SNU","4-2","공학수학 3"]},{"title":"수치미분과 수치적분","url":"/posts/111/","content":"기초 개념\n수치미분, 수치적분은 기본적으로 (xi,fi)(x_i, f_i)(xi​,fi​)가 주어졌을 때 ∑wifi\\sum w_if_i∑wi​fi​ 선형결합을 구하는 작업임\n수치미분 (테일러 전개)\n(xi,fi)(x_i, f_i)(xi​,fi​)에서의 미분값을 구하려면 테일러 전개를 쓰면 됨\n전진차분\nxi,xi+1x_i, x_{i+1}xi​,xi+1​를 사용해서 미분값을 구한다.\nfi+1=fi+hfi′+h22!fi′′+⋯+hnn!f(n)(ξ),xi≤ξ≤xi+1fi′=fi+1−fih−h2!fi′′−⋯−hn−1n!f(n)(ξ)∴fi′=fi+1−fih+O(h)\\begin{align*}\nf_{i+1} &amp;= f_i + hf_i&#x27; + \\frac{h^2}{2!}f_i&#x27;&#x27; + \\cdots + \\frac{h^n}{n!}f^{(n)}(\\xi), x_i \\le \\xi \\le x_{i+1} \\\\\nf_i&#x27; &amp;= \\frac{f_{i+1} - f_i}{h} - \\frac{h}{2!}f_i&#x27;&#x27; - \\cdots - \\frac{h^{n-1}}{n!}f^{(n)}(\\xi) \\\\\n\\therefore f_i&#x27; &amp;= \\frac{f_{i+1} - f_i}{h} + O(h)\n\\end{align*}fi+1​fi′​∴fi′​​=fi​+hfi′​+2!h2​fi′′​+⋯+n!hn​f(n)(ξ),xi​≤ξ≤xi+1​=hfi+1​−fi​​−2!h​fi′′​−⋯−n!hn−1​f(n)(ξ)=hfi+1​−fi​​+O(h)​\nh는 점 사이의 간격이고 (xi+1−xix_{i+1} - x_ixi+1​−xi​), O(h)O(h)O(h)는 오차가 h에 비례한다는 뜻이다.\n후진차분\nxi−1,xix_{i-1}, x_ixi−1​,xi​를 사용해서 미분값을 구한다.\nfi′=fi−fi−1h+O(h)f_i&#x27; = \\frac{f_i - f_{i-1}}{h} + O(h)\nfi′​=hfi​−fi−1​​+O(h)\n중앙차분\nxi−1,xi+1x_{i-1}, x_{i+1}xi−1​,xi+1​를 사용해서 미분값을 구한다.\nfi′=fi+1−fi−12h+O(h2)f_i&#x27; = \\frac{f_{i+1} - f_{i-1}}{2h} + O(h^2)\nfi′​=2hfi+1​−fi−1​​+O(h2)\n오차가 O(h2)O(h^2)O(h2)이라 정확도가 향상되었지만, 항상 중앙차분이 더 좋은건 아니다.\ne.g. 함수값이 진동하는 경우 (홀수번째는 fi=Af_i = Afi​=A, 짝수번째는 fi=Bf_i = Bfi​=B)\n일반적인 차분근사\n점의 간격이 h로 일정한 경우 사용할 수 있다.\nn계도함수를 구하기 위해서는 각 점에서 테일러 전개한 뒤, 식을 연립하여 1 ~ (n-1)계도함수를 없앤다.\n(만약 점이 더 많다면, (n+1)계도함수 이후도 없앤다.)\n본질적으로 연립방정식을 푸는 것과 다를게 없다!\ne.g. f0′′=16h2(5f−1−8f0+4f2−f3)f_0&#x27;&#x27; = \\frac{1}{6h^2}(5f_{-1} - 8f_0 + 4f_2 - f_3)f0′′​=6h21​(5f−1​−8f0​+4f2​−f3​)\n수치미분 (보간다항식)\n점들을 지나는 다항식을 구한 다음, 그 다항식의 미분값으로 근사한다.\n근데 뉴턴다항식의 극한이 테일러 급수기 떄문에, 사실 위에서 테일러 전개로 구한 것과 같은 결과가 나온다!\n뉴턴-코테스 수치적분\n핵심) f(x)f(x)f(x)를 근사하는 n차다항식을 적분함\n사다리꼴 법칙\n각 구간을 사다리꼴로 생각 (사실 이건 1차다항식으로 근사한거임)\n∫xixi+1f(x)dx≈h2(fi+fi+1)\\int_{x_i}^{x_{i+1}}f(x)dx \\approx \\frac{h}{2}(f_i + f_{i+1})\n∫xi​xi+1​​f(x)dx≈2h​(fi​+fi+1​)\n오차는 대략 −112h2(f′(b)−f′(a))-\\frac{1}{12}h^2(f&#x27;(b) - f&#x27;(a))−121​h2(f′(b)−f′(a))라고 함.\nO(h2)O(h^2)O(h2) 오차가 저거라는거지 f′(b)=f′(a)f&#x27;(b) = f&#x27;(a)f′(b)=f′(a)라고 오차가 0인게 아님!! O(h3)O(h^3)O(h3) 오차는 남아있음...\n심프슨 1/3 법칙\n1차다항식도 썼어 -&gt; 그럼 2차다항식도 쓸 수 있는거 아님?\n전체 구간을 n개 대신 2n개로 나누고, 2개씩 묶어서 하나의 구간으로 생각함. (즉, f2i,f2i+1,f2i+2f_{2i}, f_{2i+1}, f_{2i+2}f2i​,f2i+1​,f2i+2​ 사용)\n그리고 각 구간을 지나는 뉴턴 다항식을 적분하면 됨\n∫x2ix2i+2f(x)dx≈h3(f2i+4f2i+1+f2i+2)\\int_{x_{2i}}^{x_{2i+2}}f(x)dx \\approx \\frac{h}{3}(f_{2i} + 4f_{2i+1} + f_{2i+2})\n∫x2i​x2i+2​​f(x)dx≈3h​(f2i​+4f2i+1​+f2i+2​)\n계수외우기 꿀팁) 함수값이 1인 상수함수면 적분값 2h여야함 -&gt; 안쪽 계수는 6\n근데 왠지 모르겠는데 양끝이 1임 -&gt; 중간은 4\n오차는 대략 −1180h4(f′′′(b)−f′′′(a))-\\frac{1}{180}h^4(f&#x27;&#x27;&#x27;(b) - f&#x27;&#x27;&#x27;(a))−1801​h4(f′′′(b)−f′′′(a))라고 함.\n심프슨 3/8법칙\n3차 다항식도 하겠다! 마찬가지로 구간을 3n개로 나눔\n∫x3ix3i+3f(x)dx≈38h(f3i+3f3i+1+3f3i+2+f3i+3)\\int_{x_3i}^{x_{3i+3}}f(x)dx \\approx \\frac{3}{8}h(f_{3i} + 3f_{3i+1} + 3f_{3i+2} + f_{3i+3})\n∫x3​ix3i+3​​f(x)dx≈83​h(f3i​+3f3i+1​+3f3i+2​+f3i+3​)\n오차는 대략 −180h4(f′′′(b)−f′′′(a))-\\frac{1}{80}h^4(f&#x27;&#x27;&#x27;(b) - f&#x27;&#x27;&#x27;(a))−801​h4(f′′′(b)−f′′′(a))라고 함.\n뉴턴-코테스 수치적분\n심프슨은 귀찮아서 3차까지만 하고 던짐\n코테스가 이거보고 이름 남기고 싶어서 4~9차까지 전부 손으로 계산함;;;;;;;;;;\n결국 뉴턴-코테스 수치적분이라고 이름이 남게 됨\n근데 뉴턴은 아무것도 안 했는데 뉴턴다항식 적분했다고 이름붙음;;\n참고로 수치해석 관점에서 1도 쓸모없는 공식이라고 한다..\n4차부턴 오차도 심함 + 아래 방법이 더 좋음!\n수치해석은 3차까지만 돌리자\n리차드슨의 보외법\n서로 다른 두 개의 수치적분결과를 이용하여 개선된 근사값을 얻을 수 있음!\n수치적분에 의한 오차가 O(hn)O(h^n)O(hn)일 떄, 구간의 크기가 h1h_1h1​일 때, h2h_2h2​일 때 수치적분을 하면\nIex=I(h1)+Ch1nIex=I(h2)+Ch2n\\begin{align*}\nI_{ex} &amp;= I(h_1) + Ch_1^n \\\\\nI_{ex} &amp;= I(h_2) + Ch_2^n\n\\end{align*}Iex​Iex​​=I(h1​)+Ch1n​=I(h2​)+Ch2n​​\n근데 여기서 C가 같다고 가정하면 간단한 연립방정식으로 C를 소거할 수 있음 (보통 h1&gt;h2h_1 &gt; h_2h1​&gt;h2​으로 둠)\nIex=I(h2)+I(h2)−I(h1)(h1h2)n−1I_{ex} = I(h_2) + \\frac{I(h_2) - I(h_1)}{\\left( \\frac{h_1}{h_2} \\right)^n - 1}\nIex​=I(h2​)+(h2​h1​​)n−1I(h2​)−I(h1​)​\n이게 생각보다 매우 정확함!\n단순히 구간 개수를 무지성으로 늘리는 것보다 두 번 구하고 보외법을 쓰는게 나음\nc.f. 사다리꼴 법칙 쓰고 리차드슨의 보외법을 n번 적용시키는 방법을 롬버그 적분이라고 함\n가우스 구적법\n지금까지 본 수치적분은 수동적(passive)임 xi,fix_i, f_ixi​,fi​가 다 주어져 있었음\n하지만 가우스 구적법은 데이터 점을 능동적(active)하게 정해야 함\n대신 미지수가 2배라 더 정확함!\n예를 들어 기존 적분방식으로는 점 2개면 1차식만 가능 -&gt; 가우스는 미지수 4개라 3차식까지 가능!\n가우스-르장드르 구적법\n가우스 구적법이 좋은건 알겠는데 그래서 점을 어떻게 구함?\nI=∫−11f(x)dx=w1f(x1)+w2f(x2)I = \\int_{-1}^1 f(x)dx = w_1f(x_1) + w_2f(x_2)\nI=∫−11​f(x)dx=w1​f(x1​)+w2​f(x2​)\n점이 2개인 경우를 예시로 들면 원하는 서로 다른 기저함수 4개에 대해서 위 식이 성립하는 wi,xiw_i, x_iwi​,xi​를 정하면 됨\n이때 기저함수로 1,x,x2,x3,…1, x, x^2, x^3, \\ldots1,x,x2,x3,…를 쓰는게 가우스-르장드르 구적법임\ne.g. 위 식에 1,x,x2,x31, x, x^2, x^31,x,x2,x3를 집어넣고 풀면 w1=1,x1=−13,w2=1,x2=13w_1 = 1, x_1 = -\\frac{1}{\\sqrt{3}}, w_2 = 1, x_2 = \\frac{1}{\\sqrt{3}}w1​=1,x1​=−3​1​,w2​=1,x2​=3​1​이 된다.\n근데 사실 x1,x2x_1, x_2x1​,x2​는 르장드르 함수 P2(x)=12(3x2−1)P_2(x) = \\frac{1}{2}(3x^2 - 1)P2​(x)=21​(3x2−1)의 두 근임!!\n일반적으로 xix_ixi​는 르장드르 함수의 근이고, wi=2(1−xi2)(nPn−1(xi))2w_i = \\frac{2(1 - x_i^2)}{(nP_{n-1}(x_i))^2}wi​=(nPn−1​(xi​))22(1−xi2​)​라고 함.\n근데 -1 ~ 1에서 적분하는게 아니면 어떡함?\nx=a+b−a2(z+1)x = a + \\frac{b-a}{2}(z+1)x=a+2b−a​(z+1)으로 두면\n∫abf(x)dx=∫−11f(x)dxdzdz=b−a2∑i=1nwif(xi)\\int_a^b f(x)dx = \\int_{-1}^1 f(x) \\frac{dx}{dz} dz = \\frac{b-a}{2} \\sum_{i=1}^n w_i f(x_i)∫ab​f(x)dx=∫−11​f(x)dzdx​dz=2b−a​∑i=1n​wi​f(xi​)\n가우스 구적법의 장점\n\n결정점이 적분구간 내부에 있어서 경계에서 특이점을 가져도 적분을 할 수 있음\n뺼셈이 없기 때문에 유효숫자의 상실 문제가 발생하지 않음\n미지수가 2배인 효과라 결정점을 몇 개 안 잡아도 효과가 좋음\n\n","categories":["SNU","4-2","공학수학 3"]},{"title":"Basics","url":"/posts/107/","content":"Curry–Howard correspondence\nFunctional Programming Language는 Mathematics랑 일대일 대응된다!\nP: T라면 T(Type)은 proposal(명제), P(Program)은 proof(증명)이라고 생각할 수 있다.\n만약 type checking이 된다면, proposal에 맞는 proof가 있다는 뜻이므로, 명제가 참이다!\n이걸 왜 하냐?\n엄밀하게 수학, 또는 프로그램의 정확성을 증명할 수 있다!\n특히 LLM 시대에 LLM이 내놓은 출력을 사실 믿을 수가 없는 상황임 -&gt; 증명보조기로 검증할 수 있다면?\n실제로 수학자들도 증명을 (Coq이 아니라 Lean을 쓰지만) 컴퓨터로 검증하고 있다!\n더 들어가자면 T(Type)은 어떤 집합이라고 볼 수 있는데, 여기에 속한 element P(Program)을 찾을 수 있다면 T는 참이라는 뜻!\n반대로 T에 속한 element가 없다면 T는 거짓이다!\nRocq Calculus\nRocq에는 Type이라는 것만 존재함 (그냥 키워드임), 나머지는 전부 사용자가 정의하는 것!\n사실 Proposal용 Prop이랑 Value용 Set이 따로 존재하긴 하는데 일단은 둘 다 그냥 Type이라고 생각하고 무시\n실제로 Type이랑 똑같은데 그냥 편의상 구분해놓은 것\n태초에는 Type이라는 것만 존재 (그냥 키워드임)\n나머지는 전부 사용자가 정의하는거\nType은 모든 set의 set,\nMyType: Type은 특정 set,\nMyElement: MyType는 그 set의 element\n모든 element는 딱 하나의 Type에만 소속되어있음!!!\nRocq에 subtype는 존재하지 않는다!\n(예외로 Prop, Set은 Type의 subtype이지만 그냥 Type이라고 생각해도 됨)\n즉, 1이 int이면서 short이면서 bool일수는 없다는거임\nInductive\nInductive day : Type :=  | monday  | tuesday  | wednesday  | thursday  | friday  | saturday  | sunday.Check (day: Type). (* 그냥 Check day.하면 day: Set으로 뜨는데 무시!!! *)Check (monday: day).\nInductive로 set과 그 set의 element를 정의할 수 있음.\nCheck를 통해 Type 확인 가능!\n여기서 monday는 set의 element를 정의한 것이기 때문에 Check (monday: Type).는 실패하게 됨\nDefinition\nDefinition next_working_day (d: day) : day :=  match d with  | monday =&gt; tuesday  | tuesday =&gt; wednesday  | wednesday =&gt; thursday  | thursday =&gt; friday  | friday =&gt; monday  | saturday =&gt; monday  | sunday =&gt; monday  end.Compute (next_working_day friday). (* monday: day *)\nDefinition으로 함수를 정의할 수 있음.\n프로그램을 계산했을 때 같다 = 수학에서 동치관계를 의미함.\ne.g. next_working_day friday는 monday랑 동치!\n하지만 프로그램이다 보니까 &quot;계산&quot;은 아무 동치관계로나 움직일 수 있는게 아니라 제일 단순해지는 방향으로 (즉, reduced form으로) 계산됨.\n즉 monday가 갑자기 next_working_day friday로 계산될 수는 없음!\n만약 T1: Type면서 T2: Type이라면 T1-&gt;T2: Type임\n즉 T1-&gt;T2도 set임! T1 set에서 T2 set으로 가는 함수들의 집합을 나타냄.\ne.g. next_working_day: day -&gt; day\nInductive처럼 T1-&gt;T2 set의 원소를 바로 만들어내고 싶다면 lambda function을 사용할 수 있음.\nCheck ((fun d: day =&gt; monday): day -&gt; day).\n사실 Definition은 엄밀히 따지면 함수를 정의하는 syntax가 아니라 lambda function에 이름 붙이는 syntax sugar임...\nDefinition foo (b: bool): day :=  match b with  | true =&gt; monday  | false =&gt; tuesday  end.\n사실 이건\nDefinition foo: bool -&gt; day := (  fun b: bool =&gt;    match b with    | true =&gt; monday    | false =&gt; tuesday    end).\n이거랑 똑같음!\n여담) syntax의 집합은 countable이지만, set에서 set으로 가는 함수의 집합은 uncountable임.\n즉, 수학적으로 존재하지만 우리가 Rocq로 표현할 수 없는 함수가 존재함! (그것도 uncountable 만큼이나 존재함!!)\n하지만 생각해보면 굳이 Rocq가 아니라 어떤 언어여도 (심지어 자연어여도) 성립하는 말이다..?\nmatch\n왠지 모르겠는데 수학에는 dual이 있어야함 (사실 수학자는 몰랐는데 컴공한테서 자연스럽게 배움)\nSet의 element를 만들 수도 있다면 element를 없앨 수도 있어야 함?\n프로그램을 &quot;계산&quot;하면 element를 없애버림.\n그 중 Inductive는 match로 제거함.\nmatch b with| true =&gt; monday| false =&gt; tuesdayend\n\nvalue: 계산이 완전히 끝난 값\nexpression: 계산을 하면 값이 됨\n\nInductive의 value는 처음에 정의했던 값들밖에 없음!!\ne.g. day의 value는 monday ~ sunday밖에 없음.\napply\nCompute ((fun b: bool =&gt;        match b with        | true =&gt; monday        | false =&gt; tuesday        end) false).\nFunction의 elimination rule은 apply임...\n말 그대로 f x하면 function을 사용한 것!\nCompute\n요약) Type에는 Inductive랑 Function이 있음\n각 Type마다 introduction rule이랑 elimination rule이 있음\n이 네 가지 rule을 type checking이 가능하도록 조합한 모든 것을 expression이라고 부름\nInductive의 introduction은 Inductive\nInductive의 elimination은 match\nFunction의 introduction은 lambda function\nFunction의 elimination은 apply (키워드는 아니지만...)\n이제 계산을 좀 더 엄밀하게 정의할 수 있는데 같은 type(Inductive/Function)에 대해서 introduction과 elimination이 만날 경우 reduction이 일어남.\ne.g. 아래 코드를 실행시키면\n(fun x =&gt;  match x with  | monday =&gt; tuesday  | tuesday =&gt; wednesday  | wednesday =&gt; thursday  | thursday =&gt; friday  | friday =&gt; monday  | saturday =&gt; monday  | sunday =&gt; monday  end) friday\n먼저 함수 argument를 대체하고\nmatch friday with| monday =&gt; tuesday| tuesday =&gt; wednesday| wednesday =&gt; thursday| thursday =&gt; friday| friday =&gt; monday| saturday =&gt; monday| sunday =&gt; mondayend\nmatch를 맞추게 됨.\nmonday\n여담) Proof\n이것들을 이용해서 증명을 어떻게 하냐?\nDefinition test_next_working_day:  (next_working_day friday) = monday  :=  eq_refl.\neq_refl은 양변이 같은 등식들의 집합임.\n만약 (next_working_day friday)가 monday가 아니라면 eq_refl에 속하지 않으므로 type checking에 실패함!\n반대로 type checking이 된다면 (next_working_day friday)가 monday와 같다는게 증명이 되는 것\nGoal (next_working_day friday) = monday.Proof. simpl. reflexivity. Qed.\n실제 증명은 일일히 type을 지정해주는게 아니라 이런 식으로 증명하지만... 이는 후술함\nStandard library\nRocq는 사실 위에 있는 네 가지 rule과 계산만으로 정의되어있음!\n하지만 손가락의 수명을 위해서 아주 다양한 syntax sugar들과, (e.g. Definition으로 함수 introduction)\n미리 정의해둔 다양한 standard library가 있음.\n엄밀히 따지면 여기 있는 모든 내용이 standard library인건 아니긴 함;;\nbool\nInductive bool : Type :=  | true  | false.Definition negb (b:bool) : bool :=  match b with  | true =&gt; false  | false =&gt; true  end.Definition andb (b1:bool) (b2:bool) : bool :=  match b1 with  | true =&gt; b2  | false =&gt; false  end.Definition orb (b1:bool) (b2:bool) : bool :=  match b1 with  | true =&gt; true  | false =&gt; b2  end.\nRocq에는 bool과 다양한 함수들이 정의되어있음.\n위 코드는 실제 구현을 따라한 것!\n잠만 andb, orb는 뭐 어떻게 생겨먹은거임?\nDefinition andb: bool -&gt; (bool -&gt; bool) :=  fun b1: bool =&gt; (fun b2: bool =&gt;    match b1 with    | true =&gt; b2    | false =&gt; false    end) : bool -&gt; bool.\n사실 이거랑 같은거임 bool을 받아서 bool -&gt; bool fun을 리턴\n최종 타입은 bool -&gt; bool -&gt; bool\n참고로 무한 루프로 빠지는 함수는 output이 수학적으로 결정이 되지 않음..\n그래서 Rocq는 무한 루프로 빠질 가능성이 있는 함수는 애초에 type checking을 실패하도록 만들었음!\n(만약 항상 terminate하는 함수를 Rocq가 받아들이지 않는다면 수학적으로 항상 terminate한다는 것을 증명시켜야 type checking 가능함;;)\n이러면 call by value, call by name의 결과가 똑같아서 신경 안 쓰고, 또한 reduction의 순서를 어떻게 해도 항상 같은 reduced form으로 계산됨.\n즉, bool -&gt; (bool -&gt; bool)로 보든, (bool -&gt; bool) -&gt; bool로 보든 상관 없다는 소리!\n물론 실제 type checker는 단순한 프로그램이기 때문에 reduction 순서가 존재하긴 함;\nQ) 이렇게 and를 써도 되는거 아님?\nDefinition and1 (b1:bool) (b2:bool) :=  match b1 with  | true =&gt; b2  | false =&gt; false  end.Definition and2 (b1:bool) :=  match b1 with  | true =&gt; (fun b2: bool =&gt; b2)  | false =&gt; fun _ =&gt; false  end.Definition and3 (b1:bool) (b2:bool) :=  match b2 with  | true =&gt; b1  | false =&gt; false  end.\n&quot;사람이 보기엔&quot; 같은 함수가 맞음\nBut 계산 방식이 다르기 때문에 Rocq가 보기엔 같은 함수가 아님...\n정확히 말하자면 이 세 함수가 같다는 사실은 자명하지 않음\nRocq에서는 세 함수가 같다는 사실을 증명해야 함!\nif then else\nbool의 match는 Rocq에서 if then else로 바꿔줌\nmatch랑 완전 똑같지만 단순 syntax sugar임\nDefinition negb&#x27; (b:bool) : bool :=  if b then false else true.Definition andb&#x27; (b1:bool) (b2:bool) : bool :=  if b1 then b2 else false.Definition orb&#x27; (b1:bool) (b2:bool) : bool :=  if b1 then true else b2.\n사실 bool은 엄밀히 따지면 builtin이 아니라 standard library에 속함!\n그래서 if then else는 bool이 아니여도 쓸 수 있음...!! (실용성은 글쎄..)\nInductive bw : Type :=| bw_black| bw_white.Definition invert (x: bw) : bw :=  if x then bw_white  else bw_black.Compute (invert bw_black). (* bw_white : bw *)\n정확히 clause가 2개인 모든 Inductive는 if then else syntax를 사용할 수 있음!\n이때 첫 번째 clause를 true(?)로 취급함\nNotation\nNotation &quot;x &amp;&amp; y&quot; := (andb x y).Notation &quot;x || y&quot; := (orb x y).Compute false || false || true. (* true: bool *)\nRocq에서는 내 맘대로 syntax를 정할 수 있음!!\n예를 들어 위 Notation을 사용하면 다른 프로그래밍 언어처럼 &amp;&amp;랑 ||로 bool 값들을 계산할 수 있음!\nConstructors\nInductive rgb : Type :=  | red  | green  | blue.Inductive color : Type :=  | black  | white  | primary (p : rgb).\nInductive로부터 다른 Inductive를 정의할 수도 있음.\n사실 여기서 primary는 rgb-&gt;color, 즉 함수임!\ncolor는 black, white, primary red, primary green, primary blue 5개가 있는 set이라고 생각하면 됨\nprimary라는 함수를 갖고 있는 set이라고 생각하면 안 됨!\nDefinition isred (c : color) : bool :=  match c with  | black =&gt; false  | white =&gt; false  | primary red =&gt; true  | primary _ =&gt; false  end.\n이런 식으로 constructor 전체나 일부를 matching할 수 있음.\n사실 이것도 syntax sugar임! 원래는 Inductive에 정의된 모든 clause에 대해서 match해야 함.\nDefinition isred (c : color) : bool :=  match c with  | black =&gt; false  | white =&gt; false  | primary p =&gt;      match p with      | red =&gt; true      | green =&gt; false      | blue =&gt; false      end  end.\nprimary red랑 primary _를 풀어쓰면 이렇게 생겼을거임\nModule\nModule Playground.  Definition foo : rgb := blue.End Playground.Definition foo : bool := true.Check Playground.foo : rgb.Check foo : bool.\n단순한 이름을 숨기는 목적을 갖고 있지만...  나중에 진짜 의미가 나온다고 함..?\nTuples\nInductive bit : Type :=  | B1  | B0.Inductive nybble : Type :=  | bits (b0 b1 b2 b3 : bit).Check (bits B1 B0 B1 B0): nybble.Definition all_zero (nb : nybble) : bool :=  match nb with  | (bits B0 B0 B0 B0) =&gt; true  | (bits _ _ _ _) =&gt; false  end.\nTuple을 사용해서 여러 Inductive가 묶여있는 타입을 정의할 수 있음.\nc.f. 앞서 말한대로 첫 번째 clause를 true로 취급하기 때문에 Rocq에선 1 0 순서로 씀;\n사실 이것도 syntax sugar고 match는 원래 모든 clause에 대해서 다 쓰는게 맞음...\nDefinition all_zero&#x27; (nb : nybble) : bool :=  match nb with  | (bits b0 b1 b2 b3) =&gt;    match b0 with    | B0 =&gt; match b1 with            | B0 =&gt; match b2 with                    | B0 =&gt; match b3 with                            | B0 =&gt; true                            | B1 =&gt; false                            end                    | B1 =&gt; false                    end            | B1 =&gt; false            end    | B1 =&gt; false    end  end.\nNat\nProblem) 지금은 유한한 set밖에 못 만듦\n자연수같은 무한한 set은 어떻게 만듦?\nInductive nat: Type :=  | O  | S (n : nat).\nRocq에서는 nat으로 자연수를 정의하고 있음.\n0은 O, 1은 S O, 2는 S S O, ...\nlambda calculus에서 자연수를 정의했던 방식이랑 비슷함!\nc.f. n=λf.λx.fn(x)n = \\lambda f. \\lambda x. f^n(x)n=λf.λx.fn(x)\n자연수는 너무 많이 쓰이기 때문에 특별하게 자연수는 알아서 계산해서 보여줌\n예를 들어 Check (S (S (S (S O)))).는 알아서 4 : nat로 계산해서 나옴.\nSet Printing All.Check 5. (* S (S (S (S (S O)))) *)\n이게 싫다면 Set Printing All.으로 syntax sugar를 꺼버릴 수 있다...\nInductive nat: Type :=  | O  | S (n : nat).Inductive otherNat: Type :=  | baba  | fofo (jiji : otherNat).\n사실 위의 otherNat이랑 nat은 같은 집합이지만 Rocq는 두 집합이 같다는 증명을 할 수가 없음...\n심지어 otherNat이랑 nat이 다르다는 증명도 할 수가 없음!\nRocq에서는 두 집합이 같다는 증명을 할 수 없다는게 증명되어있음!\nDefinition pred (n : nat) : nat :=  match n with  | O =&gt; O  | S n&#x27; =&gt; n&#x27;  end.\n-1은 이렇게 정의되어 있음\n음수는 일단 나중에\nc.f. S도 따지고 보면 nat -&gt; nat인데 +1의 역할을 하는 것 아니냐?\n하지만 pred와 S는 조금 다른데 pred는 계산을 하는 함수지만 S는 그냥 element를 만드는 정의임.\n사실 S는 애초에 계산을 하지도 않음! (e.g. S O는 더 이상 reduce할 수 없음)\n아래에서 설명하는 Fixpoint를 쓰면 덧셈 뺄셈 등등도 정의할 수 있음!\nn m: nat은 같은 타입 인자 여러 개를 한번에 정의시켜주는 syntax sugar임\nFixpoint plus (n m : nat) : nat :=  match n with  | O =&gt; m  | S n&#x27; =&gt; S (plus n&#x27; m)  end.Compute (plus 3 4).Fixpoint minus (n m : nat) : nat :=  match n, m with  | O, _ =&gt; O  | n, O =&gt; n  | S n&#x27;, S m&#x27; =&gt; minus n&#x27; m&#x27;  end.Compute (minus 6 2).Fixpoint mult (n m : nat) : nat :=  match n with  | O =&gt; O  | S n&#x27; =&gt; plus m (mult n&#x27; m)  end.Compute (mult 3 3).Fixpoint exp (base power : nat) : nat :=  match power with  | O =&gt; S O  | S p =&gt; mult base (exp base p)  end.Compute (exp 2 10).Notation &quot;x + y&quot; := (plus x y)                       (at level 50, left associativity)                       : nat_scope.Notation &quot;x - y&quot; := (minus x y)                       (at level 50, left associativity)                       : nat_scope.Notation &quot;x * y&quot; := (mult x y)                       (at level 40, left associativity)                       : nat_scope.Compute ((1 + 3 - 2) * 5).\n(Notation 뒤쪽에 나오는건 연산자 우선순위, 연산 방향 같은거 정의해준거임)\n등호나 부등호도 두 nat을 받아서 bool을 내놓는 함수로 정의할 수 있음!\nFixpoint eqb (n m : nat) : bool :=  match n with  | O =&gt; match m with         | O =&gt; true         | S m&#x27; =&gt; false         end  | S n&#x27; =&gt; match m with            | O =&gt; false            | S m&#x27; =&gt; eqb n&#x27; m&#x27;            end  end.Fixpoint leb (n m : nat) : bool :=  match n with  | O =&gt; true  | S n&#x27; =&gt;      match m with      | O =&gt; false      | S m&#x27; =&gt; leb n&#x27; m&#x27;      end  end.Notation &quot;x =? y&quot; := (eqb x y) (at level 70) : nat_scope.Notation &quot;x &lt;=? y&quot; := (leb x y) (at level 70) : nat_scope.Compute (4 &lt;=? 2).\nFixpoint\nFixpoint를 쓰면 Definition과 달리 재귀함수를 정의할 수 있음.\nDefinition은 계산을 한 번만 하지만 Fixpoint는 계산을 몇 번 할지 모름!\nFixpoint even (n:nat) : bool :=  match n with  | O =&gt; true  | S O =&gt; false  | S (S n&#x27;) =&gt; even n&#x27;  end.\nQ) 그냥 Fixpoint만 쓰면 안 됨?\nA) 그냥 Readability 용도임 전부 Fixpoint로 써도 되긴 하는데 Definition이라고 쓰면 재귀가 없다는게 확실해짐\nc.f. Inductive에도 비슷한게 있음... 사실 Inductive가 Fixpoint에 가까운 형태임!\nVariant는 Inductive랑 똑같지만 Definition처럼 재귀를 허용하지 않는 형태임. (nat 같은 것 정의 불가)\nFixpoint도 syntax sugar지만... 굳이 lambda function 형태로 재귀함수를 쓰고 싶다면 fun 대신 fix를 쓰고 알 수 없는 이유로 반드시 :=를 써야 함\n재귀함수의 함수명은 아무거나 써도 되지만 관례상 self를 쓴다고 함\nCompute (fix self n :=  match n with  | O =&gt; true  | S O =&gt; false  | S (S n&#x27;) =&gt; self n&#x27;  end) 5.\n참고로 앞서 말한대로 Rocq 함수는 반드시 끝나야함\n아래 함수는 Rocq이 보기에 안 끝날 수도 있는 함수라 reject됨\nFixpoint even (n:nat) : bool :=  match n with  | O =&gt; true  | S O =&gt; false  | S (S n&#x27;) =&gt; even (S (S n&#x27;))  end.\n하지만 당연히 끝나는 함수도 Rocq이 멍청해서 안 끝난다고 판단할 수도 있음...\n(* 외않됨 *)Fixpoint iszero (n:nat) : bool :=  match n with  | O =&gt; true  | S O =&gt; false  | S (S n&#x27;) =&gt; iszero (S n&#x27;)  end.\n하지만 모든 끝나는 함수는 Rocq가 끝난다는 것을 알 수 있게 만들 수 있다는 증명이 있다고 함?\nMutual recursion\nFixpoint even (n: nat) : bool :=  match n with  | O =&gt; true  | S n&#x27; =&gt; odd n&#x27;  endwith odd (n: nat) : bool :=  match n with  | O =&gt; false  | S n&#x27; =&gt; even n&#x27;  end.Compute (even 4).\nMutual recursion도 가능함! with를 써서 두 함수를 동시에 정의할 수 있음.\n만약 even만 정의하고 싶고 odd는 숨기고 싶다면 let in을 써야함\nFixpoint even (n: nat) : bool :=  let odd := (fix self n := match n with    | O =&gt; false    | S n&#x27; =&gt; even n&#x27;    end)  in  match n with  | O =&gt; true  | S n&#x27; =&gt; odd n&#x27;  end.\n물론 mutual recursion은 syntax sugar라서 없애버릴수는 있긴 함...\nDefinition _even (odd: nat -&gt; bool) (n: nat) : bool :=  match n with  | O =&gt; true  | S n&#x27; =&gt; odd n&#x27;  end.Definition _odd (even: nat -&gt; bool) (n: nat) : bool :=  match n with  | O =&gt; false  | S n&#x27; =&gt; even n&#x27;  end.Fixpoint even (n: nat) : bool :=  _even (_odd even) n.Fixpoint odd (n: nat) : bool :=  _odd (_even odd) n.\n좀 더 현실적인 방법으로는 일반적으로 tuple로 묶어서 recursion을 동시에 돌리는 방법이 있음\nFixpoint evenodd (n : nat) : bool * bool :=  match n with  | O     =&gt; (true, false)  | S n&#x27;  =&gt;    match evenodd n&#x27; with    | (e, o) =&gt; (o, e)    end  end.Definition even (n : nat) : bool := fst (evenodd n).Definition odd  (n : nat) : bool := snd (evenodd n).\nProof\nRocq에선 놀랍게도 1+1 = 2를 간단하게 증명할 수 있음!\nExample plus_1_1 : 1 + 1 = 2.Proof. reflexivity. Qed.\n이게 왜 증명임?\n사실 아래랑 같은 말임\nDefinition plus_1_1&#x27; : 1 + 1 = 2 := eq_refl.\n즉, 위에서 쓴 Proof는 사실 type checking 과정임!\n하지만 사람이 조금이라도 더 알아먹기 쉽게 Proof와 tactic들로 증명하게 하는 것.\nRocq에서 =는 사실 Inductive로 정의된 set(+ Notation)으로, 1 + 1 = 2는 사실 @eq nat (1 + 1) 2임.\nPrint eq.로 정의를 보면 아래처럼 나와있음.\nInductive eq (A : Type) (x : A) : A -&gt; Prop :=| eq_refl : eq A x x.\n사실 이 Inductive는 지금까지 본 Inductive보다 더 general한 버전인 Inductive family임.\nA -&gt; Prop이라는건 임의의 A를 받아서 Prop을 내놓는다는 거임. (Prop은 대충 Type이라고 생각하자)\n즉 eq nat 1 1, eq nat 1 2 이런건 전부 Type임! 다른 언어의 Generics처럼 Inductive family로 수많은 Type을 한번에 정의했다고 생각해도 됨.\n근데 이 Inductive의 construct은 eq_refl: eq A x x밖에 없기 때문에 eq nat 1 1의 element는 construct할 수 있는데 eq nat 1 2의 element는 construct할 수 없음.\n즉 1 = 1은 참이지만 1 = 2는 거짓이다!\n위 정의를 좀 더 쉽게 바꾸면\nInductive eq&#x27; (A : Type) : A -&gt; A -&gt; Type :=| eq_refl (x: A) : eq&#x27; A x x.\neq는 어떤 set A가 주어졌을 때, A의 element 두 개를 받고 set을 내놓는거라고 볼 수 있음\n근데 construct가 eq_refl밖에 없으므로 주어진 두 element가 똑같을 때만 construct 됨.\neq A a b는 a, b가 같다면 원소가 있는 set, a, b가 다르다면 원소가 없는 set이 된다!\n그럼 a와 b가 같다는 증명은 어떻게 하냐? 실제로 원소가 있다는 것을 보여주면 됨.\nRocq 식으로 표현하면 eq A a b 타입인 element를 찾으면 되는거고,\n더 일반적으로는 Proposition 타입인 Proof를 만들면 Proposition이 참이라는 것을 증명한 것임.\n즉, 단순한 type checking으로 증명할 수 있고, &quot;증명을 코딩&quot;한다는건 Proof를 type checking 통과하도록 코딩한다는 소리임.\n이렇게 보면 뭔가 특별해보이지만 사실 type checking rule은 아래 세 가지 경우에서 모두 똑같음! 지금까지 우리가 쓰던 type checking rule을 Set, Prop, Type에서 똑같이 사용함.\n다만 용도를 조금 구분하기 위해 Rocq 내부적으로는 Set이랑 Prop이라는 새로운 Type을 만들어서 사용중임.\n\nelement: Set\nproof: Prop\nprogram: Type\n\nDefinition plus_2_2 : 2+2 = 4 := @eq_refl nat 4.Definition plus_2_2&#x27; : @eq nat (2+2) 4 := @eq_refl nat 4.\n예시로 2+2 = 4를 보면 (@eq랑 =는 같은거임) 2+2 = 4는 2+2 = 4가 맞다는 set인데, @eq_refl nat 4가 이 set의 element로 type checking됐다는 소리는 2+2 = 4라는 set에 원소가 있다는거임.\n근데 앞서 말했던대로 set에 원소가 있다는거 자체가 2+2 = 4는 참이라는거임.\n즉, type checking이 성공했다는 것 자체가 2+2 = 4는 참이라는 증명이다!\nc.f. A랑 B가 둘다 C로 reduction이 되면 A, B는 convertable하다고 함. (서로 바꿀 수 있다)\nRocq에서 계산이 되는 경우 reduction은 알아서 해줘서 2+2는 알아서 4가 됨\n사실 우리는 4 = 4를 증명하고 있었던 것;;\nc.f. Definition what: 1+1 = 3 := 1+1 = 3. 같은건 올바른 증명이 아님!!\n1+1 = 3 : Type이고, what: 1+1 = 3임. Element와 Type은 다르다!\n당연히 Type은 Type의 element일 수가 없음 (2: nat이지 nat: nat이 아니다..)\nQ. 1+1 = 2, 2+2 = 4 같은 걸 쓸 때마다 계속 타입을 만듦?\nA. 그건 C, Rust같이 어셈블리 만들어야 하는 애들이나 그러는거고 functional language는 물론이고, C나 Rust도 type checking 과정에선 polynomial type 상태를 유지하면서 type checking 가능함 (심지어 아래에 나오는 forall 같은 경우도 type checking이 됨!)\n(* 이건 Typechecking 안 됨 *)Definition foo (n: nat) : (n + 1) = (1 + n) := @eq_refl nat (S n).\n위에껀 non-trivial함!\n1 + n은 S n으로 reduction이 되긴 하지만, n + 1은 n을 몰라서 reduction이 안 됨\n그래서 같다는게 trivial하지 않아서 증명이 안 됨..\nLemma bar (n : nat) : n + 1 = 1 + n.Proof.  induction n.  - simpl. reflexivity.  - simpl. rewrite IHn. simpl. reflexivity.Qed.\n그래서 이렇게 수학적 귀납법을 써서 증명해야 함.\n이게 우리가 일반적으로 Rocq에서 증명하는 과정임!\n\nLemma, Example, Theorem, Goal 등등은 전부 똑같은 keyword임; 증명하고 싶은 명제를 정의해줌 (Goal은 이름을 지정할 수 없다는 차이점이 있음)\nProof는 지금부터 명제를 증명할 프로그램을 짜겠다는 소리\ninduction, simpl, reflexivity 같은 tactic으로 프로그램을 작성함\nQed는 완성한 프로그램을 type checking 해서 맞는지 검사함.\n\nProof ~ Qed는 우리가 @eq_refl nat 4로 짜던 프로그램을 Rocq가 대신 짜주는 거라고 생각하면 됨.\ne.g. simpl.이라는 tactic은 제일 단순한 형태로 reduction시킴\n예를 들어 2+2 = 4라는 goal을 4 = 4로 만듦\n정확히 말하면 기존 goal을 수정하는게 아니라 더 쉬운 goal을 증명하면 기존 goal을 증명할 수 있게 해주는거임!\n즉, simpl.이란 tactic은 q: 4 = 4를 만족하는 q가 있다면 q를 사용해서 p: 2+2 = 4를 만족하는 p를 만들어주는 tactic임.\n사실 reduction은 항상 하기 때문에 simpl.은 안 써도 되는 tactic이지만 사람이 알아보기 편하게 사용함..\n실제로 만들어진 프로그램을 보고 싶으면 Print bar.를 해보면 됨!\nPrint bar.(*bar =fun n : nat =&gt;nat_ind (fun n0 : nat =&gt; n0 + 1 = 1 + n0) (eq_refl : 0 + 1 = 1 + 0)(fun (n0 : nat) (IHn : n0 + 1 = 1 + n0) =&gt; eq_ind_r (fun n1 : nat =&gt; S n1 = S (S n0)) (eq_refl : S (1 + n0) = S (S n0)) IHn :S n0 + 1 = 1 + S n0) n     : forall n : nat, n + 1 = 1 + n*)\n좀 더 알아듣기 쉽게 만들려면 unfold로 풀어쓸 수 있음.\nLemma bar (n : nat) : n + 1 = 1 + n.Proof.  induction n.  - simpl. reflexivity.  - simpl. rewrite IHn. simpl. reflexivity.Defined.(* Defined로 끝내면 추후에 증명 내용을 사용할 수 있음. *)Goal bar = bar.unfold bar at 1.unfold nat_ind.unfold eq_ind_r.unfold eq_ind.unfold eq_sym.(* Set Printing All. 쓰면 syntax sugar 다 없애버림 *)(*(fun n : nat =&gt;  (fix F (n0 : nat) : n0 + 1 = 1 + n0 :=  match n0 as n1 return (n1 + 1 = 1 + n1) with  | 0 =&gt; eq_refl  | S n1 =&gt; match match F n1 in ([...]) return ([...]) with      | eq_refl =&gt; eq_refl      end in (_ = a) return (S a = S (S n1)) with    | eq_refl =&gt; eq_refl    end  end) n) = bar*)\n참고로\nDefinition even (n: nat) : Type :=  exists m: nat, n = 2 * m.\n이렇게 하면 명제를 사용해서 성질을 나타낼 수가 있음!!\n정확히 말하자면 even n은 n이 짝수라는 명제를 나타냄\neven 4를 증명하면 4가 짝수라는 것을 나타낼 수 있음!\nexists는 조건을 만족하는 m들의 집합을 나타냄\n즉, 여기서 even은 n = 2 * m을 만족하는 m들의 집합으로 정의됨.\n명제는 element가 있다면 참, 없다면 거짓이므로 n = 2 * m인 m이 있다면 even n은 참이 됨!\nTactics\n아래 예제를 통해 다른 tactic들에 대해 알아보자!\nTheorem plus_0_n : forall n : nat, 0 + n = n.Proof.  intros n. simpl. reflexivity.Qed.\n참고로 Require Import Utf8. 써주면 ∀로 forall을 대체할 수 있음!\ne.g. Theorem plus_0_n : ∀ n : nat, 0 + n = n.\nfun도 λ로 바뀌고 =&gt;도 ⇒로 바뀌고 생각보다 유니코드로 바뀌는게 많음...\n아니 근데 forall은 뭔데??\n사실 그냥 function임 (n : nat) -&gt; 0 + n = n\n근데 우리가 알던 function A -&gt; B랑 다른게 뭐냐?\n우리가 지금까지 쓰던 A -&gt; B는 non-dependent function; input/return type이 항상 같음\n근데 (n : nat) -&gt; 0 + n = n은 dependent function; input에 따라 return type이 다름!!\ne.g. 1을 넣으면 return type은 0 + 1 = 1!\n하여튼 (n : nat) -&gt; 0 + n = n은 nat을 받아서 0 + n = n의 element를 내놓는 (즉 0 + n = n의 증명을 내놓는) 함수들의 집합임.\n즉 plus_0_n : (n : nat) -&gt; 0 + n = n이라는 프로그램이 있다면 이 집합에 속하는 element plus_0_n이 있다; 즉 이 집합은 공집합이 아니다; 즉 이 명제(모든 n에 대해서 0+n = n이다)를 증명할 수 있다!\n사실 이거 n+0이면 non trivial해서 바로 증명 안 됨\nintros\nintros n.은 n을 fix시켜놓고 어떤 fix된 n 하나에 대해서 goal을 보이도록 만듦. 사실상 forall을 풀어주는 역할!\n위 예시에선 n이 변수로 올라오고 goal은 0+n = n만 증명하면 됨\n(단순히 forall만 풀어주는건 아니고 p -&gt; q의 경우 가정 p를 올려주는 등 더 다양한 사용처가 있음)\nsimpl\n앞서 봤던대로 simpl은 reduction을 알아서 해주는 역할, Rocq는 항상 reduction을 하기 때문에 안 써도 아무 문제 없음!\n순수하게 사람이 알아보기 편하라고 만들어진 tactic임;\n만약 너무 한번에 reduction이 돼서 이해하기가 어렵다면 simpl Nat.eqb.처럼 단계별로 reduction을 할 수 있음.\nreflexivity\n지금까지 봤던 eq_refl을 만들어주는 역할!\n등호 양변이 같으면 goal을 풀어버림\ninduction\nTheorem plus_n_0: forall n: nat, n + 0 = n.Proof.  intros n.  induction n.  - simpl. reflexivity.  - simpl. rewrite IHn. reflexivity.Qed.\n0+n은 simpl.이 먹히지만 n+0은 simpl.이 안 먹힘..\n이 경우 induction으로 수학적 귀납법을 써야 함!\ninduction n을 하면 2개의 subgoal을 만들어 냄\n\nn = 0일때 성립\nn 일때 성립하면 S n에서 성립\n\nsubgoal을 증명하고 싶으면 -를 사용해서 subgoal을 하나씩 증명할 수 있음.\nc.f. subgoal 단계가 깊어지면 +, *, --, ++, **를 쓰거나 다른 프로그래밍 언어처럼 중괄호를 여닫는 식으로 subgoal을 표시할 수 있음.\n아니면 아예 subgoal이 너무 많다면 ;를 사용해서 모든 subgoal마다 똑같은 tactic을 사용해서 증명하라고 할 수 있음!\nrewrite\nrewrite는 등식을 바꿔주는 tactic임.\n위에서 induction을 하면 두 번째 subgoal에서 귀납가정이 (n일때 n + 0 = n이 성립) IHn이라는 이름으로 생기는데,\nrewrite IHn.은 귀납가정을 사용해서 n + 0을 n으로 바꾸라는 소리임.\n기본적으로 rewrite H.는 왼쪽에서 오른쪽으로 치환하지만,\nrewrite &lt;- H.로 쓰면 오른쪽에서 왼쪽으로 치환해줌.\ndestruct\nNotation &quot;x =? y&quot; := (Nat.eqb x y) (at level 70) : nat_scope.Theorem plus_1_neq_0: forall n: nat,  ((n + 1) =? 0) = false.Proof.  intros n.  simpl. (* n + 1은 reduction이 안 됨... *)  destruct n.  - simpl. reflexivity. (* 0 + 1은 reduction이 됨! *)  - simpl. reflexivity. (* S n + 1은 reduction이 됨! *)Qed.\ndestruct는 Introduction에서 정의했던 constructor들로 나눠버림\n이게 뭔 소리냐? 위의 경우에는 n은 nat임\n이때 그냥 nat을 정의했던대로 O인 경우, S n인 경우로 풀어서 쓸 수 있음!\n원래 (n + 1 =? 0) = false인 goal을 두 개의 subgoal (0 + 1 =? 0) = false이랑 (S n + 1 =? 0) = false으로 풀어줌.\n만약 두 subgoal 모두 증명된다면 모든 constructor에서 증명했으니 모든 nat에 대해서 증명한게 되는거임!\n사실 보통은 destruct n as [| n'] eqn:E.로 씀\nn'은 n 돌려쓰면 헷갈리니까 subgoal에서는 S n'처럼 다른 변수로 쓰라고 하는거고,\neqn:E는 destruct한 가정을 E로 정의함 (첫 번째 subgoal에선 E: n = 0, 두 번째 subgoal에선 E: n = S n')\ninduction도 비슷하게 induction n as [| n' IHn']처럼 정의할 수 있음!\nRocq의 한계?\nRocq는 constructive logic이기 때문에 명제를 증명할 때는 항상 실제로 증명 방법을 제시해야 함.\n하지만 constructive하게 증명을 만들어낼 수 없는 명제도 있음...\nTheorem baejungrule: forall p: Prop,  p \\/ ~p.Proof. Admitted.\n이런 명제들은 Rocq에서 참이라는 증명을 할 수가 없음...\n왜냐? evidence(실제 증명)을 줄 수가 없음 만약 evidence를 만들 수 있으면 p에 밀레니엄 문제 넣고 참인 evidence가 있는지 거짓인 evidence가 있는지 보고 10억 타면 됨\nRocq는 반드시 프로그램을 짜야 증명이 되기 때문에, 저걸 증명하려면 p의 프로그램을 짜거나 ~p의 프로그램을 짜야함. 하지만 이걸 임의의 Prop에 대해서 증명할 수가 없음...\n이런 경우 Adimitted.를 써서 강제로 증명하거나 아예 Axiom baejungrule: forall p: Prop, p \\/ ~p.처럼 애초에 이 명제는 참이라고 정의할 수 있음.\n하지만 이렇게 하면 Rocq를 사용하는 이유인 엄밀성이 깨지니까 웬만하면 쓰지 말자...\nc.f. 괴델의 불완전성 정리: 수학체계는 스스로 수학체계가 무모순이라는 증명을 할 수가 없음\n","categories":["SNU","4-2","소프트웨어 엄밀 검증"]},{"title":"Introduction","url":"/posts/100/","content":"Why OS?\nTo protect and to serve (from NYPD)\nOperating System (kernel) is a software that converts hardware into a useful form for applications.\nMiddleware, Software Development Environment (compilers, loaders, editors, libraries, ...) are not OS!\nHistory of OS\n1G (1945 ~ 1955)\nVacuum tubes and plugboards\nNo programming/assembly languages!\nObviously no OS (coded with 0 or 1)\n2G (1955 ~ 1965)\nTransistors and mainframes\nBatch systems - Submit program and run it (e.g. using cards) Can run only one job at a time\nOS is always resident in memory and merely transfers a control (just a library).\nProblem) CPU is underutilized due to the bottleneck in I/O (e.g. Card readers, tape drivers, line printers)\n3G (1965 ~ 1980)\nIC (Integrated Circuits), disk drives\nMultiprogrammed systems - Program as jobs, CPU utilization increased!\nNow looks like a real OS:\n\nJob scheduling\nMemory management\nCPU scheduling\nConcurrency\nProtection\nSpooling\n\nEven better: Time sharing systems - Program as process, response time improved!\nMore OS features:\n\nSophisticated CPU scheduling\nVirtual memory and swapping\nFile system\nSynchronization\nIPC (Interprocess communication)\nInteractive shell\nMore protection\n\n4G (1980 ~)\nMicroprocessors (LSIs, VLSIs)\nEra of personal computers!\nModern OS features:\n\nGUI (Graphical User Interface)\nMultimedia\nInternet &amp; Web (Network stack)\nMobile / Networked / Distributed\nThreads\nVirtualization\n\nMultics\nMultiplexed Information and Computing Service\nFirst time-shared, multi-processor OS!\nVery influential; Nearly every feature was made from Multics!\nThey claimed they would create an OS that combined all existing technology at the time, but it failed... (c.f. second-system effect)\n\nHierarchical file system\n\nACL (Access Control List)\nHard / symbolic links\n\n\nVirtual memory (segmentation and paging)\nUser-level command shell\nDynamic linking, shared memory\nImplemented in high-level language (PL/I)\nLogical disk volumes\nSupports every programming languages at that time (e.g. Fortran, Lisp, C, Cobol, ...)\n\nUnix\nBell labs was tired of slow development of Multics, so they quit and made Unix.\nMotivation) Putting everything is impossible, only implement necessary features!\n\nHierarchical file system\n\nSpecial files: Everything is file (e.g. I/O, drivers, ...), we can use uniform I/O, naming, and protection\nRemovable file systems via mount/umount\ni-node (metadata)\n\n\nProcess control\n\ne.g. fork, exec, wait, exit\nPipes for IPC\n\n\nShells\n\nStandard I/O and I/O redirection\nFilters, shell scripts\n\n\nSignals\n\nOS\n\nVirtualization: How to make each application believe it has each resource to itself?\nConcurrency: How to handle concurrent events correctly and efficiently?\nPersistence: How to make information survive power loss?\n\nApplication view\nOS provides an abstract view of the underlying computer system.\nAbstractions:\n\nCPU cores -&gt; processes, threads\nPhysical memory -&gt; Virtual memory\nStorage -&gt; Volumes, directories, files\nI/O devices -&gt; Files\n\nSystem view\nOS manages various resources of a computer system.\nManages resources:\n\nCPU\nMemory\nI/O devices\nQueues\nEnergy\n\nby:\n\nSharing (time, space, ...)\nProtection\nFairness\nEfficiency\n\nImplementation view\nOS is highly-concurrent, event-driven software.\nTwo kinds of events:\n\nSystem calls (by software)\nInterrupts (by hardware)\n\n","categories":["SNU","4-2","운영체제"]},{"title":"Architectural Support","url":"/posts/103/","content":"Introduction\nHardware should support certain features to run OS properly!\nI/O\n\nI/O devices and CPU can execute concurrently\nEach device controller is in charge of a particular device type\nEach device has a local buffer\nCPU issues specific commands to I/O devices\nCPU moves data between main memory and local buffers\n\nProblem: I/O is too slow (sometimes, its time is unknown e.g. keyboards)\nCPU is a precious resource, it should be freed from time-consuming tasks!\nInterrupts\nInstead of polling (periodically checking whether the issued command has been completed or not), we use (hardware) interrupts.\ne.g. We want to read a file from the disk.\n\nCPU asks disk controller to read a file. (Then it runs other processes until interrupt happens.)\nDisk controller reads file.\nDisk controller read file completely and send interrupt signal to interrupt controller.\nInterrupt controller sends interrupt signal to CPU.\nInterrupt controller sends interrupt information (e.g. who interrupted) to CPU.\nCPU perform current instruction and preserves the state of the CPU. (e.g. registers, program counter)\nCPU determines the interrupt's type. (polling or vectored interrupt system)\nCPU transfers control to the ISR (interrupt service routine) or interrupt handler.\nISR or interrupt handler transfers control back to the next instruction.\n\nSome I/O devices are too fast!\nTo reduce context switching with interrupts, some user programs directly polls I/O hardware.\nData Transfer Modes\n\nPIO (Programmed I/O)\n\nCPU moves data between I/O devices and memory.\nCan use special I/O instructions or memory-mapped I/O.\nProblem: CPU can only use registers! CPU can only move up to 64bit.\n\n\nDMA (Direct Memory Access)\n\nDevice controller transfers blocks of data from the local buffer directly to main memory without CPU intervention.\nOnly one interrupt is generated per request (after DMA finished).\nWe don't have to waste CPU time!\n\n\n\nProtection\nHow to prevent user applications from harming the system?\n\nApplication shouldn't access disk drives directly.\nApplication shouldn't execute the privileged instructions. (e.g. HLT instruction stops the processor)\n\nPriviledged instructions (protected instructions)\nOnly kernel mode can perform certain tasks.\ne.g. direct I/O access, accessing system registers, memory state management (page table, TLB, etc)\nHow does the CPU know if a protected instruction can be executed?\nThe architecture supports several modes of operation.\n\nx86_64: Ring 0 (kernel) &gt; Ring 1 &gt; Ring 2 &gt; Ring 3 (user)\nRing 1, 2 were ment to be used by device drivers, but using system call was too slow.\nMost device drivers only uses Ring 0.\nARM: EL3 &gt; EL2 &gt; EL1 &gt; EL0\nRISC-V: Machine (only when booting) &gt; Supervisor (kernel) &gt; User\n\nMode can be set by a status bit in a protected register.\nServicing Requests\nOkay, OS prevents applications from performing certain tasks.\nThen how do we ask services to the OS?\ne.g. How can an application read a file if it cannot access disk drives?\nSystem Calls\nOS defines a set of system calls - a programming interface to the services provided by OS.\nOS may reject an illegal request, impose a quota on a certain resources, or consider fairness when sharing resources.\nA system call is a protected procedure call!\nOn entry, CPU switch to the kernel mode.\nOn exit, CPU switch back to the user mode.\nExceptional Events\n\nInterrupts\n\nGenerated by hardware devices\nAsynchronous\n\n\nExceptions\n\nGenerated by software executing instructions\n\nCan be unintentional; e.g. divide by zero\nCan be intentional; e.g. syscall instruction\n\n\nSynchronous\nException handling is same as interrupt handling\n\n\n\nExceptions are called differently for each CPU.\ne.g. In x86_64, exceptions are divided into three categories.\n\nTraps\n\nIntentional\ne.g. system call, breakpoint, special instructions, ...\nReturn control to next instruction\n\n\nFaults\n\nUnintentional but possibly recoverable\ne.g. page faults, protection faults, ...\nRe-execute faulting instruction or abort\n\n\nAbort\n\nUnintentional and unrecoverable\ne.g. parity error, machine check, ...\nAbort the current program or halt the system\n\n\n\nOS Trap\nThere must be a special trap instruction that:\n\nCauses an exception, which invokes a kernel handler (i.e. runs in kernel mode)\nPasses a parameter indicating which system call to invoke\nSave caller's state (e.g. registers, mode bits)\nReturns to user mode when done with restoring its state\nOS must verify caller's parameters\n\ne.g. x86_64 have SYSCALL instruction\nc.f. Monolithic kernel: Kernel provide all services: memory, file system, device driver, etc.\nMicrokernel: Kernel only support basic memory management, process scheduling, etc. File system, device driver is in user level.\nMicrokernel is safer (if file system has error, we can just reboot file system), but it is slower (message should be sent between kernel and file system, can't be done in single system call).\nControl\nHow to take the control of the CPU back from the running program?\nWe need to run other processes, system calls, etc.\n\nEach application periodically transfers the control of the CPU to OS by calling various system calls.\nA special system call can be used just to release the CPU. (e.g. yield())\n\nProblem) What if application doesn't call system calls?\nWhat if a process ends up in an infinite loop?\nTimers\nA non-cooperative approach: Use a hardware timer that generates a periodic interrupt.\nThe OS is guaranteed to always get CPU back within a fixed time period!\nThe timer is privileged - Only the OS can load it!\ne.g. Linux use 10ms for 2.4, 1ms for 2.6, 4ms  for 5.5\nMemory\nApplications can't access hardware resources.\nBut applications can access (read/write) memory directly!!!\nTheoretically, memory should be accessed by a system call, but that will make process too slow!\nAlso, since memory should be fast, hardware should support memory protection instead of software.\nBut if the OS has all the information, how can hardware determine whether memory access is valid?\nSimplest Memory Protection\nUse base/limit registers!\nEach process can only read/write memory if the memory address is out of bound.\nCan be useful in a simple embedded environment!\nVirtual Memory\nModern CPU have dedicated memory management hardware! (MMU - Memory Management Unit)\nMMU can provide more sophisticated memory protection mechanisms.\n\nVirtual memory\nPaging, page tables, page protections, TLBs\nMemory segmentation\n\nObviously manipulating MMU is a privileged operation\nSynchronization\nProblem) Interrupt can occur at any time and may interfere with the interrupted code.\nHow do we coordinate concurrent activities?\nc.f. Heisenbug: Bug disappears or alter its behavior when one attempts to study it.\nThis is because thread timing is almost random!\nAtomic Instructions\nCPU supports atomic instructions which help with writing multithreaded code!\ne.g. RISC-V has AMO (Atomic Memory Operation) instructions which can swap, add integer, bitwise and/or/xor directly to memory's value.\nThis makes implementing locks and mutexes easier!\nBut it is still possible to implementing them without atomic instructions...\n","categories":["SNU","4-2","운영체제"]},{"title":"Introduction","url":"/posts/99/","content":"Cryptography\n\n1G: Classical Cryptography\n(Oxford) The art of writing or solving &quot;codes&quot;\nIs wasn't even called cryptography...\n2G: Modern Cryptography\nResearch has been active enough to establish cryptography since the 1970s\n3G: Advanced Cryptography\nThe real &quot;modern cryptography&quot;\n\nThe book only focus on 1970s modern cryptography... But we will learn some advanced cryptography too.\nWhat is cryptography?\nNot cryptocurrency\nIs cryptography cyber security?\nSort of... Cryptography can be viewed as a subcategory of cyber security, but it deals with a much more primitive level.\nCyber security - Talks about software security, hardware security, ...\nCryptography - Talks about file encryption, data integrity, ...\nCryptography is not a panacea!\ne.g. Offline attack (rubber-hose attack): Instead of attacking victim's device, attack victime directly...\ne.g. Side-channel attack: Measure physical information (temperature, program's running time) to extract data\nCryptography is all about encryption schemes: definition of how to encrypt (or decrypt) messages\nClassical cryptography used deterministic encryption scheme, but modern cryptography uses probablistic encryption scheme.\nKerckhoffs' Principle\n\nThe cipher method must not be required to be secret, and it must be able to fall into the hands of the enemy without inconvenience.\n\ni.e. An encryption scheme should be designed to be secure even if an eavesdropper knows all the details of the scheme.\ni.e. Security should rely solely on secrecy of the key (not secrecy of the encryption scheme).\nIt is VERY EASY for an encryption scheme to be leaked.\ne.g. reverse engineering, leak by employee, ...\nClassical cryptography\nUsually we assume private-key encryption scheme.\nA private-key encryption scheme is defined by specifying a message space MMM and a key space KKK along with three algorithms: key generation GenGenGen, encryption EncEncEnc, and decryption DecDecDec.\n\nGen(⋅)Gen(\\cdot)Gen(⋅) is a probabilistic algorithm tjhat outputs a key k∈Kk \\in Kk∈K chosen according to some distribution. (e.g. uniform distribution)\nEnck(m)Enc_k(m)Enck​(m) takes a key kkk and a message mmm as a input and outputs a ciphertext ccc.\nDeck(c)Dec_k(c)Deck​(c) takes a key kkk and a ciphertext ccc as a input and outputs a plaintext mmm.\n∀k←Gen(⋅),∀m∈M,Deck(Enck(m))=m\\forall k \\leftarrow Gen(\\cdot), \\forall m \\in M, Dec_k(Enc_k(m)) = m∀k←Gen(⋅),∀m∈M,Deck​(Enck​(m))=m\n\nShift Cipher (Ceasar Cipher)\nClassic Cipher (e.g. a -&gt; d, b -&gt; e, ...)\n\nM=K={0,1,…,25}M = K = \\{ 0, 1, \\ldots, 25\\}M=K={0,1,…,25}\nG(⋅)=k∈{0,1,…,25}G(\\cdot) = k \\in \\{ 0, 1, \\ldots, 25\\}G(⋅)=k∈{0,1,…,25}\nEnck(mi)=(mi+k) mod 26Enc_k(m_i) = (m_i + k) \\bmod 26Enck​(mi​)=(mi​+k)mod26\nDeck(ci)=(ci−k) mod 26Dec_k(c_i) = (c_i - k) \\bmod 26Deck​(ci​)=(ci​−k)mod26\n\nThe biggest problem: ∣K∣=26|K| = 26∣K∣=26\nExhaustive search (brute-force attack) works!\nSubstitution Cipher\nNow the key is chosen as arbitrary permutation of the alphabet.\nNow the key space is sufficiently large! ∣K∣=26!≈288.4|K| = 26! \\approx 2^{88.4}∣K∣=26!≈288.4\nHowever, frequency analysis can break it...\nc.f. We call an encryption scheme is a n-bit security level if the attacker must perform 2n2^n2n operations to break it.\n128-bit security level is considered the baseline for safety, but 80-bit security level is also quite secure.\nc.f. Bitcoin hashrate (number of computed hash) has already surpassed 2802^80280.\nNote that computing a hash requires several operations!\nIn theory, if everyone in the world worked together, an 80-bit security level could be broken..?\nEnigma\nEach keystroke rotates rotor, just like 4-digit base-26 number. ∣K∣=264≈218|K| = 26^4 \\approx 2^{18}∣K∣=264≈218\nThe enigma machine also had an additional secondary key called a plugboard. Total size of key space is about ∣K∣≈236|K| \\approx 2^{36}∣K∣≈236!\nIt was a perfect encryption scheme that couldn't be cracked  by the computers used back then!\nBut stupid German army used the same code for several days in a row when they were supposed to change it daily...\nModern cryptography\nHistorical approach didn't have an agree-upon notion of what requirements a secure scheme should satisfy.\nSchemes were designed in an heuristic manner: If any attack is found, the scheme would be patched to thwart that attack.\nModern cryptography have a formal definitions of security - with assumptions and proofs.\nIts scope is much broader than just encryption; e.g. authentication, protocols, group communication\nTerminology\nAlice sends data to Bob, but Eve (name from the word eavesdropper) tries to steal the data.\n\nPrivacy: Eve should not be able to read message.\nIntegrity: Eve should not be able to modify message.\nAuthenticity: Bob should know that message is really originated from Alice.\n\nAdvanced crpytography\nPreviously, we only needed that the data transmission is secure.\nNowdays, we also have to guarantee that the data transmission, storing data, and using data is secure.\nWe should protect computation, not just data itself!\na.k.a. Privacy-Enhancing Cryptography (PEC)\nCentral theorem (what cryptographers want to believe): Anything that can be done with a trusted authority can also be done without it!\nIronically, that's why cryptographers never assumes trusted authority.\nInternet Casino Problem\nPlayer gives Casino $1 and GGG (Guess).\nCasino draw TTT (target) randomly from 1~100, and give ddd (reward) to Player.\nIf G=TG = TG=T, casino gives $200.\nIf G≠TG \\neq TG=T, casino gives $0.\nTheoretically, expected value of ddd is $2, so you should play it!\nProblem: Casino can cheat... How can we ensure safe internet casino?\nPlayer and casino need to exchange G, T that satisfies:\n\nCasino cannot choose T as a function of G\nPlayer cannot choose G as a function of T\n\n\n\nPlayer gives Casino $1 and GGG in the box.\nCasino gives Player T.\nPlayer gives Casino the key (for the GGG in the box).\nCasino gets GGG with the key and give Player d according to the G.\n\nThis is perfect scheme for the player... but now player can cheat!\nIf we make message well enough, we can make a box that can be decrypted into any number!\nPlayer first look at T, and then give kTk_TkT​ that decrypts the box into T.\nHow can we prevent casino or player from cheating?\nMutual Interest\nAlice and Bob have a secret - their mutual interest toward each other.\nIf they both have mutual interest toward each other, they will go on a date!\nBut Alice and Bob don't want to share its mutual interest to each other.\n(Of course, you will eventually know other's mutual interest if you say YES, but you don't want to know other's mutual interest if you say NO.)\n\nA simple(?) solution with pen and paper exists!\nAlice and Bob place the smiley face in correct position if they say YES, and place in wrong position if they say NO.\nThen, Alice and Bob rotate the smiley face without watching it.\nIf smiley face appears, both said YES!\nIf smiley face isn't present, someone said NO...\nTry using this on a date!\nCard shuffling\nAlice wants to convince Bob that she can distinguish two cards, but she doesn't want to reveal any information about the cards.\n\nAlice arranges the cards in order and gives them to Bob.\nBob may or may not shuffle the cards.\nAlice answers whether or not the cards have been shuffled.\nSince Alice can guess the answer with a 50% probability, Alice and Bob repeat this process (about 100 times).\n\nPrivacy-preserving personalized services\nCan we get personalized services without actually providing personal data?\nIn fact this is possible!\nBut for technological advancement of AI, companies/government are recklessly collecting data...\nCryptographers are hoping for a major crisis to break out in order to adopt privacy-preserving personalized services.\n","categories":["SNU","4-2","현대암호학 개론"]},{"title":"Block Ciphers","url":"/posts/105/","content":"Motivation\nOTP, Stream Ciphers only assumes ciphertext-only attack.\nWe want to securely send multiple ciphertexts using same key!\nBlock ciphers are used to build CPA-secure encryption schemes.\nBlock ciphers are not encryption schemes!\nBut it can used as encryptions, MACs(Message Authentication Code), hash functions, PRGs, ...\nPRFs and PRPs\nA function E:X→XE: X \\rightarrow XE:X→X is a permutation if there exists and inverse function such that ∀x,E−1(E(x))=x\\forall x, E^{-1}(E(x)) = x∀x,E−1(E(x))=x.\nA pseudorandom function (PRF) is a keyed function F:K×X→YF: K \\times X \\rightarrow YF:K×X→Y that is efficiently computable by a deterministic algorithm.\nUsually K={0,1}n,X={0,1}lin,Y={0,1}loutK = \\{0,1\\}^n, X = \\{0,1\\}^{l_{in}}, Y = \\{0,1\\}^{l_{out}}K={0,1}n,X={0,1}lin​,Y={0,1}lout​, and we can get a function Fk:X→YF_k: X \\rightarrow YFk​:X→Y for each k∈Kk \\in Kk∈K.\nA pseudorandom permutation (PRP) is a keyed function E:K×X→XE: K \\times X \\rightarrow XE:K×X→X such that for all k∈Kk \\in Kk∈K, Ek(x)=E(k,x)E_k(x) = E(k, x)Ek​(x)=E(k,x) is a permutation, and Ek,Ek−1E_k, E_k^{-1}Ek​,Ek−1​ are efficiently computable by a deterministic algorithm.\nUsually K={0,1}n,X={0,1}lK = \\{0,1\\}^n, X = \\{0,1\\}^lK={0,1}n,X={0,1}l, and we can get a permutation Ek:X→XE_k: X \\rightarrow XEk​:X→X for each k∈Kk \\in Kk∈K.\nBasically PRF and PRP is a function/permutation version of PRG!\nNote that this is way harder than PRG; We can only have ∣K∣|K|∣K∣ functions, but there are ∣Y∣∣X∣|Y|^{|X|}∣Y∣∣X∣ distinct functions and ∣X∣!|X|!∣X∣! distinct permutations!\nSecure PRFs\nLet F\\mathcal{F}F is the set of all functions from X to Y.\nFkF_kFk​ (for uniform k∈Kk \\in Kk∈K) should be indistinguishable from fff (for uniform f∈Ff \\in \\mathcal{F}f∈F)!\n\nProblem: We can't give local table of function fff, it's too large!\nInstead, adversary can query up to qqq times.\nLet F:K×X→YF: K \\times X \\rightarrow YF:K×X→Y be a PRF. For a distinguisher A\\mathcal{A}A, its advantage is defined as\nAdvF(A)≔∣Pr⁡[EXP0,F(A)=1]−Pr⁡[EXP1,F(A)=1]∣Adv_F(\\mathcal{A}) \\coloneqq \\left| \\Pr[EXP_{0,F}(\\mathcal{A}) = 1] - \\Pr[EXP_{1,F}(\\mathcal{A}) = 1] \\right|\nAdvF​(A):=∣Pr[EXP0,F​(A)=1]−Pr[EXP1,F​(A)=1]∣\nWe say that F is secure if AdvF(A)Adv_F(\\mathcal{A})AdvF​(A) is negligible for any efficient A\\mathcal{A}A.\nWe simply denote EXP0,F(A)=AFk(⋅),EXP1,F(A)=Af(⋅)EXP_{0,F}(\\mathcal{A}) = \\mathcal{A}^{F_k(\\cdot)}, EXP_{1,F}(\\mathcal{A}) = \\mathcal{A}^{f(\\cdot)}EXP0,F​(A)=AFk​(⋅),EXP1,F​(A)=Af(⋅).\ncf. Af(⋅)\\mathcal{A}^{f(\\cdot)}Af(⋅) actually means that adversary has a oracle access to fff, i.e. adversary can use fff freely (up to qqq times).\nSecure PRPs\nLet P\\mathcal{P}P is the set of all permutations of X.\nEkE_kEk​ (for uniform k∈Kk \\in Kk∈K) should be indistinguishable from fff (for uniform f∈Pf \\in \\mathcal{P}f∈P)!\n\nLet E:K×X→XE: K \\times X \\rightarrow XE:K×X→X be a PRP. For a distinguisher A\\mathcal{A}A, its advantage is defined as\nAdvE(A)≔∣Pr⁡[EXP0,E(A)=1]−Pr⁡[EXP1,E(A)=1]∣Adv_E(\\mathcal{A}) \\coloneqq \\left| \\Pr[EXP_{0,E}(\\mathcal{A}) = 1] - \\Pr[EXP_{1,E}(\\mathcal{A}) = 1] \\right|\nAdvE​(A):=∣Pr[EXP0,E​(A)=1]−Pr[EXP1,E​(A)=1]∣\nWe say that E is secure if AdvE(A)Adv_E(\\mathcal{A})AdvE​(A) is negligible for any efficient A\\mathcal{A}A.\nWe simply denote EXP0,E(A)=AEk(⋅),EXP1,E(A)=Af(⋅)EXP_{0,E}(\\mathcal{A}) = \\mathcal{A}^{E_k(\\cdot)}, EXP_{1,E}(\\mathcal{A}) = \\mathcal{A}^{f(\\cdot)}EXP0,E​(A)=AEk​(⋅),EXP1,E​(A)=Af(⋅).\nPRG from PRF\nWe can make secure PRG from secure PRF!\nLet F:{0,1}n×{0,1}n→{0,1}nF: \\{0, 1\\}^n \\times \\{0, 1\\}^n \\rightarrow \\{0, 1\\}^nF:{0,1}n×{0,1}n→{0,1}n be a secure PRF. Then, the following G:{0,1}n→{0,1}nmG: \\{0, 1\\}^n \\rightarrow \\{0, 1\\}^{nm}G:{0,1}n→{0,1}nm is a secure PRG.\nG(k)≔Fk(0)∥Fk(1)∥⋯∥Fk(m−1)G(k) \\coloneqq F_k(0) \\Vert F_k(1) \\Vert \\cdots \\Vert F_k(m-1)\nG(k):=Fk​(0)∥Fk​(1)∥⋯∥Fk​(m−1)\nProof\n\nLet's assume that GGG is not secure, i.e. there exists an adversary A\\mathcal{A}A and non-negligible ϵ&gt;0\\epsilon &gt; 0ϵ&gt;0 such that\n∣Pr⁡k[A(G(k))=1]−Pr⁡r[A(r)=1]∣&gt;ϵ.\\left| \\Pr_k[ \\mathcal{A}(G(k)) = 1 ] - \\Pr_r[ \\mathcal{A}(r) = 1 ] \\right| &gt; \\epsilon.\n​kPr​[A(G(k))=1]−rPr​[A(r)=1]​&gt;ϵ.\nWe define an adversary B\\mathcal{B}B for PRF experiment.\n\nQuery mmm times and get yiy_iyi​ (Fk(i)F_k(i)Fk​(i) or f(i)f(i)f(i)) for 0≤i≤m−10 \\leq i \\leq m-10≤i≤m−1.\nCreate string ω≔y0∥y1∥⋯∥ym−1\\omega \\coloneqq y_0 \\Vert y_1 \\Vert \\cdots \\Vert y_{m-1}ω:=y0​∥y1​∥⋯∥ym−1​ by appending yiy_iyi​.\nPerform secure PRG experiment and get the output b′≔A(ω)b&#x27; \\coloneqq \\mathcal{A}(\\omega)b′:=A(ω).\nOutput b′b&#x27;b′ directly.\n\nWe'll prove that B\\mathcal{B}B can distinguish PRF with a non-negligible probability.\nIf EXP0EXP_0EXP0​ (i.e. yi=Fk(i)y_i = F_k(i)yi​=Fk​(i)), ω\\omegaω has the same distribution as G(k)G(k)G(k). ∴,Pr⁡[EXP0,F(B)=1]=Pr⁡k[A(G(k))=1]\\therefore, \\Pr[EXP_{0,F}(\\mathcal{B}) = 1] = \\Pr_k[\\mathcal{A}(G(k)) = 1]∴,Pr[EXP0,F​(B)=1]=Prk​[A(G(k))=1].\nIf EXP1EXP_1EXP1​ (i.e. yi=f(i)y_i = f(i)yi​=f(i)), ω\\omegaω is uniformly random string. ∴,Pr⁡[EXP1,F(B)=1]=Pr⁡r[A(r)=1]\\therefore, \\Pr[EXP_{1,F}(\\mathcal{B}) = 1] = \\Pr_r[\\mathcal{A}(r) = 1]∴,Pr[EXP1,F​(B)=1]=Prr​[A(r)=1].\n∴AdvF(B)=∣Pr⁡[EXP0,F(B)=1]−Pr⁡[EXP1,F(B)=1]∣=∣Pr⁡k[A(G(k))=1]−Pr⁡r[A(r)=1]∣&gt;ϵ&gt;0\\begin{align*}\n\\therefore Adv_F(\\mathcal{B}) &amp;= |\\Pr[EXP_{0,F}(\\mathcal{B}) = 1] - \\Pr[EXP_{1,F}(\\mathcal{B}) = 1]| \\\\\n&amp;= \\left| \\Pr_k[ \\mathcal{A}(G(k)) = 1 ] - \\Pr_r[ \\mathcal{A}(r) = 1 ] \\right| \\\\\n&amp;&gt; \\epsilon &gt; 0\n\\end{align*}∴AdvF​(B)​=∣Pr[EXP0,F​(B)=1]−Pr[EXP1,F​(B)=1]∣=​kPr​[A(G(k))=1]−rPr​[A(r)=1]​&gt;ϵ&gt;0​\nSince F is not secure when G is not secure, G is secure when F is secure!\n\n\nNote that this PRG is also parallelizable.\nPRF switching lemma (PRF from PRP)\nAny secure PRP is also a secure PRF, if |X| is sufficiently large.\nLet E:{0,1}n×{0,1}n→{0,1}nE: \\{0, 1\\}^n \\times \\{0, 1\\}^n \\rightarrow \\{0, 1\\}^nE:{0,1}n×{0,1}n→{0,1}n be a PRP. Then, for any qqq-query adversary A\\mathcal{A}A:\n∣AdvEPRF(A)−AdvEPRP(A)∣≤q22∣X∣\\left| Adv_E^{PRF}(\\mathcal{A}) - Adv_E^{PRP}(\\mathcal{A}) \\right| \\leq \\frac{q^2}{2|X|}\n​AdvEPRF​(A)−AdvEPRP​(A)​≤2∣X∣q2​\nc.f. q22∣X∣\\frac{q^2}{2|X|}2∣X∣q2​ comes from the birthday paradox.\nLet's choose qqq values from XXX uniformly at random thereby allowing repetitions.\nThe probability of at least one value is chosen more than once is less than q22∣X∣\\frac\n{q^2}{2|X|}2∣X∣q2​ (also called the birthday bound).\nProof of the birthday bound\n\nLet's say the i-th value is xix_ixi​.\nPr⁡[∃i&lt;j:xi=xj]≤∑1≤i&lt;j≤qPr⁡[xi=xj]=(q2)⋅1∣X∣&lt;q22∣X∣\\begin{align*}\n\\Pr[\\exists i &lt; j : x_i = x_j] &amp;\\leq \\sum_{1 \\leq i &lt; j \\leq q} \\Pr[x_i = x_j] \\\\\n&amp;= \\binom{q}{2} \\cdot \\frac{1}{|X|} &lt; \\frac{q^2}{2|X|}\n\\end{align*}Pr[∃i&lt;j:xi​=xj​]​≤1≤i&lt;j≤q∑​Pr[xi​=xj​]=(2q​)⋅∣X∣1​&lt;2∣X∣q2​​\n\n\nProof of the switching lemma\n\nLet fff is a random PRF and ggg is a random PRP. From the definition,\nAdvEPRF(A)=∣Pr⁡[AEk(⋅)=1]−Pr⁡[Af(⋅)=1]∣AdvEPRP(A)=∣Pr⁡[AEk(⋅)=1]−Pr⁡[Ag(⋅)=1]∣\\begin{align*}\nAdv_E^{PRF}(\\mathcal{A}) &amp;= \\left| \\Pr[\\mathcal{A}^{E_k(\\cdot)} = 1] - \\Pr[\\mathcal{A}^{f(\\cdot)} = 1] \\right| \\\\\nAdv_E^{PRP}(\\mathcal{A}) &amp;= \\left| \\Pr[\\mathcal{A}^{E_k(\\cdot)} = 1] - \\Pr[\\mathcal{A}^{g(\\cdot)} = 1] \\right|\n\\end{align*}AdvEPRF​(A)AdvEPRP​(A)​=​Pr[AEk​(⋅)=1]−Pr[Af(⋅)=1]​=​Pr[AEk​(⋅)=1]−Pr[Ag(⋅)=1]​​\nTherefore, from the reverse triangular inequality ∣∣a∣−∣b∣∣≤∣a−b∣||a|-|b|| \\leq |a-b|∣∣a∣−∣b∣∣≤∣a−b∣,\n∣AdvEPRF(A)−AdvEPRP(A)∣≤∣Pr⁡[Ag(⋅)=1]−Pr⁡[Af(⋅)=1]∣\\begin{align*}\n\\left| Adv_E^{PRF}(\\mathcal{A}) - Adv_E^{PRP}(\\mathcal{A}) \\right| \\leq \\left| \\Pr[\\mathcal{A}^{g(\\cdot)} = 1] - \\Pr[\\mathcal{A}^{f(\\cdot)} = 1] \\right|\n\\end{align*}​AdvEPRF​(A)−AdvEPRP​(A)​≤​Pr[Ag(⋅)=1]−Pr[Af(⋅)=1]​​\nNow we prove the difference lemma: Let Z,W0,W1Z, W_0, W_1Z,W0​,W1​ be the events defined over some probability space, and W0∧¬ZW_0 \\land \\lnot ZW0​∧¬Z occurs if and only if W1∧¬ZW_1 \\land \\lnot ZW1​∧¬Z. (i.e. Pr⁡[W0∧¬Z]=Pr⁡[W1∧¬Z]\\Pr[W_0 \\land \\lnot Z] = \\Pr[W_1 \\land \\lnot Z]Pr[W0​∧¬Z]=Pr[W1​∧¬Z]) Then the following holds.\n∣Pr⁡[W0]−Pr⁡[W1]=∣(Pr⁡[W0∧Z]+Pr⁡[W0∧¬Z])−(Pr⁡[W1∧Z]+Pr⁡[W1∧¬Z])=∣Pr⁡[W0∧Z]−Pr⁡[W1∧Z]∣≤Pr⁡[Z](∵0≤Pr⁡[W0∧Z],Pr⁡[W1∧Z]≤Pr⁡[Z])\\begin{align*}\n|\\Pr[W_0] - \\Pr[W_1] &amp;= |(\\Pr[W_0 \\land Z] + \\Pr[W_0 \\land \\lnot Z]) - (\\Pr[W_1 \\land Z] + \\Pr[W_1 \\land \\lnot Z]) \\\\\n&amp;= |\\Pr[W_0 \\land Z] - \\Pr[W_1 \\land Z]| \\\\\n&amp;\\leq \\Pr[Z] \\quad (\\because 0 \\leq \\Pr[W_0 \\land Z], \\Pr[W_1 \\land Z] \\leq \\Pr[Z])\n\\end{align*}∣Pr[W0​]−Pr[W1​]​=∣(Pr[W0​∧Z]+Pr[W0​∧¬Z])−(Pr[W1​∧Z]+Pr[W1​∧¬Z])=∣Pr[W0​∧Z]−Pr[W1​∧Z]∣≤Pr[Z](∵0≤Pr[W0​∧Z],Pr[W1​∧Z]≤Pr[Z])​\nLet x1,…xqx_1, \\ldots x_qx1​,…xq​ are the variables that A\\mathcal{A}A queried, and ZZZ is an event that at least two of f(xi)f(x_i)f(xi​) have the same value.\nIf ¬Z\\lnot Z¬Z occurs, because f(xi)f(x_i)f(xi​) and g(xi)g(x_i)g(xi​) are random values with no collisions, f(xi)f(x_i)f(xi​) and g(xi)g(x_i)g(xi​) have same distribution.\n∴Pr⁡[Ag(⋅)=1∧¬Z]=Pr⁡[Af(⋅)=1∧¬Z]\\therefore \\Pr[\\mathcal{A}^{g(\\cdot)} = 1 \\land \\lnot Z] = \\Pr[\\mathcal{A}^{f(\\cdot)} = 1 \\land \\lnot Z]∴Pr[Ag(⋅)=1∧¬Z]=Pr[Af(⋅)=1∧¬Z]\nAlso, from the birthday bound, Pr⁡[Z]&lt;q22∣X∣\\Pr[Z] &lt; \\frac{q^2}{2|X|}Pr[Z]&lt;2∣X∣q2​.\nTherefore, by using difference lemma,\n∣AdvEPRF(A)−AdvEPRP(A)∣≤∣Pr⁡[Ag(⋅)=1]−Pr⁡[Af(⋅)=1]∣≤Pr⁡[Z]&lt;q22∣X∣\\begin{align*}\n\\left| Adv_E^{PRF}(\\mathcal{A}) - Adv_E^{PRP}(\\mathcal{A}) \\right| &amp;\\leq \\left| \\Pr[\\mathcal{A}^{g(\\cdot)} = 1] - \\Pr[\\mathcal{A}^{f(\\cdot)} = 1] \\right| \\\\\n&amp;\\leq \\Pr[Z] &lt; \\frac{q^2}{2|X|}\n\\end{align*}​AdvEPRF​(A)−AdvEPRP​(A)​​≤​Pr[Ag(⋅)=1]−Pr[Af(⋅)=1]​≤Pr[Z]&lt;2∣X∣q2​​\n\n\nLemma: If ∣X∣|X|∣X∣ is sufficiently large so that q22∣X∣\\frac{q^2}{2|X|}2∣X∣q2​ is negligible, then AdvEPRP(A)Adv_E^{PRP}(\\mathcal{A})AdvEPRP​(A) is negligible implies that AdvEPRF(A)Adv_E^{PRF}(\\mathcal{A})AdvEPRF​(A) is negligible.\ni.e. Any secure PRP is also a secure PRF, if |X| is sufficiently large!\nRecall) We can make secure stream cipher from secure PRG!\nTherefore secure PRP can make secure PRF, secure PRF can make secure PRG, and secure PRG can make secure stream cipher!\nProblem) How do we make secure PRP?\nBlock Ciphers\n\nBlock ciphers are actually just PRPs with same plaintext/ciphertext size (key size can be different).\n\nM=C={0,1}lM = C = \\{ 0, 1 \\}^lM=C={0,1}l\nK={0,1}nK = \\{  0, 1 \\}^nK={0,1}n\n\nBlock cipher generates r keys (k1,…,krk_1, \\ldots, k_rk1​,…,kr​) from k∈Kk \\in Kk∈K, then compute round function mi=Rnd(ki,mi−1)m_i = Rnd(k_i, m_{i - 1})mi​=Rnd(ki​,mi−1​) (where m0=mm_0 = mm0​=m, mr=cm_r = cmr​=c).\nRound function can be anything: lookup table, xor, shift operation, ...\nData Encryption Standard (DES)\nIn 1972, NIST calls for a block cipher standard.\nIn 1974, IBM designs Lucifer.\nLucifer was later renamed to DES.\nn=56,l=64,r=16n = 56, l = 64, r = 16\nn=56,l=64,r=16\nIt was secure, but it was broken by exhaustive search in 1997.\nLater, DES was replaced by AES in 2001.\nAdvanced Encryption Standard (AES)\nAES used the similar method as DES, but with an increased key size.\nIn fact, IBM proposed 128-bit key, but NSA reduced it to 56-bit key.\nThere is a conspiracy theory that the key size was reduced to allow the NSA to break it.\nn=128/192/256,l=128,r=10/12/14n = 128/192/256, l = 128, r = 10/12/14\nn=128/192/256,l=128,r=10/12/14\nAES is designed with simple operations, with a mixture of linear and non-linear operation. (Looks like NN...?)\nSimple operations were used for faster running time and hardware efficiency. Modern CPU can compute one round of AES in one cycle!!!\nOne can increase code size (pre-compute) for faster AES, or reduce code size for smaller AES (used in low memory systems).\nAttacks on Block Ciphers\nDES have some advanced attacks: e.g. differential cryptanalysis, linear cryptanalysis.\nAES is designed to resist both attacks, and only brute force attack is known.\nThere is a best key recovery attack which can crack AES four times better than exhaustive search... But it is necessary? (2128→21262^{128} \\rightarrow 2^{126}2128→2126)\nJust run with four CPUs...\nGrover's algorithm can recover the key using quantum computer in time O(∣K∣12)O \\left( |K|^{\\frac{1}{2}} \\right)O(∣K∣21​)!\nHowever, quantum computer's one operation is very different from standard computer's operation.\nEven when quantum computers are invented, it is expected that the running time will be similar.\nStrengthening Block Ciphers?\nLet E:K×M→ME: K \\times M \\rightarrow ME:K×M→M be a block cipher.\nCan we make better block cipher by using block cipher twice? 2E:K2×M→M2E: K^2 \\times M \\rightarrow M2E:K2×M→M, c=2Ek1,k2(m)=Ek1(Ek2(m))c = 2E_{k_1, k_2}(m) = E_{k_1}(E_{k_2}(m))c=2Ek1​,k2​​(m)=Ek1​​(Ek2​​(m))\n2E is two times slower than E, but key size is squared!\nHowever, this leaks critical information: Dk1(c)=Ek2(m)D_{k_1}(c) = E_{k_2}(m)Dk1​​(c)=Ek2​​(m)\nWe can use Meet-in-the-Middle (MitM) Attack!\n\nGet a plaintext-ciphertext pair (m, c).\nBuild the table of (k2,Ek2(m))\\left( k_2, E_{k_2}(m) \\right)(k2​,Ek2​​(m)) for all k2∈Kk_2 \\in Kk2​∈K, and sort on the second column (Ek2(m)E_{k_2}(m)Ek2​​(m)).\nFor all k1∈Kk_1 \\in Kk1​∈K, check if Dk1(c)D_{k_1}(c)Dk1​​(c) is in the table.\n\n\nSpace complexity: ∣K∣⋅log⁡∣M∣|K| \\cdot \\log |M|∣K∣⋅log∣M∣\nTime complexity: ∣K∣⋅log⁡∣K∣+∣K∣⋅log⁡∣K∣|K| \\cdot \\log |K| + |K| \\cdot \\log |K|∣K∣⋅log∣K∣+∣K∣⋅log∣K∣ (building + lookup)\n\nThen how about using block cipher thrice...???\nThis is actually quite secure!!\nBecause space constraint is more challenging, we build the table of (k3,Ek3(m))\\left( k_3, E_{k_3}(m) \\right)(k3​,Ek3​​(m)) and check if Dk2(Dk1(c))D_{k_2}(D_{k_1}(c))Dk2​​(Dk1​​(c)) is in the table.\nTime complexity is larger than ∣K∣2|K|^2∣K∣2!\nTriple-DES was actually used (and it's standard!) when people were migrating to AES.\nYou could use existing DES hardware to achieve AES-level security!\n","categories":["SNU","4-2","현대암호학 개론"]},{"title":"Perfect Secrecy and Computational Security","url":"/posts/104/","content":"Secure Encryption\nA security definition has two components:\n\nA security guarantee: A successful attack from the adversary's point of view\nA threat model: What actions the attacker is assumed able to carry out\nWe cannot prove scheme is secure if we just assume adversary is god (e.g. adversary can look at any information)\n\nMost modern cryptographic constructions cannot be proven secure unconditionally. (Mostly because of efficiency)\nWhy not simply assume that the scheme itself is secure?\n\nWe want to use existing assumptions because the reliability of the assumption that the scheme itself is secure is too low. (Why do we have to assume this? It is actually a realistic assumption?)\nWe want to make assumption as simple as possible; Most assumption are not even related to cryptography!\n\nSecurity guarantees\n\nIt should be impossible for an attacker to recover the key.\nIt should be impossible for an attacker to recover the plaintext from the ciphertext.\nIt should be impossible for an attacker to recover any character of the plaintext from the ciphertext.\nRegardless of any information an attacker already has (i.e. more than just plain ciphertext), a ciphertext should leak no additional information about the underlying plaintext. (The best assumption!)\n\nThreat models\n\nCiphertext-only attack: The adversary just observes ciphertext(s) and attempts to determine information about the underlying plaintext(s).\nKnown-plaintext attack: The adversary is able to learn plaintext/ciphertext pair(s) generated using an unknown key. The adversary aims to deduce information about the underlying plaintext of some other ciphertext produced using the same key.\nChosen-plaintext attack (CPA): The adversary can obtain plaintext/ciphertext pairs for plaintexts of its choice.\nIn fact, it was very common in WW2!\ne.g. If you leak fake information about Korea, you are likely to get ciphertext about Korea.\nChosen-ciphertext attack (CCA): The adversary is additionally able to obtain some information about the decryption of ciphertexts of its choice. The adversary's aim is to learn information about the underlying plaintext of some other ciphertext under the same key.\nActually this is quite realistic threat model! e.g. Some schemes return error when ciphertext is invalid. This method has successfully attacked some CPA-secure schemes!\n\nPerfect secrecy\nConsider a ciphertext-only attack. (i.e. Attacker sees only a single ciphertext)\nShanon's definition (1949): An encryption scheme E=(Gen,Enc,Dec)\\mathcal{E} = (Gen, Enc, Dec)E=(Gen,Enc,Dec) with message space MMM is perfectly secret if and only if the following holds for all message m0,m1∈Mm_0, m_1 \\in Mm0​,m1​∈M and ciphertext c∈Cc \\in Cc∈C:\nPr⁡[Enck(m0)=c]=Pr⁡[Enck(m1)=c]\\Pr[Enc_k(m_0) = c] = \\Pr[Enc_k(m_1) = c]\nPr[Enck​(m0​)=c]=Pr[Enck​(m1​)=c]\nIdea: The ciphertext should reveal no information about the plaintext!\nAn equivalent definition: An encryption scheme is perfectly secret if and only if for every probabilistic distribution for MMM, every message mmm and every ciphertext ccc in which Pr⁡[c]≠0\\Pr[c] \\neq 0Pr[c]=0 should hold the following: Pr⁡[m∣c]=Pr⁡[m]\\Pr[m|c] = \\Pr[m]Pr[m∣c]=Pr[m].\nMore intuitive?\nPerfect Indistinguishability\nAnother equivalent definition of perfect secrecy!\nHere we will introduce the concept of an &quot;experiment&quot; (or a security game).\nGenerally, a challenge ciphertext will be given to adversary, and adversary need to guess information from it.\n\n\nThe adversary A\\mathcal{A}A outputs a pair of messages m0,m1∈Mm_0, m_1 \\in Mm0​,m1​∈M.\nA key k←Gk \\leftarrow Gk←G is generated, and a uniform bit b∈{0,1}b \\in \\{0, 1\\}b∈{0,1} is chosen.\nA challenge ciphertext C←Ek(mb)C \\leftarrow E_k(m_b)C←Ek​(mb​) is computed and given to A\\mathcal{A}A.\nA\\mathcal{A}A outputs a bit b′∈{0,1}b&#x27; \\in \\{0, 1\\}b′∈{0,1}. The output of the experiment EXPA,EEXP_{\\mathcal{A,E}}EXPA,E​ is 1 if b′=bb&#x27; = bb′=b, and 0 otherwise.\n\nThe advantage of A\\mathcal{A}A is defined as AdvE(A)≔∣2⋅Pr⁡[EXPE(A)=1]−1∣Adv_{\\mathcal{E}}(\\mathcal{A}) \\coloneqq | 2 \\cdot \\Pr[EXP_{\\mathcal{E}}(\\mathcal{A}) = 1] - 1 |AdvE​(A):=∣2⋅Pr[EXPE​(A)=1]−1∣.\nIdea: If probability is not 0.5, adversary got information from ciphertext! We want the range of advantage is between 0 and 1, so we multiply by two.\nDef: An encryption scheme E\\mathcal{E}E is perfectly indistinguishable if ∀A,AdvE(A)=0\\forall \\mathcal{A}, Adv_{\\mathcal{E}}(\\mathcal{A}) = 0∀A,AdvE​(A)=0.\nOTP (One-Time Pad)\nIn fact, we are already using perfectly secret scheme!\n\nM=K=C={0,1}lM = K = C = \\{ 0, 1 \\}^lM=K=C={0,1}l (set of all binary strings of length lll)\nGenGenGen: sample a key k∈Kk \\in Kk∈K uniformly at random.\nEnck(m)=k⊕mEnc_k(m) = k \\oplus mEnck​(m)=k⊕m\nDeck(c)=k⊕cDec_k(c) = k \\oplus cDeck​(c)=k⊕c\n\nOTP is perfectly secret against a single ciphertext attack!\nObviously ∀m∈M,∀c∈C,Pr⁡[Enck(m)=c]=2−l\\forall m \\in M, \\forall c \\in C, \\Pr[Enc_k(m) = c] = 2^{-l}∀m∈M,∀c∈C,Pr[Enck​(m)=c]=2−l\nProblem) Perfectly secret scheme is not usable...\nIt has been proven that if a scheme is perfectly secret, ∣K∣≥∣M∣|K| \\geq |M|∣K∣≥∣M∣ should hold.\nKey management becomes very difficult... In fact sending key is harder than sending message! Why are we using it then..?\nComputational security\nMotivation\nPerfect secrecy is an information-theoretic security notion that requires that absolutely no information about an encrypted message is leaked, even to an adversary with unlimited computational power.\nHowever, real world doesn't require perfect secrecy.\n\nReal adversary doesn't have unlimited power.\nReal security should be fast enough.\n\nWe need a compromise solution! (The difference between math and engineering?)\nConcrete/Asymptotic Definition\nComputational security relax two definitions:\n\nSecurity is only guaranteed against efficient adversaries that run for some feasible amount of time.\nAdversary can potentially succeed (i.e., security can fail) with some negligible probability.\n\nFor example, any adversary running for time at most ttt succeeds in breaking the scheme with probability at most ϵ\\epsilonϵ for some (t,ϵ)(t, \\epsilon)(t,ϵ).\n\nConcrete approach: Explicity bound the maximum success probability of an adversary running for some specified amount of time.\ne.g. No adversary running for at most t=280t = 2^{80}t=280 cycles can succeed in breaking the scheme with probability better than ϵ=2−60\\epsilon = 2^{-60}ϵ=2−60.\nUsually we consider tϵ\\frac{t}{\\epsilon}ϵt​, so this is very secure scheme! (140 bit)\nAsymptotic approach: Introduce a security parameter nnn (or λ\\lambdaλ), and bound probability and running time using asymptotic notation.\nEfficient adversaries are probabilistic algorithms running in polynomial time in nnn.\nNegligible probability is smaller than any inverse polynomial in nnn. (e.g. inverse of super-poly, such as 12n\\frac{1}{2^n}2n1​)\n\nIn asymptotic approach, we also call efficient adversaries have probabilistic polynomial time algorithm (PPTA).\nIndistinguishability Experiment\nNow we can define which scheme has indistinguishable encryptions (also called the scheme provides indistinguishability)!\n\nFor a private-key encryption scheme E=(G,E,D)\\mathcal{E}=(G,E,D)E=(G,E,D) and an efficient (PPTA) adversary A\\mathcal{A}A,\n\nThe adversary A\\mathcal{A}A outputs a pair of messages m0,m1∈Mm_0, m_1 \\in Mm0​,m1​∈M.\nA key k←Gen(1λ)k \\leftarrow Gen(1^{\\lambda})k←Gen(1λ) is generated, and a uniform bit b∈{0,1}b \\in \\{0, 1\\}b∈{0,1} is chosen.\nA ciphertext C←Enck(mb)C \\leftarrow Enc_k(m_b)C←Enck​(mb​) is computed and given to A\\mathcal{A}A.\nA\\mathcal{A}A outputs a bit b′∈{0,1}b&#x27; \\in \\{0, 1\\}b′∈{0,1}. The output of the experiment EXPA,EEXP_{\\mathcal{A,E}}EXPA,E​ is 1 if b′=bb&#x27; = bb′=b, and 0 otherwise.\n\nThe advantage of A\\mathcal{A}A is defined as AdvE(A)≔∣2⋅Pr⁡[EXPE(A)=1]−1∣Adv_{\\mathcal{E}}(\\mathcal{A}) \\coloneqq | 2 \\cdot \\Pr[EXP_{\\mathcal{E}}(\\mathcal{A}) = 1] - 1 |AdvE​(A):=∣2⋅Pr[EXPE​(A)=1]−1∣.\nDef: An encryption scheme E\\mathcal{E}E has indistinguishable encryptions if for any efficient A\\mathcal{A}A, AdvE(A)Adv_{\\mathcal{E}}(\\mathcal{A})AdvE​(A) is negligible in λ\\lambdaλ.\n\nBut in fact, we normaly use this equivalent formulation.\nIdea: We want to fix adversary, and run different experiments.\nIf adversary reacts differently, adversary got information!\nThese experiments are almost the same as previous one, except that now bit b∈{0,1}b \\in \\{0, 1\\}b∈{0,1} is fixed. (0 for EXP0EXP_0EXP0​, 1 for EXP1EXP_1EXP1​)\nOutput of the each experiment is b′b&#x27;b′, and it is denoted as EXP0,E(A)EXP_{0,\\mathcal{E}}(\\mathcal{A})EXP0,E​(A) and EXP1,E(A)EXP_{1,\\mathcal{E}}(\\mathcal{A})EXP1,E​(A).\nThe advantage of A\\mathcal{A}A is defined as AdvE(A)≔∣Pr⁡[EXP0,E(A)=1]−Pr⁡[EXP1,E(A)=1]∣Adv_{\\mathcal{E}}(\\mathcal{A}) \\coloneqq | \\Pr[EXP_{0,\\mathcal{E}}(\\mathcal{A}) = 1] - \\Pr[EXP_{1,\\mathcal{E}}(\\mathcal{A}) = 1] |AdvE​(A):=∣Pr[EXP0,E​(A)=1]−Pr[EXP1,E​(A)=1]∣.\nDef: An encryption scheme E\\mathcal{E}E has indistinguishable encryptions if for any efficient A\\mathcal{A}A, AdvE(A)Adv_{\\mathcal{E}}(\\mathcal{A})AdvE​(A) is negligible in λ\\lambdaλ.\nProof\n\nLet Pr⁡[EXP0,E(A)=1]=p0\\Pr[EXP_{0,\\mathcal{E}}(\\mathcal{A}) = 1] = p_0Pr[EXP0,E​(A)=1]=p0​, and Pr⁡[EXP1,E(A)=1]=p1\\Pr[EXP_{1,\\mathcal{E}}(\\mathcal{A}) = 1] = p_1Pr[EXP1,E​(A)=1]=p1​.\n∣2⋅Pr⁡[EXPE(A)=1]−1∣=∣2⋅(12Pr⁡[EXP0,E(A)=0]+12Pr⁡[EXP1,E(A)=1])−1∣=∣2⋅(1−p02+p12)−1∣=∣p1−p0∣=∣p0−p1∣=∣Pr⁡[EXP0,E(A)=1]−Pr⁡[EXP1,E(A)=1]∣\\begin{align*}\n| 2 \\cdot \\Pr[EXP_{\\mathcal{E}}(\\mathcal{A}) = 1] - 1 | &amp;= | 2 \\cdot \\left( \\frac{1}{2}\\Pr[EXP_{\\mathcal{0, E}}(\\mathcal{A}) = 0] + \\frac{1}{2}\\Pr[EXP_{\\mathcal{1, E}}(\\mathcal{A}) = 1] \\right) - 1 | \\\\\n&amp;= | 2 \\cdot \\left( \\frac{1 - p_0}{2} + \\frac{p_1}{2} \\right) - 1 | \\\\\n&amp;= | p_1 - p_0 | \\\\\n&amp;= | p_0 - p_1 | \\\\\n&amp;= | \\Pr[EXP_{0,\\mathcal{E}}(\\mathcal{A}) = 1] - \\Pr[EXP_{1,\\mathcal{E}}(\\mathcal{A}) = 1] |\n\\end{align*}∣2⋅Pr[EXPE​(A)=1]−1∣​=∣2⋅(21​Pr[EXP0,E​(A)=0]+21​Pr[EXP1,E​(A)=1])−1∣=∣2⋅(21−p0​​+2p1​​)−1∣=∣p1​−p0​∣=∣p0​−p1​∣=∣Pr[EXP0,E​(A)=1]−Pr[EXP1,E​(A)=1]∣​\n\n\nc.f. Why do we want to prove indistinguishability? Actually we want to prove semantic security.\nA scheme is semantically secure if any efficient adversary that is given the ciphertext and the message's length cannot determine any partial information on the message with probability non-negligibly higher than all other efficient adversary that is only given the message's length.\nBut directly proving sematic security is really hard... Instead, we prove that a scheme is semantically secure if and only if a scheme has indistinguishable encryption, then we prove that a scheme has indistinguishable encryption.\nPseudorandom Generators (PRG)\nPRG is an efficient and deterministic algorithm for transforming a short, uniform string (called a seed) into a longer, &quot;uniform-looking&quot; (a.k.a. &quot;pseudorandom&quot;) output string.\nIf PRG exists, we can use a small amount of true randomness to generate a large amount of pseudorandomness.\nWhy do we use this? We want a long true random string. However, generating true random string is often difficult and slow.\nObviously, information theoretically(?), it is impossible to get a true random string from shorter true random string.\nBut we can get pseudorandom string computationally!\n\nHistorical approach: Use statistical tests (e.g. each n-th bit should be equal to 1 with probability 0.5, numbers of 0's and 1's in the string are approximately the same)\nCryptographic approach: Use computational security - A good PRG should pass all efficient statistical tests!\n\nDefinition of secure PRG\nPRG is a function G:{0,1}n→{0,1}lG: \\{ 0, 1 \\}^n \\rightarrow \\{ 0, 1 \\}^lG:{0,1}n→{0,1}l for some n&lt;ln &lt; ln&lt;l that is efficiently computable by a deterministic algorithm.\nIdea) Pseudorandom should look like true random!\nIntuitively, {G(s):s←{0,1}n}\\{ G(s): s \\leftarrow \\{ 0, 1 \\}^n \\}{G(s):s←{0,1}n} should be indistinguishable from {r:r←{0,1}l}\\{ r: r \\leftarrow \\{ 0, 1 \\}^l \\}{r:r←{0,1}l}.\n\nG(s)G(s)G(s) shouldn't give any information about sss.\nAdversary shouldn't distinguish whether a given l-bit string is G(s)G(s)G(s) or rrr.\n\n\nDef: Let G:{0,1}n→{0,1}lG: \\{ 0, 1 \\}^n \\rightarrow \\{ 0, 1 \\}^lG:{0,1}n→{0,1}l be a PRG. For any distinguisher (adversary) A\\mathcal{A}A, its advantage is defined as AdvG(A)≔∣Pr⁡s[A(G(s))=1]−Pr⁡r[A(r)=1]∣Adv_G(\\mathcal{A}) \\coloneqq \\left| \\Pr_s[ \\mathcal{A}(G(s)) = 1 ] - \\Pr_r[ \\mathcal{A}(r) = 1 ] \\right|AdvG​(A):=∣Prs​[A(G(s))=1]−Prr​[A(r)=1]∣. We say that GGG is secure (a.k.a. pseudorandom) if for any efficient A\\mathcal{A}A, AdvG(A)Adv_G(\\mathcal{A})AdvG​(A) is negligible.\nPredictable PRG\nDef: Let G:{0,1}n→{0,1}lG: \\{ 0, 1 \\}^n \\rightarrow \\{ 0, 1 \\}^lG:{0,1}n→{0,1}l be a PRG. We say tha GGG is predictable if there exists an efficient algorithm A\\mathcal{A}A and i&lt;li &lt; li&lt;l such that\nPr⁡s←{0,1}n[A(G(s)[0,1,…,i−1])=G(s)[i]]&gt;12+ϵ\\Pr_{s \\leftarrow \\{ 0, 1 \\}^n}[ \\mathcal{A}(G(s) [0, 1, \\ldots, i-1]) = G(s) [i] ] &gt; \\frac{1}{2} + \\epsilon\ns←{0,1}nPr​[A(G(s)[0,1,…,i−1])=G(s)[i]]&gt;21​+ϵ\nfor some non-negligible ϵ&gt;0\\epsilon &gt; 0ϵ&gt;0.\ni.e. GGG is predictable if there exists an adversary that can predict the (i+1)th bit when given the first i bits of G(s)G(s)G(s).\nSince predictable PRG is not secure (adversary can be trivially made), secure PRGs are unpredictable.\nBut are unpredictable PRGs secure?\nIt's true! (So unpredictable PRG and secure PRG are equivalent definition!) However proof is very complex...\nProof\n\nLet's assume that GGG is not secure, i.e. there exists an adversary A\\mathcal{A}A and non-negligible ϵ&gt;0\\epsilon &gt; 0ϵ&gt;0 such that\n∣Pr⁡s[A(G(s))=1]−Pr⁡r[A(r)=1]∣&gt;ϵ.\\left| \\Pr_s[ \\mathcal{A}(G(s)) = 1 ] - \\Pr_r[ \\mathcal{A}(r) = 1 ] \\right| &gt; \\epsilon.\n​sPr​[A(G(s))=1]−rPr​[A(r)=1]​&gt;ϵ.\nWe define the hybrid distribution HiH_iHi​ for 0≤i≤l0 \\leq i \\leq l0≤i≤l, where the first i bits of HiH_iHi​ are generated from G(s)G(s)G(s), and the remaining l−il-il−i bits are generated uniformly random.\nPr⁡s[A(G(s))=1]−Pr⁡r[A(r)=1]=Pr⁡[A(Hl)=1]−Pr⁡[A(H0)=1]=∑i=1l(Pr⁡[A(Hi)=1]−Pr⁡[A(Hi−1)=1])∴∣Pr⁡s[A(G(s))=1]−Pr⁡r[A(r)=1]∣≤∑i=1l∣Pr⁡[A(Hi)=1]−Pr⁡[A(Hi−1)=1]∣∴∃j(1≤j≤l) s.t. ∣Pr⁡[A(Hj)=1]−Pr⁡[A(Hj−1)=1]∣&gt;ϵl.\\begin{align*}\n\\Pr_s[ \\mathcal{A}(G(s)) = 1 ] - \\Pr_r[ \\mathcal{A}(r) = 1 ] &amp;= \\Pr[ \\mathcal{A}(H_l) = 1 ] - \\Pr[ \\mathcal{A}(H_0) = 1 ] \\\\\n&amp;= \\sum_{i=1}^l \\left( \\Pr[ \\mathcal{A}(H_i) = 1 ] - \\Pr[ \\mathcal{A}(H_{i-1}) = 1 ] \\right) \\\\\n\\therefore \\left| \\Pr_s[ \\mathcal{A}(G(s)) = 1 ] - \\Pr_r[ \\mathcal{A}(r) = 1 ] \\right| &amp;\\leq \\sum_{i=1}^l \\left| \\Pr[ \\mathcal{A}(H_i) = 1 ] - \\Pr[ \\mathcal{A}(H_{i-1}) = 1 ] \\right| \\\\\n\\therefore \\exists j (1 \\leq j \\leq l) \\  s.t. \\ &amp; \\left| \\Pr[ \\mathcal{A}(H_j) = 1 ] - \\Pr[ \\mathcal{A}(H_{j-1}) = 1 ] \\right| &gt; \\frac{\\epsilon}{l}.\n\\end{align*}sPr​[A(G(s))=1]−rPr​[A(r)=1]∴​sPr​[A(G(s))=1]−rPr​[A(r)=1]​∴∃j(1≤j≤l) s.t. ​=Pr[A(Hl​)=1]−Pr[A(H0​)=1]=i=1∑l​(Pr[A(Hi​)=1]−Pr[A(Hi−1​)=1])≤i=1∑l​∣Pr[A(Hi​)=1]−Pr[A(Hi−1​)=1]∣∣Pr[A(Hj​)=1]−Pr[A(Hj−1​)=1]∣&gt;lϵ​.​\nWithout loss of generality, assume Pr⁡[A(Hj−1)=1]−Pr⁡[A(Hj)=1]&gt;ϵl\\Pr[ \\mathcal{A}(H_{j-1}) = 1 ] - \\Pr[ \\mathcal{A}(H_j) = 1 ] &gt; \\frac{\\epsilon}{l}Pr[A(Hj−1​)=1]−Pr[A(Hj​)=1]&gt;lϵ​.\nWe define an adversary B\\mathcal{B}B for predictable PRG experiment.\n\nGet the first j−1j-1j−1 bits of G(s)G(s)G(s), denoted as ppp. (prefix)\nGenerate random bit b∈{0,1}b \\in \\{ 0, 1\\}b∈{0,1} and uniformly random l−jl-jl−j bit string rrr.\nCreate string ω≔p∥b∥r\\omega \\coloneqq p \\Vert b \\Vert rω:=p∥b∥r by appending strings.\nPerform secure PRG experiment and get the output b′≔A(ω)b&#x27; \\coloneqq \\mathcal{A}(\\omega)b′:=A(ω).\nIf b′=1b&#x27; = 1b′=1 (i.e. adversary output that this was a random string), output 1−b1-b1−b.\nIf b′=0b&#x27; = 0b′=0 (i.e. adversary output that this was a PRG), output bbb.\n\nWe'll prove that B\\mathcal{B}B can predict PRG with a non-negligible probability.\nLet B≔G(s)[j]B \\coloneqq G(s) [j]B:=G(s)[j] (the real j-th bit of the PRG), pj≔Pr⁡[A(Hj)=1]p_j \\coloneqq \\Pr[ \\mathcal{A}(H_j) = 1 ]pj​:=Pr[A(Hj​)=1], pj−1≔Pr⁡[A(Hj−1)=1]p_{j-1} \\coloneqq \\Pr[ \\mathcal{A}(H_{j-1}) = 1 ]pj−1​:=Pr[A(Hj−1​)=1].\nIf b=Bb = Bb=B, ω\\omegaω has the same distribution as HjH_jHj​. Therefore, Pr⁡[B(p)=B∣b=B]=Pr⁡[A(Hj)=0]=1−pj\\Pr[ \\mathcal{B}(p) = B | b = B ] = \\Pr[ \\mathcal{A}(H_j) = 0 ] = 1 - p_jPr[B(p)=B∣b=B]=Pr[A(Hj​)=0]=1−pj​.\nIf b≠Bb \\neq Bb=B, ω\\omegaω has the same distribution as Hj−1H_{j-1}Hj−1​. Therefore, Pr⁡[B(p)=B∣b≠B]=Pr⁡[A(Hj−1)=1]=pj−1\\Pr[ \\mathcal{B}(p) = B | b \\neq B ] = \\Pr[ \\mathcal{A}(H_{j-1}) = 1 ] = p_{j-1}Pr[B(p)=B∣b=B]=Pr[A(Hj−1​)=1]=pj−1​.\n∴Pr⁡s←{0,1}n[B(G(s)[0,1,…,j−1])=G(s)[j]]=Pr⁡[B(p)=B]=12⋅(1−pj)+12⋅pj−1=12+12(pj−1−pj)&gt;12+ϵ2l.\\begin{align*}\n\\therefore \\Pr_{s \\leftarrow \\{ 0, 1 \\}^n}[ \\mathcal{B}(G(s) [0, 1, \\ldots, j-1]) = G(s) [j] ] &amp;= \\Pr[ \\mathcal{B}(p) = B ] \\\\\n&amp;= \\frac{1}{2} \\cdot (1 - p_j) + \\frac{1}{2} \\cdot p_{j-1} \\\\\n&amp;= \\frac{1}{2} + \\frac{1}{2} (p_{j-1} - p_j) \\\\\n&amp;&gt; \\frac{1}{2} + \\frac{\\epsilon}{2l}.\n\\end{align*}∴s←{0,1}nPr​[B(G(s)[0,1,…,j−1])=G(s)[j]]​=Pr[B(p)=B]=21​⋅(1−pj​)+21​⋅pj−1​=21​+21​(pj−1​−pj​)&gt;21​+2lϵ​.​\n(If Pr⁡[A(Hj−1)=1]−Pr⁡[A(Hj)=1]&lt;ϵl\\Pr[ \\mathcal{A}(H_{j-1}) = 1 ] - \\Pr[ \\mathcal{A}(H_j) = 1 ] &lt; \\frac{\\epsilon}{l}Pr[A(Hj−1​)=1]−Pr[A(Hj​)=1]&lt;lϵ​, invert the output.)\nTherefore, B\\mathcal{B}B can predict PRG with a non-negligible probability ϵ2l\\frac{\\epsilon}{2l}2lϵ​.\n(You can also repeat this process and take the majority vote to further increase the probability.)\nSince insecure PRGs are predictable, unpredictable PRGs are secure!\n\n\nThe real problem) Does secure PRG exists?\nIn fact we don't know...!\nIt is a troublesome problem in cryptography; We can prove that if A is secure, then B is secure. However, it's often not proven that a specific A is secure!\nWe have a modern PRG that doesn't have any known attack better than exhaustive search, but we don't know whether it is secure or not.\nHowever, because predictable PRGs are not secure, famous PRG algorithms are not secure!\ne.g. glibc random is not secure!\nint random() &#123;  // Assume that r[i] has previous return values of random(),  // and i is the number of times random() is called.  r[i] = (r[i - 3] + r[i - 31]) % (1 &lt;&lt; 32);  return r[i] &gt;&gt; 1;&#125;\nStream Ciphers\nProblem of OTP: ∣k∣=∣m∣|k| = |m|∣k∣=∣m∣\nStream Ciphers: Make OTP practical by replacing random key with pseudorandom key!\n\nLet G:{0,1}n→{0,1}lG: \\{ 0, 1 \\}^n \\rightarrow \\{ 0, 1 \\}^lG:{0,1}n→{0,1}l be a PRG.\nK={0,1}n,M={0,1}lK = \\{ 0, 1 \\}^n, M = \\{ 0, 1 \\}^lK={0,1}n,M={0,1}l\nGenGenGen samples a uniform random key s∈Ks \\in Ks∈K.\nEncs(m)≔G(s)⊕mEnc_s(m) \\coloneqq G(s) \\oplus mEncs​(m):=G(s)⊕m\nDecs(c)≔G(s)⊕cDec_s(c) \\coloneqq G(s) \\oplus cDecs​(c):=G(s)⊕c\n\nSince ∣K∣&lt;∣M∣|K| &lt; |M|∣K∣&lt;∣M∣, stream cipher cannot be perfectly secure.\nHowever, if PRG is secure, stream cipher is semantically secure!\nProofs by reduction\nHow do we prove that a given construction (e.g. encryption scheme) is secure?\nWe first assume that low-level cryptographic primitive is secure, then prove that the given construction based on that primitive is secure.\nWe can prove this by presenting a reduction showing how to transform any efficient adversary A\\mathcal{A}A that succeeds in solving/breaking the given construction into an efficient algorithm $\\mathcal{B} that solves problem/primitive.\na.k.a. If construction is not secure, then primitive is not secure.\na.k.a. If primitive is secure, then construction is secure!\nOf course, we don't know if given stream cipher is secure...\nBut we can prove that if PRG is secure, then stream cipher is semantically secure.\nSince we don't know if modern PRG is broken, so maybe modern stream cipher is secure..?\nRecall) Semantically secure scheme is equivalent to indistinguishable scheme.\nTheorem: Let G:{0,1}n→{0,1}lG: \\{ 0, 1 \\}^n \\rightarrow \\{ 0, 1 \\}^lG:{0,1}n→{0,1}l be a secure PRG. Then, the stream cipher E\\mathcal{E}E derived from GGG is semantically secure. More specifically, for any adversary A\\mathcal{A}A for stream cipher E\\mathcal{E}E, there exists a PRG adversary B\\mathcal{B}B such that\nAdvG(B)≥12AdvE(A)Adv_G(\\mathcal{B}) \\geq \\frac{1}{2} Adv_\\mathcal{E}(\\mathcal{A})\nAdvG​(B)≥21​AdvE​(A)\nThis is slightly stronger than what we wanted to prove: If there exists an adversary for stream cipher, there exists an adversary for PRG.\nProof\n\nLet's assume that A\\mathcal{A}A is an adversary for a stream cipher E\\mathcal{E}E which uses PRG G(s)G(s)G(s).\nThen we can define adversary B\\mathcal{B}B for PRG GGG.\n\nA challenge string ω\\omegaω is given to B\\mathcal{B}B.\nB\\mathcal{B}B gets two messages m0,m1m_0, m_1m0​,m1​ from A\\mathcal{A}A.\nB\\mathcal{B}B randomly choose b∈{0,1}b \\in \\{ 0, 1 \\}b∈{0,1}, then give ciphertext ω⊕mb\\omega \\oplus m_bω⊕mb​ back to A\\mathcal{A}A. (i.e. B\\mathcal{B}B just construct stream cipher for A\\mathcal{A}A.)\nB\\mathcal{B}B gets A\\mathcal{A}A's output b′b&#x27;b′.\nB\\mathcal{B}B outputs 0 if b=b′b = b&#x27;b=b′ (this is a PRG), and 1 if b≠b′b \\neq b&#x27;b=b′ (this is a random string).\n\nWe'll prove that B\\mathcal{B}B's advantage is half of A\\mathcal{A}A's advantage.\nIf a challenge string was a PRG, this is a stream cipher experiment from A\\mathcal{A}A's point of view.\n∴Pr⁡s[B(G(s))=1]=Pr⁡[b′=1∣b=0]+Pr⁡[b′=0∣b=1]=12⋅Pr⁡[EXP0,E(A)=1]+12⋅Pr⁡[EXP1,E(A)=0]=12⋅(Pr⁡[EXP0,E(A)=1]+1−Pr⁡[EXP1,E(A)=1])=12+12⋅(Pr⁡[EXP0,E(A)=1]−Pr⁡[EXP1,E(A)=1])\\begin{align*}\n\\therefore \\Pr_s[ \\mathcal{B}(G(s)) = 1 ] &amp;= \\Pr[b&#x27; = 1 | b = 0] + \\Pr[b&#x27; = 0 | b = 1] \\\\\n&amp;= \\frac{1}{2} \\cdot \\Pr[EXP_{0,\\mathcal{E}}(\\mathcal{A}) = 1] + \\frac{1}{2} \\cdot \\Pr[EXP_{1,\\mathcal{E}}(\\mathcal{A}) = 0] \\\\\n&amp;= \\frac{1}{2} \\cdot (\\Pr[EXP_{0,\\mathcal{E}}(\\mathcal{A}) = 1] + 1 - \\Pr[EXP_{1,\\mathcal{E}}(\\mathcal{A}) = 1]) \\\\\n&amp;= \\frac{1}{2} + \\frac{1}{2} \\cdot (\\Pr[EXP_{0,\\mathcal{E}}(\\mathcal{A}) = 1] - \\Pr[EXP_{1,\\mathcal{E}}(\\mathcal{A}) = 1])\n\\end{align*}∴sPr​[B(G(s))=1]​=Pr[b′=1∣b=0]+Pr[b′=0∣b=1]=21​⋅Pr[EXP0,E​(A)=1]+21​⋅Pr[EXP1,E​(A)=0]=21​⋅(Pr[EXP0,E​(A)=1]+1−Pr[EXP1,E​(A)=1])=21​+21​⋅(Pr[EXP0,E​(A)=1]−Pr[EXP1,E​(A)=1])​\nIf a challenge string was a random string, ω⊕mb\\omega \\oplus m_bω⊕mb​ is also a random string. ∴Pr⁡r[B(r)=1]=12\\therefore \\Pr_r[ \\mathcal{B}(r) = 1 ] = \\frac{1}{2}∴Prr​[B(r)=1]=21​\n∴AdvG(B)=∣Pr⁡s[B(G(s))=1]−Pr⁡r[B(r)=1]∣=12⋅∣Pr⁡[EXP0,E(A)=1]−Pr⁡[EXP1,E(A)=1]∣=12AdvE(A)\\begin{align*}\n\\therefore Adv_G(\\mathcal{B}) &amp;= \\left| \\Pr_s[ \\mathcal{B}(G(s)) = 1 ] - \\Pr_r[ \\mathcal{B}(r) = 1 ] \\right| \\\\\n&amp;= \\frac{1}{2} \\cdot \\left| \\Pr[EXP_{0,\\mathcal{E}}(\\mathcal{A}) = 1] - \\Pr[EXP_{1,\\mathcal{E}}(\\mathcal{A}) = 1] \\right| \\\\\n&amp;= \\frac{1}{2} Adv_{\\mathcal{E}}(\\mathcal{A})\n\\end{align*}∴AdvG​(B)​=​sPr​[B(G(s))=1]−rPr​[B(r)=1]​=21​⋅∣Pr[EXP0,E​(A)=1]−Pr[EXP1,E​(A)=1]∣=21​AdvE​(A)​\n\n\n","categories":["SNU","4-2","현대암호학 개론"]},{"title":"CPA Security and Private-key Encryptions","url":"/posts/106/","content":"Security for multiple encryptions\nGenerally, a secret key is used more than once.\nWe cannot assume single ciphertext attack!\nWe'll now assume chosen-plaintext attacks (CPA).\nAdversary can control over what the honest parties encrypt.\nThe adversary can influence a party to encrypt certain messages and observe the resulting ciphertexts.\nLater the attacker observes a ciphertext of some unknown meessage.\nCPA-security\n\n\nA key k←Genk \\leftarrow Genk←Gen is generated.\nThe adversary A\\mathcal{A}A is given oracle access to Enck(⋅)Enc_k(\\cdot)Enck​(⋅). i.e. A\\mathcal{A}A can receive any number of ciphertexts c∗←Enck(m∗)c^* \\leftarrow Enc_k(m^*)c∗←Enck​(m∗) for messages m∗m^*m∗ of its choice.\nA\\mathcal{A}A outputs a pair of messages m0,m1m_0, m_1m0​,m1​. A ciphertext c←Enck(mb)c \\leftarrow Enc_k(m_b)c←Enck​(mb​) is computed and given to A\\mathcal{A}A.\nA\\mathcal{A}A continues to have oracle access to Enck(⋅)Enc_k(\\cdot)Enck​(⋅). Then it outputs a bit b′b&#x27;b′.\n\nThe advantage of A\\mathcal{A}A is defined to be AdvE(A)=∣Pr⁡[EXP0,E(A)=1]−Pr⁡[EXP1,E(A)=1]∣Adv_{\\mathcal{E}}(\\mathcal{A}) = |\\Pr[EXP_{0, \\mathcal{E}}(\\mathcal{A}) = 1] - \\Pr[EXP_{1, \\mathcal{E}}(\\mathcal{A}) = 1]|AdvE​(A)=∣Pr[EXP0,E​(A)=1]−Pr[EXP1,E​(A)=1]∣.\nDef: A private-key encryption scheme E\\mathcal{E}E is indistinguishable under chosen plaintext atack, or simply CPA-secure, if for any efficient A\\mathcal{A}A, AdvE(A)Adv_{\\mathcal{E}}(\\mathcal{A})AdvE​(A) is negligible.\nNote that every CPA-secure scheme should be probabilistic!\nIf scheme is deterministic, A\\mathcal{A}A can just query m0,m1m_0, m_1m0​,m1​.\nCPA-security for Multiple Encryptions\nWe can extend CPA-security definition to the case of multiple encryptions.\nInstead of single challenge ciphertext ccc, adversary now get lists of challenge ciphertexts, using lists of plaintexts.\n\nWe now give the adversary access to left-or-right oracle LRk,b(⋅,⋅)LR_{k,b}(\\cdot, \\cdot)LRk,b​(⋅,⋅). The oracle, on input a pair of messages (m0,m1)(m_0, m_1)(m0​,m1​), computes and returns c←Enck(mb)c \\leftarrow Enc_k(m_b)c←Enck​(mb​).\nThis is stronger oracle than Enck(⋅)Enc_k(\\cdot)Enck​(⋅), because we can get Enck(m)Enc_k(m)Enck​(m) with left-or-right oracle by querying the oracle with (m,m)(m, m)(m,m).\n\nA key k←Genk \\leftarrow Genk←Gen is generated.\nThe adversary A\\mathcal{A}A is given oracle access to LRk,b(⋅,⋅)LR_{k,b}(\\cdot, \\cdot)LRk,b​(⋅,⋅). i.e. A\\mathcal{A}A can receive any number of ciphertext pairs (ci,0,ci,1)←LRk,b(mi,0,mi,1)(c_{i,0}, c_{i,1}) \\leftarrow LR_{k,b}(m_{i,0}, m_{i,1})(ci,0​,ci,1​)←LRk,b​(mi,0​,mi,1​) for message pairs (mi,0,mi,1)(m_{i,0}, m_{i,1})(mi,0​,mi,1​) of its choice.\nA\\mathcal{A}A outputs a bit b′b&#x27;b′.\n\nThe advantage of A\\mathcal{A}A is defined to be AdvE(A)=∣Pr⁡[EXP0,E(A)=1]−Pr⁡[EXP1,E(A)=1]∣Adv_{\\mathcal{E}}(\\mathcal{A}) = |\\Pr[EXP_{0, \\mathcal{E}}(\\mathcal{A}) = 1] - \\Pr[EXP_{1, \\mathcal{E}}(\\mathcal{A}) = 1]|AdvE​(A)=∣Pr[EXP0,E​(A)=1]−Pr[EXP1,E​(A)=1]∣.\nDef: A private-key encryption scheme E\\mathcal{E}E is CPA-secure for multiple encryptions, if for any efficient A\\mathcal{A}A, AdvE(A)Adv_{\\mathcal{E}}(\\mathcal{A})AdvE​(A) is negligible.\nSince left-or-right oracle is stronger, if E\\mathcal{E}E is CPA-secure for multiple encryptions, E\\mathcal{E}E is CPA-secure.\nHowever, it is also known that if E\\mathcal{E}E is CPA-secure, E\\mathcal{E}E is CPA-secure for multiple encryptions! (Although this is very hard to prove...)\nTherefore, two definitions are equivalent!\nProof\n\nLet's assume that E\\mathcal{E}E is not CPA-secure for multiple encryptions, i.e. there exists an adversary A\\mathcal{A}A and non-negligible ϵ&gt;0\\epsilon &gt; 0ϵ&gt;0 such that\n∣Pr⁡[EXP0,Emulti(A)=1]−Pr⁡[EXP1,Emulti(A)=1]∣&gt;ϵ.|\\Pr[EXP_{0, \\mathcal{E}}^{multi}(\\mathcal{A}) = 1] - \\Pr[EXP_{1, \\mathcal{E}}^{multi}(\\mathcal{A}) = 1]| &gt; \\epsilon.\n∣Pr[EXP0,Emulti​(A)=1]−Pr[EXP1,Emulti​(A)=1]∣&gt;ϵ.\nLet QQQ is the total number of queries that A\\mathcal{A}A asks.\nWe define the hybrid experimetns HiH_iHi​ for 0≤i≤Q0 \\leq i \\leq Q0≤i≤Q, where HiH_iHi​ gives Enck(m0)Enc_k(m_0)Enck​(m0​) for iii queries, then gives Enck(m1)Enc_k(m_1)Enck​(m1​) for the remaining Q−iQ-iQ−i queries.\nLet pi≔Pr⁡[EXPHi,Emulti(A)=1]p_i \\coloneqq \\Pr[EXP_{H_i, \\mathcal{E}}^{multi}(\\mathcal{A}) = 1]pi​:=Pr[EXPHi​,Emulti​(A)=1].\nAdvE(A)=∣pQ−p0∣≤∑i=1Q∣pi−pi−1∣∴∃j(1≤j≤Q) s.t. ∣pj−pj−1∣&gt;ϵQ\\begin{align*}\nAdv_{\\mathcal{E}}(\\mathcal{A}) = |p_Q - p_0| &amp;\\leq \\sum_{i = 1}^Q |p_i - p_{i-1}| \\\\\n\\therefore \\exists j (1 \\leq j \\leq Q) \\  s.t. \\ &amp; |p_j - p_{j-1}| &gt; \\frac{\\epsilon}{Q}\n\\end{align*}AdvE​(A)=∣pQ​−p0​∣∴∃j(1≤j≤Q) s.t. ​≤i=1∑Q​∣pi​−pi−1​∣∣pj​−pj−1​∣&gt;Qϵ​​\nNow we define adversaries Bi\\mathcal{B_i}Bi​ (1≤i≤Q1 \\leq i \\leq Q1≤i≤Q) for CPA-secure experiment.\n\nGet queries (mj,0,mj,1)(m_{j,0}, m_{j,1})(mj,0​,mj,1​) from A\\mathcal{A}A.\nFor j&lt;ij &lt; ij&lt;i, give Enck(mj,0)Enc_k(m_{j,0})Enck​(mj,0​) to A\\mathcal{A}A.\nFor j=ij = ij=i, output (mj,0,mj,1)(m_{j,0}, m_{j,1})(mj,0​,mj,1​) as a challenge pair, and give the result ccc to A\\mathcal{A}A.\nFor j&gt;ij &gt; ij&gt;i, give Enck(mj,1)Enc_k(m_{j,1})Enck​(mj,1​) to A\\mathcal{A}A.\nGet b′b&#x27;b′ from A\\mathcal{A}A, and output b′b&#x27;b′.\n\nIf b=0b = 0b=0 (i.e. c=Enck(mi,0)c = Enc_k(m_{i,0})c=Enck​(mi,0​)), this is a HiH_iHi​ experiment from A\\mathcal{A}A's point of view.\n∴Pr⁡[EXP0,Esingle(Bi)=1]=pi\\therefore \\Pr[EXP_{0, \\mathcal{E}}^{single}(\\mathcal{B_i}) = 1] = p_i\n∴Pr[EXP0,Esingle​(Bi​)=1]=pi​\nIf b=1b = 1b=1 (i.e. c=Enck(mi,1)c = Enc_k(m_{i,1})c=Enck​(mi,1​)), this is a Hi−1H_{i-1}Hi−1​ experiment from A\\mathcal{A}A's point of view.\n∴Pr⁡[EXP1,Esingle(Bi)=1]=pi−1\\therefore \\Pr[EXP_{1, \\mathcal{E}}^{single}(\\mathcal{B_i}) = 1] = p_{i-1}\n∴Pr[EXP1,Esingle​(Bi​)=1]=pi−1​\n∴AdvE(Bi)=∣pi−pi−1∣∴∃j(1≤j≤Q) s.t. AdvE(Bj)&gt;ϵQ\\begin{align*}\n&amp;\\therefore Adv_{\\mathcal{E}}(\\mathcal{B_i}) = |p_i - p_{i-1}| \\\\\n&amp;\\therefore \\exists j (1 \\leq j \\leq Q) \\  s.t. \\  Adv_{\\mathcal{E}}(\\mathcal{B_j}) &gt; \\frac{\\epsilon}{Q}\n\\end{align*}​∴AdvE​(Bi​)=∣pi​−pi−1​∣∴∃j(1≤j≤Q) s.t. AdvE​(Bj​)&gt;Qϵ​​\nTherefore, Bj\\mathcal{B_j}Bj​ can distinguish E\\mathcal{E}E under chosen plaintext attack with a non-negligible probability ϵQ\\frac{\\epsilon}{Q}Qϵ​.\nSince E\\mathcal{E}E is not CPA-secure if it is not CPA-secure for multiple encryptions, E\\mathcal{E}E is CPA-secure for multiple encryptions if E\\mathcal{E}E is CPA-secure.\n\n\nplaintext length\nDefinition of secure encryptions does not require the encryption scheme to hide the plaintext length!\nAll commonly used encryption schemes reveal the plaintext length (or a close approximation).\nc.f. If the encryption scheme doesn't hide the plaintext length, the adversary must choose messages of the same length in the security game.\nIt is impossible to support arbitrary length messages while hiding all information about the plaintext length. (However encryption scheme can hide the exact length, while revealing an approximation.)\nThis is due to information theory, a set of arbitrary length plaintext is larger than a set of fixed length ciphertext. (The ciphertext has fixed length because we obviously know its length when we receive it.)\nIn many cases, revealing the plaintext length is acceptable as it is already public or is not sensitive.\nHowever, leaking the plaintext length can be problematic in some cases. (e.g. simple numberic/text data, compression)\nSolution: Pad all messages to some pre-determined length before encrypting them.\ne.g. append 1 followed by several 0's. (message and plaintext have one-to-one correspondence!)\nNow the plaintext's length is fixed and known to the adversary, but we can't get &quot;actual&quot; length of the message!\nNote that due to information theory, padded message should be longer than previous message. We cannot have one-to-one correspondence between ⋃i=1n{0,1}i\\bigcup_{i=1}^n \\{0, 1\\}^i⋃i=1n​{0,1}i and {0,1}n\\{ 0, 1 \\}^n{0,1}n.\nCPA security from PRF\nRecall) Block cipher cannot be CPA secure! (Because block ciphertext is deterministic!)\ne.g. Block cipher will encrypt sex data (man/woman) into cmc_mcm​ or cwc_wcw​... Encryption is meaningless!\nIf your security company claims that they encrypted data using a plain block cipher (e.g. AES), don't believe them...\nBut we want to use block cipher.... How do we make CPA-secure scheme with block cipher?\nLet F:{0,1}n×{0,1}l→{0,1}lF: \\{ 0, 1 \\}^n \\times \\{ 0, 1 \\}^l \\rightarrow \\{ 0, 1 \\}^lF:{0,1}n×{0,1}l→{0,1}l be a PRF. Then we can make a construction E=(Gen,Enc,Dec)\\mathcal{E} = (Gen, Enc, Dec)E=(Gen,Enc,Dec):\n\nGenGenGen: Choose uniform k∈{0,1}nk \\in \\{ 0, 1 \\}^nk∈{0,1}n.\nEnck(m)Enc_k(m)Enck​(m): On input a key k∈{0,1}nk \\in \\{ 0, 1 \\}^nk∈{0,1}n and a message m∈{0,1}lm \\in \\{ 0, 1 \\}^lm∈{0,1}l, choose uniform r∈{0,1}lr \\in \\{ 0, 1 \\}^lr∈{0,1}l and output the ciphertext c≔(r,Fk(r)⊕m)∈{0,1}2lc \\coloneqq (r, F_k(r) \\oplus m) \\in \\{ 0, 1 \\}^{2l}c:=(r,Fk​(r)⊕m)∈{0,1}2l.\nDeck(c)Dec_k(c)Deck​(c): On input a key k∈{0,1}nk \\in \\{ 0, 1 \\}^nk∈{0,1}n and a ciphertext c=(r,s)c = (r, s)c=(r,s), output the message m≔Fk(r)⊕sm \\coloneqq F_k(r) \\oplus sm:=Fk​(r)⊕s.\n\nTheorem: For any qqq-query adversary A\\mathcal{A}A, there exists a qqq-query adversary B\\mathcal{B}B such that AdvE(A)≤2⋅(AdvF(B)+q2l)Adv_{\\mathcal{E}}(\\mathcal{A}) \\leq 2 \\cdot \\left( Adv_F(\\mathcal{B}) + \\frac{q}{2^l} \\right)AdvE​(A)≤2⋅(AdvF​(B)+2lq​).\ni.e. If F is secure PRF, then the construction is CPA-secure!\nProof\n\nLet A\\mathcal{A}A is an adversary for the CPA indistinguishability experiment.\nWe define an adversary B\\mathcal{B}B for PRF experiment.\n\nFor each query mmm from A\\mathcal{A}A, generate uniformly random lll bit string rrr, query yry_ryr​ (B\\mathcal{B}B gets either Fk(r)F_k(r)Fk​(r) or f(r)f(r)f(r)), then give c=(r,yr⊕m)c = (r, y_r \\oplus m)c=(r,yr​⊕m) to A\\mathcal{A}A.\nFor challenge query (m0,m1)(m_0, m_1)(m0​,m1​) from A\\mathcal{A}A, generate random bit b∈{0,1}b \\in \\{ 0, 1 \\}b∈{0,1} and random lll bit string r∗r^*r∗, query yr∗y_{r^*}yr∗​, then give cb=(r∗,yr∗⊕mb)c_b = (r^*, y_{r^*} \\oplus m_b)cb​=(r∗,yr∗​⊕mb​) to A\\mathcal{A}A.\nGet the output b′b&#x27;b′ from A\\mathcal{A}A.\nIf b=b′b = b&#x27;b=b′ (i.e. adversary was correct), output 0 (i.e. this was a PRF).\nIf b≠b′b \\neq b&#x27;b=b′ (i.e. adversary was wrong), output 1 (i.e. this was a random function).\n\nLet pA,b≔Pr⁡[EXPb,E(A)=1]p_{\\mathcal{A},b} \\coloneqq \\Pr[EXP_{b, \\mathcal{E}}(\\mathcal{A}) = 1]pA,b​:=Pr[EXPb,E​(A)=1], pB,b≔Pr⁡[EXPb,F(B)=1]p_{\\mathcal{B},b} \\coloneqq \\Pr[EXP_{b, F}(\\mathcal{B}) = 1]pB,b​:=Pr[EXPb,F​(B)=1].\nIf this was a experiment 0 (i.e. PRF was given), this is same experiment from A\\mathcal{A}A's point of view.\n∴pB,0=Pr⁡[b=0]⋅Pr⁡[EXP0,E(A)=1]+Pr⁡[b=1]⋅Pr⁡[EXP1,E(A)=0]=12⋅pA,0+12⋅(1−pA,1)=12+12⋅(pA,0−pA,1)\\begin{align*}\n\\therefore p_{\\mathcal{B},0} &amp;= \\Pr[b = 0] \\cdot \\Pr[EXP_{0, \\mathcal{E}}(\\mathcal{A}) = 1] + \\Pr[b = 1] \\cdot \\Pr[EXP_{1, \\mathcal{E}}(\\mathcal{A}) = 0] \\\\\n&amp;= \\frac{1}{2} \\cdot p_{\\mathcal{A},0} + \\frac{1}{2} \\cdot (1 - p_{\\mathcal{A},1}) \\\\\n&amp;= \\frac{1}{2} + \\frac{1}{2} \\cdot (p_{\\mathcal{A},0} - p_{\\mathcal{A},1})\n\\end{align*}∴pB,0​​=Pr[b=0]⋅Pr[EXP0,E​(A)=1]+Pr[b=1]⋅Pr[EXP1,E​(A)=0]=21​⋅pA,0​+21​⋅(1−pA,1​)=21​+21​⋅(pA,0​−pA,1​)​\nIf this was a experiment 1 (i.e. random function was given), yr∗⊕mby_{r^*} \\oplus m_byr∗​⊕mb​ is a random string. Therefore, A\\mathcal{A}A cannot distinguish c0,c1c_0, c_1c0​,c1​ unless r∗r^*r∗ is the same as rrr from one of the other queries.\n∴∣pB,1−12∣≤Pr⁡[r∗∈{r1,…,rq}]≤q2l\\therefore \\left| p_{\\mathcal{B},1} - \\frac{1}{2} \\right| \\leq \\Pr[r^* \\in \\{ r_1, \\ldots, r_q \\}] \\leq \\frac{q}{2^l}\n∴​pB,1​−21​​≤Pr[r∗∈{r1​,…,rq​}]≤2lq​\n∵AdvF(B)=∣pB,0−pB,1∣,∣pB,0−12∣≤∣pB,0−pB,1∣+∣pB,1−12∣≤AdvF(B)+q2l∴AdvE(A)=∣pA,0−pA,1∣=2⋅∣pB,0−12∣≤2⋅(AdvF(B)+q2l)\\begin{align*}\n\\because Adv_F(\\mathcal{B}) &amp;= |p_{\\mathcal{B},0} - p_{\\mathcal{B},1}|, \\\\\n\\left| p_{\\mathcal{B},0} - \\frac{1}{2} \\right| &amp;\\leq |p_{\\mathcal{B},0} - p_{\\mathcal{B},1}| + \\left| p_{\\mathcal{B},1} - \\frac{1}{2} \\right| \\\\\n&amp;\\leq Adv_F(\\mathcal{B}) + \\frac{q}{2^l} \\\\\n\\therefore Adv_{\\mathcal{E}}(\\mathcal{A}) &amp;= |p_{\\mathcal{A},0} - p_{\\mathcal{A},1}| \\\\\n&amp;= 2 \\cdot \\left| p_{\\mathcal{B},0} - \\frac{1}{2} \\right| \\\\\n&amp;\\leq 2 \\cdot \\left( Adv_F(\\mathcal{B}) + \\frac{q}{2^l} \\right)\n\\end{align*}∵AdvF​(B)​pB,0​−21​​∴AdvE​(A)​=∣pB,0​−pB,1​∣,≤∣pB,0​−pB,1​∣+​pB,1​−21​​≤AdvF​(B)+2lq​=∣pA,0​−pA,1​∣=2⋅​pB,0​−21​​≤2⋅(AdvF​(B)+2lq​)​\n\n\nProblem) Expansion rate ∣C∣∣M∣\\frac{|C|}{|M|}∣M∣∣C∣​ is not good.\nSince CPA-secure scheme is probablistic, plaintext can be encrypted to multiple ciphertexts. (i.e. expansion rate is larger than 1.)\nHowever, an expansion rate of 2 is too high...\nModes of operation\nThe previous construction is defined only for the encryption of fixed-length messages. Can we encrypt arbitrary-length messages?\nWe can divide a long message into multiple short messages and encrypting them one-by-one.\nThis is called mode of operation: an efficient symmetric-key encryption schemes from block ciphers.\nElectronic Code Book (ECB) Mode\n\nEnck(m)≔Ek(m1)∥⋯∥Ek(mL)Enc_k(m) \\coloneqq E_k(m_1) \\Vert \\cdots \\Vert E_k(m_L)\nEnck​(m):=Ek​(m1​)∥⋯∥Ek​(mL​)\nECB Mode just concatenate ciphertexts... although this has very serious problems!!!!\nProblem 1) Since block cipher is deterministic, ECB mode is also deterministic.\nGenerally, secure modes use a uniform initialization vector (IV) to introduce randomness.\n\nProblem 2) ECB preserves patterns of the original message because same block of message will encrypted to same ciphertext.\ne.g. If every block is just 0s or 1s, this will be encrypted to two different ciphertext... which is very predictable!\ne.g. ECB penguin (in the above) still looks like penguin because same pixels are encrypted to same value.\nCipher Block Chaining (CBC) Mode\n\n\nA uniform initialization vector (IV) is chosen as the initial ciphertext block. (c0≔IVc_0 \\coloneqq IVc0​:=IV)\nEach ciphertext blocks are generated by applying the block cipher to the XOR of the current plaintext block and the previous ciphertext block. (ci≔Ek(ci−1⊕mi)c_i \\coloneqq E_k(c_{i-1} \\oplus m_i)ci​:=Ek​(ci−1​⊕mi​))\n\nTheorem (Security of CBC mode): If E:K×X→XE: K \\times X \\rightarrow XE:K×X→X is a secure PRP, then CBC mode is CPA-secure.\nFor any qqq-query adversary A\\mathcal{A}A against ECBC\\mathcal{E}_{CBC}ECBC​, there exists a PRP adversary B\\mathcal{B}B against EEE such that\nAdvECBC(A)≤2⋅AdvE(B)+2q2L2∣X∣Adv_{\\mathcal{E_{CBC}}}(\\mathcal{A}) \\leq 2 \\cdot Adv_{E}(\\mathcal{B}) + 2\\frac{q^2L^2}{|X|}\nAdvECBC​​(A)≤2⋅AdvE​(B)+2∣X∣q2L2​\nProof sketch: Each query use LLL block ciphers, resulting total of qLqLqL block ciphers. By birthday paradox, we have q2L2q^2L^2q2L2 term.\nThis is secure, and have better expansion rate (ciphertext is only l bit longer), but it is not used today.\n\nEncryption is sequential; Not parallelizable!\nNote that decryption is parallelizable.\nDecryption need inverse of block cipher.\nWe need different algorithms for encryption and decryption.\nThe message must be padded to a multiple of the block size. (This can be handled by ciphertext stealing with an extra complexity.)\n\nSince padded message should be longer than the original message, if the message is already a multiple of the block size, padding can be up to block size!\nExtremely, when sending 1 byte message, message should be padded up to block size.\n\n\n\nWarning: CBC mode is not secure if the adversary can predict IV for the next message!\ne.g. A stateful variant of CBC-mode encryption (a.k.a. chained CBC mode) use the last block of the previous ciphertext as the IV of the next message. (This was actually used in SSL 3.0 and TLS 1.0!!!!!)\nThis is vulnerable to a chosen-plaintext attack! (And again, this was used in SSL 3.0 and TLS 1.0!)\nLesson: NEVER make modifications to cryptographic schemes, even if those modifications seem benign.\nCounter (CTR) Mode\n\n\nA uniform initialization vector (IV) is chosen as the initial ciphertext block. (c0≔IVc_0 \\coloneqq IVc0​:=IV)\nEach ciphertext blocks are generated by applying the block cipher to IV+iIV + iIV+i, then XORing with the current plaintext block. (ci≔Fk(IV+i)⊕mic_i \\coloneqq F_k(IV + i) \\oplus m_ici​:=Fk​(IV+i)⊕mi​)\n\nTechnically, IVIVIV is a random string, but we view as number and compute IV+iIV + iIV+i.\nTheorem (Security of CTR mode): If F:K×X→XF: K \\times X \\rightarrow XF:K×X→X is a secure PRF, then CTR mode is CPA-secure.\nFor any qqq-query adversary A\\mathcal{A}A against ECTR\\mathcal{E}_{CTR}ECTR​, there exists a PRF adversary B\\mathcal{B}B against FFF such that\nAdvECTR(A)≤2⋅AdvF(B)+2q2L∣X∣Adv_{\\mathcal{E_{CTR}}}(\\mathcal{A}) \\leq 2 \\cdot Adv_{F}(\\mathcal{B}) + 2\\frac{q^2L}{|X|}\nAdvECTR​​(A)≤2⋅AdvF​(B)+2∣X∣q2L​\nProof sketch: Each query use LLL block ciphers, resulting total of qLqLqL block ciphers. However, because we are using consecutive strings (IV+1IV+1IV+1 ~ IV+LIV+LIV+L) instead of random LLL strings, collision probability is q2Lq^2Lq2L instead of q2L2q^2L^2q2L2.\nThis is a complete upgrade over CBC mode!\n\nWe only need PRF instead of PRP!\nEncryption/decryption is parallelizable!\nIn fact CTR can precompute encryption! We generate IV and precompute block ciphers before receiving message.\nFactor is q2Lq^2Lq2L instead of q2L2q^2L^2q2L2!\nThis means that CTR takes less time to change keys than CBC.\nThis was meaningless when CBC came out, (old systems didn't track q,Lq, Lq,L at all) but it's important nowadays because of too many queries.\nCTR can reuse same algorithm for encryption/decryption!\nCTR doesn't need padding!\n\nNonce-based Encryption\nIV should be random and generated for each message. This is very tedious...\nInstead, we can make nonce-based encryption, where the encryption and decryption algorithms additionally accept a nonce as input. Enck(m,n)Enc_k(m,n)Enck​(m,n)\n\nNonce should be used once, and never repeated.\nNonce doesn't have to be a true random string, it can be a counter value, or the current time!\nThis is useful for lightweight systems where generating high-quality randomness is expensive or impossible.\nYou can even reveal the nonce, and it's still secure!\nSince nonce provides sort of non-deterministic way, encryption/decryption algorithm can be deterministic (and it's still secure)!\nCPA security definition can be defined in a similar manner, but the adversary cannot choose the same nonce more than once during the security game.\n\nNonce-based encryption is very good!\nHowever, it's not suitable for communicating with millions of clients. (You have to ensure that each client's nonce is unique!)\nGenerating IV from your side is much easier and faster way to achieve security.\n","categories":["SNU","4-2","현대암호학 개론"]}]